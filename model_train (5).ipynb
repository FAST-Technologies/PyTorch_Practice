{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TRAIN AND LEARN OF word_language_model"
      ],
      "metadata": {
        "id": "SsK88NLWSwjo"
      },
      "id": "SsK88NLWSwjo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model code"
      ],
      "metadata": {
        "id": "fFwuP5dYS2Tx"
      },
      "id": "fFwuP5dYS2Tx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading files from Google Disk"
      ],
      "metadata": {
        "id": "pJHzuwzIThUp"
      },
      "id": "pJHzuwzIThUp"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxeRfC-STmyY",
        "outputId": "638e5972-74ed-45d5-a794-8ecebc785656"
      },
      "id": "WxeRfC-STmyY",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "import os\n",
        "from io import open\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from typing import Dict, List, Union, Tuple, Optional"
      ],
      "metadata": {
        "id": "w5z0Vi8DVzgk"
      },
      "id": "w5z0Vi8DVzgk",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data.py"
      ],
      "metadata": {
        "id": "vHTBXIgBS5Xa"
      },
      "id": "vHTBXIgBS5Xa"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self) -> None:\n",
        "        self.word2idx: Dict[str, int] = {}\n",
        "        self.idx2word: List[str] = []\n",
        "        self.word2count: Counter = Counter()\n",
        "\n",
        "    def add_word(self, word: str) -> int:\n",
        "        self.word2count[word] += 1\n",
        "        return self.word2idx.get(word, -1)\n",
        "\n",
        "    def finalize(self, min_freq: int = 5) -> None:\n",
        "        for word, count in self.word2count.items():\n",
        "            if count >= min_freq and word not in self.word2idx:\n",
        "                self.idx2word.append(word)\n",
        "                self.word2idx[word] = len(self.idx2word) - 1\n",
        "        self.word2idx['<unk>'] = len(self.idx2word)\n",
        "        self.idx2word.append('<unk>')\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.idx2word)\n",
        "\n",
        "class Corpus(object):\n",
        "  \"\"\"Corpus class for loading and tokenizing text data.\"\"\"\n",
        "  def __init__(self, path: str, min_freq: int = 5) -> None:\n",
        "      self.dictionary: Dictionary = Dictionary()\n",
        "      self.min_freq: int = min_freq\n",
        "      self.train: torch.Tensor = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "      self.valid: torch.Tensor = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "      self.test: torch.Tensor = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "  def tokenize(self, path: str) -> torch.Tensor:\n",
        "      \"\"\"Tokenizes a text file.\"\"\"\n",
        "      assert os.path.exists(path)\n",
        "      # Add words to the dictionary\n",
        "      with open(path, 'r', encoding=\"utf8\") as f:\n",
        "          for line in f:\n",
        "              words: List[str] = line.split() + ['<eos>']\n",
        "              for word in words:\n",
        "                  self.dictionary.add_word(word)\n",
        "      self.dictionary.finalize(self.min_freq)\n",
        "      # Tokenize file content\n",
        "      with open(path, 'r', encoding=\"utf8\") as f:\n",
        "          idss: List[torch.Tensor] = []\n",
        "          for line in f:\n",
        "              words: List[str] = line.split() + ['<eos>']\n",
        "              ids: List[int] = []\n",
        "              for word in words:\n",
        "                  idx = self.dictionary.word2idx.get(word, self.dictionary.word2idx['<unk>'])\n",
        "                  ids.append(idx)\n",
        "              idss.append(torch.tensor(ids, dtype=torch.int64))\n",
        "          ids_tensor: torch.Tensor = torch.cat(idss)\n",
        "\n",
        "      return ids_tensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Old_Dictionary(object):\n",
        "    \"\"\"Dictionary for word-to-index mapping.\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.word2idx: Dict[str, int] = {}\n",
        "        self.idx2word: List[str] = []\n",
        "\n",
        "    def add_word(self, word: str) -> int:\n",
        "        \"\"\"Add a word to the dictionary.\"\"\"\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Old_Corpus(object):\n",
        "    \"\"\"Corpus class for loading and tokenizing text data.\"\"\"\n",
        "\n",
        "    def __init__(self, path: str) -> None:\n",
        "        self.dictionary: Old_Dictionary = Old_Dictionary()\n",
        "        self.train: torch.Tensor = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid: torch.Tensor = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test: torch.Tensor = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path: str) -> torch.Tensor:\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path), f\"Path {path} does not exist\"\n",
        "\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words: List[str] = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss: List[torch.Tensor] = []\n",
        "            for line in f:\n",
        "                words: List[str] = line.split() + ['<eos>']\n",
        "                ids: List[int] = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids, dtype=torch.int64))\n",
        "            ids_tensor: torch.Tensor = torch.cat(idss)\n",
        "\n",
        "        return ids_tensor"
      ],
      "metadata": {
        "id": "FwJICJSw5Rs3"
      },
      "id": "FwJICJSw5Rs3",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model.py"
      ],
      "metadata": {
        "id": "xkMHPnmLWEKE"
      },
      "id": "xkMHPnmLWEKE"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# MODEL ARCHITECTURES (model.py)\n",
        "# ===============================\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    \"\"\"RNN-based language model (LSTM/GRU/RNN).\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        rnn_type: str,\n",
        "        ntoken: int,\n",
        "        ninp: int,\n",
        "        nhid: int,\n",
        "        nlayers: int,\n",
        "        dropout: float = 0.5,\n",
        "        tie_weights: bool = False\n",
        "    ) -> None:\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.ntoken: int = ntoken\n",
        "        self.rnn_type: str = rnn_type\n",
        "        self.nhid: int = nhid\n",
        "        self.nlayers: int = nlayers\n",
        "\n",
        "        self.drop: nn.Dropout = nn.Dropout(dropout)\n",
        "        self.encoder: nn.Embedding = nn.Embedding(ntoken, ninp)\n",
        "\n",
        "        if rnn_type in ['LSTM', 'GRU']:\n",
        "            self.rnn: nn.Module = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
        "        else:\n",
        "            try:\n",
        "                nonlinearity: str = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
        "            except KeyError as e:\n",
        "                raise ValueError(\n",
        "                    \"Invalid option for `--model`. \"\n",
        "                    \"Options are ['LSTM', 'GRU', 'RNN_TANH', 'RNN_RELU']\"\n",
        "                ) from e\n",
        "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
        "\n",
        "        self.decoder: nn.Linear = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        # Tie weights\n",
        "        if tie_weights:\n",
        "            if nhid != ninp:\n",
        "                raise ValueError('When using tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        \"\"\"Initialize weights.\"\"\"\n",
        "        initrange: float = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, input: torch.Tensor, hidden: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        emb: torch.Tensor = self.drop(self.encoder(input))\n",
        "        output: torch.Tensor\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded: torch.Tensor = self.decoder(output)\n",
        "        decoded = decoded.view(-1, self.ntoken)\n",
        "        return F.log_softmax(decoded, dim=1), hidden\n",
        "\n",
        "    def init_hidden(self, bsz: int) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\"Initialize hidden state.\"\"\"\n",
        "        weight: torch.Tensor = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (\n",
        "                weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
        "            )\n",
        "        else:\n",
        "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
        "\n",
        "# Temporarily leave PositionalEncoding module here. Will be moved somewhere else.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
        "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
        "        Here, we use sine and cosine functions of different frequencies.\n",
        "    .. math:\n",
        "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
        "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "        \\text{where pos is the word position and i is the embed idx)\n",
        "    Args:\n",
        "        d_model: the embed dim (required).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        max_len: the max. length of the incoming sequence (default=5000).\n",
        "    Examples:\n",
        "        >>> pos_encoder = PositionalEncoding(d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"Positional encoding for Transformer.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000) -> None:\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout: nn.Dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe: torch.Tensor = torch.zeros(max_len, d_model)\n",
        "        position: torch.Tensor = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term: torch.Tensor = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        r\"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            x: the sequence fed to the positional encoder model (required).\n",
        "        Shape:\n",
        "            x: [sequence length, batch size, embed dim]\n",
        "            output: [sequence length, batch size, embed dim]\n",
        "        Examples:\n",
        "            >>> output = pos_encoder(x)\n",
        "        \"\"\"\n",
        "\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerModel(nn.Transformer):\n",
        "    \"\"\"Transformer-based language model.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        ntoken: int,\n",
        "        ninp: int,\n",
        "        nhead: int,\n",
        "        nhid: int,\n",
        "        nlayers: int,\n",
        "        dropout: float = 0.5\n",
        "    ) -> None:\n",
        "        super(TransformerModel, self).__init__(\n",
        "            d_model=ninp,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=nhid,\n",
        "            num_encoder_layers=nlayers\n",
        "        )\n",
        "        self.model_type: str = 'Transformer'\n",
        "        self.src_mask: Optional[torch.Tensor] = None\n",
        "        self.pos_encoder: PositionalEncoding = PositionalEncoding(ninp, dropout)\n",
        "\n",
        "        self.input_emb: nn.Embedding = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp: int = ninp\n",
        "        self.decoder: nn.Linear = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz: int) -> torch.Tensor:\n",
        "        \"\"\"Generate mask for causal attention.\"\"\"\n",
        "        return torch.log(torch.tril(torch.ones(sz, sz)))\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        \"\"\"Initialize weights.\"\"\"\n",
        "        initrange: float = 0.1\n",
        "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, src: torch.Tensor, has_mask: bool = True) -> torch.Tensor:\n",
        "        if has_mask:\n",
        "            device: torch.device = src.device\n",
        "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "                mask: torch.Tensor = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "                self.src_mask = mask\n",
        "        else:\n",
        "            self.src_mask = None\n",
        "\n",
        "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output: torch.Tensor = self.encoder(src, mask=self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return F.log_softmax(output, dim=-1)"
      ],
      "metadata": {
        "id": "9BXrfBWHWKGj"
      },
      "id": "9BXrfBWHWKGj",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label smoothing loss\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, smoothing: float = 0.0) -> None:\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.smoothing: float = smoothing\n",
        "        self.confidence: float = 1.0 - smoothing\n",
        "\n",
        "    def forward(self, output: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        log_probs: torch.Tensor = output\n",
        "        n_classes: int = log_probs.size(-1)\n",
        "        with torch.no_grad():\n",
        "            true_dist: torch.Tensor = torch.zeros_like(log_probs)\n",
        "            true_dist.fill_(self.smoothing / (n_classes - 1))\n",
        "            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * log_probs, dim=-1))"
      ],
      "metadata": {
        "id": "xY5KPVooRtAt"
      },
      "id": "xY5KPVooRtAt",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test train on original data"
      ],
      "metadata": {
        "id": "4OZuV_gLZxBg"
      },
      "id": "4OZuV_gLZxBg"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# TRAINING & EVALUATION (main.py)\n",
        "# ===============================\n",
        "\n",
        "def get_lr(step: float, d_model: float, warmup_steps: int) -> float:\n",
        "    \"\"\"Gets the learning rate step.\"\"\"\n",
        "    lr: float = d_model ** -0.5 * min(step ** -0.5, step * warmup_steps ** -1.5)\n",
        "    return lr\n",
        "\n",
        "def batchify(data: torch.Tensor, bsz: int, device: torch.device) -> torch.Tensor:\n",
        "    \"\"\"Divide data into batches.\"\"\"\n",
        "    nbatch: int = data.size(0) // bsz\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "\n",
        "def get_batch(source: torch.Tensor, i: int, bptt: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Get a batch of data.\"\"\"\n",
        "    seq_len: int = min(bptt, len(source) - 1 - i)\n",
        "    data: torch.Tensor = source[i:i+seq_len]\n",
        "    target: torch.Tensor = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def repackage_hidden(h: Union[torch.Tensor, Tuple]) -> Union[torch.Tensor, Tuple]:\n",
        "    \"\"\"Detach hidden state from history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def top_k_sampling(logits: torch.Tensor, k: int, temperature: float = 1.0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Top k-sampling in generate function.\"\"\"\n",
        "    values, indices = torch.topk(logits, k)\n",
        "    values = values.div(temperature).exp()\n",
        "    values = values / values.sum()\n",
        "    return torch.multinomial(values, 1), indices\n",
        "\n",
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    data_source: torch.Tensor,\n",
        "    criterion: nn.Module,\n",
        "    bptt: int,\n",
        "    ntokens: int,\n",
        "    eval_batch_size: int,\n",
        "    is_transformer: bool\n",
        ") -> float:\n",
        "    \"\"\"Evaluate the model.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss: float = 0.0\n",
        "    if not is_transformer:\n",
        "        hidden = model.init_hidden(eval_batch_size)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data: torch.Tensor\n",
        "            targets: torch.Tensor\n",
        "            data, targets = get_batch(data_source, i, bptt)\n",
        "\n",
        "            if is_transformer:\n",
        "                output: torch.Tensor = model(data)\n",
        "                output = output.view(-1, ntokens)\n",
        "            else:\n",
        "                output, hidden = model(data, hidden)\n",
        "                hidden = repackage_hidden(hidden)\n",
        "\n",
        "            total_loss += len(data) * criterion(output, targets).item()\n",
        "\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train_epoch(\n",
        "    model: nn.Module,\n",
        "    train_data: torch.Tensor,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    epoch: int,\n",
        "    bptt: int,\n",
        "    ntokens: int,\n",
        "    batch_size: int,\n",
        "    clip: float,\n",
        "    log_interval: int,\n",
        "    is_transformer: bool,\n",
        "    use_optimizer: bool = True,\n",
        "    use_warmup: bool = False,\n",
        "    step: int = 0,\n",
        "    d_model: int = 512,\n",
        "    warmup_steps: int = 4000,\n",
        "    dry_run: bool = False\n",
        ") -> int:\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss: float = 0.0\n",
        "    start_time: float = time.time()\n",
        "\n",
        "    if not is_transformer:\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data: torch.Tensor\n",
        "        targets: torch.Tensor\n",
        "        data, targets = get_batch(train_data, i, bptt)\n",
        "\n",
        "        optimizer.zero_grad() if use_optimizer else model.zero_grad()\n",
        "\n",
        "        if is_transformer:\n",
        "            output: torch.Tensor = model(data)\n",
        "            output = output.view(-1, ntokens)\n",
        "        else:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            output, hidden = model(data, hidden)\n",
        "\n",
        "        loss: torch.Tensor = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        if use_warmup:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = get_lr(step + 1, d_model, warmup_steps)\n",
        "        if use_optimizer:\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            for p in model.parameters():\n",
        "                p.data.add_(p.grad, alpha=-optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        step += 1\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss: float = total_loss / log_interval\n",
        "            elapsed: float = time.time() - start_time\n",
        "            print(\n",
        "                f'| epoch {epoch:3d} | {batch:5d}/{len(train_data) // bptt:5d} batches | '\n",
        "                f'lr {optimizer.param_groups[0][\"lr\"]:02.6f} | ms/batch {elapsed * 1000 / log_interval:5.2f} | '\n",
        "                f'loss {cur_loss:5.2f} | ppl {math.exp(cur_loss):8.2f}'\n",
        "            )\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "        if dry_run:\n",
        "            break\n",
        "    return step\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model_type: str = 'LSTM', # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "    data_path: str = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize: int = 200,\n",
        "    nhid: int = 200,\n",
        "    nlayers: int = 2,\n",
        "    lr: float = 0.001,\n",
        "    clip: float = 0.25,\n",
        "    epochs: int = 60,\n",
        "    batch_size: int = 20,\n",
        "    bptt: int = 35,\n",
        "    dropout: float = 0.2,\n",
        "    tied: bool = False,\n",
        "    nhead: int = 2,\n",
        "    log_interval: int = 200,\n",
        "    save_path: str = 'model.pt',\n",
        "    onnx_export: str = '',\n",
        "    dry_run: bool = False,\n",
        "    accel: bool = True,\n",
        "    use_optimizer: bool = True,\n",
        "    optimizer_type: str = 'AdamW',\n",
        "    weight_decay: Optional[float] = None,\n",
        "    use_betas: bool = False,\n",
        "    betas: Optional[Tuple[float, float]] = (0.9, 0.98),\n",
        "    use_eps: bool = False,\n",
        "    eps: float = 1e-9,\n",
        "    criterion: Optional[nn.Module] = None,\n",
        "    use_label_smoothing: bool = False,\n",
        "    label_smoothing: float = 0.1,\n",
        "    use_warmup: bool = False,\n",
        "    warmup_steps: int = 4000,\n",
        "    min_freq: int = 5,\n",
        "    seed: int = 1111,\n",
        "    old_version: bool = True\n",
        ") -> None:\n",
        "    \"\"\"Main training function.\"\"\"\n",
        "\n",
        "    if data_path == '/content/drive/MyDrive/data_word_train/wikitext-2':\n",
        "      if not os.path.exists(data_path):\n",
        "          print(\"Downloading Wikitext-2 dataset...\")\n",
        "          !wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip -P /content/\n",
        "          !unzip /content/wikitext-2-v1.zip -d /content/data_word_train/\n",
        "          !mkdir -p /content/drive/MyDrive/data_word_train/\n",
        "          !mv /content/data_word_train/wikitext-2 /content/drive/MyDrive/data_word_train/\n",
        "          print(\"Wikitext-2 dataset moved to Google Drive\")\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Set device\n",
        "    device: torch.device = torch.device('cuda' if accel and torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load data\n",
        "    corpus: Union[Corpus, Old_Corpus] = Old_Corpus(data_path) if old_version else Corpus(data_path, min_freq=min_freq)\n",
        "    print(f\"Vocabulary size: {len(corpus.dictionary)}\")\n",
        "\n",
        "    eval_batch_size: int = 10\n",
        "    train_data: torch.Tensor = batchify(corpus.train, batch_size, device)\n",
        "    val_data: torch.Tensor = batchify(corpus.valid, eval_batch_size, device)\n",
        "    test_data: torch.Tensor = batchify(corpus.test, eval_batch_size, device)\n",
        "\n",
        "    # Build model\n",
        "    ntokens: int = len(corpus.dictionary)\n",
        "    is_transformer: bool = model_type == 'Transformer'\n",
        "\n",
        "    model: nn.Module = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device) if is_transformer else RNNModel(model_type, ntokens, emsize, nhid, nlayers, dropout, tied).to(device)\n",
        "    # Loss and optimizer (Adam with weight_decay as in Transformer paper)\n",
        "    criterion: nn.Module = criterion if criterion is not None else (LabelSmoothingLoss(smoothing=label_smoothing) if use_label_smoothing else nn.NLLLoss())\n",
        "    if use_betas == True and use_eps == True:\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=betas, eps=eps) if optimizer_type == 'AdamW' else optim.Adam(model.parameters(), lr=lr, betas=betas, eps=eps)\n",
        "    elif use_betas == False and use_eps == True:\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, eps=eps) if optimizer_type == 'AdamW' else optim.Adam(model.parameters(), lr=lr, eps=eps)\n",
        "    elif use_betas == False and use_eps == False:\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay) if optimizer_type == 'AdamW' else optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    scheduler: optim.lr_scheduler.ReduceLROnPlateau = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=2\n",
        "    ) if not use_warmup else None\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss: Optional[float] = None\n",
        "    global_step: int = 0\n",
        "\n",
        "    try:\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            epoch_start_time: float = time.time()\n",
        "\n",
        "            global_step = train_epoch(\n",
        "                model, train_data, criterion, optimizer, epoch, bptt, ntokens, batch_size, clip, log_interval, is_transformer,\n",
        "                use_optimizer, use_warmup, global_step, emsize, warmup_steps, dry_run\n",
        "            )\n",
        "\n",
        "            val_loss: float = evaluate(\n",
        "                model, val_data, criterion, bptt, ntokens,\n",
        "                eval_batch_size, is_transformer\n",
        "            )\n",
        "\n",
        "            print('-' * 89)\n",
        "            print(\n",
        "                f'| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s | '\n",
        "                f'valid loss {val_loss:5.2f} | valid ppl {math.exp(val_loss):8.2f}'\n",
        "            )\n",
        "            print('-' * 89)\n",
        "\n",
        "            # Save best model\n",
        "            if not best_val_loss or val_loss < best_val_loss:\n",
        "                with open(save_path, 'wb') as f:\n",
        "                    torch.save(model, f)\n",
        "                best_val_loss = val_loss\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            # lr /= 4.0\n",
        "            if use_warmup:\n",
        "                print(f\"Current learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "            else:\n",
        "                scheduler.step(val_loss)\n",
        "                print(f\"Current learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('-' * 89)\n",
        "        print('Exiting from training early')\n",
        "    safe_globals: List = [\n",
        "        RNNModel, TransformerModel, PositionalEncoding, Old_Dictionary, Dictionary, Old_Corpus, Corpus,\n",
        "        nn.Dropout, nn.Linear, nn.GRU, nn.LSTM, nn.RNN, nn.Embedding,\n",
        "        nn.TransformerEncoder, nn.TransformerEncoderLayer, nn.MultiheadAttention,\n",
        "        nn.LayerNorm, F.relu, nn.ModuleList, nn.modules.linear.NonDynamicallyQuantizableLinear\n",
        "    ]\n",
        "    with torch.serialization.safe_globals(safe_globals):\n",
        "        with open(save_path, 'rb') as f:\n",
        "            model = torch.load(f, map_location=device)\n",
        "    test_loss: float = evaluate(\n",
        "        model, test_data, criterion, bptt, ntokens, eval_batch_size, is_transformer\n",
        "    )\n",
        "    print('=' * 89)\n",
        "    print(\n",
        "        f'| End of training | test loss {test_loss:5.2f} | '\n",
        "        f'test ppl {math.exp(test_loss):8.2f}'\n",
        "    )\n",
        "    print('=' * 89)\n"
      ],
      "metadata": {
        "id": "zb-MESoeRJq0"
      },
      "execution_count": 51,
      "outputs": [],
      "id": "zb-MESoeRJq0"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# TEXT GENERATION (generate.py)\n",
        "# ===============================\n",
        "\n",
        "def generate_text(\n",
        "    checkpoint: str = 'model.pt',\n",
        "    data_path: str = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf: str = 'generated.txt',\n",
        "    words: int = 1000,\n",
        "    temperature: float = 1.0,\n",
        "    top_k: int = 40,\n",
        "    seed: int = 1111,\n",
        "    log_interval: int = 100,\n",
        "    accel: bool = True,\n",
        "    min_freq: int = 5,\n",
        "    use_top_k: bool = False,\n",
        "    old_version: bool = True\n",
        ") -> None:\n",
        "    \"\"\"Generate text from trained model.\"\"\"\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    device: torch.device = torch.device('cuda' if accel and torch.cuda.is_available() else 'cpu')\n",
        "    assert len(corpus.dictionary) == model.ntoken, f\"Vocabulary size mismatch: {len(corpus.dictionary)} vs {model.ntoken}\"\n",
        "\n",
        "    # Load model\n",
        "    safe_globals: List = [\n",
        "        RNNModel, TransformerModel, PositionalEncoding, Old_Dictionary, Dictionary, Old_Corpus, Corpus,\n",
        "        nn.Dropout, nn.Linear, nn.GRU, nn.LSTM, nn.RNN, nn.Embedding,\n",
        "        nn.TransformerEncoder, nn.TransformerEncoderLayer, nn.MultiheadAttention,\n",
        "        nn.LayerNorm, F.relu, nn.ModuleList, nn.modules.linear.NonDynamicallyQuantizableLinear\n",
        "    ]\n",
        "    with torch.serialization.safe_globals(safe_globals):\n",
        "        with open(checkpoint, 'rb') as f:\n",
        "            model: nn.Module = torch.load(f, map_location=device)\n",
        "    model.eval()\n",
        "\n",
        "    # Load corpus\n",
        "    corpus: Union[Corpus, Old_Corpus] = Old_Corpus(data_path) if old_version else Corpus(data_path, min_freq=min_freq)\n",
        "    print(f\"Vocabulary size: {len(corpus.dictionary)}\")\n",
        "    print(f\"Vocabulary size: {len(corpus.dictionary)}\")\n",
        "    ntokens: int = len(corpus.dictionary)\n",
        "\n",
        "    is_transformer: bool = hasattr(model, 'model_type') and model.model_type == 'Transformer'\n",
        "    if not is_transformer:\n",
        "        hidden = model.init_hidden(1)\n",
        "\n",
        "    input: torch.Tensor = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "    with open(outf, 'w') as outfile:\n",
        "        with torch.no_grad():\n",
        "            for i in range(words):\n",
        "                if is_transformer:\n",
        "                    output: torch.Tensor = model(input, False)\n",
        "                    if use_top_k:\n",
        "                        word_weights: torch.Tensor = output[-1].squeeze().cpu()\n",
        "                        prob, top_indices = top_k_sampling(word_weights, top_k, temperature)\n",
        "                        word_idx: int = top_indices[prob.item()].item()\n",
        "                    else:\n",
        "                        word_weights: torch.Tensor = output[-1].squeeze().div(temperature).exp().cpu()\n",
        "                        word_idx: int = torch.multinomial(word_weights, 1)[0].item()\n",
        "                    word_tensor: torch.Tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
        "                    input = torch.cat([input, word_tensor], 0)\n",
        "\n",
        "                else:\n",
        "                    output, hidden = model(input, hidden)\n",
        "                    word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "                    word_idx = torch.multinomial(word_weights, 1)[0].item()\n",
        "                    input.fill_(word_idx)\n",
        "\n",
        "                word: str = corpus.dictionary.idx2word[word_idx]\n",
        "                if word == '@-@' or word == '@.@' or word == '@,@':\n",
        "                  word = ' '\n",
        "                outfile.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "                if i % log_interval == 0:\n",
        "                    print(f'| Generated {i}/{words} words')"
      ],
      "metadata": {
        "id": "DOCVuP6LUpRS"
      },
      "id": "DOCVuP6LUpRS",
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Train on WikiText-2\n",
        "print(\"Training LSTM on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type = 'LSTM', # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "    data_path = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize = 400,\n",
        "    nhid = 400,\n",
        "    nlayers = 4,\n",
        "    lr = 0.001,\n",
        "    clip = 0.25,\n",
        "    epochs = 60,\n",
        "    batch_size = 20,\n",
        "    bptt = 35,\n",
        "    dropout = 0.2,\n",
        "    tied = False,\n",
        "    nhead = 4,\n",
        "    log_interval = 200,\n",
        "    save_path = 'model.pt',\n",
        "    onnx_export = '',\n",
        "    dry_run = False,\n",
        "    accel = True,\n",
        "    use_optimizer = True,\n",
        "    optimizer_type = 'AdamW',\n",
        "    weight_decay=1e-5,\n",
        "    use_betas = False,\n",
        "    use_eps = False,\n",
        "    criterion = nn.NLLLoss(),\n",
        "    use_label_smoothing = False,\n",
        "    label_smoothing = 0.1,\n",
        "    use_warmup = False,\n",
        "    warmup_steps = 4000,\n",
        "    min_freq = 5,\n",
        "    seed = 1111,\n",
        "    old_version = True\n",
        ")\n",
        "\n",
        "# Example 2: Generate text\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    words=1000,\n",
        "    temperature=1.0,\n",
        "    old_version=True,\n",
        "    use_top_k=False,\n",
        "    accel = True\n",
        ")\n",
        "\n",
        "!cat generated.txt\n",
        "\n",
        "# Example 3: Train on custom names dataset\n",
        "# First, create the data files (see instructions below)\n",
        "print(\"\\nTraining on custom names dataset...\")\n",
        "# train_model(\n",
        "#     model_type='LSTM',\n",
        "#     data_path='./data/names',\n",
        "#     emsize=128,\n",
        "#     nhid=128,\n",
        "#     nlayers=2,\n",
        "#     epochs=20,\n",
        "#     lr=0.001\n",
        "# )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlFnK0TWRNX0",
        "outputId": "2907aa69-70b3-4088-feda-d6b9e3135b87"
      },
      "id": "jlFnK0TWRNX0",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM on WikiText-2...\n",
            "Using device: cuda\n",
            "Vocabulary size: 33278\n",
            "| epoch   1 |   200/ 2983 batches | lr 0.001000 | ms/batch 24.86 | loss  7.33 | ppl  1522.35\n",
            "| epoch   1 |   400/ 2983 batches | lr 0.001000 | ms/batch 24.81 | loss  7.10 | ppl  1207.10\n",
            "| epoch   1 |   600/ 2983 batches | lr 0.001000 | ms/batch 24.89 | loss  7.09 | ppl  1196.08\n",
            "| epoch   1 |   800/ 2983 batches | lr 0.001000 | ms/batch 24.92 | loss  7.09 | ppl  1196.80\n",
            "| epoch   1 |  1000/ 2983 batches | lr 0.001000 | ms/batch 24.99 | loss  7.10 | ppl  1214.24\n",
            "| epoch   1 |  1200/ 2983 batches | lr 0.001000 | ms/batch 25.00 | loss  7.11 | ppl  1228.74\n",
            "| epoch   1 |  1400/ 2983 batches | lr 0.001000 | ms/batch 25.00 | loss  6.94 | ppl  1031.50\n",
            "| epoch   1 |  1600/ 2983 batches | lr 0.001000 | ms/batch 25.05 | loss  6.63 | ppl   758.50\n",
            "| epoch   1 |  1800/ 2983 batches | lr 0.001000 | ms/batch 25.11 | loss  6.46 | ppl   636.73\n",
            "| epoch   1 |  2000/ 2983 batches | lr 0.001000 | ms/batch 25.14 | loss  6.40 | ppl   602.21\n",
            "| epoch   1 |  2200/ 2983 batches | lr 0.001000 | ms/batch 25.16 | loss  6.26 | ppl   524.79\n",
            "| epoch   1 |  2400/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  6.22 | ppl   504.94\n",
            "| epoch   1 |  2600/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  6.19 | ppl   486.94\n",
            "| epoch   1 |  2800/ 2983 batches | lr 0.001000 | ms/batch 25.36 | loss  6.08 | ppl   436.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 76.88s | valid loss  5.91 | valid ppl   367.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   2 |   200/ 2983 batches | lr 0.001000 | ms/batch 25.62 | loss  5.77 | ppl   321.80\n",
            "| epoch   2 |   400/ 2983 batches | lr 0.001000 | ms/batch 25.43 | loss  5.72 | ppl   303.89\n",
            "| epoch   2 |   600/ 2983 batches | lr 0.001000 | ms/batch 25.37 | loss  5.64 | ppl   282.60\n",
            "| epoch   2 |   800/ 2983 batches | lr 0.001000 | ms/batch 25.37 | loss  5.68 | ppl   293.98\n",
            "| epoch   2 |  1000/ 2983 batches | lr 0.001000 | ms/batch 25.35 | loss  5.64 | ppl   281.28\n",
            "| epoch   2 |  1200/ 2983 batches | lr 0.001000 | ms/batch 25.37 | loss  5.66 | ppl   286.92\n",
            "| epoch   2 |  1400/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  5.62 | ppl   275.84\n",
            "| epoch   2 |  1600/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  5.63 | ppl   277.30\n",
            "| epoch   2 |  1800/ 2983 batches | lr 0.001000 | ms/batch 25.35 | loss  5.54 | ppl   254.42\n",
            "| epoch   2 |  2000/ 2983 batches | lr 0.001000 | ms/batch 25.25 | loss  5.56 | ppl   260.03\n",
            "| epoch   2 |  2200/ 2983 batches | lr 0.001000 | ms/batch 25.24 | loss  5.46 | ppl   234.41\n",
            "| epoch   2 |  2400/ 2983 batches | lr 0.001000 | ms/batch 25.24 | loss  5.47 | ppl   237.27\n",
            "| epoch   2 |  2600/ 2983 batches | lr 0.001000 | ms/batch 25.24 | loss  5.48 | ppl   238.70\n",
            "| epoch   2 |  2800/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  5.40 | ppl   221.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 77.60s | valid loss  5.56 | valid ppl   258.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   3 |   200/ 2983 batches | lr 0.001000 | ms/batch 25.43 | loss  5.27 | ppl   193.78\n",
            "| epoch   3 |   400/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  5.25 | ppl   190.32\n",
            "| epoch   3 |   600/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  5.15 | ppl   171.83\n",
            "| epoch   3 |   800/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  5.20 | ppl   181.67\n",
            "| epoch   3 |  1000/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  5.18 | ppl   177.72\n",
            "| epoch   3 |  1200/ 2983 batches | lr 0.001000 | ms/batch 25.33 | loss  5.20 | ppl   181.47\n",
            "| epoch   3 |  1400/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  5.18 | ppl   178.44\n",
            "| epoch   3 |  1600/ 2983 batches | lr 0.001000 | ms/batch 25.32 | loss  5.22 | ppl   185.62\n",
            "| epoch   3 |  1800/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  5.14 | ppl   171.37\n",
            "| epoch   3 |  2000/ 2983 batches | lr 0.001000 | ms/batch 25.33 | loss  5.16 | ppl   175.01\n",
            "| epoch   3 |  2200/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  5.07 | ppl   159.19\n",
            "| epoch   3 |  2400/ 2983 batches | lr 0.001000 | ms/batch 25.32 | loss  5.08 | ppl   160.81\n",
            "| epoch   3 |  2600/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  5.10 | ppl   163.68\n",
            "| epoch   3 |  2800/ 2983 batches | lr 0.001000 | ms/batch 25.26 | loss  5.04 | ppl   153.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 77.54s | valid loss  5.39 | valid ppl   219.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   4 |   200/ 2983 batches | lr 0.001000 | ms/batch 25.39 | loss  4.95 | ppl   141.85\n",
            "| epoch   4 |   400/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  4.97 | ppl   143.91\n",
            "| epoch   4 |   600/ 2983 batches | lr 0.001000 | ms/batch 25.25 | loss  4.82 | ppl   123.62\n",
            "| epoch   4 |   800/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.88 | ppl   131.14\n",
            "| epoch   4 |  1000/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.88 | ppl   132.15\n",
            "| epoch   4 |  1200/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.90 | ppl   134.12\n",
            "| epoch   4 |  1400/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.90 | ppl   134.56\n",
            "| epoch   4 |  1600/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.95 | ppl   141.65\n",
            "| epoch   4 |  1800/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.87 | ppl   130.13\n",
            "| epoch   4 |  2000/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  4.89 | ppl   132.89\n",
            "| epoch   4 |  2200/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.81 | ppl   122.18\n",
            "| epoch   4 |  2400/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.82 | ppl   124.12\n",
            "| epoch   4 |  2600/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.84 | ppl   126.56\n",
            "| epoch   4 |  2800/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.79 | ppl   120.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 77.49s | valid loss  5.32 | valid ppl   205.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   5 |   200/ 2983 batches | lr 0.001000 | ms/batch 25.43 | loss  4.74 | ppl   113.94\n",
            "| epoch   5 |   400/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.75 | ppl   115.66\n",
            "| epoch   5 |   600/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.59 | ppl    98.72\n",
            "| epoch   5 |   800/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.65 | ppl   104.25\n",
            "| epoch   5 |  1000/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.66 | ppl   106.02\n",
            "| epoch   5 |  1200/ 2983 batches | lr 0.001000 | ms/batch 25.32 | loss  4.68 | ppl   107.78\n",
            "| epoch   5 |  1400/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.69 | ppl   109.19\n",
            "| epoch   5 |  1600/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.74 | ppl   114.82\n",
            "| epoch   5 |  1800/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.66 | ppl   105.73\n",
            "| epoch   5 |  2000/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  4.68 | ppl   108.30\n",
            "| epoch   5 |  2200/ 2983 batches | lr 0.001000 | ms/batch 25.32 | loss  4.61 | ppl   100.60\n",
            "| epoch   5 |  2400/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.62 | ppl   101.75\n",
            "| epoch   5 |  2600/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.64 | ppl   104.02\n",
            "| epoch   5 |  2800/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.59 | ppl    98.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 77.52s | valid loss  5.28 | valid ppl   196.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   6 |   200/ 2983 batches | lr 0.001000 | ms/batch 25.39 | loss  4.56 | ppl    95.84\n",
            "| epoch   6 |   400/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.57 | ppl    96.68\n",
            "| epoch   6 |   600/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.40 | ppl    81.85\n",
            "| epoch   6 |   800/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  4.46 | ppl    86.60\n",
            "| epoch   6 |  1000/ 2983 batches | lr 0.001000 | ms/batch 25.32 | loss  4.49 | ppl    88.98\n",
            "| epoch   6 |  1200/ 2983 batches | lr 0.001000 | ms/batch 25.26 | loss  4.51 | ppl    90.81\n",
            "| epoch   6 |  1400/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.52 | ppl    92.28\n",
            "| epoch   6 |  1600/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  4.59 | ppl    98.41\n",
            "| epoch   6 |  1800/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.50 | ppl    90.41\n",
            "| epoch   6 |  2000/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.53 | ppl    92.35\n",
            "| epoch   6 |  2200/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.46 | ppl    86.15\n",
            "| epoch   6 |  2400/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.46 | ppl    86.44\n",
            "| epoch   6 |  2600/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.48 | ppl    88.22\n",
            "| epoch   6 |  2800/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.43 | ppl    84.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 77.50s | valid loss  5.28 | valid ppl   196.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   7 |   200/ 2983 batches | lr 0.001000 | ms/batch 25.41 | loss  4.42 | ppl    82.73\n",
            "| epoch   7 |   400/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.43 | ppl    84.16\n",
            "| epoch   7 |   600/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.26 | ppl    70.75\n",
            "| epoch   7 |   800/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.31 | ppl    74.80\n",
            "| epoch   7 |  1000/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.35 | ppl    77.72\n",
            "| epoch   7 |  1200/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.37 | ppl    78.91\n",
            "| epoch   7 |  1400/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.39 | ppl    80.56\n",
            "| epoch   7 |  1600/ 2983 batches | lr 0.001000 | ms/batch 25.26 | loss  4.46 | ppl    86.45\n",
            "| epoch   7 |  1800/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.38 | ppl    79.56\n",
            "| epoch   7 |  2000/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.39 | ppl    80.96\n",
            "| epoch   7 |  2200/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.33 | ppl    75.62\n",
            "| epoch   7 |  2400/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.33 | ppl    75.66\n",
            "| epoch   7 |  2600/ 2983 batches | lr 0.001000 | ms/batch 25.32 | loss  4.36 | ppl    78.06\n",
            "| epoch   7 |  2800/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.31 | ppl    74.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 77.51s | valid loss  5.29 | valid ppl   199.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   8 |   200/ 2983 batches | lr 0.001000 | ms/batch 25.44 | loss  4.30 | ppl    73.39\n",
            "| epoch   8 |   400/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  4.31 | ppl    74.35\n",
            "| epoch   8 |   600/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.14 | ppl    62.77\n",
            "| epoch   8 |   800/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  4.20 | ppl    66.40\n",
            "| epoch   8 |  1000/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  4.23 | ppl    68.94\n",
            "| epoch   8 |  1200/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.25 | ppl    70.42\n",
            "| epoch   8 |  1400/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.28 | ppl    71.92\n",
            "| epoch   8 |  1600/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.35 | ppl    77.58\n",
            "| epoch   8 |  1800/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  4.26 | ppl    70.91\n",
            "| epoch   8 |  2000/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.30 | ppl    73.33\n",
            "| epoch   8 |  2200/ 2983 batches | lr 0.001000 | ms/batch 25.32 | loss  4.21 | ppl    67.66\n",
            "| epoch   8 |  2400/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.22 | ppl    67.70\n",
            "| epoch   8 |  2600/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.25 | ppl    70.38\n",
            "| epoch   8 |  2800/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.20 | ppl    66.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 77.52s | valid loss  5.34 | valid ppl   208.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   9 |   200/ 2983 batches | lr 0.001000 | ms/batch 25.41 | loss  4.19 | ppl    66.18\n",
            "| epoch   9 |   400/ 2983 batches | lr 0.001000 | ms/batch 25.32 | loss  4.22 | ppl    67.76\n",
            "| epoch   9 |   600/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.04 | ppl    56.77\n",
            "| epoch   9 |   800/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.10 | ppl    60.18\n",
            "| epoch   9 |  1000/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.14 | ppl    62.98\n",
            "| epoch   9 |  1200/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.16 | ppl    63.89\n",
            "| epoch   9 |  1400/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.17 | ppl    64.98\n",
            "| epoch   9 |  1600/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.25 | ppl    70.38\n",
            "| epoch   9 |  1800/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.17 | ppl    64.60\n",
            "| epoch   9 |  2000/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.20 | ppl    66.80\n",
            "| epoch   9 |  2200/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.12 | ppl    61.39\n",
            "| epoch   9 |  2400/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.12 | ppl    61.66\n",
            "| epoch   9 |  2600/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.15 | ppl    63.61\n",
            "| epoch   9 |  2800/ 2983 batches | lr 0.001000 | ms/batch 25.32 | loss  4.11 | ppl    61.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 77.53s | valid loss  5.38 | valid ppl   216.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch  10 |   200/ 2983 batches | lr 0.000500 | ms/batch 25.43 | loss  4.13 | ppl    62.30\n",
            "| epoch  10 |   400/ 2983 batches | lr 0.000500 | ms/batch 25.28 | loss  4.13 | ppl    62.11\n",
            "| epoch  10 |   600/ 2983 batches | lr 0.000500 | ms/batch 25.28 | loss  3.95 | ppl    51.94\n",
            "| epoch  10 |   800/ 2983 batches | lr 0.000500 | ms/batch 25.26 | loss  4.00 | ppl    54.85\n",
            "| epoch  10 |  1000/ 2983 batches | lr 0.000500 | ms/batch 25.27 | loss  4.05 | ppl    57.37\n",
            "| epoch  10 |  1200/ 2983 batches | lr 0.000500 | ms/batch 25.30 | loss  4.04 | ppl    56.69\n",
            "| epoch  10 |  1400/ 2983 batches | lr 0.000500 | ms/batch 25.29 | loss  4.05 | ppl    57.29\n",
            "| epoch  10 |  1600/ 2983 batches | lr 0.000500 | ms/batch 25.31 | loss  4.12 | ppl    61.84\n",
            "| epoch  10 |  1800/ 2983 batches | lr 0.000500 | ms/batch 25.31 | loss  4.04 | ppl    56.68\n",
            "| epoch  10 |  2000/ 2983 batches | lr 0.000500 | ms/batch 25.30 | loss  4.05 | ppl    57.33\n",
            "| epoch  10 |  2200/ 2983 batches | lr 0.000500 | ms/batch 25.32 | loss  3.96 | ppl    52.70\n",
            "| epoch  10 |  2400/ 2983 batches | lr 0.000500 | ms/batch 25.31 | loss  3.97 | ppl    53.05\n",
            "| epoch  10 |  2600/ 2983 batches | lr 0.000500 | ms/batch 25.32 | loss  3.99 | ppl    54.27\n",
            "| epoch  10 |  2800/ 2983 batches | lr 0.000500 | ms/batch 25.29 | loss  3.95 | ppl    51.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 77.52s | valid loss  5.37 | valid ppl   213.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch  11 |   200/ 2983 batches | lr 0.000500 | ms/batch 25.42 | loss  4.04 | ppl    56.68\n",
            "| epoch  11 |   400/ 2983 batches | lr 0.000500 | ms/batch 25.29 | loss  4.05 | ppl    57.15\n",
            "| epoch  11 |   600/ 2983 batches | lr 0.000500 | ms/batch 25.28 | loss  3.87 | ppl    47.89\n",
            "| epoch  11 |   800/ 2983 batches | lr 0.000500 | ms/batch 25.25 | loss  3.93 | ppl    50.73\n",
            "| epoch  11 |  1000/ 2983 batches | lr 0.000500 | ms/batch 25.29 | loss  3.97 | ppl    52.91\n",
            "| epoch  11 |  1200/ 2983 batches | lr 0.000500 | ms/batch 25.28 | loss  3.96 | ppl    52.39\n",
            "| epoch  11 |  1400/ 2983 batches | lr 0.000500 | ms/batch 25.28 | loss  3.98 | ppl    53.51\n",
            "| epoch  11 |  1600/ 2983 batches | lr 0.000500 | ms/batch 25.30 | loss  4.05 | ppl    57.47\n",
            "| epoch  11 |  1800/ 2983 batches | lr 0.000500 | ms/batch 25.29 | loss  3.98 | ppl    53.28\n",
            "| epoch  11 |  2000/ 2983 batches | lr 0.000500 | ms/batch 25.29 | loss  4.00 | ppl    54.37\n",
            "| epoch  11 |  2200/ 2983 batches | lr 0.000500 | ms/batch 25.31 | loss  3.91 | ppl    49.88\n",
            "| epoch  11 |  2400/ 2983 batches | lr 0.000500 | ms/batch 25.30 | loss  3.91 | ppl    49.81\n",
            "| epoch  11 |  2600/ 2983 batches | lr 0.000500 | ms/batch 25.28 | loss  3.94 | ppl    51.39\n",
            "| epoch  11 |  2800/ 2983 batches | lr 0.000500 | ms/batch 25.32 | loss  3.90 | ppl    49.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 77.51s | valid loss  5.39 | valid ppl   219.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch  12 |   200/ 2983 batches | lr 0.000500 | ms/batch 25.46 | loss  3.98 | ppl    53.47\n",
            "| epoch  12 |   400/ 2983 batches | lr 0.000500 | ms/batch 25.30 | loss  3.98 | ppl    53.77\n",
            "| epoch  12 |   600/ 2983 batches | lr 0.000500 | ms/batch 25.29 | loss  3.81 | ppl    44.97\n",
            "| epoch  12 |   800/ 2983 batches | lr 0.000500 | ms/batch 25.30 | loss  3.87 | ppl    47.82\n",
            "| epoch  12 |  1000/ 2983 batches | lr 0.000500 | ms/batch 25.27 | loss  3.91 | ppl    49.89\n",
            "| epoch  12 |  1200/ 2983 batches | lr 0.000500 | ms/batch 25.32 | loss  3.90 | ppl    49.44\n",
            "| epoch  12 |  1400/ 2983 batches | lr 0.000500 | ms/batch 25.32 | loss  3.92 | ppl    50.56\n",
            "| epoch  12 |  1600/ 2983 batches | lr 0.000500 | ms/batch 25.30 | loss  4.00 | ppl    54.69\n",
            "| epoch  12 |  1800/ 2983 batches | lr 0.000500 | ms/batch 25.30 | loss  3.93 | ppl    51.05\n",
            "| epoch  12 |  2000/ 2983 batches | lr 0.000500 | ms/batch 25.30 | loss  3.95 | ppl    51.79\n",
            "| epoch  12 |  2200/ 2983 batches | lr 0.000500 | ms/batch 25.33 | loss  3.86 | ppl    47.34\n",
            "| epoch  12 |  2400/ 2983 batches | lr 0.000500 | ms/batch 25.30 | loss  3.86 | ppl    47.47\n",
            "| epoch  12 |  2600/ 2983 batches | lr 0.000500 | ms/batch 25.34 | loss  3.90 | ppl    49.17\n",
            "| epoch  12 |  2800/ 2983 batches | lr 0.000500 | ms/batch 25.31 | loss  3.85 | ppl    47.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 77.59s | valid loss  5.41 | valid ppl   223.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  13 |   200/ 2983 batches | lr 0.000250 | ms/batch 25.48 | loss  3.97 | ppl    52.89\n",
            "| epoch  13 |   400/ 2983 batches | lr 0.000250 | ms/batch 25.30 | loss  3.96 | ppl    52.59\n",
            "| epoch  13 |   600/ 2983 batches | lr 0.000250 | ms/batch 25.32 | loss  3.80 | ppl    44.55\n",
            "| epoch  13 |   800/ 2983 batches | lr 0.000250 | ms/batch 25.29 | loss  3.84 | ppl    46.41\n",
            "| epoch  13 |  1000/ 2983 batches | lr 0.000250 | ms/batch 25.33 | loss  3.88 | ppl    48.25\n",
            "| epoch  13 |  1200/ 2983 batches | lr 0.000250 | ms/batch 25.33 | loss  3.86 | ppl    47.44\n",
            "| epoch  13 |  1400/ 2983 batches | lr 0.000250 | ms/batch 25.30 | loss  3.88 | ppl    48.23\n",
            "| epoch  13 |  1600/ 2983 batches | lr 0.000250 | ms/batch 25.33 | loss  3.95 | ppl    51.90\n",
            "| epoch  13 |  1800/ 2983 batches | lr 0.000250 | ms/batch 25.30 | loss  3.88 | ppl    48.24\n",
            "| epoch  13 |  2000/ 2983 batches | lr 0.000250 | ms/batch 25.30 | loss  3.88 | ppl    48.66\n",
            "| epoch  13 |  2200/ 2983 batches | lr 0.000250 | ms/batch 25.31 | loss  3.80 | ppl    44.51\n",
            "| epoch  13 |  2400/ 2983 batches | lr 0.000250 | ms/batch 25.28 | loss  3.80 | ppl    44.72\n",
            "| epoch  13 |  2600/ 2983 batches | lr 0.000250 | ms/batch 25.30 | loss  3.83 | ppl    46.04\n",
            "| epoch  13 |  2800/ 2983 batches | lr 0.000250 | ms/batch 25.28 | loss  3.79 | ppl    44.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 77.59s | valid loss  5.40 | valid ppl   220.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  14 |   200/ 2983 batches | lr 0.000250 | ms/batch 25.43 | loss  3.92 | ppl    50.32\n",
            "| epoch  14 |   400/ 2983 batches | lr 0.000250 | ms/batch 25.31 | loss  3.92 | ppl    50.64\n",
            "| epoch  14 |   600/ 2983 batches | lr 0.000250 | ms/batch 25.29 | loss  3.76 | ppl    42.74\n",
            "| epoch  14 |   800/ 2983 batches | lr 0.000250 | ms/batch 25.30 | loss  3.79 | ppl    44.42\n",
            "| epoch  14 |  1000/ 2983 batches | lr 0.000250 | ms/batch 25.34 | loss  3.84 | ppl    46.34\n",
            "| epoch  14 |  1200/ 2983 batches | lr 0.000250 | ms/batch 25.31 | loss  3.81 | ppl    45.27\n",
            "| epoch  14 |  1400/ 2983 batches | lr 0.000250 | ms/batch 25.33 | loss  3.84 | ppl    46.48\n",
            "| epoch  14 |  1600/ 2983 batches | lr 0.000250 | ms/batch 25.37 | loss  3.91 | ppl    49.90\n",
            "| epoch  14 |  1800/ 2983 batches | lr 0.000250 | ms/batch 25.37 | loss  3.85 | ppl    46.76\n",
            "| epoch  14 |  2000/ 2983 batches | lr 0.000250 | ms/batch 25.36 | loss  3.85 | ppl    46.94\n",
            "| epoch  14 |  2200/ 2983 batches | lr 0.000250 | ms/batch 25.32 | loss  3.77 | ppl    43.30\n",
            "| epoch  14 |  2400/ 2983 batches | lr 0.000250 | ms/batch 25.31 | loss  3.77 | ppl    43.48\n",
            "| epoch  14 |  2600/ 2983 batches | lr 0.000250 | ms/batch 25.30 | loss  3.81 | ppl    45.12\n",
            "| epoch  14 |  2800/ 2983 batches | lr 0.000250 | ms/batch 25.32 | loss  3.76 | ppl    43.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 77.61s | valid loss  5.40 | valid ppl   221.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  15 |   200/ 2983 batches | lr 0.000250 | ms/batch 25.44 | loss  3.88 | ppl    48.34\n",
            "| epoch  15 |   400/ 2983 batches | lr 0.000250 | ms/batch 25.28 | loss  3.89 | ppl    48.81\n",
            "| epoch  15 |   600/ 2983 batches | lr 0.000250 | ms/batch 25.29 | loss  3.72 | ppl    41.20\n",
            "| epoch  15 |   800/ 2983 batches | lr 0.000250 | ms/batch 25.31 | loss  3.76 | ppl    42.80\n",
            "| epoch  15 |  1000/ 2983 batches | lr 0.000250 | ms/batch 25.28 | loss  3.80 | ppl    44.74\n",
            "| epoch  15 |  1200/ 2983 batches | lr 0.000250 | ms/batch 25.31 | loss  3.79 | ppl    44.24\n",
            "| epoch  15 |  1400/ 2983 batches | lr 0.000250 | ms/batch 25.32 | loss  3.81 | ppl    44.99\n",
            "| epoch  15 |  1600/ 2983 batches | lr 0.000250 | ms/batch 25.31 | loss  3.88 | ppl    48.61\n",
            "| epoch  15 |  1800/ 2983 batches | lr 0.000250 | ms/batch 25.33 | loss  3.81 | ppl    45.35\n",
            "| epoch  15 |  2000/ 2983 batches | lr 0.000250 | ms/batch 25.31 | loss  3.82 | ppl    45.51\n",
            "| epoch  15 |  2200/ 2983 batches | lr 0.000250 | ms/batch 25.33 | loss  3.73 | ppl    41.88\n",
            "| epoch  15 |  2400/ 2983 batches | lr 0.000250 | ms/batch 25.28 | loss  3.74 | ppl    42.09\n",
            "| epoch  15 |  2600/ 2983 batches | lr 0.000250 | ms/batch 25.30 | loss  3.77 | ppl    43.56\n",
            "| epoch  15 |  2800/ 2983 batches | lr 0.000250 | ms/batch 25.28 | loss  3.74 | ppl    41.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 77.54s | valid loss  5.41 | valid ppl   223.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  16 |   200/ 2983 batches | lr 0.000125 | ms/batch 25.40 | loss  3.91 | ppl    49.88\n",
            "| epoch  16 |   400/ 2983 batches | lr 0.000125 | ms/batch 25.28 | loss  3.89 | ppl    49.07\n",
            "| epoch  16 |   600/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.75 | ppl    42.57\n",
            "| epoch  16 |   800/ 2983 batches | lr 0.000125 | ms/batch 25.27 | loss  3.76 | ppl    43.02\n",
            "| epoch  16 |  1000/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.80 | ppl    44.67\n",
            "| epoch  16 |  1200/ 2983 batches | lr 0.000125 | ms/batch 25.30 | loss  3.79 | ppl    44.29\n",
            "| epoch  16 |  1400/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.80 | ppl    44.62\n",
            "| epoch  16 |  1600/ 2983 batches | lr 0.000125 | ms/batch 25.31 | loss  3.87 | ppl    48.12\n",
            "| epoch  16 |  1800/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.80 | ppl    44.71\n",
            "| epoch  16 |  2000/ 2983 batches | lr 0.000125 | ms/batch 25.34 | loss  3.81 | ppl    45.35\n",
            "| epoch  16 |  2200/ 2983 batches | lr 0.000125 | ms/batch 25.32 | loss  3.72 | ppl    41.25\n",
            "| epoch  16 |  2400/ 2983 batches | lr 0.000125 | ms/batch 25.32 | loss  3.73 | ppl    41.75\n",
            "| epoch  16 |  2600/ 2983 batches | lr 0.000125 | ms/batch 25.32 | loss  3.77 | ppl    43.41\n",
            "| epoch  16 |  2800/ 2983 batches | lr 0.000125 | ms/batch 25.31 | loss  3.71 | ppl    40.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 77.55s | valid loss  5.37 | valid ppl   214.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  17 |   200/ 2983 batches | lr 0.000125 | ms/batch 25.42 | loss  3.88 | ppl    48.52\n",
            "| epoch  17 |   400/ 2983 batches | lr 0.000125 | ms/batch 25.30 | loss  3.87 | ppl    47.92\n",
            "| epoch  17 |   600/ 2983 batches | lr 0.000125 | ms/batch 25.27 | loss  3.71 | ppl    41.02\n",
            "| epoch  17 |   800/ 2983 batches | lr 0.000125 | ms/batch 25.28 | loss  3.74 | ppl    41.94\n",
            "| epoch  17 |  1000/ 2983 batches | lr 0.000125 | ms/batch 25.28 | loss  3.79 | ppl    44.09\n",
            "| epoch  17 |  1200/ 2983 batches | lr 0.000125 | ms/batch 25.28 | loss  3.76 | ppl    43.03\n",
            "| epoch  17 |  1400/ 2983 batches | lr 0.000125 | ms/batch 25.33 | loss  3.78 | ppl    43.82\n",
            "| epoch  17 |  1600/ 2983 batches | lr 0.000125 | ms/batch 25.30 | loss  3.86 | ppl    47.35\n",
            "| epoch  17 |  1800/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.78 | ppl    43.63\n",
            "| epoch  17 |  2000/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.80 | ppl    44.80\n",
            "| epoch  17 |  2200/ 2983 batches | lr 0.000125 | ms/batch 25.30 | loss  3.70 | ppl    40.49\n",
            "| epoch  17 |  2400/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.72 | ppl    41.06\n",
            "| epoch  17 |  2600/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.76 | ppl    42.98\n",
            "| epoch  17 |  2800/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.69 | ppl    40.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 77.50s | valid loss  5.37 | valid ppl   215.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  18 |   200/ 2983 batches | lr 0.000125 | ms/batch 25.40 | loss  3.86 | ppl    47.23\n",
            "| epoch  18 |   400/ 2983 batches | lr 0.000125 | ms/batch 25.27 | loss  3.85 | ppl    47.02\n",
            "| epoch  18 |   600/ 2983 batches | lr 0.000125 | ms/batch 25.31 | loss  3.68 | ppl    39.72\n",
            "| epoch  18 |   800/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.72 | ppl    41.20\n",
            "| epoch  18 |  1000/ 2983 batches | lr 0.000125 | ms/batch 25.27 | loss  3.76 | ppl    42.94\n",
            "| epoch  18 |  1200/ 2983 batches | lr 0.000125 | ms/batch 25.31 | loss  3.74 | ppl    42.27\n",
            "| epoch  18 |  1400/ 2983 batches | lr 0.000125 | ms/batch 25.31 | loss  3.75 | ppl    42.69\n",
            "| epoch  18 |  1600/ 2983 batches | lr 0.000125 | ms/batch 25.32 | loss  3.84 | ppl    46.34\n",
            "| epoch  18 |  1800/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.76 | ppl    42.91\n",
            "| epoch  18 |  2000/ 2983 batches | lr 0.000125 | ms/batch 25.30 | loss  3.79 | ppl    44.09\n",
            "| epoch  18 |  2200/ 2983 batches | lr 0.000125 | ms/batch 25.32 | loss  3.69 | ppl    39.99\n",
            "| epoch  18 |  2400/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.69 | ppl    39.98\n",
            "| epoch  18 |  2600/ 2983 batches | lr 0.000125 | ms/batch 25.30 | loss  3.74 | ppl    42.16\n",
            "| epoch  18 |  2800/ 2983 batches | lr 0.000125 | ms/batch 25.30 | loss  3.68 | ppl    39.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 77.55s | valid loss  5.37 | valid ppl   215.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  19 |   200/ 2983 batches | lr 0.000063 | ms/batch 25.43 | loss  3.87 | ppl    48.05\n",
            "| epoch  19 |   400/ 2983 batches | lr 0.000063 | ms/batch 25.32 | loss  3.90 | ppl    49.56\n",
            "| epoch  19 |   600/ 2983 batches | lr 0.000063 | ms/batch 25.31 | loss  3.74 | ppl    42.05\n",
            "| epoch  19 |   800/ 2983 batches | lr 0.000063 | ms/batch 25.29 | loss  3.74 | ppl    42.06\n",
            "| epoch  19 |  1000/ 2983 batches | lr 0.000063 | ms/batch 25.28 | loss  3.79 | ppl    44.05\n",
            "| epoch  19 |  1200/ 2983 batches | lr 0.000063 | ms/batch 25.28 | loss  3.77 | ppl    43.38\n",
            "| epoch  19 |  1400/ 2983 batches | lr 0.000063 | ms/batch 25.30 | loss  3.79 | ppl    44.28\n",
            "| epoch  19 |  1600/ 2983 batches | lr 0.000063 | ms/batch 25.29 | loss  3.86 | ppl    47.40\n",
            "| epoch  19 |  1800/ 2983 batches | lr 0.000063 | ms/batch 25.30 | loss  3.77 | ppl    43.47\n",
            "| epoch  19 |  2000/ 2983 batches | lr 0.000063 | ms/batch 25.31 | loss  3.80 | ppl    44.59\n",
            "| epoch  19 |  2200/ 2983 batches | lr 0.000063 | ms/batch 25.29 | loss  3.72 | ppl    41.27\n",
            "| epoch  19 |  2400/ 2983 batches | lr 0.000063 | ms/batch 25.32 | loss  3.71 | ppl    40.89\n",
            "| epoch  19 |  2600/ 2983 batches | lr 0.000063 | ms/batch 25.30 | loss  3.75 | ppl    42.35\n",
            "| epoch  19 |  2800/ 2983 batches | lr 0.000063 | ms/batch 25.30 | loss  3.69 | ppl    40.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 77.53s | valid loss  5.30 | valid ppl   201.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  20 |   200/ 2983 batches | lr 0.000063 | ms/batch 25.42 | loss  3.87 | ppl    47.93\n",
            "| epoch  20 |   400/ 2983 batches | lr 0.000063 | ms/batch 25.30 | loss  3.87 | ppl    47.77\n",
            "| epoch  20 |   600/ 2983 batches | lr 0.000063 | ms/batch 25.29 | loss  3.72 | ppl    41.37\n",
            "| epoch  20 |   800/ 2983 batches | lr 0.000063 | ms/batch 25.32 | loss  3.71 | ppl    41.06\n",
            "| epoch  20 |  1000/ 2983 batches | lr 0.000063 | ms/batch 25.30 | loss  3.77 | ppl    43.33\n",
            "| epoch  20 |  1200/ 2983 batches | lr 0.000063 | ms/batch 25.29 | loss  3.74 | ppl    42.20\n",
            "| epoch  20 |  1400/ 2983 batches | lr 0.000063 | ms/batch 25.31 | loss  3.79 | ppl    44.40\n",
            "| epoch  20 |  1600/ 2983 batches | lr 0.000063 | ms/batch 25.31 | loss  3.84 | ppl    46.49\n",
            "| epoch  20 |  1800/ 2983 batches | lr 0.000063 | ms/batch 25.34 | loss  3.77 | ppl    43.36\n",
            "| epoch  20 |  2000/ 2983 batches | lr 0.000063 | ms/batch 25.32 | loss  3.79 | ppl    44.26\n",
            "| epoch  20 |  2200/ 2983 batches | lr 0.000063 | ms/batch 25.32 | loss  3.71 | ppl    41.00\n",
            "| epoch  20 |  2400/ 2983 batches | lr 0.000063 | ms/batch 25.29 | loss  3.71 | ppl    40.78\n",
            "| epoch  20 |  2600/ 2983 batches | lr 0.000063 | ms/batch 25.32 | loss  3.73 | ppl    41.84\n",
            "| epoch  20 |  2800/ 2983 batches | lr 0.000063 | ms/batch 25.32 | loss  3.68 | ppl    39.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 77.60s | valid loss  5.31 | valid ppl   202.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  21 |   200/ 2983 batches | lr 0.000063 | ms/batch 25.50 | loss  3.85 | ppl    46.96\n",
            "| epoch  21 |   400/ 2983 batches | lr 0.000063 | ms/batch 25.30 | loss  3.83 | ppl    46.23\n",
            "| epoch  21 |   600/ 2983 batches | lr 0.000063 | ms/batch 25.33 | loss  3.69 | ppl    40.04\n",
            "| epoch  21 |   800/ 2983 batches | lr 0.000063 | ms/batch 25.31 | loss  3.70 | ppl    40.58\n",
            "| epoch  21 |  1000/ 2983 batches | lr 0.000063 | ms/batch 25.30 | loss  3.76 | ppl    42.82\n",
            "| epoch  21 |  1200/ 2983 batches | lr 0.000063 | ms/batch 25.34 | loss  3.74 | ppl    42.02\n",
            "| epoch  21 |  1400/ 2983 batches | lr 0.000063 | ms/batch 25.29 | loss  3.77 | ppl    43.54\n",
            "| epoch  21 |  1600/ 2983 batches | lr 0.000063 | ms/batch 25.31 | loss  3.83 | ppl    45.96\n",
            "| epoch  21 |  1800/ 2983 batches | lr 0.000063 | ms/batch 25.30 | loss  3.76 | ppl    42.85\n",
            "| epoch  21 |  2000/ 2983 batches | lr 0.000063 | ms/batch 25.29 | loss  3.77 | ppl    43.55\n",
            "| epoch  21 |  2200/ 2983 batches | lr 0.000063 | ms/batch 25.29 | loss  3.70 | ppl    40.29\n",
            "| epoch  21 |  2400/ 2983 batches | lr 0.000063 | ms/batch 25.29 | loss  3.69 | ppl    39.99\n",
            "| epoch  21 |  2600/ 2983 batches | lr 0.000063 | ms/batch 25.31 | loss  3.73 | ppl    41.77\n",
            "| epoch  21 |  2800/ 2983 batches | lr 0.000063 | ms/batch 25.30 | loss  3.67 | ppl    39.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 77.58s | valid loss  5.33 | valid ppl   205.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  22 |   200/ 2983 batches | lr 0.000031 | ms/batch 25.42 | loss  3.87 | ppl    48.14\n",
            "| epoch  22 |   400/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.91 | ppl    49.84\n",
            "| epoch  22 |   600/ 2983 batches | lr 0.000031 | ms/batch 25.32 | loss  3.76 | ppl    42.95\n",
            "| epoch  22 |   800/ 2983 batches | lr 0.000031 | ms/batch 25.36 | loss  3.73 | ppl    41.72\n",
            "| epoch  22 |  1000/ 2983 batches | lr 0.000031 | ms/batch 25.34 | loss  3.79 | ppl    44.27\n",
            "| epoch  22 |  1200/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.77 | ppl    43.48\n",
            "| epoch  22 |  1400/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.85 | ppl    46.88\n",
            "| epoch  22 |  1600/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.86 | ppl    47.57\n",
            "| epoch  22 |  1800/ 2983 batches | lr 0.000031 | ms/batch 25.32 | loss  3.81 | ppl    45.06\n",
            "| epoch  22 |  2000/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.80 | ppl    44.64\n",
            "| epoch  22 |  2200/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.76 | ppl    42.76\n",
            "| epoch  22 |  2400/ 2983 batches | lr 0.000031 | ms/batch 25.33 | loss  3.75 | ppl    42.36\n",
            "| epoch  22 |  2600/ 2983 batches | lr 0.000031 | ms/batch 25.32 | loss  3.74 | ppl    41.99\n",
            "| epoch  22 |  2800/ 2983 batches | lr 0.000031 | ms/batch 25.32 | loss  3.72 | ppl    41.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 77.59s | valid loss  5.28 | valid ppl   195.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  23 |   200/ 2983 batches | lr 0.000031 | ms/batch 25.41 | loss  3.88 | ppl    48.55\n",
            "| epoch  23 |   400/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.91 | ppl    49.80\n",
            "| epoch  23 |   600/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.73 | ppl    41.85\n",
            "| epoch  23 |   800/ 2983 batches | lr 0.000031 | ms/batch 25.32 | loss  3.72 | ppl    41.20\n",
            "| epoch  23 |  1000/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.80 | ppl    44.48\n",
            "| epoch  23 |  1200/ 2983 batches | lr 0.000031 | ms/batch 25.32 | loss  3.75 | ppl    42.35\n",
            "| epoch  23 |  1400/ 2983 batches | lr 0.000031 | ms/batch 25.33 | loss  3.82 | ppl    45.77\n",
            "| epoch  23 |  1600/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.84 | ppl    46.72\n",
            "| epoch  23 |  1800/ 2983 batches | lr 0.000031 | ms/batch 25.32 | loss  3.79 | ppl    44.39\n",
            "| epoch  23 |  2000/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.78 | ppl    43.80\n",
            "| epoch  23 |  2200/ 2983 batches | lr 0.000031 | ms/batch 25.33 | loss  3.74 | ppl    41.90\n",
            "| epoch  23 |  2400/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.72 | ppl    41.40\n",
            "| epoch  23 |  2600/ 2983 batches | lr 0.000031 | ms/batch 25.32 | loss  3.74 | ppl    42.02\n",
            "| epoch  23 |  2800/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.70 | ppl    40.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 77.57s | valid loss  5.27 | valid ppl   195.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  24 |   200/ 2983 batches | lr 0.000031 | ms/batch 25.42 | loss  3.87 | ppl    48.01\n",
            "| epoch  24 |   400/ 2983 batches | lr 0.000031 | ms/batch 25.39 | loss  3.89 | ppl    49.04\n",
            "| epoch  24 |   600/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.72 | ppl    41.15\n",
            "| epoch  24 |   800/ 2983 batches | lr 0.000031 | ms/batch 25.33 | loss  3.71 | ppl    40.79\n",
            "| epoch  24 |  1000/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.78 | ppl    44.03\n",
            "| epoch  24 |  1200/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.74 | ppl    42.16\n",
            "| epoch  24 |  1400/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.82 | ppl    45.67\n",
            "| epoch  24 |  1600/ 2983 batches | lr 0.000031 | ms/batch 25.33 | loss  3.84 | ppl    46.45\n",
            "| epoch  24 |  1800/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.79 | ppl    44.15\n",
            "| epoch  24 |  2000/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.78 | ppl    43.84\n",
            "| epoch  24 |  2200/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.71 | ppl    40.77\n",
            "| epoch  24 |  2400/ 2983 batches | lr 0.000031 | ms/batch 25.34 | loss  3.72 | ppl    41.18\n",
            "| epoch  24 |  2600/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.73 | ppl    41.79\n",
            "| epoch  24 |  2800/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.69 | ppl    39.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 77.61s | valid loss  5.27 | valid ppl   193.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  25 |   200/ 2983 batches | lr 0.000031 | ms/batch 25.39 | loss  3.88 | ppl    48.18\n",
            "| epoch  25 |   400/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.87 | ppl    48.12\n",
            "| epoch  25 |   600/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.71 | ppl    40.99\n",
            "| epoch  25 |   800/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.70 | ppl    40.40\n",
            "| epoch  25 |  1000/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.77 | ppl    43.59\n",
            "| epoch  25 |  1200/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.74 | ppl    41.98\n",
            "| epoch  25 |  1400/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.81 | ppl    45.19\n",
            "| epoch  25 |  1600/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.83 | ppl    45.98\n",
            "| epoch  25 |  1800/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.77 | ppl    43.33\n",
            "| epoch  25 |  2000/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.77 | ppl    43.56\n",
            "| epoch  25 |  2200/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.70 | ppl    40.61\n",
            "| epoch  25 |  2400/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.71 | ppl    40.91\n",
            "| epoch  25 |  2600/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.73 | ppl    41.51\n",
            "| epoch  25 |  2800/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.68 | ppl    39.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 77.51s | valid loss  5.26 | valid ppl   192.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  26 |   200/ 2983 batches | lr 0.000031 | ms/batch 25.42 | loss  3.87 | ppl    47.80\n",
            "| epoch  26 |   400/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.86 | ppl    47.43\n",
            "| epoch  26 |   600/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.69 | ppl    40.17\n",
            "| epoch  26 |   800/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.70 | ppl    40.40\n",
            "| epoch  26 |  1000/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.77 | ppl    43.56\n",
            "| epoch  26 |  1200/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.75 | ppl    42.39\n",
            "| epoch  26 |  1400/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.80 | ppl    44.80\n",
            "| epoch  26 |  1600/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.82 | ppl    45.44\n",
            "| epoch  26 |  1800/ 2983 batches | lr 0.000031 | ms/batch 25.27 | loss  3.77 | ppl    43.28\n",
            "| epoch  26 |  2000/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.76 | ppl    43.03\n",
            "| epoch  26 |  2200/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.69 | ppl    39.91\n",
            "| epoch  26 |  2400/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.70 | ppl    40.58\n",
            "| epoch  26 |  2600/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.72 | ppl    41.39\n",
            "| epoch  26 |  2800/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.67 | ppl    39.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 77.53s | valid loss  5.26 | valid ppl   193.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  27 |   200/ 2983 batches | lr 0.000031 | ms/batch 25.43 | loss  3.85 | ppl    47.20\n",
            "| epoch  27 |   400/ 2983 batches | lr 0.000031 | ms/batch 25.32 | loss  3.86 | ppl    47.33\n",
            "| epoch  27 |   600/ 2983 batches | lr 0.000031 | ms/batch 25.26 | loss  3.68 | ppl    39.64\n",
            "| epoch  27 |   800/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.69 | ppl    39.88\n",
            "| epoch  27 |  1000/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.77 | ppl    43.20\n",
            "| epoch  27 |  1200/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.72 | ppl    41.18\n",
            "| epoch  27 |  1400/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.80 | ppl    44.87\n",
            "| epoch  27 |  1600/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.81 | ppl    45.19\n",
            "| epoch  27 |  1800/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.75 | ppl    42.47\n",
            "| epoch  27 |  2000/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.76 | ppl    43.12\n",
            "| epoch  27 |  2200/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.68 | ppl    39.75\n",
            "| epoch  27 |  2400/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.69 | ppl    39.88\n",
            "| epoch  27 |  2600/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.71 | ppl    40.90\n",
            "| epoch  27 |  2800/ 2983 batches | lr 0.000031 | ms/batch 25.33 | loss  3.67 | ppl    39.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 77.51s | valid loss  5.26 | valid ppl   192.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  28 |   200/ 2983 batches | lr 0.000031 | ms/batch 25.43 | loss  3.86 | ppl    47.41\n",
            "| epoch  28 |   400/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.85 | ppl    46.76\n",
            "| epoch  28 |   600/ 2983 batches | lr 0.000031 | ms/batch 25.27 | loss  3.67 | ppl    39.34\n",
            "| epoch  28 |   800/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.68 | ppl    39.65\n",
            "| epoch  28 |  1000/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.75 | ppl    42.63\n",
            "| epoch  28 |  1200/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.72 | ppl    41.12\n",
            "| epoch  28 |  1400/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.80 | ppl    44.70\n",
            "| epoch  28 |  1600/ 2983 batches | lr 0.000031 | ms/batch 25.27 | loss  3.80 | ppl    44.82\n",
            "| epoch  28 |  1800/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.75 | ppl    42.34\n",
            "| epoch  28 |  2000/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.75 | ppl    42.57\n",
            "| epoch  28 |  2200/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.66 | ppl    38.82\n",
            "| epoch  28 |  2400/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.69 | ppl    39.86\n",
            "| epoch  28 |  2600/ 2983 batches | lr 0.000031 | ms/batch 25.33 | loss  3.71 | ppl    40.74\n",
            "| epoch  28 |  2800/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.66 | ppl    39.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 77.53s | valid loss  5.27 | valid ppl   194.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  29 |   200/ 2983 batches | lr 0.000016 | ms/batch 25.41 | loss  3.87 | ppl    47.79\n",
            "| epoch  29 |   400/ 2983 batches | lr 0.000016 | ms/batch 25.27 | loss  3.93 | ppl    50.78\n",
            "| epoch  29 |   600/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.73 | ppl    41.57\n",
            "| epoch  29 |   800/ 2983 batches | lr 0.000016 | ms/batch 25.26 | loss  3.74 | ppl    41.92\n",
            "| epoch  29 |  1000/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.87 | ppl    48.16\n",
            "| epoch  29 |  1200/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.75 | ppl    42.45\n",
            "| epoch  29 |  1400/ 2983 batches | lr 0.000016 | ms/batch 25.32 | loss  3.92 | ppl    50.41\n",
            "| epoch  29 |  1600/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.86 | ppl    47.52\n",
            "| epoch  29 |  1800/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.81 | ppl    45.31\n",
            "| epoch  29 |  2000/ 2983 batches | lr 0.000016 | ms/batch 25.32 | loss  3.83 | ppl    46.11\n",
            "| epoch  29 |  2200/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.75 | ppl    42.45\n",
            "| epoch  29 |  2400/ 2983 batches | lr 0.000016 | ms/batch 25.33 | loss  3.71 | ppl    40.79\n",
            "| epoch  29 |  2600/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.74 | ppl    42.03\n",
            "| epoch  29 |  2800/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.71 | ppl    41.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 77.52s | valid loss  5.21 | valid ppl   183.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  30 |   200/ 2983 batches | lr 0.000016 | ms/batch 25.42 | loss  3.89 | ppl    48.97\n",
            "| epoch  30 |   400/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.92 | ppl    50.61\n",
            "| epoch  30 |   600/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.72 | ppl    41.11\n",
            "| epoch  30 |   800/ 2983 batches | lr 0.000016 | ms/batch 25.32 | loss  3.74 | ppl    42.08\n",
            "| epoch  30 |  1000/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.82 | ppl    45.56\n",
            "| epoch  30 |  1200/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.73 | ppl    41.52\n",
            "| epoch  30 |  1400/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.89 | ppl    49.00\n",
            "| epoch  30 |  1600/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.82 | ppl    45.75\n",
            "| epoch  30 |  1800/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.79 | ppl    44.26\n",
            "| epoch  30 |  2000/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.81 | ppl    45.15\n",
            "| epoch  30 |  2200/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.73 | ppl    41.61\n",
            "| epoch  30 |  2400/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.71 | ppl    40.68\n",
            "| epoch  30 |  2600/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.73 | ppl    41.69\n",
            "| epoch  30 |  2800/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.70 | ppl    40.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 77.52s | valid loss  5.21 | valid ppl   183.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  31 |   200/ 2983 batches | lr 0.000016 | ms/batch 25.42 | loss  3.87 | ppl    48.07\n",
            "| epoch  31 |   400/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.94 | ppl    51.27\n",
            "| epoch  31 |   600/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.71 | ppl    40.70\n",
            "| epoch  31 |   800/ 2983 batches | lr 0.000016 | ms/batch 25.27 | loss  3.74 | ppl    42.16\n",
            "| epoch  31 |  1000/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.79 | ppl    44.04\n",
            "| epoch  31 |  1200/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.74 | ppl    41.96\n",
            "| epoch  31 |  1400/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.88 | ppl    48.34\n",
            "| epoch  31 |  1600/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.84 | ppl    46.45\n",
            "| epoch  31 |  1800/ 2983 batches | lr 0.000016 | ms/batch 25.27 | loss  3.79 | ppl    44.13\n",
            "| epoch  31 |  2000/ 2983 batches | lr 0.000016 | ms/batch 25.32 | loss  3.79 | ppl    44.19\n",
            "| epoch  31 |  2200/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.73 | ppl    41.51\n",
            "| epoch  31 |  2400/ 2983 batches | lr 0.000016 | ms/batch 25.27 | loss  3.70 | ppl    40.42\n",
            "| epoch  31 |  2600/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.72 | ppl    41.33\n",
            "| epoch  31 |  2800/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.69 | ppl    40.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 77.52s | valid loss  5.20 | valid ppl   182.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  32 |   200/ 2983 batches | lr 0.000016 | ms/batch 25.40 | loss  3.87 | ppl    47.97\n",
            "| epoch  32 |   400/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.92 | ppl    50.48\n",
            "| epoch  32 |   600/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.69 | ppl    40.12\n",
            "| epoch  32 |   800/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.72 | ppl    41.36\n",
            "| epoch  32 |  1000/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.80 | ppl    44.89\n",
            "| epoch  32 |  1200/ 2983 batches | lr 0.000016 | ms/batch 25.27 | loss  3.74 | ppl    41.91\n",
            "| epoch  32 |  1400/ 2983 batches | lr 0.000016 | ms/batch 25.32 | loss  3.87 | ppl    47.84\n",
            "| epoch  32 |  1600/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.82 | ppl    45.41\n",
            "| epoch  32 |  1800/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.77 | ppl    43.47\n",
            "| epoch  32 |  2000/ 2983 batches | lr 0.000016 | ms/batch 25.33 | loss  3.78 | ppl    43.91\n",
            "| epoch  32 |  2200/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.71 | ppl    41.05\n",
            "| epoch  32 |  2400/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.69 | ppl    40.06\n",
            "| epoch  32 |  2600/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.72 | ppl    41.36\n",
            "| epoch  32 |  2800/ 2983 batches | lr 0.000016 | ms/batch 25.33 | loss  3.68 | ppl    39.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 77.55s | valid loss  5.21 | valid ppl   183.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  33 |   200/ 2983 batches | lr 0.000016 | ms/batch 25.45 | loss  3.87 | ppl    47.82\n",
            "| epoch  33 |   400/ 2983 batches | lr 0.000016 | ms/batch 25.34 | loss  3.91 | ppl    49.90\n",
            "| epoch  33 |   600/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.69 | ppl    39.95\n",
            "| epoch  33 |   800/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.72 | ppl    41.09\n",
            "| epoch  33 |  1000/ 2983 batches | lr 0.000016 | ms/batch 25.26 | loss  3.83 | ppl    46.24\n",
            "| epoch  33 |  1200/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.74 | ppl    41.89\n",
            "| epoch  33 |  1400/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.87 | ppl    47.80\n",
            "| epoch  33 |  1600/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.83 | ppl    45.84\n",
            "| epoch  33 |  1800/ 2983 batches | lr 0.000016 | ms/batch 25.27 | loss  3.78 | ppl    43.62\n",
            "| epoch  33 |  2000/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.77 | ppl    43.29\n",
            "| epoch  33 |  2200/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.70 | ppl    40.53\n",
            "| epoch  33 |  2400/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.69 | ppl    40.06\n",
            "| epoch  33 |  2600/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.71 | ppl    40.81\n",
            "| epoch  33 |  2800/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.68 | ppl    39.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 77.53s | valid loss  5.22 | valid ppl   184.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  34 |   200/ 2983 batches | lr 0.000016 | ms/batch 25.43 | loss  3.86 | ppl    47.31\n",
            "| epoch  34 |   400/ 2983 batches | lr 0.000016 | ms/batch 25.27 | loss  3.90 | ppl    49.57\n",
            "| epoch  34 |   600/ 2983 batches | lr 0.000016 | ms/batch 25.32 | loss  3.69 | ppl    39.95\n",
            "| epoch  34 |   800/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.70 | ppl    40.47\n",
            "| epoch  34 |  1000/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.83 | ppl    45.90\n",
            "| epoch  34 |  1200/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.72 | ppl    41.20\n",
            "| epoch  34 |  1400/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.86 | ppl    47.24\n",
            "| epoch  34 |  1600/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.82 | ppl    45.66\n",
            "| epoch  34 |  1800/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.78 | ppl    43.62\n",
            "| epoch  34 |  2000/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.77 | ppl    43.33\n",
            "| epoch  34 |  2200/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.70 | ppl    40.41\n",
            "| epoch  34 |  2400/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.68 | ppl    39.75\n",
            "| epoch  34 |  2600/ 2983 batches | lr 0.000016 | ms/batch 25.32 | loss  3.71 | ppl    40.79\n",
            "| epoch  34 |  2800/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.67 | ppl    39.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 77.54s | valid loss  5.21 | valid ppl   183.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  35 |   200/ 2983 batches | lr 0.000008 | ms/batch 25.42 | loss  3.88 | ppl    48.50\n",
            "| epoch  35 |   400/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.98 | ppl    53.46\n",
            "| epoch  35 |   600/ 2983 batches | lr 0.000008 | ms/batch 25.27 | loss  3.74 | ppl    42.27\n",
            "| epoch  35 |   800/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.77 | ppl    43.31\n",
            "| epoch  35 |  1000/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.89 | ppl    48.85\n",
            "| epoch  35 |  1200/ 2983 batches | lr 0.000008 | ms/batch 25.27 | loss  3.79 | ppl    44.32\n",
            "| epoch  35 |  1400/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.96 | ppl    52.38\n",
            "| epoch  35 |  1600/ 2983 batches | lr 0.000008 | ms/batch 25.27 | loss  3.93 | ppl    50.84\n",
            "| epoch  35 |  1800/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.83 | ppl    46.01\n",
            "| epoch  35 |  2000/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.84 | ppl    46.45\n",
            "| epoch  35 |  2200/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.73 | ppl    41.85\n",
            "| epoch  35 |  2400/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.73 | ppl    41.57\n",
            "| epoch  35 |  2600/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.73 | ppl    41.87\n",
            "| epoch  35 |  2800/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.72 | ppl    41.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 77.52s | valid loss  5.20 | valid ppl   180.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  36 |   200/ 2983 batches | lr 0.000008 | ms/batch 25.44 | loss  3.98 | ppl    53.56\n",
            "| epoch  36 |   400/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.91 | ppl    49.68\n",
            "| epoch  36 |   600/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.76 | ppl    42.96\n",
            "| epoch  36 |   800/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.78 | ppl    43.63\n",
            "| epoch  36 |  1000/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.91 | ppl    49.99\n",
            "| epoch  36 |  1200/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.77 | ppl    43.45\n",
            "| epoch  36 |  1400/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.95 | ppl    52.14\n",
            "| epoch  36 |  1600/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.92 | ppl    50.56\n",
            "| epoch  36 |  1800/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.81 | ppl    45.29\n",
            "| epoch  36 |  2000/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.83 | ppl    46.12\n",
            "| epoch  36 |  2200/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.73 | ppl    41.74\n",
            "| epoch  36 |  2400/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.72 | ppl    41.13\n",
            "| epoch  36 |  2600/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.75 | ppl    42.51\n",
            "| epoch  36 |  2800/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.72 | ppl    41.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 77.56s | valid loss  5.19 | valid ppl   180.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  37 |   200/ 2983 batches | lr 0.000008 | ms/batch 25.43 | loss  3.96 | ppl    52.72\n",
            "| epoch  37 |   400/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.90 | ppl    49.40\n",
            "| epoch  37 |   600/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.73 | ppl    41.81\n",
            "| epoch  37 |   800/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.78 | ppl    43.69\n",
            "| epoch  37 |  1000/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.91 | ppl    50.04\n",
            "| epoch  37 |  1200/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.76 | ppl    43.07\n",
            "| epoch  37 |  1400/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.96 | ppl    52.56\n",
            "| epoch  37 |  1600/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.91 | ppl    49.90\n",
            "| epoch  37 |  1800/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.82 | ppl    45.68\n",
            "| epoch  37 |  2000/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.83 | ppl    46.13\n",
            "| epoch  37 |  2200/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.72 | ppl    41.08\n",
            "| epoch  37 |  2400/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.71 | ppl    40.82\n",
            "| epoch  37 |  2600/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.74 | ppl    42.27\n",
            "| epoch  37 |  2800/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.70 | ppl    40.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 77.53s | valid loss  5.19 | valid ppl   180.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  38 |   200/ 2983 batches | lr 0.000008 | ms/batch 25.42 | loss  3.95 | ppl    51.98\n",
            "| epoch  38 |   400/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.94 | ppl    51.55\n",
            "| epoch  38 |   600/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.71 | ppl    40.83\n",
            "| epoch  38 |   800/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.76 | ppl    42.93\n",
            "| epoch  38 |  1000/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.91 | ppl    49.80\n",
            "| epoch  38 |  1200/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.76 | ppl    42.83\n",
            "| epoch  38 |  1400/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.95 | ppl    51.77\n",
            "| epoch  38 |  1600/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.90 | ppl    49.18\n",
            "| epoch  38 |  1800/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.80 | ppl    44.74\n",
            "| epoch  38 |  2000/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.83 | ppl    46.06\n",
            "| epoch  38 |  2200/ 2983 batches | lr 0.000008 | ms/batch 25.34 | loss  3.72 | ppl    41.42\n",
            "| epoch  38 |  2400/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.70 | ppl    40.57\n",
            "| epoch  38 |  2600/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.74 | ppl    42.17\n",
            "| epoch  38 |  2800/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.70 | ppl    40.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 77.56s | valid loss  5.19 | valid ppl   178.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  39 |   200/ 2983 batches | lr 0.000008 | ms/batch 25.43 | loss  3.96 | ppl    52.25\n",
            "| epoch  39 |   400/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.91 | ppl    49.98\n",
            "| epoch  39 |   600/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.71 | ppl    40.74\n",
            "| epoch  39 |   800/ 2983 batches | lr 0.000008 | ms/batch 25.27 | loss  3.77 | ppl    43.28\n",
            "| epoch  39 |  1000/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.91 | ppl    49.96\n",
            "| epoch  39 |  1200/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.75 | ppl    42.37\n",
            "| epoch  39 |  1400/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.95 | ppl    51.70\n",
            "| epoch  39 |  1600/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.90 | ppl    49.30\n",
            "| epoch  39 |  1800/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.81 | ppl    44.95\n",
            "| epoch  39 |  2000/ 2983 batches | lr 0.000008 | ms/batch 25.33 | loss  3.84 | ppl    46.33\n",
            "| epoch  39 |  2200/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.71 | ppl    40.87\n",
            "| epoch  39 |  2400/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.69 | ppl    40.17\n",
            "| epoch  39 |  2600/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.74 | ppl    42.09\n",
            "| epoch  39 |  2800/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.68 | ppl    39.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 77.53s | valid loss  5.19 | valid ppl   179.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  40 |   200/ 2983 batches | lr 0.000008 | ms/batch 25.42 | loss  3.94 | ppl    51.20\n",
            "| epoch  40 |   400/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.90 | ppl    49.60\n",
            "| epoch  40 |   600/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.70 | ppl    40.53\n",
            "| epoch  40 |   800/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.76 | ppl    42.74\n",
            "| epoch  40 |  1000/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.90 | ppl    49.63\n",
            "| epoch  40 |  1200/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.75 | ppl    42.32\n",
            "| epoch  40 |  1400/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.94 | ppl    51.33\n",
            "| epoch  40 |  1600/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.89 | ppl    49.03\n",
            "| epoch  40 |  1800/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.81 | ppl    45.23\n",
            "| epoch  40 |  2000/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.83 | ppl    46.01\n",
            "| epoch  40 |  2200/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.71 | ppl    40.87\n",
            "| epoch  40 |  2400/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.70 | ppl    40.37\n",
            "| epoch  40 |  2600/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.74 | ppl    42.04\n",
            "| epoch  40 |  2800/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.69 | ppl    39.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 77.55s | valid loss  5.18 | valid ppl   177.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  41 |   200/ 2983 batches | lr 0.000008 | ms/batch 25.45 | loss  3.96 | ppl    52.40\n",
            "| epoch  41 |   400/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.90 | ppl    49.26\n",
            "| epoch  41 |   600/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.70 | ppl    40.34\n",
            "| epoch  41 |   800/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.75 | ppl    42.48\n",
            "| epoch  41 |  1000/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.89 | ppl    49.09\n",
            "| epoch  41 |  1200/ 2983 batches | lr 0.000008 | ms/batch 25.33 | loss  3.75 | ppl    42.48\n",
            "| epoch  41 |  1400/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.95 | ppl    51.77\n",
            "| epoch  41 |  1600/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.88 | ppl    48.62\n",
            "| epoch  41 |  1800/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.80 | ppl    44.83\n",
            "| epoch  41 |  2000/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.83 | ppl    45.89\n",
            "| epoch  41 |  2200/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.72 | ppl    41.14\n",
            "| epoch  41 |  2400/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.69 | ppl    39.97\n",
            "| epoch  41 |  2600/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.73 | ppl    41.73\n",
            "| epoch  41 |  2800/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.68 | ppl    39.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 77.57s | valid loss  5.18 | valid ppl   178.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  42 |   200/ 2983 batches | lr 0.000008 | ms/batch 25.42 | loss  3.93 | ppl    51.00\n",
            "| epoch  42 |   400/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.90 | ppl    49.26\n",
            "| epoch  42 |   600/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.69 | ppl    39.98\n",
            "| epoch  42 |   800/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.76 | ppl    43.03\n",
            "| epoch  42 |  1000/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.90 | ppl    49.34\n",
            "| epoch  42 |  1200/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.74 | ppl    42.03\n",
            "| epoch  42 |  1400/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.95 | ppl    51.79\n",
            "| epoch  42 |  1600/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.89 | ppl    48.76\n",
            "| epoch  42 |  1800/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.79 | ppl    44.21\n",
            "| epoch  42 |  2000/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.83 | ppl    45.90\n",
            "| epoch  42 |  2200/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.71 | ppl    40.90\n",
            "| epoch  42 |  2400/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.69 | ppl    39.91\n",
            "| epoch  42 |  2600/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.73 | ppl    41.62\n",
            "| epoch  42 |  2800/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.68 | ppl    39.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 77.53s | valid loss  5.18 | valid ppl   177.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  43 |   200/ 2983 batches | lr 0.000008 | ms/batch 25.42 | loss  3.96 | ppl    52.41\n",
            "| epoch  43 |   400/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.90 | ppl    49.37\n",
            "| epoch  43 |   600/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.69 | ppl    40.22\n",
            "| epoch  43 |   800/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.75 | ppl    42.70\n",
            "| epoch  43 |  1000/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.85 | ppl    46.93\n",
            "| epoch  43 |  1200/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.74 | ppl    42.12\n",
            "| epoch  43 |  1400/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.95 | ppl    52.18\n",
            "| epoch  43 |  1600/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.88 | ppl    48.62\n",
            "| epoch  43 |  1800/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.80 | ppl    44.54\n",
            "| epoch  43 |  2000/ 2983 batches | lr 0.000008 | ms/batch 25.27 | loss  3.82 | ppl    45.82\n",
            "| epoch  43 |  2200/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.72 | ppl    41.46\n",
            "| epoch  43 |  2400/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.68 | ppl    39.61\n",
            "| epoch  43 |  2600/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.73 | ppl    41.72\n",
            "| epoch  43 |  2800/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.67 | ppl    39.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 77.52s | valid loss  5.19 | valid ppl   179.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000004\n",
            "| epoch  44 |   200/ 2983 batches | lr 0.000004 | ms/batch 25.42 | loss  3.98 | ppl    53.40\n",
            "| epoch  44 |   400/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  4.08 | ppl    59.40\n",
            "| epoch  44 |   600/ 2983 batches | lr 0.000004 | ms/batch 25.31 | loss  3.85 | ppl    46.96\n",
            "| epoch  44 |   800/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  3.82 | ppl    45.44\n",
            "| epoch  44 |  1000/ 2983 batches | lr 0.000004 | ms/batch 25.32 | loss  3.99 | ppl    54.11\n",
            "| epoch  44 |  1200/ 2983 batches | lr 0.000004 | ms/batch 25.28 | loss  3.97 | ppl    52.75\n",
            "| epoch  44 |  1400/ 2983 batches | lr 0.000004 | ms/batch 25.27 | loss  3.90 | ppl    49.55\n",
            "| epoch  44 |  1600/ 2983 batches | lr 0.000004 | ms/batch 25.28 | loss  4.01 | ppl    55.15\n",
            "| epoch  44 |  1800/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  3.92 | ppl    50.21\n",
            "| epoch  44 |  2000/ 2983 batches | lr 0.000004 | ms/batch 25.31 | loss  3.92 | ppl    50.32\n",
            "| epoch  44 |  2200/ 2983 batches | lr 0.000004 | ms/batch 25.30 | loss  3.77 | ppl    43.48\n",
            "| epoch  44 |  2400/ 2983 batches | lr 0.000004 | ms/batch 25.31 | loss  3.72 | ppl    41.08\n",
            "| epoch  44 |  2600/ 2983 batches | lr 0.000004 | ms/batch 25.31 | loss  3.75 | ppl    42.60\n",
            "| epoch  44 |  2800/ 2983 batches | lr 0.000004 | ms/batch 25.30 | loss  3.70 | ppl    40.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 77.54s | valid loss  5.19 | valid ppl   179.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000004\n",
            "| epoch  45 |   200/ 2983 batches | lr 0.000004 | ms/batch 25.43 | loss  4.01 | ppl    55.37\n",
            "| epoch  45 |   400/ 2983 batches | lr 0.000004 | ms/batch 25.31 | loss  4.09 | ppl    59.76\n",
            "| epoch  45 |   600/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  3.89 | ppl    49.14\n",
            "| epoch  45 |   800/ 2983 batches | lr 0.000004 | ms/batch 25.28 | loss  3.84 | ppl    46.63\n",
            "| epoch  45 |  1000/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  3.95 | ppl    52.10\n",
            "| epoch  45 |  1200/ 2983 batches | lr 0.000004 | ms/batch 25.30 | loss  3.85 | ppl    47.14\n",
            "| epoch  45 |  1400/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  3.95 | ppl    52.11\n",
            "| epoch  45 |  1600/ 2983 batches | lr 0.000004 | ms/batch 25.30 | loss  4.02 | ppl    55.58\n",
            "| epoch  45 |  1800/ 2983 batches | lr 0.000004 | ms/batch 25.30 | loss  3.88 | ppl    48.64\n",
            "| epoch  45 |  2000/ 2983 batches | lr 0.000004 | ms/batch 25.32 | loss  3.90 | ppl    49.52\n",
            "| epoch  45 |  2200/ 2983 batches | lr 0.000004 | ms/batch 25.31 | loss  3.76 | ppl    42.86\n",
            "| epoch  45 |  2400/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  3.71 | ppl    40.93\n",
            "| epoch  45 |  2600/ 2983 batches | lr 0.000004 | ms/batch 25.31 | loss  3.75 | ppl    42.40\n",
            "| epoch  45 |  2800/ 2983 batches | lr 0.000004 | ms/batch 25.31 | loss  3.70 | ppl    40.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 77.52s | valid loss  5.19 | valid ppl   179.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000004\n",
            "| epoch  46 |   200/ 2983 batches | lr 0.000004 | ms/batch 25.44 | loss  4.03 | ppl    56.25\n",
            "| epoch  46 |   400/ 2983 batches | lr 0.000004 | ms/batch 25.30 | loss  4.10 | ppl    60.24\n",
            "| epoch  46 |   600/ 2983 batches | lr 0.000004 | ms/batch 25.28 | loss  3.88 | ppl    48.60\n",
            "| epoch  46 |   800/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  3.82 | ppl    45.54\n",
            "| epoch  46 |  1000/ 2983 batches | lr 0.000004 | ms/batch 25.30 | loss  3.97 | ppl    52.93\n",
            "| epoch  46 |  1200/ 2983 batches | lr 0.000004 | ms/batch 25.30 | loss  3.83 | ppl    46.04\n",
            "| epoch  46 |  1400/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  3.94 | ppl    51.56\n",
            "| epoch  46 |  1600/ 2983 batches | lr 0.000004 | ms/batch 25.30 | loss  4.00 | ppl    54.67\n",
            "| epoch  46 |  1800/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  3.86 | ppl    47.26\n",
            "| epoch  46 |  2000/ 2983 batches | lr 0.000004 | ms/batch 25.31 | loss  3.89 | ppl    48.92\n",
            "| epoch  46 |  2200/ 2983 batches | lr 0.000004 | ms/batch 25.31 | loss  3.76 | ppl    43.03\n",
            "| epoch  46 |  2400/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  3.71 | ppl    41.03\n",
            "| epoch  46 |  2600/ 2983 batches | lr 0.000004 | ms/batch 25.32 | loss  3.74 | ppl    42.28\n",
            "| epoch  46 |  2800/ 2983 batches | lr 0.000004 | ms/batch 25.32 | loss  3.69 | ppl    40.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 77.55s | valid loss  5.19 | valid ppl   179.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000002\n",
            "| epoch  47 |   200/ 2983 batches | lr 0.000002 | ms/batch 25.41 | loss  4.07 | ppl    58.29\n",
            "| epoch  47 |   400/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  4.16 | ppl    63.90\n",
            "| epoch  47 |   600/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  4.04 | ppl    56.67\n",
            "| epoch  47 |   800/ 2983 batches | lr 0.000002 | ms/batch 25.27 | loss  4.13 | ppl    62.29\n",
            "| epoch  47 |  1000/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  4.10 | ppl    60.27\n",
            "| epoch  47 |  1200/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  3.91 | ppl    50.00\n",
            "| epoch  47 |  1400/ 2983 batches | lr 0.000002 | ms/batch 25.28 | loss  3.94 | ppl    51.24\n",
            "| epoch  47 |  1600/ 2983 batches | lr 0.000002 | ms/batch 25.28 | loss  4.00 | ppl    54.60\n",
            "| epoch  47 |  1800/ 2983 batches | lr 0.000002 | ms/batch 25.28 | loss  3.89 | ppl    48.97\n",
            "| epoch  47 |  2000/ 2983 batches | lr 0.000002 | ms/batch 25.34 | loss  3.89 | ppl    49.06\n",
            "| epoch  47 |  2200/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  3.79 | ppl    44.21\n",
            "| epoch  47 |  2400/ 2983 batches | lr 0.000002 | ms/batch 25.31 | loss  3.80 | ppl    44.80\n",
            "| epoch  47 |  2600/ 2983 batches | lr 0.000002 | ms/batch 25.32 | loss  3.78 | ppl    43.93\n",
            "| epoch  47 |  2800/ 2983 batches | lr 0.000002 | ms/batch 25.31 | loss  3.72 | ppl    41.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 77.54s | valid loss  5.19 | valid ppl   179.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000002\n",
            "| epoch  48 |   200/ 2983 batches | lr 0.000002 | ms/batch 25.41 | loss  4.03 | ppl    56.10\n",
            "| epoch  48 |   400/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  4.09 | ppl    59.98\n",
            "| epoch  48 |   600/ 2983 batches | lr 0.000002 | ms/batch 25.28 | loss  3.95 | ppl    52.16\n",
            "| epoch  48 |   800/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  3.99 | ppl    54.17\n",
            "| epoch  48 |  1000/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  4.01 | ppl    55.31\n",
            "| epoch  48 |  1200/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  3.91 | ppl    50.14\n",
            "| epoch  48 |  1400/ 2983 batches | lr 0.000002 | ms/batch 25.31 | loss  3.93 | ppl    50.79\n",
            "| epoch  48 |  1600/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  4.04 | ppl    56.90\n",
            "| epoch  48 |  1800/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  3.91 | ppl    49.68\n",
            "| epoch  48 |  2000/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  3.91 | ppl    49.74\n",
            "| epoch  48 |  2200/ 2983 batches | lr 0.000002 | ms/batch 25.32 | loss  3.80 | ppl    44.53\n",
            "| epoch  48 |  2400/ 2983 batches | lr 0.000002 | ms/batch 25.31 | loss  3.78 | ppl    43.86\n",
            "| epoch  48 |  2600/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  3.78 | ppl    43.72\n",
            "| epoch  48 |  2800/ 2983 batches | lr 0.000002 | ms/batch 25.31 | loss  3.71 | ppl    40.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 77.53s | valid loss  5.19 | valid ppl   178.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000002\n",
            "| epoch  49 |   200/ 2983 batches | lr 0.000002 | ms/batch 25.47 | loss  4.06 | ppl    57.84\n",
            "| epoch  49 |   400/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  4.10 | ppl    60.38\n",
            "| epoch  49 |   600/ 2983 batches | lr 0.000002 | ms/batch 25.34 | loss  3.96 | ppl    52.59\n",
            "| epoch  49 |   800/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  4.01 | ppl    54.89\n",
            "| epoch  49 |  1000/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  4.01 | ppl    55.31\n",
            "| epoch  49 |  1200/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  3.90 | ppl    49.39\n",
            "| epoch  49 |  1400/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  3.93 | ppl    50.80\n",
            "| epoch  49 |  1600/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  4.05 | ppl    57.43\n",
            "| epoch  49 |  1800/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  3.92 | ppl    50.23\n",
            "| epoch  49 |  2000/ 2983 batches | lr 0.000002 | ms/batch 25.31 | loss  3.91 | ppl    50.02\n",
            "| epoch  49 |  2200/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  3.79 | ppl    44.12\n",
            "| epoch  49 |  2400/ 2983 batches | lr 0.000002 | ms/batch 25.28 | loss  3.77 | ppl    43.59\n",
            "| epoch  49 |  2600/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  3.77 | ppl    43.29\n",
            "| epoch  49 |  2800/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  3.70 | ppl    40.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 77.54s | valid loss  5.18 | valid ppl   177.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  50 |   200/ 2983 batches | lr 0.000001 | ms/batch 25.43 | loss  4.09 | ppl    59.49\n",
            "| epoch  50 |   400/ 2983 batches | lr 0.000001 | ms/batch 25.31 | loss  4.16 | ppl    64.19\n",
            "| epoch  50 |   600/ 2983 batches | lr 0.000001 | ms/batch 25.32 | loss  4.04 | ppl    56.69\n",
            "| epoch  50 |   800/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  4.13 | ppl    62.25\n",
            "| epoch  50 |  1000/ 2983 batches | lr 0.000001 | ms/batch 25.32 | loss  4.11 | ppl    61.00\n",
            "| epoch  50 |  1200/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  4.06 | ppl    58.03\n",
            "| epoch  50 |  1400/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  4.05 | ppl    57.63\n",
            "| epoch  50 |  1600/ 2983 batches | lr 0.000001 | ms/batch 25.28 | loss  4.04 | ppl    57.03\n",
            "| epoch  50 |  1800/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  3.89 | ppl    48.78\n",
            "| epoch  50 |  2000/ 2983 batches | lr 0.000001 | ms/batch 25.34 | loss  3.88 | ppl    48.40\n",
            "| epoch  50 |  2200/ 2983 batches | lr 0.000001 | ms/batch 25.32 | loss  3.82 | ppl    45.67\n",
            "| epoch  50 |  2400/ 2983 batches | lr 0.000001 | ms/batch 25.34 | loss  3.84 | ppl    46.74\n",
            "| epoch  50 |  2600/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  3.86 | ppl    47.27\n",
            "| epoch  50 |  2800/ 2983 batches | lr 0.000001 | ms/batch 25.31 | loss  3.82 | ppl    45.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 77.57s | valid loss  5.13 | valid ppl   169.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  51 |   200/ 2983 batches | lr 0.000001 | ms/batch 25.43 | loss  4.05 | ppl    57.61\n",
            "| epoch  51 |   400/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  4.09 | ppl    59.86\n",
            "| epoch  51 |   600/ 2983 batches | lr 0.000001 | ms/batch 25.27 | loss  3.96 | ppl    52.21\n",
            "| epoch  51 |   800/ 2983 batches | lr 0.000001 | ms/batch 25.28 | loss  4.03 | ppl    56.16\n",
            "| epoch  51 |  1000/ 2983 batches | lr 0.000001 | ms/batch 25.27 | loss  4.07 | ppl    58.47\n",
            "| epoch  51 |  1200/ 2983 batches | lr 0.000001 | ms/batch 25.32 | loss  4.00 | ppl    54.56\n",
            "| epoch  51 |  1400/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  3.98 | ppl    53.61\n",
            "| epoch  51 |  1600/ 2983 batches | lr 0.000001 | ms/batch 25.31 | loss  4.01 | ppl    54.99\n",
            "| epoch  51 |  1800/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  3.91 | ppl    49.95\n",
            "| epoch  51 |  2000/ 2983 batches | lr 0.000001 | ms/batch 25.28 | loss  3.93 | ppl    50.89\n",
            "| epoch  51 |  2200/ 2983 batches | lr 0.000001 | ms/batch 25.34 | loss  3.85 | ppl    46.87\n",
            "| epoch  51 |  2400/ 2983 batches | lr 0.000001 | ms/batch 25.34 | loss  3.85 | ppl    46.95\n",
            "| epoch  51 |  2600/ 2983 batches | lr 0.000001 | ms/batch 25.33 | loss  3.85 | ppl    46.89\n",
            "| epoch  51 |  2800/ 2983 batches | lr 0.000001 | ms/batch 25.31 | loss  3.78 | ppl    43.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 77.56s | valid loss  5.15 | valid ppl   172.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  52 |   200/ 2983 batches | lr 0.000001 | ms/batch 25.43 | loss  4.02 | ppl    55.55\n",
            "| epoch  52 |   400/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  4.09 | ppl    59.44\n",
            "| epoch  52 |   600/ 2983 batches | lr 0.000001 | ms/batch 25.28 | loss  3.96 | ppl    52.52\n",
            "| epoch  52 |   800/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  4.04 | ppl    56.83\n",
            "| epoch  52 |  1000/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  4.06 | ppl    58.06\n",
            "| epoch  52 |  1200/ 2983 batches | lr 0.000001 | ms/batch 25.28 | loss  4.00 | ppl    54.53\n",
            "| epoch  52 |  1400/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  3.97 | ppl    53.06\n",
            "| epoch  52 |  1600/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  4.02 | ppl    55.71\n",
            "| epoch  52 |  1800/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  3.92 | ppl    50.20\n",
            "| epoch  52 |  2000/ 2983 batches | lr 0.000001 | ms/batch 25.31 | loss  3.94 | ppl    51.48\n",
            "| epoch  52 |  2200/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  3.85 | ppl    47.10\n",
            "| epoch  52 |  2400/ 2983 batches | lr 0.000001 | ms/batch 25.31 | loss  3.85 | ppl    46.95\n",
            "| epoch  52 |  2600/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  3.84 | ppl    46.45\n",
            "| epoch  52 |  2800/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  3.77 | ppl    43.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 77.53s | valid loss  5.16 | valid ppl   173.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  53 |   200/ 2983 batches | lr 0.000001 | ms/batch 25.42 | loss  4.02 | ppl    55.67\n",
            "| epoch  53 |   400/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  4.09 | ppl    59.82\n",
            "| epoch  53 |   600/ 2983 batches | lr 0.000001 | ms/batch 25.27 | loss  3.97 | ppl    53.00\n",
            "| epoch  53 |   800/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  4.05 | ppl    57.13\n",
            "| epoch  53 |  1000/ 2983 batches | lr 0.000001 | ms/batch 25.27 | loss  4.06 | ppl    58.00\n",
            "| epoch  53 |  1200/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  4.00 | ppl    54.49\n",
            "| epoch  53 |  1400/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  3.97 | ppl    53.03\n",
            "| epoch  53 |  1600/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  4.02 | ppl    55.60\n",
            "| epoch  53 |  1800/ 2983 batches | lr 0.000001 | ms/batch 25.32 | loss  3.92 | ppl    50.55\n",
            "| epoch  53 |  2000/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  3.94 | ppl    51.56\n",
            "| epoch  53 |  2200/ 2983 batches | lr 0.000001 | ms/batch 25.32 | loss  3.84 | ppl    46.68\n",
            "| epoch  53 |  2400/ 2983 batches | lr 0.000001 | ms/batch 25.32 | loss  3.84 | ppl    46.72\n",
            "| epoch  53 |  2600/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  3.84 | ppl    46.54\n",
            "| epoch  53 |  2800/ 2983 batches | lr 0.000001 | ms/batch 25.32 | loss  3.76 | ppl    42.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 77.54s | valid loss  5.16 | valid ppl   174.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  54 |   200/ 2983 batches | lr 0.000000 | ms/batch 25.43 | loss  4.04 | ppl    56.57\n",
            "| epoch  54 |   400/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.13 | ppl    62.31\n",
            "| epoch  54 |   600/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.02 | ppl    55.80\n",
            "| epoch  54 |   800/ 2983 batches | lr 0.000000 | ms/batch 25.27 | loss  4.13 | ppl    61.97\n",
            "| epoch  54 |  1000/ 2983 batches | lr 0.000000 | ms/batch 25.26 | loss  4.13 | ppl    62.40\n",
            "| epoch  54 |  1200/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.08 | ppl    59.00\n",
            "| epoch  54 |  1400/ 2983 batches | lr 0.000000 | ms/batch 25.27 | loss  4.08 | ppl    59.26\n",
            "| epoch  54 |  1600/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.11 | ppl    61.14\n",
            "| epoch  54 |  1800/ 2983 batches | lr 0.000000 | ms/batch 25.28 | loss  3.98 | ppl    53.72\n",
            "| epoch  54 |  2000/ 2983 batches | lr 0.000000 | ms/batch 25.32 | loss  3.94 | ppl    51.65\n",
            "| epoch  54 |  2200/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.84 | ppl    46.50\n",
            "| epoch  54 |  2400/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.83 | ppl    46.23\n",
            "| epoch  54 |  2600/ 2983 batches | lr 0.000000 | ms/batch 25.33 | loss  3.86 | ppl    47.26\n",
            "| epoch  54 |  2800/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.81 | ppl    45.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 77.53s | valid loss  5.12 | valid ppl   167.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  55 |   200/ 2983 batches | lr 0.000000 | ms/batch 25.42 | loss  4.06 | ppl    58.16\n",
            "| epoch  55 |   400/ 2983 batches | lr 0.000000 | ms/batch 25.32 | loss  4.13 | ppl    62.07\n",
            "| epoch  55 |   600/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.00 | ppl    54.82\n",
            "| epoch  55 |   800/ 2983 batches | lr 0.000000 | ms/batch 25.28 | loss  4.09 | ppl    59.93\n",
            "| epoch  55 |  1000/ 2983 batches | lr 0.000000 | ms/batch 25.32 | loss  4.09 | ppl    60.02\n",
            "| epoch  55 |  1200/ 2983 batches | lr 0.000000 | ms/batch 25.28 | loss  4.03 | ppl    56.51\n",
            "| epoch  55 |  1400/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.03 | ppl    56.41\n",
            "| epoch  55 |  1600/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.04 | ppl    56.73\n",
            "| epoch  55 |  1800/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.92 | ppl    50.53\n",
            "| epoch  55 |  2000/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.93 | ppl    50.87\n",
            "| epoch  55 |  2200/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.85 | ppl    46.80\n",
            "| epoch  55 |  2400/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.87 | ppl    48.05\n",
            "| epoch  55 |  2600/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.90 | ppl    49.62\n",
            "| epoch  55 |  2800/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.84 | ppl    46.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 77.54s | valid loss  5.12 | valid ppl   167.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  56 |   200/ 2983 batches | lr 0.000000 | ms/batch 25.44 | loss  4.08 | ppl    59.02\n",
            "| epoch  56 |   400/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.12 | ppl    61.41\n",
            "| epoch  56 |   600/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.98 | ppl    53.32\n",
            "| epoch  56 |   800/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.07 | ppl    58.36\n",
            "| epoch  56 |  1000/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.09 | ppl    59.64\n",
            "| epoch  56 |  1200/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.02 | ppl    55.72\n",
            "| epoch  56 |  1400/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.02 | ppl    55.56\n",
            "| epoch  56 |  1600/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.03 | ppl    56.22\n",
            "| epoch  56 |  1800/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.93 | ppl    51.09\n",
            "| epoch  56 |  2000/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  3.94 | ppl    51.47\n",
            "| epoch  56 |  2200/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.86 | ppl    47.39\n",
            "| epoch  56 |  2400/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  3.89 | ppl    48.72\n",
            "| epoch  56 |  2600/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  3.90 | ppl    49.59\n",
            "| epoch  56 |  2800/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  3.83 | ppl    46.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 77.54s | valid loss  5.12 | valid ppl   167.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  57 |   200/ 2983 batches | lr 0.000000 | ms/batch 25.43 | loss  4.05 | ppl    57.60\n",
            "| epoch  57 |   400/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.10 | ppl    60.51\n",
            "| epoch  57 |   600/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.97 | ppl    52.91\n",
            "| epoch  57 |   800/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.06 | ppl    57.91\n",
            "| epoch  57 |  1000/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.09 | ppl    59.46\n",
            "| epoch  57 |  1200/ 2983 batches | lr 0.000000 | ms/batch 25.28 | loss  4.02 | ppl    55.79\n",
            "| epoch  57 |  1400/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.01 | ppl    55.20\n",
            "| epoch  57 |  1600/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.04 | ppl    56.73\n",
            "| epoch  57 |  1800/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.95 | ppl    52.15\n",
            "| epoch  57 |  2000/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.96 | ppl    52.61\n",
            "| epoch  57 |  2200/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  3.87 | ppl    47.78\n",
            "| epoch  57 |  2400/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.88 | ppl    48.66\n",
            "| epoch  57 |  2600/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  3.90 | ppl    49.40\n",
            "| epoch  57 |  2800/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  3.81 | ppl    45.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 77.52s | valid loss  5.12 | valid ppl   167.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  58 |   200/ 2983 batches | lr 0.000000 | ms/batch 25.40 | loss  4.04 | ppl    56.94\n",
            "| epoch  58 |   400/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.10 | ppl    60.45\n",
            "| epoch  58 |   600/ 2983 batches | lr 0.000000 | ms/batch 25.28 | loss  3.97 | ppl    53.00\n",
            "| epoch  58 |   800/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.06 | ppl    57.91\n",
            "| epoch  58 |  1000/ 2983 batches | lr 0.000000 | ms/batch 25.28 | loss  4.08 | ppl    59.33\n",
            "| epoch  58 |  1200/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.02 | ppl    55.91\n",
            "| epoch  58 |  1400/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.02 | ppl    55.67\n",
            "| epoch  58 |  1600/ 2983 batches | lr 0.000000 | ms/batch 25.28 | loss  4.04 | ppl    56.66\n",
            "| epoch  58 |  1800/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  3.94 | ppl    51.52\n",
            "| epoch  58 |  2000/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  3.96 | ppl    52.59\n",
            "| epoch  58 |  2200/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.86 | ppl    47.45\n",
            "| epoch  58 |  2400/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.88 | ppl    48.41\n",
            "| epoch  58 |  2600/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.89 | ppl    48.96\n",
            "| epoch  58 |  2800/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.81 | ppl    45.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 77.51s | valid loss  5.12 | valid ppl   167.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  59 |   200/ 2983 batches | lr 0.000000 | ms/batch 25.43 | loss  4.05 | ppl    57.40\n",
            "| epoch  59 |   400/ 2983 batches | lr 0.000000 | ms/batch 25.28 | loss  4.10 | ppl    60.44\n",
            "| epoch  59 |   600/ 2983 batches | lr 0.000000 | ms/batch 25.27 | loss  4.00 | ppl    54.36\n",
            "| epoch  59 |   800/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.11 | ppl    61.01\n",
            "| epoch  59 |  1000/ 2983 batches | lr 0.000000 | ms/batch 25.28 | loss  4.12 | ppl    61.67\n",
            "| epoch  59 |  1200/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.07 | ppl    58.34\n",
            "| epoch  59 |  1400/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.08 | ppl    58.98\n",
            "| epoch  59 |  1600/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.11 | ppl    60.83\n",
            "| epoch  59 |  1800/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.01 | ppl    54.95\n",
            "| epoch  59 |  2000/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.99 | ppl    54.19\n",
            "| epoch  59 |  2200/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  3.87 | ppl    47.85\n",
            "| epoch  59 |  2400/ 2983 batches | lr 0.000000 | ms/batch 25.28 | loss  3.87 | ppl    47.94\n",
            "| epoch  59 |  2600/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.88 | ppl    48.31\n",
            "| epoch  59 |  2800/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.81 | ppl    45.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 77.52s | valid loss  5.12 | valid ppl   166.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  60 |   200/ 2983 batches | lr 0.000000 | ms/batch 25.41 | loss  4.06 | ppl    58.15\n",
            "| epoch  60 |   400/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.12 | ppl    61.56\n",
            "| epoch  60 |   600/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.00 | ppl    54.75\n",
            "| epoch  60 |   800/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.11 | ppl    60.69\n",
            "| epoch  60 |  1000/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.11 | ppl    60.96\n",
            "| epoch  60 |  1200/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.05 | ppl    57.58\n",
            "| epoch  60 |  1400/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.06 | ppl    57.83\n",
            "| epoch  60 |  1600/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.09 | ppl    59.79\n",
            "| epoch  60 |  1800/ 2983 batches | lr 0.000000 | ms/batch 25.34 | loss  3.99 | ppl    54.06\n",
            "| epoch  60 |  2000/ 2983 batches | lr 0.000000 | ms/batch 25.38 | loss  3.97 | ppl    52.95\n",
            "| epoch  60 |  2200/ 2983 batches | lr 0.000000 | ms/batch 25.32 | loss  3.86 | ppl    47.40\n",
            "| epoch  60 |  2400/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.87 | ppl    47.75\n",
            "| epoch  60 |  2600/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.90 | ppl    49.23\n",
            "| epoch  60 |  2800/ 2983 batches | lr 0.000000 | ms/batch 25.34 | loss  3.83 | ppl    46.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 77.58s | valid loss  5.11 | valid ppl   166.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "=========================================================================================\n",
            "| End of training | test loss  5.05 | test ppl   155.26\n",
            "=========================================================================================\n",
            "\n",
            "Generating text...\n",
            "Vocabulary size: 33278\n",
            "Vocabulary size: 33278\n",
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n",
            "to do not incorporate , there was a little written gate in the top seven episode . It usually included\n",
            "as Main 's most direct personalities of My America , but it remained often survives . During the 1970s .\n",
            "Tourism from the 9th century were a leap travel from the music between the Moon , billed as several \"\n",
            "emerging \" and \" <unk> \" <unk> , the historical pressure of the center is of between the modern 80s\n",
            "tours from the ship . Immediately after the race , the University of Southern Bay generated an impressive field of\n",
            "workers in Europe and some of the national   Technical <unk> Well , a long race with its fifth depth\n",
            "rear rate Metro , and how the Bill 's brother are this running past the entry of the Mississippi River\n",
            "which reduces a more height . They respected the music distribution where Douglas was intentionally also successful on the PlayStation\n",
            ", Earth deriving to strips , posters and oak . Moored buoys begin , further in the <unk>   <unk>\n",
            "row and blue liquids , similar to moving ahead of these research that utilizes the event and would interfere with\n",
            "other stages of the game . <eos> Hoover counties that finished the tracks with several <unk> passes , featuring the\n",
            "removal of the kakapo to sell the sequel a large track . <eos> There is a low  approximately three\n",
            "24   foot ( 15 m ) stucco system and with 7   inch ( 6   3 for \n",
            "2   9 m2 ) gross water . The midrange was 1   3 billion and an 8   2\n",
            "numerous tons ( 7   0 kg ) number 110 ( 1   2 lb ) per year , and\n",
            "slightly 59 kg ( 250 cm ) long . The most common female supply was darker , while the background\n",
            "appeared again , but these was the open   back airport temperature to up a traverse to match the heightened\n",
            "ocean view was branched . The rivalry was lost at Ceres in 2013 , and they again absorbed the Stanley\n",
            "Park Forest , though it was discovered in the 5000 National 3 / 1 L / <unk> : Motor Directing\n",
            "( 2015 ) for the Leckwith <unk> 's <unk> toy . <eos> <eos> = = = Armament = = =\n",
            "<eos> <eos> The engine also replaced the outer grout power located within their own <unk> . A second male Fish\n",
            "'s barbarian for a diet that includes the Rockefeller represents the factory , and later underwent the <unk> . This\n",
            "featured an active percentage of 20   000 to 10   000 people , a day home although most of\n",
            "the passengers were mobilized partly with its previous lack and stay them across it inside approximately 10 to 50 tenths\n",
            ". Accordingly , the tour still increased a earlier series of 1897 competition before teams have been mounting the name\n",
            "double <unk> . Construction represented by flash computing regulation and fixed <unk> became the longest   held total of 3\n",
            "% . Tech 's architectural advantage was Montagne as a state for six years earlier , but they introduced it\n",
            "in baseball . Another version that lasted for several years , was now held in <unk> by the A Easter\n",
            "<unk> since the late 1970s , but would briefly be green . Many of the continued <unk> , titled \"\n",
            "Songs 's ball the <unk> \" , allows it from an attraction that would be occasionally perfect at the entrance\n",
            "to the cargo people . The <unk> Millennium officially revised today glass ballet , producing radiation leg developer William Flynn\n",
            ", whose father <unk> , Viscount Fusiliers , Sri <unk> , Martin Ray Cunningham , and Shane <unk> <unk> ,\n",
            "who had previously decided to look for charge over events , and rather than assured use of the nature of\n",
            "the inspiration and risked Byung   <unk> followed with direct offenders . According to a revival known as the \"\n",
            "<unk> \" boat , Watkins admitted , \" Ten songs take precedence for the party when the <unk> , or\n",
            "experience to <unk> <unk> you inflict support we could in a standing fan . He finds me out . \"\n",
            "<eos> He was commissioned into the benefit problems of this highly active frequency \" , a contest on red music\n",
            "and a short black memory genre . \" The video was defended by Don Fei and Paul <unk> by <unk>\n",
            "The Dawn Barbara <unk> as a <unk> . <eos> <eos> = = Personal popular literature = = <eos> <eos> Barbarian\n",
            "II is a simple review in New York and instead resulted in three complaints as the third highest day ,\n",
            "a record of three years , a team called remained with a Journal of <unk> , 1   2 and\n",
            "15 % share , using a single seasonal range of blue <unk> and the white bar , a projecting <unk>\n",
            "factory . It also is the expansion of the falsetto object , a three   70 piece with negative rivals\n",
            "tips or each time . <eos> <eos> = = World War II = = <eos> <eos> <unk> since the 15th\n",
            "century , the Tom Salmon Authority Company was discovered in 1985 in the 1980s for the President . The group\n",
            "unveiled 33   8 % of deaths for the shares during the event 's conclusion . Most research applications have\n",
            "made the largest element of the people and sold the James Ball Award , which contributed to the <unk> Register\n",
            "in modern history . The understanding of economic submarines from the Jewish Civil War would Sosa as to destroy the\n",
            "titles of the invasion . Numerous important charitable sites in their new site has been exercised by the British government\n",
            ". In April 1947 , the British Ministry of Mogadiscio was given for two nations : the <unk> Range of\n",
            "\n",
            "Training on custom names dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Transformer on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type='Transformer',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize=512,\n",
        "    nhid=2048,\n",
        "    nlayers=6,\n",
        "    epochs=60,\n",
        "    lr=0.0003,\n",
        "    batch_size = 32,\n",
        "    bptt = 50,\n",
        "    dropout = 0.1,\n",
        "    nhead = 8,\n",
        "    save_path='model_1.pt',\n",
        "    use_optimizer = True,\n",
        "    optimizer_type = 'AdamW',\n",
        "    weight_decay=1e-4,\n",
        "    use_betas=True,\n",
        "    betas=(0.9, 0.98),\n",
        "    use_eps=True,\n",
        "    eps=1e-9,\n",
        "    criterion = nn.NLLLoss(),\n",
        "    use_label_smoothing = False,\n",
        "    use_warmup = False,\n",
        "    seed = 1111,\n",
        "    old_version = True\n",
        ")\n",
        "\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model_1.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    words=1000,\n",
        "    temperature=1.0,\n",
        "    seed=1111,\n",
        "    old_version=True,\n",
        "    use_top_k=False,\n",
        "    accel = True\n",
        ")"
      ],
      "metadata": {
        "id": "Ay8aCfM-YMIw"
      },
      "id": "Ay8aCfM-YMIw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Transformer on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type='Transformer',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize=512,\n",
        "    nhid=2048,\n",
        "    nlayers=6,\n",
        "    lr=0.0001,\n",
        "    clip=0.25,\n",
        "    epochs=60,\n",
        "    batch_size=32,\n",
        "    bptt=50,\n",
        "    dropout=0.1,\n",
        "    tied=False,\n",
        "    nhead=8,\n",
        "    log_interval=200,\n",
        "    save_path='model_2.pt',\n",
        "    dry_run=False,\n",
        "    accel=True,\n",
        "    use_optimizer=True,\n",
        "    optimizer_type='AdamW',\n",
        "    weight_decay=1e-4,\n",
        "    use_betas=True,\n",
        "    betas=(0.9, 0.98),\n",
        "    use_eps=True,\n",
        "    eps=1e-9,\n",
        "    criterion=None,\n",
        "    use_label_smoothing=True,\n",
        "    label_smoothing=0.1,\n",
        "    use_warmup=True,\n",
        "    warmup_steps=4000,\n",
        "    min_freq=5,\n",
        "    seed=1111,\n",
        "    old_version=False\n",
        "    # batch_size=64,  #     \n",
        "    # bptt=64,     #   \n",
        ")\n",
        "\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model_2.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf='generated_2.txt',\n",
        "    words=1000,\n",
        "    temperature=1.0,\n",
        "    top_k=40,\n",
        "    seed=1111,\n",
        "    log_interval=100,\n",
        "    use_top_k=True,  # Use top-k sampling\n",
        "    min_freq=5\n",
        ")\n",
        "\n",
        "!cat generated_2.txt"
      ],
      "metadata": {
        "id": "rKoNQPO4jYZB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "81aec876-131b-4be3-a29c-a8cdbda3a93c"
      },
      "id": "rKoNQPO4jYZB",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Transformer on WikiText-2...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AcceleratorError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-282939656.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Transformer on WikiText-2...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_model(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Transformer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/data_word_train/wikitext-2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0memsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-184860079.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_type, data_path, emsize, nhid, nlayers, lr, clip, epochs, batch_size, bptt, dropout, tied, nhead, log_interval, save_path, onnx_export, dry_run, accel, use_optimizer, optimizer_type, weight_decay, use_betas, betas, use_eps, eps, criterion, use_label_smoothing, label_smoothing, use_warmup, warmup_steps, min_freq, seed, old_version)\u001b[0m\n\u001b[1;32m    190\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Wikitext-2 dataset moved to Google Drive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;31m# Set device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    927\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_callback_from_stance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                     \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/random.py\u001b[0m in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_in_bad_fork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_initialization_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mcb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mdefault_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_generators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# DEBUG SETUP\n",
        "# ===============================\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # Synchronous CUDA errors\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'    # Device-side asserts\n",
        "print(\"Training Transformer on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type='Transformer',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize=512,\n",
        "    nhid=2048,\n",
        "    nlayers=6,\n",
        "    lr=0.0001,\n",
        "    clip=0.25,\n",
        "    epochs=1,\n",
        "    batch_size=32,\n",
        "    bptt=50,\n",
        "    dropout=0.1,\n",
        "    tied=False,\n",
        "    nhead=8,\n",
        "    log_interval=200,\n",
        "    save_path='model_3.pt',\n",
        "    dry_run=False,\n",
        "    accel=True,\n",
        "    use_optimizer=True,\n",
        "    optimizer_type='AdamW',\n",
        "    weight_decay=1e-4,\n",
        "    use_betas=True,\n",
        "    betas=(0.9, 0.98),\n",
        "    use_eps=True,\n",
        "    eps=1e-9,\n",
        "    criterion=None,\n",
        "    use_label_smoothing=True,\n",
        "    label_smoothing=0.1,\n",
        "    use_warmup=True,\n",
        "    warmup_steps=4000,\n",
        "    min_freq=5,\n",
        "    seed=1111,\n",
        "    old_version=False\n",
        "    # batch_size=64,  #     \n",
        "    # bptt=64,     #   \n",
        ")\n",
        "\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model_3.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf='generated_3.txt',\n",
        "    words=1000,\n",
        "    temperature=1.0,\n",
        "    top_k=40,\n",
        "    seed=1111,\n",
        "    log_interval=100,\n",
        "    use_top_k=True,  # Use top-k sampling\n",
        "    min_freq=5,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "!cat generated_2.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "BS8m8883i7E2",
        "outputId": "48951331-75c9-4115-a112-cc2af02f8eb1"
      },
      "id": "BS8m8883i7E2",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AcceleratorError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3143978272.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# os.environ['TORCH_USE_CUDA_DSA'] = '1'    # Device-side asserts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# print(\"Training Transformer on WikiText-2...\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m train_model(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Transformer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/data_word_train/wikitext-2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-184860079.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_type, data_path, emsize, nhid, nlayers, lr, clip, epochs, batch_size, bptt, dropout, tied, nhead, log_interval, save_path, onnx_export, dry_run, accel, use_optimizer, optimizer_type, weight_decay, use_betas, betas, use_eps, eps, criterion, use_label_smoothing, label_smoothing, use_warmup, warmup_steps, min_freq, seed, old_version)\u001b[0m\n\u001b[1;32m    190\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Wikitext-2 dataset moved to Google Drive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;31m# Set device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    927\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_callback_from_stance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                     \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/random.py\u001b[0m in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_in_bad_fork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_initialization_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/random.py\u001b[0m in \u001b[0;36mcb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mdefault_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_generators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mdefault_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0m_lazy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MAIN PART"
      ],
      "metadata": {
        "id": "BSnSaA8FXKGj"
      },
      "id": "BSnSaA8FXKGj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Data Preparation for Custom Data (English-French dataset as example)\n",
        "# Extract French sentences and create train/valid/test.txt\n",
        "DATA_PATH = '/content/drive/MyDrive/data_word_train/custom'\n",
        "INPUT_FILE = '/content/drive/MyDrive/data/eng-fra.txt'\n",
        "os.makedirs(DATA_PATH, exist_ok=True)\n",
        "random.seed(1111)\n",
        "\n",
        "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "french_sentences = [line.strip().split('\\t')[1] for line in lines if len(line.strip().split('\\t')) == 2]\n",
        "\n",
        "random.shuffle(french_sentences)\n",
        "n = len(french_sentences)\n",
        "train_end = int(n * 0.8)\n",
        "valid_end = train_end + int(n * 0.1)\n",
        "train_data = french_sentences[:train_end]\n",
        "valid_data = french_sentences[train_end:valid_end]\n",
        "test_data = french_sentences[valid_end:]\n",
        "\n",
        "def save_sentences(sentences, filename):\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.replace('.', ' .').replace(',', ' ,').replace('!', ' !').replace('?', ' ?')\n",
        "            f.write(sentence + ' <eos>\\n')\n",
        "\n",
        "save_sentences(train_data, os.path.join(DATA_PATH, 'train.txt'))\n",
        "save_sentences(valid_data, os.path.join(DATA_PATH, 'valid.txt'))\n",
        "save_sentences(test_data, os.path.join(DATA_PATH, 'test.txt'))\n",
        "\n",
        "print(f\"Created datasets: {len(train_data)} train, {len(valid_data)} valid, {len(test_data)} test sentences\")\n",
        "\n",
        "# Step 5: Training Function from main.py (adapted for Colab)\n",
        "# Define args as a class for Colab\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.data = '/content/drive/MyDrive/data_word_train/custom'  # Custom data path\n",
        "        self.model = 'LSTM'  # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "        self.emsize = 200\n",
        "        self.nhid = 200\n",
        "        self.nlayers = 2\n",
        "        self.lr = 0.001\n",
        "        self.clip = 0.25\n",
        "        self.epochs = 20  # Reduced for faster training\n",
        "        self.batch_size = 20\n",
        "        self.bptt = 35\n",
        "        self.dropout = 0.2\n",
        "        self.tied = False\n",
        "        self.seed = 1111\n",
        "        self.log_interval = 200\n",
        "        self.save = 'model.pt'\n",
        "        self.onnx_export = ''\n",
        "        self.nhead = 2\n",
        "        self.dry_run = False\n",
        "        self.accel = True\n",
        "        self.use_optimizer = True  # Use AdamW\n",
        "\n",
        "args = Args()\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "if args.accel and torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "corpus = Corpus(args.data)\n",
        "\n",
        "# Starting from sequential data, batchify arranges the dataset into columns.\n",
        "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
        "#  a g m s \n",
        "#  b h n t \n",
        "#  c i o u \n",
        "#  d j p v \n",
        "#  e k q w \n",
        "#  f l r x .\n",
        "# These columns are treated as independent by the model, which means that the\n",
        "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
        "# batch processing.\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    nbatch = data.size(0) // bsz\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(corpus.train, args.batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "\n",
        "ntokens = len(corpus.dictionary)\n",
        "if args.model == 'Transformer':\n",
        "    model = TransformerModel(ntokens, args.emsize, args.nhead, args.nhid, args.nlayers, args.dropout).to(device)\n",
        "else:\n",
        "    model = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied).to(device)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "if args.use_optimizer:\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
        "\n",
        "###############################################################################\n",
        "# Training code\n",
        "###############################################################################\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "# get_batch subdivides the source data into chunks of length args.bptt.\n",
        "# If source is equal to the example output of the batchify function, with\n",
        "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
        "#  a g m s   b h n t \n",
        "#  b h n t   c i o u \n",
        "# Note that despite the name of the function, the subdivison of data is not\n",
        "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
        "# by the batchify function. The chunks are along dimension 0, corresponding\n",
        "# to the seq_len dimension in the LSTM.\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "def evaluate(data_source):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if args.model != 'Transformer':\n",
        "        hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, args.bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            if args.model == 'Transformer':\n",
        "                output = model(data)\n",
        "                output = output.view(-1, ntokens)\n",
        "            else:\n",
        "                output, hidden = model(data, hidden)\n",
        "                hidden = repackage_hidden(hidden)\n",
        "            total_loss += len(data) * criterion(output, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "def train_func():\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if args.model != 'Transformer':\n",
        "        hidden = model.init_hidden(args.batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        optimizer.zero_grad() if args.use_optimizer else model.zero_grad()\n",
        "        if args.model == 'Transformer':\n",
        "            output = model(data)\n",
        "            output = output.view(-1, ntokens)\n",
        "        else:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            output, hidden = model(data, hidden)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
        "        if args.use_optimizer:\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            for p in model.parameters():\n",
        "                p.data.add_(p.grad, alpha=-lr)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % args.log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / args.log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // args.bptt, lr,\n",
        "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "        if args.dry_run:\n",
        "            break\n",
        "\n",
        "lr = args.lr\n",
        "best_val_loss = None\n",
        "\n",
        "try:\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        train_func()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                         val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(args.save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(args.save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)\n",
        "\n",
        "# Step 6: Generate Text from generate.py (adapted for Colab)\n",
        "checkpoint = 'model.pt'  # Your saved model\n",
        "outf = 'generated.txt'\n",
        "words = 1000\n",
        "temperature = 1.0\n",
        "log_interval = 100\n",
        "accel = True  # Use CUDA\n",
        "\n",
        "torch.manual_seed(1111)\n",
        "\n",
        "if accel and torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "with open(checkpoint, 'rb') as f:\n",
        "    model = torch.load(f, map_location=device)\n",
        "model.eval()\n",
        "\n",
        "corpus = Corpus(args.data)\n",
        "ntokens = len(corpus.dictionary)\n",
        "\n",
        "is_transformer_model = hasattr(model, 'model_type') and model.model_type == 'Transformer'\n",
        "if not is_transformer_model:\n",
        "    hidden = model.init_hidden(1)\n",
        "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "with open(outf, 'w') as outf:\n",
        "    with torch.no_grad():\n",
        "        for i in range(words):\n",
        "            if is_transformer_model:\n",
        "                output = model(input, False)\n",
        "                word_weights = output[-1].squeeze().div(temperature).exp().cpu()\n",
        "                word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "                word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
        "                input = torch.cat([input, word_tensor], 0)\n",
        "            else:\n",
        "                output, hidden = model(input, hidden)\n",
        "                word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "                word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "                input.fill_(word_idx)\n",
        "\n",
        "            word = corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "            if i % log_interval == 0:\n",
        "                print('| Generated {}/{} words'.format(i, words))\n",
        "\n",
        "# Print generated text\n",
        "!head -n 20 generated.txt"
      ],
      "metadata": {
        "id": "cmr9R-Z5XIPa"
      },
      "id": "cmr9R-Z5XIPa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Names dataset learn"
      ],
      "metadata": {
        "id": "eteX5K-YGnxt"
      },
      "id": "eteX5K-YGnxt"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Improved Word-Level Language Modeling with Full Type Hints\n",
        "Based on PyTorch RNN/Transformer example with enhancements\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "from io import open\n",
        "from typing import List, Dict, Tuple, Optional, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# ===============================\n",
        "# DATA PREPARATION (data.py)\n",
        "# ===============================\n",
        "\n",
        "class Dictionary:\n",
        "    \"\"\"Dictionary for word-to-index mapping.\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.word2idx: Dict[str, int] = {}\n",
        "        self.idx2word: List[str] = []\n",
        "\n",
        "    def add_word(self, word: str) -> int:\n",
        "        \"\"\"Add a word to the dictionary.\"\"\"\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus:\n",
        "    \"\"\"Corpus class for loading and tokenizing text data.\"\"\"\n",
        "\n",
        "    def __init__(self, path: str) -> None:\n",
        "        self.dictionary: Dictionary = Dictionary()\n",
        "        self.train: torch.Tensor = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid: torch.Tensor = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test: torch.Tensor = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path: str) -> torch.Tensor:\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path), f\"Path {path} does not exist\"\n",
        "\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words: List[str] = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss: List[torch.Tensor] = []\n",
        "            for line in f:\n",
        "                words: List[str] = line.split() + ['<eos>']\n",
        "                ids: List[int] = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids, dtype=torch.int64))\n",
        "            ids_tensor: torch.Tensor = torch.cat(idss)\n",
        "\n",
        "        return ids_tensor\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# MODEL ARCHITECTURES (model.py)\n",
        "# ===============================\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Positional encoding for Transformer.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000) -> None:\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout: nn.Dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe: torch.Tensor = torch.zeros(max_len, d_model)\n",
        "        position: torch.Tensor = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term: torch.Tensor = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    \"\"\"RNN-based language model (LSTM/GRU/RNN).\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        rnn_type: str,\n",
        "        ntoken: int,\n",
        "        ninp: int,\n",
        "        nhid: int,\n",
        "        nlayers: int,\n",
        "        dropout: float = 0.5,\n",
        "        tie_weights: bool = False\n",
        "    ) -> None:\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.ntoken: int = ntoken\n",
        "        self.rnn_type: str = rnn_type\n",
        "        self.nhid: int = nhid\n",
        "        self.nlayers: int = nlayers\n",
        "\n",
        "        self.drop: nn.Dropout = nn.Dropout(dropout)\n",
        "        self.encoder: nn.Embedding = nn.Embedding(ntoken, ninp)\n",
        "\n",
        "        if rnn_type in ['LSTM', 'GRU']:\n",
        "            self.rnn: nn.Module = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
        "        else:\n",
        "            try:\n",
        "                nonlinearity: str = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
        "            except KeyError as e:\n",
        "                raise ValueError(\n",
        "                    \"Invalid option for `--model`. \"\n",
        "                    \"Options are ['LSTM', 'GRU', 'RNN_TANH', 'RNN_RELU']\"\n",
        "                ) from e\n",
        "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
        "\n",
        "        self.decoder: nn.Linear = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        # Tie weights\n",
        "        if tie_weights:\n",
        "            if nhid != ninp:\n",
        "                raise ValueError('When using tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        \"\"\"Initialize weights.\"\"\"\n",
        "        initrange: float = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, input: torch.Tensor, hidden: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        emb: torch.Tensor = self.drop(self.encoder(input))\n",
        "        output: torch.Tensor\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded: torch.Tensor = self.decoder(output)\n",
        "        decoded = decoded.view(-1, self.ntoken)\n",
        "        return F.log_softmax(decoded, dim=1), hidden\n",
        "\n",
        "    def init_hidden(self, bsz: int) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\"Initialize hidden state.\"\"\"\n",
        "        weight: torch.Tensor = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (\n",
        "                weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
        "            )\n",
        "        else:\n",
        "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Transformer):\n",
        "    \"\"\"Transformer-based language model.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        ntoken: int,\n",
        "        ninp: int,\n",
        "        nhead: int,\n",
        "        nhid: int,\n",
        "        nlayers: int,\n",
        "        dropout: float = 0.5\n",
        "    ) -> None:\n",
        "        super(TransformerModel, self).__init__(\n",
        "            d_model=ninp,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=nhid,\n",
        "            num_encoder_layers=nlayers\n",
        "        )\n",
        "        self.model_type: str = 'Transformer'\n",
        "        self.src_mask: Optional[torch.Tensor] = None\n",
        "        self.pos_encoder: PositionalEncoding = PositionalEncoding(ninp, dropout)\n",
        "\n",
        "        self.input_emb: nn.Embedding = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp: int = ninp\n",
        "        self.decoder: nn.Linear = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz: int) -> torch.Tensor:\n",
        "        \"\"\"Generate mask for causal attention.\"\"\"\n",
        "        return torch.log(torch.tril(torch.ones(sz, sz)))\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        \"\"\"Initialize weights.\"\"\"\n",
        "        initrange: float = 0.1\n",
        "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, src: torch.Tensor, has_mask: bool = True) -> torch.Tensor:\n",
        "        if has_mask:\n",
        "            device: torch.device = src.device\n",
        "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "                mask: torch.Tensor = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "                self.src_mask = mask\n",
        "        else:\n",
        "            self.src_mask = None\n",
        "\n",
        "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output: torch.Tensor = self.encoder(src, mask=self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return F.log_softmax(output, dim=-1)\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# TRAINING & EVALUATION (main.py)\n",
        "# ===============================\n",
        "\n",
        "def batchify(data: torch.Tensor, bsz: int, device: torch.device) -> torch.Tensor:\n",
        "    \"\"\"Divide data into batches.\"\"\"\n",
        "    nbatch: int = data.size(0) // bsz\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "\n",
        "def get_batch(source: torch.Tensor, i: int, bptt: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Get a batch of data.\"\"\"\n",
        "    seq_len: int = min(bptt, len(source) - 1 - i)\n",
        "    data: torch.Tensor = source[i:i+seq_len]\n",
        "    target: torch.Tensor = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def repackage_hidden(h: Union[torch.Tensor, Tuple]) -> Union[torch.Tensor, Tuple]:\n",
        "    \"\"\"Detach hidden state from history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    data_source: torch.Tensor,\n",
        "    criterion: nn.Module,\n",
        "    bptt: int,\n",
        "    ntokens: int,\n",
        "    eval_batch_size: int,\n",
        "    is_transformer: bool\n",
        ") -> float:\n",
        "    \"\"\"Evaluate the model.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss: float = 0.0\n",
        "    if not is_transformer:\n",
        "        hidden = model.init_hidden(eval_batch_size)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data: torch.Tensor\n",
        "            targets: torch.Tensor\n",
        "            data, targets = get_batch(data_source, i, bptt)\n",
        "\n",
        "            if is_transformer:\n",
        "                output: torch.Tensor = model(data)\n",
        "                output = output.view(-1, ntokens)\n",
        "            else:\n",
        "                output, hidden = model(data, hidden)\n",
        "                hidden = repackage_hidden(hidden)\n",
        "\n",
        "            total_loss += len(data) * criterion(output, targets).item()\n",
        "\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train_epoch(\n",
        "    model: nn.Module,\n",
        "    train_data: torch.Tensor,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    epoch: int,\n",
        "    bptt: int,\n",
        "    ntokens: int,\n",
        "    batch_size: int,\n",
        "    clip: float,\n",
        "    log_interval: int,\n",
        "    is_transformer: bool\n",
        ") -> None:\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss: float = 0.0\n",
        "    start_time: float = time.time()\n",
        "\n",
        "    if not is_transformer:\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data: torch.Tensor\n",
        "        targets: torch.Tensor\n",
        "        data, targets = get_batch(train_data, i, bptt)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if is_transformer:\n",
        "            output: torch.Tensor = model(data)\n",
        "            output = output.view(-1, ntokens)\n",
        "        else:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            output, hidden = model(data, hidden)\n",
        "\n",
        "        loss: torch.Tensor = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss: float = total_loss / log_interval\n",
        "            elapsed: float = time.time() - start_time\n",
        "            print(\n",
        "                f'| epoch {epoch:3d} | {batch:5d}/{len(train_data) // bptt:5d} batches | '\n",
        "                f'ms/batch {elapsed * 1000 / log_interval:5.2f} | '\n",
        "                f'loss {cur_loss:5.2f} | ppl {math.exp(cur_loss):8.2f}'\n",
        "            )\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model_type: str = 'LSTM',\n",
        "    data_path: str = './data/wikitext-2',\n",
        "    emsize: int = 200,\n",
        "    nhid: int = 200,\n",
        "    nlayers: int = 2,\n",
        "    lr: float = 0.001,\n",
        "    clip: float = 0.25,\n",
        "    epochs: int = 40,\n",
        "    batch_size: int = 20,\n",
        "    bptt: int = 35,\n",
        "    dropout: float = 0.2,\n",
        "    tied: bool = False,\n",
        "    nhead: int = 2,\n",
        "    log_interval: int = 200,\n",
        "    save_path: str = 'model.pt',\n",
        "    use_cuda: bool = True\n",
        ") -> None:\n",
        "    \"\"\"Main training function.\"\"\"\n",
        "\n",
        "    # Set device\n",
        "    device: torch.device = torch.device('cuda' if use_cuda and torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load data\n",
        "    corpus: Corpus = Corpus(data_path)\n",
        "\n",
        "    eval_batch_size: int = 10\n",
        "    train_data: torch.Tensor = batchify(corpus.train, batch_size, device)\n",
        "    val_data: torch.Tensor = batchify(corpus.valid, eval_batch_size, device)\n",
        "    test_data: torch.Tensor = batchify(corpus.test, eval_batch_size, device)\n",
        "\n",
        "    # Build model\n",
        "    ntokens: int = len(corpus.dictionary)\n",
        "    is_transformer: bool = model_type == 'Transformer'\n",
        "\n",
        "    if is_transformer:\n",
        "        model: nn.Module = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
        "    else:\n",
        "        model = RNNModel(model_type, ntokens, emsize, nhid, nlayers, dropout, tied).to(device)\n",
        "\n",
        "    # Loss and optimizer (Adam with weight_decay as in Transformer paper)\n",
        "    criterion: nn.NLLLoss = nn.NLLLoss()\n",
        "    optimizer: optim.Adam = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    scheduler: optim.lr_scheduler.ReduceLROnPlateau = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=2\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss: Optional[float] = None\n",
        "\n",
        "    try:\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            epoch_start_time: float = time.time()\n",
        "\n",
        "            train_epoch(\n",
        "                model, train_data, criterion, optimizer, epoch,\n",
        "                bptt, ntokens, batch_size, clip, log_interval, is_transformer\n",
        "            )\n",
        "\n",
        "            val_loss: float = evaluate(\n",
        "                model, val_data, criterion, bptt, ntokens,\n",
        "                eval_batch_size, is_transformer\n",
        "            )\n",
        "\n",
        "            print('-' * 89)\n",
        "            print(\n",
        "                f'| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s | '\n",
        "                f'valid loss {val_loss:5.2f} | valid ppl {math.exp(val_loss):8.2f}'\n",
        "            )\n",
        "            print('-' * 89)\n",
        "\n",
        "            # Save best model\n",
        "            if not best_val_loss or val_loss < best_val_loss:\n",
        "                with open(save_path, 'wb') as f:\n",
        "                    torch.save(model, f)\n",
        "                best_val_loss = val_loss\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            scheduler.step(val_loss)\n",
        "            print(f\"Current learning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('-' * 89)\n",
        "        print('Exiting from training early')\n",
        "\n",
        "    # Load best model and test\n",
        "    with open(save_path, 'rb') as f:\n",
        "        model = torch.load(f, map_location=device)\n",
        "\n",
        "    test_loss: float = evaluate(\n",
        "        model, test_data, criterion, bptt, ntokens,\n",
        "        eval_batch_size, is_transformer\n",
        "    )\n",
        "\n",
        "    print('=' * 89)\n",
        "    print(\n",
        "        f'| End of training | test loss {test_loss:5.2f} | '\n",
        "        f'test ppl {math.exp(test_loss):8.2f}'\n",
        "    )\n",
        "    print('=' * 89)\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# TEXT GENERATION (generate.py)\n",
        "# ===============================\n",
        "\n",
        "def generate_text(\n",
        "    checkpoint: str = 'model.pt',\n",
        "    data_path: str = './data/wikitext-2',\n",
        "    outf: str = 'generated.txt',\n",
        "    words: int = 1000,\n",
        "    temperature: float = 1.0,\n",
        "    seed: int = 1111,\n",
        "    log_interval: int = 100,\n",
        "    use_cuda: bool = True\n",
        ") -> None:\n",
        "    \"\"\"Generate text from trained model.\"\"\"\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    device: torch.device = torch.device('cuda' if use_cuda and torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Load model\n",
        "    with open(checkpoint, 'rb') as f:\n",
        "        model: nn.Module = torch.load(f, map_location=device)\n",
        "    model.eval()\n",
        "\n",
        "    # Load corpus\n",
        "    corpus: Corpus = Corpus(data_path)\n",
        "    ntokens: int = len(corpus.dictionary)\n",
        "\n",
        "    is_transformer: bool = hasattr(model, 'model_type') and model.model_type == 'Transformer'\n",
        "    if not is_transformer:\n",
        "        hidden = model.init_hidden(1)\n",
        "\n",
        "    input: torch.Tensor = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "    with open(outf, 'w') as outfile:\n",
        "        with torch.no_grad():\n",
        "            for i in range(words):\n",
        "                if is_transformer:\n",
        "                    output: torch.Tensor = model(input, False)\n",
        "                    word_weights: torch.Tensor = output[-1].squeeze().div(temperature).exp().cpu()\n",
        "                    word_idx: int = torch.multinomial(word_weights, 1)[0].item()\n",
        "                    word_tensor: torch.Tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
        "                    input = torch.cat([input, word_tensor], 0)\n",
        "                else:\n",
        "                    output, hidden = model(input, hidden)\n",
        "                    word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "                    word_idx = torch.multinomial(word_weights, 1)[0].item()\n",
        "                    input.fill_(word_idx)\n",
        "\n",
        "                word: str = corpus.dictionary.idx2word[word_idx]\n",
        "                outfile.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "                if i % log_interval == 0:\n",
        "                    print(f'| Generated {i}/{words} words')\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# EXAMPLE USAGE\n",
        "# ===============================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Example 1: Train on WikiText-2\n",
        "    print(\"Training LSTM on WikiText-2...\")\n",
        "    train_model(\n",
        "        model_type='LSTM',\n",
        "        data_path='./data/wikitext-2',\n",
        "        emsize=400,\n",
        "        nhid=400,\n",
        "        nlayers=4,\n",
        "        epochs=40,\n",
        "        lr=0.001\n",
        "    )\n",
        "\n",
        "    # Example 2: Generate text\n",
        "    print(\"\\nGenerating text...\")\n",
        "    generate_text(\n",
        "        checkpoint='model.pt',\n",
        "        data_path='./data/wikitext-2',\n",
        "        words=1000,\n",
        "        temperature=1.0\n",
        "    )\n",
        "\n",
        "    # Example 3: Train on custom names dataset\n",
        "    # First, create the data files (see instructions below)\n",
        "    print(\"\\nTraining on custom names dataset...\")\n",
        "    # train_model(\n",
        "    #     model_type='LSTM',\n",
        "    #     data_path='./data/names',\n",
        "    #     emsize=128,\n",
        "    #     nhid=128,\n",
        "    #     nlayers=2,\n",
        "    #     epochs=20,\n",
        "    #     lr=0.001\n",
        "    # )"
      ],
      "metadata": {
        "id": "87WH6xgiGqmE"
      },
      "id": "87WH6xgiGqmE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Script to prepare names dataset for Word-Level Language Modeling\n",
        "Converts multiple text files with names into train/valid/test splits\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import random\n",
        "from typing import List, Tuple\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def read_names_from_files(data_dir: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Read all names from text files in the directory.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory containing .txt files with names\n",
        "\n",
        "    Returns:\n",
        "        List of all names\n",
        "    \"\"\"\n",
        "    all_names: List[str] = []\n",
        "\n",
        "    for filename in Path(data_dir).glob('*.txt'):\n",
        "        print(f\"Reading {filename.name}...\")\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            names: List[str] = [line.strip() for line in f if line.strip()]\n",
        "            all_names.extend(names)\n",
        "            print(f\"  Found {len(names)} names\")\n",
        "\n",
        "    print(f\"\\nTotal names: {len(all_names)}\")\n",
        "    return all_names\n",
        "\n",
        "\n",
        "def create_train_valid_test_splits(\n",
        "    names: List[str],\n",
        "    train_ratio: float = 0.8,\n",
        "    valid_ratio: float = 0.1,\n",
        "    test_ratio: float = 0.1,\n",
        "    seed: int = 42\n",
        ") -> Tuple[List[str], List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Split names into train, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "        names: List of names\n",
        "        train_ratio: Proportion for training (default 0.8)\n",
        "        valid_ratio: Proportion for validation (default 0.1)\n",
        "        test_ratio: Proportion for testing (default 0.1)\n",
        "        seed: Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (train_names, valid_names, test_names)\n",
        "    \"\"\"\n",
        "    assert abs(train_ratio + valid_ratio + test_ratio - 1.0) < 1e-6, \\\n",
        "        \"Ratios must sum to 1.0\"\n",
        "\n",
        "    random.seed(seed)\n",
        "    names_copy: List[str] = names.copy()\n",
        "    random.shuffle(names_copy)\n",
        "\n",
        "    n: int = len(names_copy)\n",
        "    train_end: int = int(n * train_ratio)\n",
        "    valid_end: int = train_end + int(n * valid_ratio)\n",
        "\n",
        "    train_names: List[str] = names_copy[:train_end]\n",
        "    valid_names: List[str] = names_copy[train_end:valid_end]\n",
        "    test_names: List[str] = names_copy[valid_end:]\n",
        "\n",
        "    print(f\"\\nSplit sizes:\")\n",
        "    print(f\"  Train: {len(train_names)} ({len(train_names)/n*100:.1f}%)\")\n",
        "    print(f\"  Valid: {len(valid_names)} ({len(valid_names)/n*100:.1f}%)\")\n",
        "    print(f\"  Test:  {len(test_names)} ({len(test_names)/n*100:.1f}%)\")\n",
        "\n",
        "    return train_names, valid_names, test_names\n",
        "\n",
        "\n",
        "def save_names_to_file(names: List[str], filename: str) -> None:\n",
        "    \"\"\"\n",
        "    Save names to file, one per line.\n",
        "\n",
        "    Args:\n",
        "        names: List of names\n",
        "        filename: Output filename\n",
        "    \"\"\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for name in names:\n",
        "            # Write each name as a separate \"sentence\"\n",
        "            # The model will add <eos> automatically\n",
        "            f.write(name + '\\n')\n",
        "    print(f\"Saved {len(names)} names to {filename}\")\n",
        "\n",
        "\n",
        "def prepare_names_dataset(\n",
        "    input_dir: str,\n",
        "    output_dir: str,\n",
        "    train_ratio: float = 0.8,\n",
        "    valid_ratio: float = 0.1,\n",
        "    test_ratio: float = 0.1,\n",
        "    seed: int = 42\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Main function to prepare names dataset.\n",
        "\n",
        "    Args:\n",
        "        input_dir: Directory containing raw name files\n",
        "        output_dir: Directory to save train/valid/test files\n",
        "        train_ratio: Proportion for training\n",
        "        valid_ratio: Proportion for validation\n",
        "        test_ratio: Proportion for testing\n",
        "        seed: Random seed\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"Preparing Names Dataset for Language Modeling\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Read all names\n",
        "    all_names: List[str] = read_names_from_files(input_dir)\n",
        "\n",
        "    # Split into train/valid/test\n",
        "    train_names, valid_names, test_names = create_train_valid_test_splits(\n",
        "        all_names, train_ratio, valid_ratio, test_ratio, seed\n",
        "    )\n",
        "\n",
        "    # Save splits\n",
        "    print(\"\\nSaving splits...\")\n",
        "    save_names_to_file(train_names, os.path.join(output_dir, 'train.txt'))\n",
        "    save_names_to_file(valid_names, os.path.join(output_dir, 'valid.txt'))\n",
        "    save_names_to_file(test_names, os.path.join(output_dir, 'test.txt'))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Dataset preparation complete!\")\n",
        "    print(f\"Output directory: {output_dir}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Print sample names\n",
        "    print(\"\\nSample names from train set:\")\n",
        "    for name in train_names[:10]:\n",
        "        print(f\"  {name}\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# ALTERNATIVE: Word-level format\n",
        "# ===============================\n",
        "\n",
        "def prepare_wordlevel_format(\n",
        "    input_dir: str,\n",
        "    output_dir: str,\n",
        "    train_ratio: float = 0.8,\n",
        "    valid_ratio: float = 0.1,\n",
        "    test_ratio: float = 0.1,\n",
        "    seed: int = 42\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Prepare dataset where each character is a 'word'.\n",
        "    This allows character-level language modeling using word LM code.\n",
        "\n",
        "    Args:\n",
        "        input_dir: Directory containing raw name files\n",
        "        output_dir: Directory to save train/valid/test files\n",
        "        train_ratio: Proportion for training\n",
        "        valid_ratio: Proportion for validation\n",
        "        test_ratio: Proportion for testing\n",
        "        seed: Random seed\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"Preparing Character-Level (as words) Dataset\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Read all names\n",
        "    all_names: List[str] = read_names_from_files(input_dir)\n",
        "\n",
        "    # Split\n",
        "    train_names, valid_names, test_names = create_train_valid_test_splits(\n",
        "        all_names, train_ratio, valid_ratio, test_ratio, seed\n",
        "    )\n",
        "\n",
        "    # Convert to character-level\n",
        "    def names_to_char_words(names: List[str], filename: str) -> None:\n",
        "        \"\"\"Convert names to space-separated characters.\"\"\"\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            for name in names:\n",
        "                # Each character becomes a \"word\"\n",
        "                char_sequence: str = ' '.join(list(name))\n",
        "                f.write(char_sequence + '\\n')\n",
        "        print(f\"Saved {len(names)} names (as char sequences) to {filename}\")\n",
        "\n",
        "    print(\"\\nSaving character-level splits...\")\n",
        "    names_to_char_words(train_names, os.path.join(output_dir, 'train.txt'))\n",
        "    names_to_char_words(valid_names, os.path.join(output_dir, 'valid.txt'))\n",
        "    names_to_char_words(test_names, os.path.join(output_dir, 'test.txt'))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Character-level dataset preparation complete!\")\n",
        "    print(f\"Output directory: {output_dir}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Print sample\n",
        "    print(\"\\nSample character sequences from train set:\")\n",
        "    for name in train_names[:5]:\n",
        "        char_seq: str = ' '.join(list(name))\n",
        "        print(f\"  {name} -> {char_seq}\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# EXAMPLE USAGE\n",
        "# ===============================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Example 1: Word-level (each name is a word)\n",
        "    # Best for: learning name distributions\n",
        "    prepare_names_dataset(\n",
        "        input_dir='data/names_raw',  # Your directory with German.txt, Russian.txt, etc.\n",
        "        output_dir='data/names_word',\n",
        "        train_ratio=0.8,\n",
        "        valid_ratio=0.1,\n",
        "        test_ratio=0.1\n",
        "    )\n",
        "\n",
        "    # Example 2: Character-level (each character is a word)\n",
        "    # Best for: generating new names character-by-character\n",
        "    prepare_wordlevel_format(\n",
        "        input_dir='data/names_raw',\n",
        "        output_dir='data/names_char',\n",
        "        train_ratio=0.8,\n",
        "        valid_ratio=0.1,\n",
        "        test_ratio=0.1\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"NEXT STEPS:\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"1. Use 'data/names_word' for name-level modeling\")\n",
        "    print(\"2. Use 'data/names_char' for character-level modeling\")\n",
        "    print(\"\\nTo train:\")\n",
        "    print(\"  python improved_word_lm.py --data data/names_word\")\n",
        "    print(\"\\nTo generate:\")\n",
        "    print(\"  python improved_word_lm.py --generate --checkpoint model.pt\")"
      ],
      "metadata": {
        "id": "o7FUkvYGbw7c"
      },
      "id": "o7FUkvYGbw7c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Complete example with training, evaluation, and improvements\n",
        "for Word-Level Language Modeling on Names Dataset\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "from typing import List, Dict, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "\n",
        "# Assuming improved_word_lm.py is imported\n",
        "from improved_word_lm import (\n",
        "    Corpus, RNNModel, TransformerModel,\n",
        "    batchify, evaluate, train_epoch\n",
        ")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# EXPERIMENT 1: Compare Architectures\n",
        "# ===============================\n",
        "\n",
        "def compare_architectures(\n",
        "    data_path: str = './data/names_char',\n",
        "    epochs: int = 20,\n",
        "    device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        ") -> Dict[str, Dict]:\n",
        "    \"\"\"Compare different model architectures on names dataset.\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"EXPERIMENT 1: Comparing Model Architectures\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Load data\n",
        "    corpus: Corpus = Corpus(data_path)\n",
        "    ntokens: int = len(corpus.dictionary)\n",
        "\n",
        "    batch_size: int = 20\n",
        "    eval_batch_size: int = 10\n",
        "    bptt: int = 35\n",
        "\n",
        "    train_data: torch.Tensor = batchify(corpus.train, batch_size, device)\n",
        "    val_data: torch.Tensor = batchify(corpus.valid, eval_batch_size, device)\n",
        "    test_data: torch.Tensor = batchify(corpus.test, eval_batch_size, device)\n",
        "\n",
        "    # Define architectures to test\n",
        "    architectures: Dict[str, Dict] = {\n",
        "        'LSTM-Small': {\n",
        "            'model_type': 'LSTM',\n",
        "            'emsize': 128,\n",
        "            'nhid': 128,\n",
        "            'nlayers': 2,\n",
        "            'dropout': 0.2\n",
        "        },\n",
        "        'LSTM-Large': {\n",
        "            'model_type': 'LSTM',\n",
        "            'emsize': 256,\n",
        "            'nhid': 256,\n",
        "            'nlayers': 3,\n",
        "            'dropout': 0.3\n",
        "        },\n",
        "        'GRU': {\n",
        "            'model_type': 'GRU',\n",
        "            'emsize': 200,\n",
        "            'nhid': 200,\n",
        "            'nlayers': 2,\n",
        "            'dropout': 0.2\n",
        "        },\n",
        "        'Transformer': {\n",
        "            'model_type': 'Transformer',\n",
        "            'emsize': 200,\n",
        "            'nhid': 200,\n",
        "            'nlayers': 2,\n",
        "            'nhead': 2,\n",
        "            'dropout': 0.2\n",
        "        }\n",
        "    }\n",
        "\n",
        "    results: Dict[str, Dict] = {}\n",
        "\n",
        "    for name, config in architectures.items():\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Training {name}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        # Create model\n",
        "        is_transformer: bool = config['model_type'] == 'Transformer'\n",
        "\n",
        "        if is_transformer:\n",
        "            model: nn.Module = TransformerModel(\n",
        "                ntokens,\n",
        "                config['emsize'],\n",
        "                config['nhead'],\n",
        "                config['nhid'],\n",
        "                config['nlayers'],\n",
        "                config['dropout']\n",
        "            ).to(device)\n",
        "        else:\n",
        "            model = RNNModel(\n",
        "                config['model_type'],\n",
        "                ntokens,\n",
        "                config['emsize'],\n",
        "                config['nhid'],\n",
        "                config['nlayers'],\n",
        "                config['dropout'],\n",
        "                tie_weights=False\n",
        "            ).to(device)\n",
        "\n",
        "        # Optimizer with weight decay (from Transformer paper)\n",
        "        criterion: nn.NLLLoss = nn.NLLLoss()\n",
        "        optimizer: optim.Adam = optim.Adam(\n",
        "            model.parameters(),\n",
        "            lr=0.001,\n",
        "            weight_decay=1e-5\n",
        "        )\n",
        "\n",
        "        # Training\n",
        "        train_losses: List[float] = []\n",
        "        val_losses: List[float] = []\n",
        "        best_val_loss: float = float('inf')\n",
        "\n",
        "        start_time: float = time.time()\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            train_epoch(\n",
        "                model, train_data, criterion, optimizer, epoch,\n",
        "                bptt, ntokens, batch_size, 0.25, 100, is_transformer\n",
        "            )\n",
        "\n",
        "            val_loss: float = evaluate(\n",
        "                model, val_data, criterion, bptt, ntokens,\n",
        "                eval_batch_size, is_transformer\n",
        "            )\n",
        "\n",
        "            train_losses.append(criterion(\n",
        "                model(train_data[:100])[0] if is_transformer else model(train_data[:100], model.init_hidden(batch_size))[0],\n",
        "                train_data[1:101].view(-1)\n",
        "            ).item())\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "            print(f'Epoch {epoch:3d} | val loss {val_loss:5.2f} | val ppl {math.exp(val_loss):8.2f}')\n",
        "\n",
        "            best_val_loss = min(best_val_loss, val_loss)\n",
        "\n",
        "        training_time: float = time.time() - start_time\n",
        "\n",
        "        # Test\n",
        "        test_loss: float = evaluate(\n",
        "            model, test_data, criterion, bptt, ntokens,\n",
        "            eval_batch_size, is_transformer\n",
        "        )\n",
        "\n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'best_val_loss': best_val_loss,\n",
        "            'test_loss': test_loss,\n",
        "            'test_ppl': math.exp(test_loss),\n",
        "            'training_time': training_time,\n",
        "            'config': config\n",
        "        }\n",
        "\n",
        "        print(f\"\\n{name} Results:\")\n",
        "        print(f\"  Best Val Loss: {best_val_loss:.4f}\")\n",
        "        print(f\"  Test Loss: {test_loss:.4f}\")\n",
        "        print(f\"  Test PPL: {math.exp(test_loss):.2f}\")\n",
        "        print(f\"  Training Time: {training_time:.2f}s\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# EXPERIMENT 2: Hyperparameter Tuning\n",
        "# ===============================\n",
        "\n",
        "def tune_hyperparameters(\n",
        "    data_path: str = './data/names_char',\n",
        "    base_config: Dict = None\n",
        ") -> Dict[str, List]:\n",
        "    \"\"\"Tune hyperparameters for best model.\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"EXPERIMENT 2: Hyperparameter Tuning\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    if base_config is None:\n",
        "        base_config = {\n",
        "            'model_type': 'LSTM',\n",
        "            'emsize': 200,\n",
        "            'nhid': 200,\n",
        "            'nlayers': 2,\n",
        "            'dropout': 0.2,\n",
        "            'lr': 0.001,\n",
        "            'batch_size': 20,\n",
        "            'epochs': 15\n",
        "        }\n",
        "\n",
        "    # Parameters to tune\n",
        "    param_grid: Dict[str, List] = {\n",
        "        'lr': [0.0001, 0.001, 0.01],\n",
        "        'dropout': [0.1, 0.2, 0.3],\n",
        "        'nhid': [128, 200, 256],\n",
        "        'nlayers': [2, 3, 4]\n",
        "    }\n",
        "\n",
        "    results: Dict[str, List] = {}\n",
        "\n",
        "    for param_name, param_values in param_grid.items():\n",
        "        print(f\"\\nTuning {param_name}...\")\n",
        "        results[param_name] = []\n",
        "\n",
        "        for value in param_values:\n",
        "            config: Dict = base_config.copy()\n",
        "            config[param_name] = value\n",
        "\n",
        "            print(f\"  Testing {param_name}={value}\")\n",
        "\n",
        "            # Train and evaluate (simplified version)\n",
        "            # In practice, you'd call the full training function\n",
        "            test_ppl: float = train_and_evaluate(data_path, config)\n",
        "\n",
        "            results[param_name].append({\n",
        "                'value': value,\n",
        "                'test_ppl': test_ppl\n",
        "            })\n",
        "\n",
        "            print(f\"    Test PPL: {test_ppl:.2f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def train_and_evaluate(data_path: str, config: Dict) -> float:\n",
        "    \"\"\"Simplified train and evaluate function.\"\"\"\n",
        "    # Placeholder - implement actual training\n",
        "    # This is a simplified version\n",
        "    return 10.0  # Return test perplexity\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# EXPERIMENT 3: Learning Curves\n",
        "# ===============================\n",
        "\n",
        "def analyze_learning_curves(results: Dict[str, Dict]) -> None:\n",
        "    \"\"\"Plot and analyze learning curves.\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"EXPERIMENT 3: Learning Curve Analysis\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    for idx, (name, result) in enumerate(results.items()):\n",
        "        row: int = idx // 2\n",
        "        col: int = idx % 2\n",
        "\n",
        "        ax = axes[row, col]\n",
        "\n",
        "        epochs: List[int] = list(range(1, len(result['train_losses']) + 1))\n",
        "        ax.plot(epochs, result['train_losses'], label='Train Loss', marker='o')\n",
        "        ax.plot(epochs, result['val_losses'], label='Val Loss', marker='s')\n",
        "\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Loss')\n",
        "        ax.set_title(f'{name} Learning Curves')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('learning_curves.png', dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Learning curves saved to 'learning_curves.png'\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# EXPERIMENT 4: Generate & Analyze Names\n",
        "# ===============================\n",
        "\n",
        "def generate_and_analyze_names(\n",
        "    model: nn.Module,\n",
        "    corpus: Corpus,\n",
        "    n_samples: int = 100,\n",
        "    temperature: float = 1.0,\n",
        "    device: torch.device = torch.device('cpu')\n",
        ") -> List[str]:\n",
        "    \"\"\"Generate names and analyze quality.\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"EXPERIMENT 4: Name Generation & Analysis\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    model.eval()\n",
        "    ntokens: int = len(corpus.dictionary)\n",
        "    is_transformer: bool = hasattr(model, 'model_type')\n",
        "\n",
        "    generated_names: List[str] = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(n_samples):\n",
        "            if not is_transformer:\n",
        "                hidden = model.init_hidden(1)\n",
        "\n",
        "            # Start with random character\n",
        "            input_tensor: torch.Tensor = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "            name_chars: List[str] = []\n",
        "\n",
        "            max_len: int = 20\n",
        "            for _ in range(max_len):\n",
        "                if is_transformer:\n",
        "                    output: torch.Tensor = model(input_tensor, False)\n",
        "                    word_weights: torch.Tensor = output[-1].squeeze().div(temperature).exp().cpu()\n",
        "                else:\n",
        "                    output, hidden = model(input_tensor, hidden)\n",
        "                    word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "\n",
        "                word_idx: int = torch.multinomial(word_weights, 1)[0].item()\n",
        "                word: str = corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "                if word == '<eos>':\n",
        "                    break\n",
        "\n",
        "                name_chars.append(word)\n",
        "                input_tensor.fill_(word_idx)\n",
        "\n",
        "            generated_name: str = ''.join(name_chars)\n",
        "            if generated_name:  # Only add non-empty names\n",
        "                generated_names.append(generated_name)\n",
        "\n",
        "    # Analysis\n",
        "    print(f\"\\nGenerated {len(generated_names)} names\")\n",
        "    print(f\"Sample names:\")\n",
        "    for name in generated_names[:20]:\n",
        "        print(f\"  {name}\")\n",
        "\n",
        "    # Statistics\n",
        "    avg_length: float = sum(len(name) for name in generated_names) / len(generated_names)\n",
        "    unique_names: int = len(set(generated_names))\n",
        "\n",
        "    print(f\"\\nStatistics:\")\n",
        "    print(f\"  Average length: {avg_length:.2f}\")\n",
        "    print(f\"  Unique names: {unique_names} ({unique_names/len(generated_names)*100:.1f}%)\")\n",
        "\n",
        "    return generated_names\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# EXPERIMENT 5: Temperature Sampling\n",
        "# ===============================\n",
        "\n",
        "def experiment_temperature(\n",
        "    model: nn.Module,\n",
        "    corpus: Corpus,\n",
        "    temperatures: List[float] = [0.5, 0.8, 1.0, 1.2, 1.5],\n",
        "    n_samples: int = 10,\n",
        "    device: torch.device = torch.device('cpu')\n",
        ") -> None:\n",
        "    \"\"\"Experiment with different temperature values.\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"EXPERIMENT 5: Temperature Sampling\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for temp in temperatures:\n",
        "        print(f\"\\nTemperature = {temp}\")\n",
        "        names: List[str] = generate_and_analyze_names(\n",
        "            model, corpus, n_samples, temp, device\n",
        "        )\n",
        "        print(f\"  Sample: {', '.join(names[:5])}\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# MAIN EXECUTION\n",
        "# ===============================\n",
        "\n",
        "def run_all_experiments(data_path: str = './data/names_char') -> None:\n",
        "    \"\"\"Run all experiments.\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"COMPLETE EXPERIMENTAL PIPELINE\")\n",
        "    print(\"Word-Level Language Modeling on Names Dataset\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Experiment 1: Compare architectures\n",
        "    results: Dict = compare_architectures(data_path, epochs=20, device=device)\n",
        "\n",
        "    # Experiment 3: Analyze learning curves\n",
        "    analyze_learning_curves(results)\n",
        "\n",
        "    # Find best model\n",
        "    best_model_name: str = min(\n",
        "        results.keys(),\n",
        "        key=lambda x: results[x]['test_loss']\n",
        "    )\n",
        "    best_model: nn.Module = results[best_model_name]['model']\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Best Model: {best_model_name}\")\n",
        "    print(f\"Test PPL: {results[best_model_name]['test_ppl']:.2f}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Load corpus for generation\n",
        "    corpus: Corpus = Corpus(data_path)\n",
        "\n",
        "    # Experiment 4: Generate names\n",
        "    generated_names: List[str] = generate_and_analyze_names(\n",
        "        best_model, corpus, n_samples=100, device=device\n",
        "    )\n",
        "\n",
        "    # Experiment 5: Temperature sampling\n",
        "    experiment_temperature(best_model, corpus, device=device)\n",
        "\n",
        "    # Save results\n",
        "    with open('experiment_results.txt', 'w') as f:\n",
        "        f.write(\"EXPERIMENTAL RESULTS\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "        for name, result in results.items():\n",
        "            f.write(f\"{name}:\\n\")\n",
        "            f.write(f\"  Test Loss: {result['test_loss']:.4f}\\n\")\n",
        "            f.write(f\"  Test PPL: {result['test_ppl']:.2f}\\n\")\n",
        "            f.write(f\"  Training Time: {result['training_time']:.2f}s\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "        f.write(\"\\nGenerated Names:\\n\")\n",
        "        for name in generated_names[:50]:\n",
        "            f.write(f\"  {name}\\n\")\n",
        "\n",
        "    print(\"\\nResults saved to 'experiment_results.txt'\")\n",
        "    print(\"\\nExperiments complete!\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Run all experiments\n",
        "    run_all_experiments(data_path='./data/names_char')"
      ],
      "metadata": {
        "id": "HMxRI_O3GcZl"
      },
      "id": "HMxRI_O3GcZl",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}