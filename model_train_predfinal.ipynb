{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "SsK88NLWSwjo",
      "metadata": {
        "id": "SsK88NLWSwjo"
      },
      "source": [
        "# TRAIN AND LEARN OF word_language_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fFwuP5dYS2Tx",
      "metadata": {
        "id": "fFwuP5dYS2Tx"
      },
      "source": [
        "## Model code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pJHzuwzIThUp",
      "metadata": {
        "id": "pJHzuwzIThUp"
      },
      "source": [
        "## Uploading files from Google Disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WxeRfC-STmyY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxeRfC-STmyY",
        "outputId": "0deaec71-eace-47f5-9546-6cf2bce1135d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w5z0Vi8DVzgk",
      "metadata": {
        "id": "w5z0Vi8DVzgk"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "import os\n",
        "from io import open\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from typing import Dict, List, Union, Tuple, Optional"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vHTBXIgBS5Xa",
      "metadata": {
        "id": "vHTBXIgBS5Xa"
      },
      "source": [
        "### data.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self) -> None:\n",
        "        self.word2idx: Dict[str, int] = {}\n",
        "        self.idx2word: List[str] = []\n",
        "        self.word2count: Counter = Counter()\n",
        "\n",
        "    def add_word(self, word: str) -> int:\n",
        "        self.word2count[word] += 1\n",
        "        return self.word2idx.get(word, -1)\n",
        "\n",
        "    def finalize(self, min_freq: int = 5) -> None:\n",
        "        for word, count in self.word2count.items():\n",
        "            if count >= min_freq and word not in self.word2idx:\n",
        "                self.idx2word.append(word)\n",
        "                self.word2idx[word] = len(self.idx2word) - 1\n",
        "        self.word2idx['<unk>'] = len(self.idx2word)\n",
        "        self.idx2word.append('<unk>')\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.idx2word)\n",
        "\n",
        "class Corpus(object):\n",
        "  \"\"\"Corpus class for loading and tokenizing text data.\"\"\"\n",
        "  def __init__(self, path: str, min_freq: int = 5) -> None:\n",
        "      self.dictionary: Dictionary = Dictionary()\n",
        "      self.min_freq: int = min_freq\n",
        "      self.train: torch.Tensor = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "      self.valid: torch.Tensor = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "      self.test: torch.Tensor = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "  def tokenize(self, path: str) -> torch.Tensor:\n",
        "      \"\"\"Tokenizes a text file.\"\"\"\n",
        "      assert os.path.exists(path)\n",
        "      # Add words to the dictionary\n",
        "      with open(path, 'r', encoding=\"utf8\") as f:\n",
        "          for line in f:\n",
        "              words: List[str] = line.split() + ['<eos>']\n",
        "              for word in words:\n",
        "                  self.dictionary.add_word(word)\n",
        "      self.dictionary.finalize(self.min_freq)\n",
        "      # Tokenize file content\n",
        "      with open(path, 'r', encoding=\"utf8\") as f:\n",
        "          idss: List[torch.Tensor] = []\n",
        "          for line in f:\n",
        "              words: List[str] = line.split() + ['<eos>']\n",
        "              ids: List[int] = []\n",
        "              for word in words:\n",
        "                  idx = self.dictionary.word2idx.get(word, self.dictionary.word2idx['<unk>'])\n",
        "                  ids.append(idx)\n",
        "              idss.append(torch.tensor(ids, dtype=torch.int64))\n",
        "          ids_tensor: torch.Tensor = torch.cat(idss)\n",
        "\n",
        "      return ids_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FwJICJSw5Rs3",
      "metadata": {
        "id": "FwJICJSw5Rs3"
      },
      "outputs": [],
      "source": [
        "class Old_Dictionary(object):\n",
        "    \"\"\"Dictionary for word-to-index mapping.\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.word2idx: Dict[str, int] = {}\n",
        "        self.idx2word: List[str] = []\n",
        "\n",
        "    def add_word(self, word: str) -> int:\n",
        "        \"\"\"Add a word to the dictionary.\"\"\"\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Old_Corpus(object):\n",
        "    \"\"\"Corpus class for loading and tokenizing text data.\"\"\"\n",
        "\n",
        "    def __init__(self, path: str) -> None:\n",
        "        self.dictionary: Old_Dictionary = Old_Dictionary()\n",
        "        self.train: torch.Tensor = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid: torch.Tensor = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test: torch.Tensor = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path: str) -> torch.Tensor:\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path), f\"Path {path} does not exist\"\n",
        "\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words: List[str] = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss: List[torch.Tensor] = []\n",
        "            for line in f:\n",
        "                words: List[str] = line.split() + ['<eos>']\n",
        "                ids: List[int] = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids, dtype=torch.int64))\n",
        "            ids_tensor: torch.Tensor = torch.cat(idss)\n",
        "\n",
        "        return ids_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xkMHPnmLWEKE",
      "metadata": {
        "id": "xkMHPnmLWEKE"
      },
      "source": [
        "### model.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9BXrfBWHWKGj",
      "metadata": {
        "id": "9BXrfBWHWKGj"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# MODEL ARCHITECTURES (model.py)\n",
        "# ===============================\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    \"\"\"RNN-based language model (LSTM/GRU/RNN).\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        rnn_type: str,\n",
        "        ntoken: int,\n",
        "        ninp: int,\n",
        "        nhid: int,\n",
        "        nlayers: int,\n",
        "        dropout: float = 0.5,\n",
        "        tie_weights: bool = False\n",
        "    ) -> None:\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.ntoken: int = ntoken\n",
        "        self.rnn_type: str = rnn_type\n",
        "        self.nhid: int = nhid\n",
        "        self.nlayers: int = nlayers\n",
        "\n",
        "        self.drop: nn.Dropout = nn.Dropout(dropout)\n",
        "        self.encoder: nn.Embedding = nn.Embedding(ntoken, ninp)\n",
        "\n",
        "        if rnn_type in ['LSTM', 'GRU']:\n",
        "            self.rnn: nn.Module = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
        "        else:\n",
        "            try:\n",
        "                nonlinearity: str = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
        "            except KeyError as e:\n",
        "                raise ValueError(\n",
        "                    \"Invalid option for `--model`. \"\n",
        "                    \"Options are ['LSTM', 'GRU', 'RNN_TANH', 'RNN_RELU']\"\n",
        "                ) from e\n",
        "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
        "\n",
        "        self.decoder: nn.Linear = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        # Tie weights\n",
        "        if tie_weights:\n",
        "            if nhid != ninp:\n",
        "                raise ValueError('When using tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        \"\"\"Initialize weights.\"\"\"\n",
        "        initrange: float = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, input: torch.Tensor, hidden: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        emb: torch.Tensor = self.drop(self.encoder(input))\n",
        "        output: torch.Tensor\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded: torch.Tensor = self.decoder(output)\n",
        "        decoded = decoded.view(-1, self.ntoken)\n",
        "        return F.log_softmax(decoded, dim=1), hidden\n",
        "\n",
        "    def init_hidden(self, bsz: int) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\"Initialize hidden state.\"\"\"\n",
        "        weight: torch.Tensor = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (\n",
        "                weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
        "            )\n",
        "        else:\n",
        "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
        "\n",
        "# Temporarily leave PositionalEncoding module here. Will be moved somewhere else.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
        "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
        "        Here, we use sine and cosine functions of different frequencies.\n",
        "    .. math:\n",
        "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
        "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "        \\text{where pos is the word position and i is the embed idx)\n",
        "    Args:\n",
        "        d_model: the embed dim (required).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        max_len: the max. length of the incoming sequence (default=5000).\n",
        "    Examples:\n",
        "        >>> pos_encoder = PositionalEncoding(d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"Positional encoding for Transformer.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000) -> None:\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout: nn.Dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe: torch.Tensor = torch.zeros(max_len, d_model)\n",
        "        position: torch.Tensor = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term: torch.Tensor = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        r\"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            x: the sequence fed to the positional encoder model (required).\n",
        "        Shape:\n",
        "            x: [sequence length, batch size, embed dim]\n",
        "            output: [sequence length, batch size, embed dim]\n",
        "        Examples:\n",
        "            >>> output = pos_encoder(x)\n",
        "        \"\"\"\n",
        "\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerModel(nn.Transformer):\n",
        "    \"\"\"Transformer-based language model.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        ntoken: int,\n",
        "        ninp: int,\n",
        "        nhead: int,\n",
        "        nhid: int,\n",
        "        nlayers: int,\n",
        "        dropout: float = 0.5\n",
        "    ) -> None:\n",
        "        super(TransformerModel, self).__init__(\n",
        "            d_model=ninp,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=nhid,\n",
        "            num_encoder_layers=nlayers\n",
        "        )\n",
        "        self.model_type: str = 'Transformer'\n",
        "        self.src_mask: Optional[torch.Tensor] = None\n",
        "        self.pos_encoder: PositionalEncoding = PositionalEncoding(ninp, dropout)\n",
        "\n",
        "        self.input_emb: nn.Embedding = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp: int = ninp\n",
        "        self.decoder: nn.Linear = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz: int) -> torch.Tensor:\n",
        "        \"\"\"Generate mask for causal attention.\"\"\"\n",
        "        return torch.log(torch.tril(torch.ones(sz, sz)))\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        \"\"\"Initialize weights.\"\"\"\n",
        "        initrange: float = 0.1\n",
        "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, src: torch.Tensor, has_mask: bool = True) -> torch.Tensor:\n",
        "        if has_mask:\n",
        "            device: torch.device = src.device\n",
        "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "                mask: torch.Tensor = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "                self.src_mask = mask\n",
        "        else:\n",
        "            self.src_mask = None\n",
        "\n",
        "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output: torch.Tensor = self.encoder(src, mask=self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return F.log_softmax(output, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xY5KPVooRtAt",
      "metadata": {
        "id": "xY5KPVooRtAt"
      },
      "outputs": [],
      "source": [
        "# Label smoothing loss\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, smoothing: float = 0.0) -> None:\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.smoothing: float = smoothing\n",
        "        self.confidence: float = 1.0 - smoothing\n",
        "\n",
        "    def forward(self, output: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        log_probs: torch.Tensor = output\n",
        "        n_classes: int = log_probs.size(-1)\n",
        "        with torch.no_grad():\n",
        "            true_dist: torch.Tensor = torch.zeros_like(log_probs)\n",
        "            true_dist.fill_(self.smoothing / (n_classes - 1))\n",
        "            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * log_probs, dim=-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4OZuV_gLZxBg",
      "metadata": {
        "id": "4OZuV_gLZxBg"
      },
      "source": [
        "### Test train on original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zb-MESoeRJq0",
      "metadata": {
        "id": "zb-MESoeRJq0"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# TRAINING & EVALUATION (main.py)\n",
        "# ===============================\n",
        "\n",
        "def get_lr(step: float, d_model: float, warmup_steps: int) -> float:\n",
        "    \"\"\"Gets the learning rate step.\"\"\"\n",
        "    lr: float = d_model ** -0.5 * min(step ** -0.5, step * warmup_steps ** -1.5)\n",
        "    return lr\n",
        "\n",
        "def batchify(data: torch.Tensor, bsz: int, device: torch.device) -> torch.Tensor:\n",
        "    \"\"\"Divide data into batches.\"\"\"\n",
        "    nbatch: int = data.size(0) // bsz\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "\n",
        "def get_batch(source: torch.Tensor, i: int, bptt: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Get a batch of data.\"\"\"\n",
        "    seq_len: int = min(bptt, len(source) - 1 - i)\n",
        "    data: torch.Tensor = source[i:i+seq_len]\n",
        "    target: torch.Tensor = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def repackage_hidden(h: Union[torch.Tensor, Tuple]) -> Union[torch.Tensor, Tuple]:\n",
        "    \"\"\"Detach hidden state from history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def top_k_sampling(logits: torch.Tensor, k: int, temperature: float = 1.0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Top k-sampling in generate function.\"\"\"\n",
        "    values, indices = torch.topk(logits, k)\n",
        "    values = values.div(temperature).exp()\n",
        "    values = values / values.sum()\n",
        "    return torch.multinomial(values, 1), indices\n",
        "\n",
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    data_source: torch.Tensor,\n",
        "    criterion: nn.Module,\n",
        "    bptt: int,\n",
        "    ntokens: int,\n",
        "    eval_batch_size: int,\n",
        "    is_transformer: bool\n",
        ") -> float:\n",
        "    \"\"\"Evaluate the model.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss: float = 0.0\n",
        "    if not is_transformer:\n",
        "        hidden = model.init_hidden(eval_batch_size)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data: torch.Tensor\n",
        "            targets: torch.Tensor\n",
        "            data, targets = get_batch(data_source, i, bptt)\n",
        "\n",
        "            if is_transformer:\n",
        "                output: torch.Tensor = model(data)\n",
        "                output = output.view(-1, ntokens)\n",
        "            else:\n",
        "                output, hidden = model(data, hidden)\n",
        "                hidden = repackage_hidden(hidden)\n",
        "\n",
        "            total_loss += len(data) * criterion(output, targets).item()\n",
        "\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train_epoch(\n",
        "    model: nn.Module,\n",
        "    train_data: torch.Tensor,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    epoch: int,\n",
        "    bptt: int,\n",
        "    ntokens: int,\n",
        "    batch_size: int,\n",
        "    clip: float,\n",
        "    log_interval: int,\n",
        "    is_transformer: bool,\n",
        "    use_optimizer: bool = True,\n",
        "    use_warmup: bool = False,\n",
        "    step: int = 0,\n",
        "    d_model: int = 512,\n",
        "    warmup_steps: int = 4000,\n",
        "    dry_run: bool = False\n",
        ") -> int:\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss: float = 0.0\n",
        "    start_time: float = time.time()\n",
        "\n",
        "    if not is_transformer:\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data: torch.Tensor\n",
        "        targets: torch.Tensor\n",
        "        data, targets = get_batch(train_data, i, bptt)\n",
        "\n",
        "        optimizer.zero_grad() if use_optimizer else model.zero_grad()\n",
        "\n",
        "        if is_transformer:\n",
        "            output: torch.Tensor = model(data)\n",
        "            output = output.view(-1, ntokens)\n",
        "        else:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            output, hidden = model(data, hidden)\n",
        "\n",
        "        loss: torch.Tensor = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        if use_warmup:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = get_lr(step + 1, d_model, warmup_steps)\n",
        "        if use_optimizer:\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            for p in model.parameters():\n",
        "                p.data.add_(p.grad, alpha=-optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        step += 1\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss: float = total_loss / log_interval\n",
        "            elapsed: float = time.time() - start_time\n",
        "            print(\n",
        "                f'| epoch {epoch:3d} | {batch:5d}/{len(train_data) // bptt:5d} batches | '\n",
        "                f'lr {optimizer.param_groups[0][\"lr\"]:02.6f} | ms/batch {elapsed * 1000 / log_interval:5.2f} | '\n",
        "                f'loss {cur_loss:5.2f} | ppl {math.exp(cur_loss):8.2f}'\n",
        "            )\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "        if dry_run:\n",
        "            break\n",
        "    return step\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model_type: str = 'LSTM', # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "    data_path: str = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize: int = 200,\n",
        "    nhid: int = 200,\n",
        "    nlayers: int = 2,\n",
        "    lr: float = 0.001,\n",
        "    clip: float = 0.25,\n",
        "    epochs: int = 60,\n",
        "    batch_size: int = 20,\n",
        "    bptt: int = 35,\n",
        "    dropout: float = 0.2,\n",
        "    tied: bool = False,\n",
        "    nhead: int = 2,\n",
        "    log_interval: int = 200,\n",
        "    save_path: str = 'model.pt',\n",
        "    onnx_export: str = '',\n",
        "    dry_run: bool = False,\n",
        "    accel: bool = True,\n",
        "    use_optimizer: bool = True,\n",
        "    optimizer_type: str = 'AdamW',\n",
        "    weight_decay: Optional[float] = None,\n",
        "    use_betas: bool = False,\n",
        "    betas: Optional[Tuple[float, float]] = (0.9, 0.98),\n",
        "    use_eps: bool = False,\n",
        "    eps: float = 1e-9,\n",
        "    criterion: Optional[nn.Module] = None,\n",
        "    use_label_smoothing: bool = False,\n",
        "    label_smoothing: float = 0.1,\n",
        "    use_warmup: bool = False,\n",
        "    warmup_steps: int = 4000,\n",
        "    min_freq: int = 5,\n",
        "    seed: int = 1111,\n",
        "    old_version: bool = True\n",
        ") -> None:\n",
        "    \"\"\"Main training function.\"\"\"\n",
        "\n",
        "    if data_path == '/content/drive/MyDrive/data_word_train/wikitext-2':\n",
        "      if not os.path.exists(data_path):\n",
        "          print(\"Downloading Wikitext-2 dataset...\")\n",
        "          !wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip -P /content/\n",
        "          !unzip /content/wikitext-2-v1.zip -d /content/data_word_train/\n",
        "          !mkdir -p /content/drive/MyDrive/data_word_train/\n",
        "          !mv /content/data_word_train/wikitext-2 /content/drive/MyDrive/data_word_train/\n",
        "          print(\"Wikitext-2 dataset moved to Google Drive\")\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # Set device\n",
        "    device: torch.device = torch.device('cuda' if accel and torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load data\n",
        "    corpus: Union[Corpus, Old_Corpus] = Old_Corpus(data_path) if old_version else Corpus(data_path, min_freq=min_freq)\n",
        "    print(f\"Vocabulary size: {len(corpus.dictionary)}\")\n",
        "\n",
        "    eval_batch_size: int = 10\n",
        "    train_data: torch.Tensor = batchify(corpus.train, batch_size, device)\n",
        "    val_data: torch.Tensor = batchify(corpus.valid, eval_batch_size, device)\n",
        "    test_data: torch.Tensor = batchify(corpus.test, eval_batch_size, device)\n",
        "\n",
        "    # Build model\n",
        "    ntokens: int = len(corpus.dictionary)\n",
        "    is_transformer: bool = model_type == 'Transformer'\n",
        "\n",
        "    model: nn.Module = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device) if is_transformer else RNNModel(model_type, ntokens, emsize, nhid, nlayers, dropout, tied).to(device)\n",
        "    # Loss and optimizer (Adam with weight_decay as in Transformer paper)\n",
        "    criterion: nn.Module = criterion if criterion is not None else (LabelSmoothingLoss(smoothing=label_smoothing) if use_label_smoothing else nn.NLLLoss())\n",
        "    if use_betas == True and use_eps == True:\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=betas, eps=eps) if optimizer_type == 'AdamW' else optim.Adam(model.parameters(), lr=lr, betas=betas, eps=eps)\n",
        "    elif use_betas == False and use_eps == True:\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, eps=eps) if optimizer_type == 'AdamW' else optim.Adam(model.parameters(), lr=lr, eps=eps)\n",
        "    elif use_betas == False and use_eps == False:\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay) if optimizer_type == 'AdamW' else optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    scheduler: optim.lr_scheduler.ReduceLROnPlateau = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=2\n",
        "    ) if not use_warmup else None\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss: Optional[float] = None\n",
        "    global_step: int = 0\n",
        "\n",
        "    try:\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            epoch_start_time: float = time.time()\n",
        "\n",
        "            global_step = train_epoch(\n",
        "                model, train_data, criterion, optimizer, epoch, bptt, ntokens, batch_size, clip, log_interval, is_transformer,\n",
        "                use_optimizer, use_warmup, global_step, emsize, warmup_steps, dry_run\n",
        "            )\n",
        "\n",
        "            val_loss: float = evaluate(\n",
        "                model, val_data, criterion, bptt, ntokens,\n",
        "                eval_batch_size, is_transformer\n",
        "            )\n",
        "\n",
        "            print('-' * 89)\n",
        "            print(\n",
        "                f'| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s | '\n",
        "                f'valid loss {val_loss:5.2f} | valid ppl {math.exp(val_loss):8.2f}'\n",
        "            )\n",
        "            print('-' * 89)\n",
        "\n",
        "            # Save best model\n",
        "            if not best_val_loss or val_loss < best_val_loss:\n",
        "                with open(save_path, 'wb') as f:\n",
        "                    torch.save(model, f)\n",
        "                best_val_loss = val_loss\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            # lr /= 4.0\n",
        "            if use_warmup:\n",
        "                print(f\"Current learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "            else:\n",
        "                scheduler.step(val_loss)\n",
        "                print(f\"Current learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('-' * 89)\n",
        "        print('Exiting from training early')\n",
        "    safe_globals: List = [\n",
        "        RNNModel, TransformerModel, PositionalEncoding, Old_Dictionary, Dictionary, Old_Corpus, Corpus,\n",
        "        nn.Dropout, nn.Linear, nn.GRU, nn.LSTM, nn.RNN, nn.Embedding,\n",
        "        nn.TransformerEncoder, nn.TransformerEncoderLayer, nn.MultiheadAttention,\n",
        "        nn.LayerNorm, F.relu, nn.ModuleList, nn.modules.linear.NonDynamicallyQuantizableLinear\n",
        "    ]\n",
        "    with torch.serialization.safe_globals(safe_globals):\n",
        "        with open(save_path, 'rb') as f:\n",
        "            model = torch.load(f, map_location=device)\n",
        "    test_loss: float = evaluate(\n",
        "        model, test_data, criterion, bptt, ntokens, eval_batch_size, is_transformer\n",
        "    )\n",
        "    print('=' * 89)\n",
        "    print(\n",
        "        f'| End of training | test loss {test_loss:5.2f} | '\n",
        "        f'test ppl {math.exp(test_loss):8.2f}'\n",
        "    )\n",
        "    print('=' * 89)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DOCVuP6LUpRS",
      "metadata": {
        "id": "DOCVuP6LUpRS"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# TEXT GENERATION (generate.py)\n",
        "# ===============================\n",
        "\n",
        "def generate_text(\n",
        "    checkpoint: str = 'model.pt',\n",
        "    data_path: str = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf: str = 'generated.txt',\n",
        "    words: int = 1000,\n",
        "    temperature: float = 1.0,\n",
        "    top_k: int = 40,\n",
        "    seed: int = 1111,\n",
        "    log_interval: int = 100,\n",
        "    accel: bool = True,\n",
        "    min_freq: int = 5,\n",
        "    use_top_k: bool = False,\n",
        "    old_version: bool = True\n",
        ") -> None:\n",
        "    \"\"\"Generate text from trained model.\"\"\"\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    device: torch.device = torch.device('cuda' if accel and torch.cuda.is_available() else 'cpu')\n",
        "    # assert len(corpus.dictionary) == model.ntoken, f\"Vocabulary size mismatch: {len(corpus.dictionary)} vs {model.ntoken}\"\n",
        "\n",
        "    # Load model\n",
        "    safe_globals: List = [\n",
        "        RNNModel, TransformerModel, PositionalEncoding, Old_Dictionary, Dictionary, Old_Corpus, Corpus,\n",
        "        nn.Dropout, nn.Linear, nn.GRU, nn.LSTM, nn.RNN, nn.Embedding,\n",
        "        nn.TransformerEncoder, nn.TransformerEncoderLayer, nn.MultiheadAttention,\n",
        "        nn.LayerNorm, F.relu, nn.ModuleList, nn.modules.linear.NonDynamicallyQuantizableLinear\n",
        "    ]\n",
        "    with torch.serialization.safe_globals(safe_globals):\n",
        "        with open(checkpoint, 'rb') as f:\n",
        "            model: nn.Module = torch.load(f, map_location=device)\n",
        "    model.eval()\n",
        "\n",
        "    # Load corpus\n",
        "    corpus: Union[Corpus, Old_Corpus] = Old_Corpus(data_path) if old_version else Corpus(data_path, min_freq=min_freq)\n",
        "    print(f\"Vocabulary size: {len(corpus.dictionary)}\")\n",
        "    print(f\"Vocabulary size: {len(corpus.dictionary)}\")\n",
        "    ntokens: int = len(corpus.dictionary)\n",
        "\n",
        "    is_transformer: bool = hasattr(model, 'model_type') and model.model_type == 'Transformer'\n",
        "    if not is_transformer:\n",
        "        hidden = model.init_hidden(1)\n",
        "\n",
        "    input: torch.Tensor = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "    with open(outf, 'w') as outfile:\n",
        "        with torch.no_grad():\n",
        "            for i in range(words):\n",
        "                if is_transformer:\n",
        "                    output: torch.Tensor = model(input, False)\n",
        "                    if use_top_k:\n",
        "                        word_weights: torch.Tensor = output[-1].squeeze().cpu()\n",
        "                        prob, top_indices = top_k_sampling(word_weights, top_k, temperature)\n",
        "                        word_idx: int = top_indices[prob.item()].item()\n",
        "                    else:\n",
        "                        word_weights: torch.Tensor = output[-1].squeeze().div(temperature).exp().cpu()\n",
        "                        word_idx: int = torch.multinomial(word_weights, 1)[0].item()\n",
        "                    word_tensor: torch.Tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
        "                    input = torch.cat([input, word_tensor], 0)\n",
        "\n",
        "                else:\n",
        "                    output, hidden = model(input, hidden)\n",
        "                    word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "                    word_idx = torch.multinomial(word_weights, 1)[0].item()\n",
        "                    input.fill_(word_idx)\n",
        "\n",
        "                word: str = corpus.dictionary.idx2word[word_idx]\n",
        "                if word == '@-@' or word == '@.@' or word == '@,@':\n",
        "                  word = ' '\n",
        "                outfile.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "                if i % log_interval == 0:\n",
        "                    print(f'| Generated {i}/{words} words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jlFnK0TWRNX0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlFnK0TWRNX0",
        "outputId": "2907aa69-70b3-4088-feda-d6b9e3135b87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LSTM on WikiText-2...\n",
            "Using device: cuda\n",
            "Vocabulary size: 33278\n",
            "| epoch   1 |   200/ 2983 batches | lr 0.001000 | ms/batch 24.86 | loss  7.33 | ppl  1522.35\n",
            "| epoch   1 |   400/ 2983 batches | lr 0.001000 | ms/batch 24.81 | loss  7.10 | ppl  1207.10\n",
            "| epoch   1 |   600/ 2983 batches | lr 0.001000 | ms/batch 24.89 | loss  7.09 | ppl  1196.08\n",
            "| epoch   1 |   800/ 2983 batches | lr 0.001000 | ms/batch 24.92 | loss  7.09 | ppl  1196.80\n",
            "| epoch   1 |  1000/ 2983 batches | lr 0.001000 | ms/batch 24.99 | loss  7.10 | ppl  1214.24\n",
            "| epoch   1 |  1200/ 2983 batches | lr 0.001000 | ms/batch 25.00 | loss  7.11 | ppl  1228.74\n",
            "| epoch   1 |  1400/ 2983 batches | lr 0.001000 | ms/batch 25.00 | loss  6.94 | ppl  1031.50\n",
            "| epoch   1 |  1600/ 2983 batches | lr 0.001000 | ms/batch 25.05 | loss  6.63 | ppl   758.50\n",
            "| epoch   1 |  1800/ 2983 batches | lr 0.001000 | ms/batch 25.11 | loss  6.46 | ppl   636.73\n",
            "| epoch   1 |  2000/ 2983 batches | lr 0.001000 | ms/batch 25.14 | loss  6.40 | ppl   602.21\n",
            "| epoch   1 |  2200/ 2983 batches | lr 0.001000 | ms/batch 25.16 | loss  6.26 | ppl   524.79\n",
            "| epoch   1 |  2400/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  6.22 | ppl   504.94\n",
            "| epoch   1 |  2600/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  6.19 | ppl   486.94\n",
            "| epoch   1 |  2800/ 2983 batches | lr 0.001000 | ms/batch 25.36 | loss  6.08 | ppl   436.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 76.88s | valid loss  5.91 | valid ppl   367.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   2 |   200/ 2983 batches | lr 0.001000 | ms/batch 25.62 | loss  5.77 | ppl   321.80\n",
            "| epoch   2 |   400/ 2983 batches | lr 0.001000 | ms/batch 25.43 | loss  5.72 | ppl   303.89\n",
            "| epoch   2 |   600/ 2983 batches | lr 0.001000 | ms/batch 25.37 | loss  5.64 | ppl   282.60\n",
            "| epoch   2 |   800/ 2983 batches | lr 0.001000 | ms/batch 25.37 | loss  5.68 | ppl   293.98\n",
            "| epoch   2 |  1000/ 2983 batches | lr 0.001000 | ms/batch 25.35 | loss  5.64 | ppl   281.28\n",
            "| epoch   2 |  1200/ 2983 batches | lr 0.001000 | ms/batch 25.37 | loss  5.66 | ppl   286.92\n",
            "| epoch   2 |  1400/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  5.62 | ppl   275.84\n",
            "| epoch   2 |  1600/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  5.63 | ppl   277.30\n",
            "| epoch   2 |  1800/ 2983 batches | lr 0.001000 | ms/batch 25.35 | loss  5.54 | ppl   254.42\n",
            "| epoch   2 |  2000/ 2983 batches | lr 0.001000 | ms/batch 25.25 | loss  5.56 | ppl   260.03\n",
            "| epoch   2 |  2200/ 2983 batches | lr 0.001000 | ms/batch 25.24 | loss  5.46 | ppl   234.41\n",
            "| epoch   2 |  2400/ 2983 batches | lr 0.001000 | ms/batch 25.24 | loss  5.47 | ppl   237.27\n",
            "| epoch   2 |  2600/ 2983 batches | lr 0.001000 | ms/batch 25.24 | loss  5.48 | ppl   238.70\n",
            "| epoch   2 |  2800/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  5.40 | ppl   221.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 77.60s | valid loss  5.56 | valid ppl   258.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   3 |   200/ 2983 batches | lr 0.001000 | ms/batch 25.43 | loss  5.27 | ppl   193.78\n",
            "| epoch   3 |   400/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  5.25 | ppl   190.32\n",
            "| epoch   3 |   600/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  5.15 | ppl   171.83\n",
            "| epoch   3 |   800/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  5.20 | ppl   181.67\n",
            "| epoch   3 |  1000/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  5.18 | ppl   177.72\n",
            "| epoch   3 |  1200/ 2983 batches | lr 0.001000 | ms/batch 25.33 | loss  5.20 | ppl   181.47\n",
            "| epoch   3 |  1400/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  5.18 | ppl   178.44\n",
            "| epoch   3 |  1600/ 2983 batches | lr 0.001000 | ms/batch 25.32 | loss  5.22 | ppl   185.62\n",
            "| epoch   3 |  1800/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  5.14 | ppl   171.37\n",
            "| epoch   3 |  2000/ 2983 batches | lr 0.001000 | ms/batch 25.33 | loss  5.16 | ppl   175.01\n",
            "| epoch   3 |  2200/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  5.07 | ppl   159.19\n",
            "| epoch   3 |  2400/ 2983 batches | lr 0.001000 | ms/batch 25.32 | loss  5.08 | ppl   160.81\n",
            "| epoch   3 |  2600/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  5.10 | ppl   163.68\n",
            "| epoch   3 |  2800/ 2983 batches | lr 0.001000 | ms/batch 25.26 | loss  5.04 | ppl   153.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 77.54s | valid loss  5.39 | valid ppl   219.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   4 |   200/ 2983 batches | lr 0.001000 | ms/batch 25.39 | loss  4.95 | ppl   141.85\n",
            "| epoch   4 |   400/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  4.97 | ppl   143.91\n",
            "| epoch   4 |   600/ 2983 batches | lr 0.001000 | ms/batch 25.25 | loss  4.82 | ppl   123.62\n",
            "| epoch   4 |   800/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.88 | ppl   131.14\n",
            "| epoch   4 |  1000/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.88 | ppl   132.15\n",
            "| epoch   4 |  1200/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.90 | ppl   134.12\n",
            "| epoch   4 |  1400/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.90 | ppl   134.56\n",
            "| epoch   4 |  1600/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.95 | ppl   141.65\n",
            "| epoch   4 |  1800/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.87 | ppl   130.13\n",
            "| epoch   4 |  2000/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  4.89 | ppl   132.89\n",
            "| epoch   4 |  2200/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.81 | ppl   122.18\n",
            "| epoch   4 |  2400/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.82 | ppl   124.12\n",
            "| epoch   4 |  2600/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.84 | ppl   126.56\n",
            "| epoch   4 |  2800/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.79 | ppl   120.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 77.49s | valid loss  5.32 | valid ppl   205.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   5 |   200/ 2983 batches | lr 0.001000 | ms/batch 25.43 | loss  4.74 | ppl   113.94\n",
            "| epoch   5 |   400/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.75 | ppl   115.66\n",
            "| epoch   5 |   600/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.59 | ppl    98.72\n",
            "| epoch   5 |   800/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.65 | ppl   104.25\n",
            "| epoch   5 |  1000/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.66 | ppl   106.02\n",
            "| epoch   5 |  1200/ 2983 batches | lr 0.001000 | ms/batch 25.32 | loss  4.68 | ppl   107.78\n",
            "| epoch   5 |  1400/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.69 | ppl   109.19\n",
            "| epoch   5 |  1600/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.74 | ppl   114.82\n",
            "| epoch   5 |  1800/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.66 | ppl   105.73\n",
            "| epoch   5 |  2000/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  4.68 | ppl   108.30\n",
            "| epoch   5 |  2200/ 2983 batches | lr 0.001000 | ms/batch 25.32 | loss  4.61 | ppl   100.60\n",
            "| epoch   5 |  2400/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.62 | ppl   101.75\n",
            "| epoch   5 |  2600/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.64 | ppl   104.02\n",
            "| epoch   5 |  2800/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.59 | ppl    98.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 77.52s | valid loss  5.28 | valid ppl   196.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   6 |   200/ 2983 batches | lr 0.001000 | ms/batch 25.39 | loss  4.56 | ppl    95.84\n",
            "| epoch   6 |   400/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.57 | ppl    96.68\n",
            "| epoch   6 |   600/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.40 | ppl    81.85\n",
            "| epoch   6 |   800/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  4.46 | ppl    86.60\n",
            "| epoch   6 |  1000/ 2983 batches | lr 0.001000 | ms/batch 25.32 | loss  4.49 | ppl    88.98\n",
            "| epoch   6 |  1200/ 2983 batches | lr 0.001000 | ms/batch 25.26 | loss  4.51 | ppl    90.81\n",
            "| epoch   6 |  1400/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.52 | ppl    92.28\n",
            "| epoch   6 |  1600/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  4.59 | ppl    98.41\n",
            "| epoch   6 |  1800/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.50 | ppl    90.41\n",
            "| epoch   6 |  2000/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.53 | ppl    92.35\n",
            "| epoch   6 |  2200/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.46 | ppl    86.15\n",
            "| epoch   6 |  2400/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.46 | ppl    86.44\n",
            "| epoch   6 |  2600/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.48 | ppl    88.22\n",
            "| epoch   6 |  2800/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.43 | ppl    84.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 77.50s | valid loss  5.28 | valid ppl   196.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   7 |   200/ 2983 batches | lr 0.001000 | ms/batch 25.41 | loss  4.42 | ppl    82.73\n",
            "| epoch   7 |   400/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.43 | ppl    84.16\n",
            "| epoch   7 |   600/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.26 | ppl    70.75\n",
            "| epoch   7 |   800/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.31 | ppl    74.80\n",
            "| epoch   7 |  1000/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.35 | ppl    77.72\n",
            "| epoch   7 |  1200/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.37 | ppl    78.91\n",
            "| epoch   7 |  1400/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.39 | ppl    80.56\n",
            "| epoch   7 |  1600/ 2983 batches | lr 0.001000 | ms/batch 25.26 | loss  4.46 | ppl    86.45\n",
            "| epoch   7 |  1800/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.38 | ppl    79.56\n",
            "| epoch   7 |  2000/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.39 | ppl    80.96\n",
            "| epoch   7 |  2200/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.33 | ppl    75.62\n",
            "| epoch   7 |  2400/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.33 | ppl    75.66\n",
            "| epoch   7 |  2600/ 2983 batches | lr 0.001000 | ms/batch 25.32 | loss  4.36 | ppl    78.06\n",
            "| epoch   7 |  2800/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.31 | ppl    74.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 77.51s | valid loss  5.29 | valid ppl   199.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   8 |   200/ 2983 batches | lr 0.001000 | ms/batch 25.44 | loss  4.30 | ppl    73.39\n",
            "| epoch   8 |   400/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  4.31 | ppl    74.35\n",
            "| epoch   8 |   600/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.14 | ppl    62.77\n",
            "| epoch   8 |   800/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  4.20 | ppl    66.40\n",
            "| epoch   8 |  1000/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  4.23 | ppl    68.94\n",
            "| epoch   8 |  1200/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.25 | ppl    70.42\n",
            "| epoch   8 |  1400/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.28 | ppl    71.92\n",
            "| epoch   8 |  1600/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.35 | ppl    77.58\n",
            "| epoch   8 |  1800/ 2983 batches | lr 0.001000 | ms/batch 25.27 | loss  4.26 | ppl    70.91\n",
            "| epoch   8 |  2000/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.30 | ppl    73.33\n",
            "| epoch   8 |  2200/ 2983 batches | lr 0.001000 | ms/batch 25.32 | loss  4.21 | ppl    67.66\n",
            "| epoch   8 |  2400/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.22 | ppl    67.70\n",
            "| epoch   8 |  2600/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.25 | ppl    70.38\n",
            "| epoch   8 |  2800/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.20 | ppl    66.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 77.52s | valid loss  5.34 | valid ppl   208.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   9 |   200/ 2983 batches | lr 0.001000 | ms/batch 25.41 | loss  4.19 | ppl    66.18\n",
            "| epoch   9 |   400/ 2983 batches | lr 0.001000 | ms/batch 25.32 | loss  4.22 | ppl    67.76\n",
            "| epoch   9 |   600/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.04 | ppl    56.77\n",
            "| epoch   9 |   800/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.10 | ppl    60.18\n",
            "| epoch   9 |  1000/ 2983 batches | lr 0.001000 | ms/batch 25.30 | loss  4.14 | ppl    62.98\n",
            "| epoch   9 |  1200/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.16 | ppl    63.89\n",
            "| epoch   9 |  1400/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.17 | ppl    64.98\n",
            "| epoch   9 |  1600/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.25 | ppl    70.38\n",
            "| epoch   9 |  1800/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.17 | ppl    64.60\n",
            "| epoch   9 |  2000/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.20 | ppl    66.80\n",
            "| epoch   9 |  2200/ 2983 batches | lr 0.001000 | ms/batch 25.29 | loss  4.12 | ppl    61.39\n",
            "| epoch   9 |  2400/ 2983 batches | lr 0.001000 | ms/batch 25.31 | loss  4.12 | ppl    61.66\n",
            "| epoch   9 |  2600/ 2983 batches | lr 0.001000 | ms/batch 25.28 | loss  4.15 | ppl    63.61\n",
            "| epoch   9 |  2800/ 2983 batches | lr 0.001000 | ms/batch 25.32 | loss  4.11 | ppl    61.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 77.53s | valid loss  5.38 | valid ppl   216.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch  10 |   200/ 2983 batches | lr 0.000500 | ms/batch 25.43 | loss  4.13 | ppl    62.30\n",
            "| epoch  10 |   400/ 2983 batches | lr 0.000500 | ms/batch 25.28 | loss  4.13 | ppl    62.11\n",
            "| epoch  10 |   600/ 2983 batches | lr 0.000500 | ms/batch 25.28 | loss  3.95 | ppl    51.94\n",
            "| epoch  10 |   800/ 2983 batches | lr 0.000500 | ms/batch 25.26 | loss  4.00 | ppl    54.85\n",
            "| epoch  10 |  1000/ 2983 batches | lr 0.000500 | ms/batch 25.27 | loss  4.05 | ppl    57.37\n",
            "| epoch  10 |  1200/ 2983 batches | lr 0.000500 | ms/batch 25.30 | loss  4.04 | ppl    56.69\n",
            "| epoch  10 |  1400/ 2983 batches | lr 0.000500 | ms/batch 25.29 | loss  4.05 | ppl    57.29\n",
            "| epoch  10 |  1600/ 2983 batches | lr 0.000500 | ms/batch 25.31 | loss  4.12 | ppl    61.84\n",
            "| epoch  10 |  1800/ 2983 batches | lr 0.000500 | ms/batch 25.31 | loss  4.04 | ppl    56.68\n",
            "| epoch  10 |  2000/ 2983 batches | lr 0.000500 | ms/batch 25.30 | loss  4.05 | ppl    57.33\n",
            "| epoch  10 |  2200/ 2983 batches | lr 0.000500 | ms/batch 25.32 | loss  3.96 | ppl    52.70\n",
            "| epoch  10 |  2400/ 2983 batches | lr 0.000500 | ms/batch 25.31 | loss  3.97 | ppl    53.05\n",
            "| epoch  10 |  2600/ 2983 batches | lr 0.000500 | ms/batch 25.32 | loss  3.99 | ppl    54.27\n",
            "| epoch  10 |  2800/ 2983 batches | lr 0.000500 | ms/batch 25.29 | loss  3.95 | ppl    51.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 77.52s | valid loss  5.37 | valid ppl   213.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch  11 |   200/ 2983 batches | lr 0.000500 | ms/batch 25.42 | loss  4.04 | ppl    56.68\n",
            "| epoch  11 |   400/ 2983 batches | lr 0.000500 | ms/batch 25.29 | loss  4.05 | ppl    57.15\n",
            "| epoch  11 |   600/ 2983 batches | lr 0.000500 | ms/batch 25.28 | loss  3.87 | ppl    47.89\n",
            "| epoch  11 |   800/ 2983 batches | lr 0.000500 | ms/batch 25.25 | loss  3.93 | ppl    50.73\n",
            "| epoch  11 |  1000/ 2983 batches | lr 0.000500 | ms/batch 25.29 | loss  3.97 | ppl    52.91\n",
            "| epoch  11 |  1200/ 2983 batches | lr 0.000500 | ms/batch 25.28 | loss  3.96 | ppl    52.39\n",
            "| epoch  11 |  1400/ 2983 batches | lr 0.000500 | ms/batch 25.28 | loss  3.98 | ppl    53.51\n",
            "| epoch  11 |  1600/ 2983 batches | lr 0.000500 | ms/batch 25.30 | loss  4.05 | ppl    57.47\n",
            "| epoch  11 |  1800/ 2983 batches | lr 0.000500 | ms/batch 25.29 | loss  3.98 | ppl    53.28\n",
            "| epoch  11 |  2000/ 2983 batches | lr 0.000500 | ms/batch 25.29 | loss  4.00 | ppl    54.37\n",
            "| epoch  11 |  2200/ 2983 batches | lr 0.000500 | ms/batch 25.31 | loss  3.91 | ppl    49.88\n",
            "| epoch  11 |  2400/ 2983 batches | lr 0.000500 | ms/batch 25.30 | loss  3.91 | ppl    49.81\n",
            "| epoch  11 |  2600/ 2983 batches | lr 0.000500 | ms/batch 25.28 | loss  3.94 | ppl    51.39\n",
            "| epoch  11 |  2800/ 2983 batches | lr 0.000500 | ms/batch 25.32 | loss  3.90 | ppl    49.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 77.51s | valid loss  5.39 | valid ppl   219.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch  12 |   200/ 2983 batches | lr 0.000500 | ms/batch 25.46 | loss  3.98 | ppl    53.47\n",
            "| epoch  12 |   400/ 2983 batches | lr 0.000500 | ms/batch 25.30 | loss  3.98 | ppl    53.77\n",
            "| epoch  12 |   600/ 2983 batches | lr 0.000500 | ms/batch 25.29 | loss  3.81 | ppl    44.97\n",
            "| epoch  12 |   800/ 2983 batches | lr 0.000500 | ms/batch 25.30 | loss  3.87 | ppl    47.82\n",
            "| epoch  12 |  1000/ 2983 batches | lr 0.000500 | ms/batch 25.27 | loss  3.91 | ppl    49.89\n",
            "| epoch  12 |  1200/ 2983 batches | lr 0.000500 | ms/batch 25.32 | loss  3.90 | ppl    49.44\n",
            "| epoch  12 |  1400/ 2983 batches | lr 0.000500 | ms/batch 25.32 | loss  3.92 | ppl    50.56\n",
            "| epoch  12 |  1600/ 2983 batches | lr 0.000500 | ms/batch 25.30 | loss  4.00 | ppl    54.69\n",
            "| epoch  12 |  1800/ 2983 batches | lr 0.000500 | ms/batch 25.30 | loss  3.93 | ppl    51.05\n",
            "| epoch  12 |  2000/ 2983 batches | lr 0.000500 | ms/batch 25.30 | loss  3.95 | ppl    51.79\n",
            "| epoch  12 |  2200/ 2983 batches | lr 0.000500 | ms/batch 25.33 | loss  3.86 | ppl    47.34\n",
            "| epoch  12 |  2400/ 2983 batches | lr 0.000500 | ms/batch 25.30 | loss  3.86 | ppl    47.47\n",
            "| epoch  12 |  2600/ 2983 batches | lr 0.000500 | ms/batch 25.34 | loss  3.90 | ppl    49.17\n",
            "| epoch  12 |  2800/ 2983 batches | lr 0.000500 | ms/batch 25.31 | loss  3.85 | ppl    47.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 77.59s | valid loss  5.41 | valid ppl   223.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  13 |   200/ 2983 batches | lr 0.000250 | ms/batch 25.48 | loss  3.97 | ppl    52.89\n",
            "| epoch  13 |   400/ 2983 batches | lr 0.000250 | ms/batch 25.30 | loss  3.96 | ppl    52.59\n",
            "| epoch  13 |   600/ 2983 batches | lr 0.000250 | ms/batch 25.32 | loss  3.80 | ppl    44.55\n",
            "| epoch  13 |   800/ 2983 batches | lr 0.000250 | ms/batch 25.29 | loss  3.84 | ppl    46.41\n",
            "| epoch  13 |  1000/ 2983 batches | lr 0.000250 | ms/batch 25.33 | loss  3.88 | ppl    48.25\n",
            "| epoch  13 |  1200/ 2983 batches | lr 0.000250 | ms/batch 25.33 | loss  3.86 | ppl    47.44\n",
            "| epoch  13 |  1400/ 2983 batches | lr 0.000250 | ms/batch 25.30 | loss  3.88 | ppl    48.23\n",
            "| epoch  13 |  1600/ 2983 batches | lr 0.000250 | ms/batch 25.33 | loss  3.95 | ppl    51.90\n",
            "| epoch  13 |  1800/ 2983 batches | lr 0.000250 | ms/batch 25.30 | loss  3.88 | ppl    48.24\n",
            "| epoch  13 |  2000/ 2983 batches | lr 0.000250 | ms/batch 25.30 | loss  3.88 | ppl    48.66\n",
            "| epoch  13 |  2200/ 2983 batches | lr 0.000250 | ms/batch 25.31 | loss  3.80 | ppl    44.51\n",
            "| epoch  13 |  2400/ 2983 batches | lr 0.000250 | ms/batch 25.28 | loss  3.80 | ppl    44.72\n",
            "| epoch  13 |  2600/ 2983 batches | lr 0.000250 | ms/batch 25.30 | loss  3.83 | ppl    46.04\n",
            "| epoch  13 |  2800/ 2983 batches | lr 0.000250 | ms/batch 25.28 | loss  3.79 | ppl    44.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 77.59s | valid loss  5.40 | valid ppl   220.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  14 |   200/ 2983 batches | lr 0.000250 | ms/batch 25.43 | loss  3.92 | ppl    50.32\n",
            "| epoch  14 |   400/ 2983 batches | lr 0.000250 | ms/batch 25.31 | loss  3.92 | ppl    50.64\n",
            "| epoch  14 |   600/ 2983 batches | lr 0.000250 | ms/batch 25.29 | loss  3.76 | ppl    42.74\n",
            "| epoch  14 |   800/ 2983 batches | lr 0.000250 | ms/batch 25.30 | loss  3.79 | ppl    44.42\n",
            "| epoch  14 |  1000/ 2983 batches | lr 0.000250 | ms/batch 25.34 | loss  3.84 | ppl    46.34\n",
            "| epoch  14 |  1200/ 2983 batches | lr 0.000250 | ms/batch 25.31 | loss  3.81 | ppl    45.27\n",
            "| epoch  14 |  1400/ 2983 batches | lr 0.000250 | ms/batch 25.33 | loss  3.84 | ppl    46.48\n",
            "| epoch  14 |  1600/ 2983 batches | lr 0.000250 | ms/batch 25.37 | loss  3.91 | ppl    49.90\n",
            "| epoch  14 |  1800/ 2983 batches | lr 0.000250 | ms/batch 25.37 | loss  3.85 | ppl    46.76\n",
            "| epoch  14 |  2000/ 2983 batches | lr 0.000250 | ms/batch 25.36 | loss  3.85 | ppl    46.94\n",
            "| epoch  14 |  2200/ 2983 batches | lr 0.000250 | ms/batch 25.32 | loss  3.77 | ppl    43.30\n",
            "| epoch  14 |  2400/ 2983 batches | lr 0.000250 | ms/batch 25.31 | loss  3.77 | ppl    43.48\n",
            "| epoch  14 |  2600/ 2983 batches | lr 0.000250 | ms/batch 25.30 | loss  3.81 | ppl    45.12\n",
            "| epoch  14 |  2800/ 2983 batches | lr 0.000250 | ms/batch 25.32 | loss  3.76 | ppl    43.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 77.61s | valid loss  5.40 | valid ppl   221.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  15 |   200/ 2983 batches | lr 0.000250 | ms/batch 25.44 | loss  3.88 | ppl    48.34\n",
            "| epoch  15 |   400/ 2983 batches | lr 0.000250 | ms/batch 25.28 | loss  3.89 | ppl    48.81\n",
            "| epoch  15 |   600/ 2983 batches | lr 0.000250 | ms/batch 25.29 | loss  3.72 | ppl    41.20\n",
            "| epoch  15 |   800/ 2983 batches | lr 0.000250 | ms/batch 25.31 | loss  3.76 | ppl    42.80\n",
            "| epoch  15 |  1000/ 2983 batches | lr 0.000250 | ms/batch 25.28 | loss  3.80 | ppl    44.74\n",
            "| epoch  15 |  1200/ 2983 batches | lr 0.000250 | ms/batch 25.31 | loss  3.79 | ppl    44.24\n",
            "| epoch  15 |  1400/ 2983 batches | lr 0.000250 | ms/batch 25.32 | loss  3.81 | ppl    44.99\n",
            "| epoch  15 |  1600/ 2983 batches | lr 0.000250 | ms/batch 25.31 | loss  3.88 | ppl    48.61\n",
            "| epoch  15 |  1800/ 2983 batches | lr 0.000250 | ms/batch 25.33 | loss  3.81 | ppl    45.35\n",
            "| epoch  15 |  2000/ 2983 batches | lr 0.000250 | ms/batch 25.31 | loss  3.82 | ppl    45.51\n",
            "| epoch  15 |  2200/ 2983 batches | lr 0.000250 | ms/batch 25.33 | loss  3.73 | ppl    41.88\n",
            "| epoch  15 |  2400/ 2983 batches | lr 0.000250 | ms/batch 25.28 | loss  3.74 | ppl    42.09\n",
            "| epoch  15 |  2600/ 2983 batches | lr 0.000250 | ms/batch 25.30 | loss  3.77 | ppl    43.56\n",
            "| epoch  15 |  2800/ 2983 batches | lr 0.000250 | ms/batch 25.28 | loss  3.74 | ppl    41.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 77.54s | valid loss  5.41 | valid ppl   223.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  16 |   200/ 2983 batches | lr 0.000125 | ms/batch 25.40 | loss  3.91 | ppl    49.88\n",
            "| epoch  16 |   400/ 2983 batches | lr 0.000125 | ms/batch 25.28 | loss  3.89 | ppl    49.07\n",
            "| epoch  16 |   600/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.75 | ppl    42.57\n",
            "| epoch  16 |   800/ 2983 batches | lr 0.000125 | ms/batch 25.27 | loss  3.76 | ppl    43.02\n",
            "| epoch  16 |  1000/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.80 | ppl    44.67\n",
            "| epoch  16 |  1200/ 2983 batches | lr 0.000125 | ms/batch 25.30 | loss  3.79 | ppl    44.29\n",
            "| epoch  16 |  1400/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.80 | ppl    44.62\n",
            "| epoch  16 |  1600/ 2983 batches | lr 0.000125 | ms/batch 25.31 | loss  3.87 | ppl    48.12\n",
            "| epoch  16 |  1800/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.80 | ppl    44.71\n",
            "| epoch  16 |  2000/ 2983 batches | lr 0.000125 | ms/batch 25.34 | loss  3.81 | ppl    45.35\n",
            "| epoch  16 |  2200/ 2983 batches | lr 0.000125 | ms/batch 25.32 | loss  3.72 | ppl    41.25\n",
            "| epoch  16 |  2400/ 2983 batches | lr 0.000125 | ms/batch 25.32 | loss  3.73 | ppl    41.75\n",
            "| epoch  16 |  2600/ 2983 batches | lr 0.000125 | ms/batch 25.32 | loss  3.77 | ppl    43.41\n",
            "| epoch  16 |  2800/ 2983 batches | lr 0.000125 | ms/batch 25.31 | loss  3.71 | ppl    40.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 77.55s | valid loss  5.37 | valid ppl   214.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  17 |   200/ 2983 batches | lr 0.000125 | ms/batch 25.42 | loss  3.88 | ppl    48.52\n",
            "| epoch  17 |   400/ 2983 batches | lr 0.000125 | ms/batch 25.30 | loss  3.87 | ppl    47.92\n",
            "| epoch  17 |   600/ 2983 batches | lr 0.000125 | ms/batch 25.27 | loss  3.71 | ppl    41.02\n",
            "| epoch  17 |   800/ 2983 batches | lr 0.000125 | ms/batch 25.28 | loss  3.74 | ppl    41.94\n",
            "| epoch  17 |  1000/ 2983 batches | lr 0.000125 | ms/batch 25.28 | loss  3.79 | ppl    44.09\n",
            "| epoch  17 |  1200/ 2983 batches | lr 0.000125 | ms/batch 25.28 | loss  3.76 | ppl    43.03\n",
            "| epoch  17 |  1400/ 2983 batches | lr 0.000125 | ms/batch 25.33 | loss  3.78 | ppl    43.82\n",
            "| epoch  17 |  1600/ 2983 batches | lr 0.000125 | ms/batch 25.30 | loss  3.86 | ppl    47.35\n",
            "| epoch  17 |  1800/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.78 | ppl    43.63\n",
            "| epoch  17 |  2000/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.80 | ppl    44.80\n",
            "| epoch  17 |  2200/ 2983 batches | lr 0.000125 | ms/batch 25.30 | loss  3.70 | ppl    40.49\n",
            "| epoch  17 |  2400/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.72 | ppl    41.06\n",
            "| epoch  17 |  2600/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.76 | ppl    42.98\n",
            "| epoch  17 |  2800/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.69 | ppl    40.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 77.50s | valid loss  5.37 | valid ppl   215.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  18 |   200/ 2983 batches | lr 0.000125 | ms/batch 25.40 | loss  3.86 | ppl    47.23\n",
            "| epoch  18 |   400/ 2983 batches | lr 0.000125 | ms/batch 25.27 | loss  3.85 | ppl    47.02\n",
            "| epoch  18 |   600/ 2983 batches | lr 0.000125 | ms/batch 25.31 | loss  3.68 | ppl    39.72\n",
            "| epoch  18 |   800/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.72 | ppl    41.20\n",
            "| epoch  18 |  1000/ 2983 batches | lr 0.000125 | ms/batch 25.27 | loss  3.76 | ppl    42.94\n",
            "| epoch  18 |  1200/ 2983 batches | lr 0.000125 | ms/batch 25.31 | loss  3.74 | ppl    42.27\n",
            "| epoch  18 |  1400/ 2983 batches | lr 0.000125 | ms/batch 25.31 | loss  3.75 | ppl    42.69\n",
            "| epoch  18 |  1600/ 2983 batches | lr 0.000125 | ms/batch 25.32 | loss  3.84 | ppl    46.34\n",
            "| epoch  18 |  1800/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.76 | ppl    42.91\n",
            "| epoch  18 |  2000/ 2983 batches | lr 0.000125 | ms/batch 25.30 | loss  3.79 | ppl    44.09\n",
            "| epoch  18 |  2200/ 2983 batches | lr 0.000125 | ms/batch 25.32 | loss  3.69 | ppl    39.99\n",
            "| epoch  18 |  2400/ 2983 batches | lr 0.000125 | ms/batch 25.29 | loss  3.69 | ppl    39.98\n",
            "| epoch  18 |  2600/ 2983 batches | lr 0.000125 | ms/batch 25.30 | loss  3.74 | ppl    42.16\n",
            "| epoch  18 |  2800/ 2983 batches | lr 0.000125 | ms/batch 25.30 | loss  3.68 | ppl    39.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 77.55s | valid loss  5.37 | valid ppl   215.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  19 |   200/ 2983 batches | lr 0.000063 | ms/batch 25.43 | loss  3.87 | ppl    48.05\n",
            "| epoch  19 |   400/ 2983 batches | lr 0.000063 | ms/batch 25.32 | loss  3.90 | ppl    49.56\n",
            "| epoch  19 |   600/ 2983 batches | lr 0.000063 | ms/batch 25.31 | loss  3.74 | ppl    42.05\n",
            "| epoch  19 |   800/ 2983 batches | lr 0.000063 | ms/batch 25.29 | loss  3.74 | ppl    42.06\n",
            "| epoch  19 |  1000/ 2983 batches | lr 0.000063 | ms/batch 25.28 | loss  3.79 | ppl    44.05\n",
            "| epoch  19 |  1200/ 2983 batches | lr 0.000063 | ms/batch 25.28 | loss  3.77 | ppl    43.38\n",
            "| epoch  19 |  1400/ 2983 batches | lr 0.000063 | ms/batch 25.30 | loss  3.79 | ppl    44.28\n",
            "| epoch  19 |  1600/ 2983 batches | lr 0.000063 | ms/batch 25.29 | loss  3.86 | ppl    47.40\n",
            "| epoch  19 |  1800/ 2983 batches | lr 0.000063 | ms/batch 25.30 | loss  3.77 | ppl    43.47\n",
            "| epoch  19 |  2000/ 2983 batches | lr 0.000063 | ms/batch 25.31 | loss  3.80 | ppl    44.59\n",
            "| epoch  19 |  2200/ 2983 batches | lr 0.000063 | ms/batch 25.29 | loss  3.72 | ppl    41.27\n",
            "| epoch  19 |  2400/ 2983 batches | lr 0.000063 | ms/batch 25.32 | loss  3.71 | ppl    40.89\n",
            "| epoch  19 |  2600/ 2983 batches | lr 0.000063 | ms/batch 25.30 | loss  3.75 | ppl    42.35\n",
            "| epoch  19 |  2800/ 2983 batches | lr 0.000063 | ms/batch 25.30 | loss  3.69 | ppl    40.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 77.53s | valid loss  5.30 | valid ppl   201.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  20 |   200/ 2983 batches | lr 0.000063 | ms/batch 25.42 | loss  3.87 | ppl    47.93\n",
            "| epoch  20 |   400/ 2983 batches | lr 0.000063 | ms/batch 25.30 | loss  3.87 | ppl    47.77\n",
            "| epoch  20 |   600/ 2983 batches | lr 0.000063 | ms/batch 25.29 | loss  3.72 | ppl    41.37\n",
            "| epoch  20 |   800/ 2983 batches | lr 0.000063 | ms/batch 25.32 | loss  3.71 | ppl    41.06\n",
            "| epoch  20 |  1000/ 2983 batches | lr 0.000063 | ms/batch 25.30 | loss  3.77 | ppl    43.33\n",
            "| epoch  20 |  1200/ 2983 batches | lr 0.000063 | ms/batch 25.29 | loss  3.74 | ppl    42.20\n",
            "| epoch  20 |  1400/ 2983 batches | lr 0.000063 | ms/batch 25.31 | loss  3.79 | ppl    44.40\n",
            "| epoch  20 |  1600/ 2983 batches | lr 0.000063 | ms/batch 25.31 | loss  3.84 | ppl    46.49\n",
            "| epoch  20 |  1800/ 2983 batches | lr 0.000063 | ms/batch 25.34 | loss  3.77 | ppl    43.36\n",
            "| epoch  20 |  2000/ 2983 batches | lr 0.000063 | ms/batch 25.32 | loss  3.79 | ppl    44.26\n",
            "| epoch  20 |  2200/ 2983 batches | lr 0.000063 | ms/batch 25.32 | loss  3.71 | ppl    41.00\n",
            "| epoch  20 |  2400/ 2983 batches | lr 0.000063 | ms/batch 25.29 | loss  3.71 | ppl    40.78\n",
            "| epoch  20 |  2600/ 2983 batches | lr 0.000063 | ms/batch 25.32 | loss  3.73 | ppl    41.84\n",
            "| epoch  20 |  2800/ 2983 batches | lr 0.000063 | ms/batch 25.32 | loss  3.68 | ppl    39.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 77.60s | valid loss  5.31 | valid ppl   202.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  21 |   200/ 2983 batches | lr 0.000063 | ms/batch 25.50 | loss  3.85 | ppl    46.96\n",
            "| epoch  21 |   400/ 2983 batches | lr 0.000063 | ms/batch 25.30 | loss  3.83 | ppl    46.23\n",
            "| epoch  21 |   600/ 2983 batches | lr 0.000063 | ms/batch 25.33 | loss  3.69 | ppl    40.04\n",
            "| epoch  21 |   800/ 2983 batches | lr 0.000063 | ms/batch 25.31 | loss  3.70 | ppl    40.58\n",
            "| epoch  21 |  1000/ 2983 batches | lr 0.000063 | ms/batch 25.30 | loss  3.76 | ppl    42.82\n",
            "| epoch  21 |  1200/ 2983 batches | lr 0.000063 | ms/batch 25.34 | loss  3.74 | ppl    42.02\n",
            "| epoch  21 |  1400/ 2983 batches | lr 0.000063 | ms/batch 25.29 | loss  3.77 | ppl    43.54\n",
            "| epoch  21 |  1600/ 2983 batches | lr 0.000063 | ms/batch 25.31 | loss  3.83 | ppl    45.96\n",
            "| epoch  21 |  1800/ 2983 batches | lr 0.000063 | ms/batch 25.30 | loss  3.76 | ppl    42.85\n",
            "| epoch  21 |  2000/ 2983 batches | lr 0.000063 | ms/batch 25.29 | loss  3.77 | ppl    43.55\n",
            "| epoch  21 |  2200/ 2983 batches | lr 0.000063 | ms/batch 25.29 | loss  3.70 | ppl    40.29\n",
            "| epoch  21 |  2400/ 2983 batches | lr 0.000063 | ms/batch 25.29 | loss  3.69 | ppl    39.99\n",
            "| epoch  21 |  2600/ 2983 batches | lr 0.000063 | ms/batch 25.31 | loss  3.73 | ppl    41.77\n",
            "| epoch  21 |  2800/ 2983 batches | lr 0.000063 | ms/batch 25.30 | loss  3.67 | ppl    39.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 77.58s | valid loss  5.33 | valid ppl   205.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  22 |   200/ 2983 batches | lr 0.000031 | ms/batch 25.42 | loss  3.87 | ppl    48.14\n",
            "| epoch  22 |   400/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.91 | ppl    49.84\n",
            "| epoch  22 |   600/ 2983 batches | lr 0.000031 | ms/batch 25.32 | loss  3.76 | ppl    42.95\n",
            "| epoch  22 |   800/ 2983 batches | lr 0.000031 | ms/batch 25.36 | loss  3.73 | ppl    41.72\n",
            "| epoch  22 |  1000/ 2983 batches | lr 0.000031 | ms/batch 25.34 | loss  3.79 | ppl    44.27\n",
            "| epoch  22 |  1200/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.77 | ppl    43.48\n",
            "| epoch  22 |  1400/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.85 | ppl    46.88\n",
            "| epoch  22 |  1600/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.86 | ppl    47.57\n",
            "| epoch  22 |  1800/ 2983 batches | lr 0.000031 | ms/batch 25.32 | loss  3.81 | ppl    45.06\n",
            "| epoch  22 |  2000/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.80 | ppl    44.64\n",
            "| epoch  22 |  2200/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.76 | ppl    42.76\n",
            "| epoch  22 |  2400/ 2983 batches | lr 0.000031 | ms/batch 25.33 | loss  3.75 | ppl    42.36\n",
            "| epoch  22 |  2600/ 2983 batches | lr 0.000031 | ms/batch 25.32 | loss  3.74 | ppl    41.99\n",
            "| epoch  22 |  2800/ 2983 batches | lr 0.000031 | ms/batch 25.32 | loss  3.72 | ppl    41.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 77.59s | valid loss  5.28 | valid ppl   195.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  23 |   200/ 2983 batches | lr 0.000031 | ms/batch 25.41 | loss  3.88 | ppl    48.55\n",
            "| epoch  23 |   400/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.91 | ppl    49.80\n",
            "| epoch  23 |   600/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.73 | ppl    41.85\n",
            "| epoch  23 |   800/ 2983 batches | lr 0.000031 | ms/batch 25.32 | loss  3.72 | ppl    41.20\n",
            "| epoch  23 |  1000/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.80 | ppl    44.48\n",
            "| epoch  23 |  1200/ 2983 batches | lr 0.000031 | ms/batch 25.32 | loss  3.75 | ppl    42.35\n",
            "| epoch  23 |  1400/ 2983 batches | lr 0.000031 | ms/batch 25.33 | loss  3.82 | ppl    45.77\n",
            "| epoch  23 |  1600/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.84 | ppl    46.72\n",
            "| epoch  23 |  1800/ 2983 batches | lr 0.000031 | ms/batch 25.32 | loss  3.79 | ppl    44.39\n",
            "| epoch  23 |  2000/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.78 | ppl    43.80\n",
            "| epoch  23 |  2200/ 2983 batches | lr 0.000031 | ms/batch 25.33 | loss  3.74 | ppl    41.90\n",
            "| epoch  23 |  2400/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.72 | ppl    41.40\n",
            "| epoch  23 |  2600/ 2983 batches | lr 0.000031 | ms/batch 25.32 | loss  3.74 | ppl    42.02\n",
            "| epoch  23 |  2800/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.70 | ppl    40.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 77.57s | valid loss  5.27 | valid ppl   195.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  24 |   200/ 2983 batches | lr 0.000031 | ms/batch 25.42 | loss  3.87 | ppl    48.01\n",
            "| epoch  24 |   400/ 2983 batches | lr 0.000031 | ms/batch 25.39 | loss  3.89 | ppl    49.04\n",
            "| epoch  24 |   600/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.72 | ppl    41.15\n",
            "| epoch  24 |   800/ 2983 batches | lr 0.000031 | ms/batch 25.33 | loss  3.71 | ppl    40.79\n",
            "| epoch  24 |  1000/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.78 | ppl    44.03\n",
            "| epoch  24 |  1200/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.74 | ppl    42.16\n",
            "| epoch  24 |  1400/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.82 | ppl    45.67\n",
            "| epoch  24 |  1600/ 2983 batches | lr 0.000031 | ms/batch 25.33 | loss  3.84 | ppl    46.45\n",
            "| epoch  24 |  1800/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.79 | ppl    44.15\n",
            "| epoch  24 |  2000/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.78 | ppl    43.84\n",
            "| epoch  24 |  2200/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.71 | ppl    40.77\n",
            "| epoch  24 |  2400/ 2983 batches | lr 0.000031 | ms/batch 25.34 | loss  3.72 | ppl    41.18\n",
            "| epoch  24 |  2600/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.73 | ppl    41.79\n",
            "| epoch  24 |  2800/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.69 | ppl    39.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 77.61s | valid loss  5.27 | valid ppl   193.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  25 |   200/ 2983 batches | lr 0.000031 | ms/batch 25.39 | loss  3.88 | ppl    48.18\n",
            "| epoch  25 |   400/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.87 | ppl    48.12\n",
            "| epoch  25 |   600/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.71 | ppl    40.99\n",
            "| epoch  25 |   800/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.70 | ppl    40.40\n",
            "| epoch  25 |  1000/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.77 | ppl    43.59\n",
            "| epoch  25 |  1200/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.74 | ppl    41.98\n",
            "| epoch  25 |  1400/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.81 | ppl    45.19\n",
            "| epoch  25 |  1600/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.83 | ppl    45.98\n",
            "| epoch  25 |  1800/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.77 | ppl    43.33\n",
            "| epoch  25 |  2000/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.77 | ppl    43.56\n",
            "| epoch  25 |  2200/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.70 | ppl    40.61\n",
            "| epoch  25 |  2400/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.71 | ppl    40.91\n",
            "| epoch  25 |  2600/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.73 | ppl    41.51\n",
            "| epoch  25 |  2800/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.68 | ppl    39.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 77.51s | valid loss  5.26 | valid ppl   192.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  26 |   200/ 2983 batches | lr 0.000031 | ms/batch 25.42 | loss  3.87 | ppl    47.80\n",
            "| epoch  26 |   400/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.86 | ppl    47.43\n",
            "| epoch  26 |   600/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.69 | ppl    40.17\n",
            "| epoch  26 |   800/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.70 | ppl    40.40\n",
            "| epoch  26 |  1000/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.77 | ppl    43.56\n",
            "| epoch  26 |  1200/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.75 | ppl    42.39\n",
            "| epoch  26 |  1400/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.80 | ppl    44.80\n",
            "| epoch  26 |  1600/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.82 | ppl    45.44\n",
            "| epoch  26 |  1800/ 2983 batches | lr 0.000031 | ms/batch 25.27 | loss  3.77 | ppl    43.28\n",
            "| epoch  26 |  2000/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.76 | ppl    43.03\n",
            "| epoch  26 |  2200/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.69 | ppl    39.91\n",
            "| epoch  26 |  2400/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.70 | ppl    40.58\n",
            "| epoch  26 |  2600/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.72 | ppl    41.39\n",
            "| epoch  26 |  2800/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.67 | ppl    39.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 77.53s | valid loss  5.26 | valid ppl   193.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  27 |   200/ 2983 batches | lr 0.000031 | ms/batch 25.43 | loss  3.85 | ppl    47.20\n",
            "| epoch  27 |   400/ 2983 batches | lr 0.000031 | ms/batch 25.32 | loss  3.86 | ppl    47.33\n",
            "| epoch  27 |   600/ 2983 batches | lr 0.000031 | ms/batch 25.26 | loss  3.68 | ppl    39.64\n",
            "| epoch  27 |   800/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.69 | ppl    39.88\n",
            "| epoch  27 |  1000/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.77 | ppl    43.20\n",
            "| epoch  27 |  1200/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.72 | ppl    41.18\n",
            "| epoch  27 |  1400/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.80 | ppl    44.87\n",
            "| epoch  27 |  1600/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.81 | ppl    45.19\n",
            "| epoch  27 |  1800/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.75 | ppl    42.47\n",
            "| epoch  27 |  2000/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.76 | ppl    43.12\n",
            "| epoch  27 |  2200/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.68 | ppl    39.75\n",
            "| epoch  27 |  2400/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.69 | ppl    39.88\n",
            "| epoch  27 |  2600/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.71 | ppl    40.90\n",
            "| epoch  27 |  2800/ 2983 batches | lr 0.000031 | ms/batch 25.33 | loss  3.67 | ppl    39.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 77.51s | valid loss  5.26 | valid ppl   192.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  28 |   200/ 2983 batches | lr 0.000031 | ms/batch 25.43 | loss  3.86 | ppl    47.41\n",
            "| epoch  28 |   400/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.85 | ppl    46.76\n",
            "| epoch  28 |   600/ 2983 batches | lr 0.000031 | ms/batch 25.27 | loss  3.67 | ppl    39.34\n",
            "| epoch  28 |   800/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.68 | ppl    39.65\n",
            "| epoch  28 |  1000/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.75 | ppl    42.63\n",
            "| epoch  28 |  1200/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.72 | ppl    41.12\n",
            "| epoch  28 |  1400/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.80 | ppl    44.70\n",
            "| epoch  28 |  1600/ 2983 batches | lr 0.000031 | ms/batch 25.27 | loss  3.80 | ppl    44.82\n",
            "| epoch  28 |  1800/ 2983 batches | lr 0.000031 | ms/batch 25.30 | loss  3.75 | ppl    42.34\n",
            "| epoch  28 |  2000/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.75 | ppl    42.57\n",
            "| epoch  28 |  2200/ 2983 batches | lr 0.000031 | ms/batch 25.31 | loss  3.66 | ppl    38.82\n",
            "| epoch  28 |  2400/ 2983 batches | lr 0.000031 | ms/batch 25.28 | loss  3.69 | ppl    39.86\n",
            "| epoch  28 |  2600/ 2983 batches | lr 0.000031 | ms/batch 25.33 | loss  3.71 | ppl    40.74\n",
            "| epoch  28 |  2800/ 2983 batches | lr 0.000031 | ms/batch 25.29 | loss  3.66 | ppl    39.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 77.53s | valid loss  5.27 | valid ppl   194.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  29 |   200/ 2983 batches | lr 0.000016 | ms/batch 25.41 | loss  3.87 | ppl    47.79\n",
            "| epoch  29 |   400/ 2983 batches | lr 0.000016 | ms/batch 25.27 | loss  3.93 | ppl    50.78\n",
            "| epoch  29 |   600/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.73 | ppl    41.57\n",
            "| epoch  29 |   800/ 2983 batches | lr 0.000016 | ms/batch 25.26 | loss  3.74 | ppl    41.92\n",
            "| epoch  29 |  1000/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.87 | ppl    48.16\n",
            "| epoch  29 |  1200/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.75 | ppl    42.45\n",
            "| epoch  29 |  1400/ 2983 batches | lr 0.000016 | ms/batch 25.32 | loss  3.92 | ppl    50.41\n",
            "| epoch  29 |  1600/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.86 | ppl    47.52\n",
            "| epoch  29 |  1800/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.81 | ppl    45.31\n",
            "| epoch  29 |  2000/ 2983 batches | lr 0.000016 | ms/batch 25.32 | loss  3.83 | ppl    46.11\n",
            "| epoch  29 |  2200/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.75 | ppl    42.45\n",
            "| epoch  29 |  2400/ 2983 batches | lr 0.000016 | ms/batch 25.33 | loss  3.71 | ppl    40.79\n",
            "| epoch  29 |  2600/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.74 | ppl    42.03\n",
            "| epoch  29 |  2800/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.71 | ppl    41.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 77.52s | valid loss  5.21 | valid ppl   183.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  30 |   200/ 2983 batches | lr 0.000016 | ms/batch 25.42 | loss  3.89 | ppl    48.97\n",
            "| epoch  30 |   400/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.92 | ppl    50.61\n",
            "| epoch  30 |   600/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.72 | ppl    41.11\n",
            "| epoch  30 |   800/ 2983 batches | lr 0.000016 | ms/batch 25.32 | loss  3.74 | ppl    42.08\n",
            "| epoch  30 |  1000/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.82 | ppl    45.56\n",
            "| epoch  30 |  1200/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.73 | ppl    41.52\n",
            "| epoch  30 |  1400/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.89 | ppl    49.00\n",
            "| epoch  30 |  1600/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.82 | ppl    45.75\n",
            "| epoch  30 |  1800/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.79 | ppl    44.26\n",
            "| epoch  30 |  2000/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.81 | ppl    45.15\n",
            "| epoch  30 |  2200/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.73 | ppl    41.61\n",
            "| epoch  30 |  2400/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.71 | ppl    40.68\n",
            "| epoch  30 |  2600/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.73 | ppl    41.69\n",
            "| epoch  30 |  2800/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.70 | ppl    40.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 77.52s | valid loss  5.21 | valid ppl   183.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  31 |   200/ 2983 batches | lr 0.000016 | ms/batch 25.42 | loss  3.87 | ppl    48.07\n",
            "| epoch  31 |   400/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.94 | ppl    51.27\n",
            "| epoch  31 |   600/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.71 | ppl    40.70\n",
            "| epoch  31 |   800/ 2983 batches | lr 0.000016 | ms/batch 25.27 | loss  3.74 | ppl    42.16\n",
            "| epoch  31 |  1000/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.79 | ppl    44.04\n",
            "| epoch  31 |  1200/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.74 | ppl    41.96\n",
            "| epoch  31 |  1400/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.88 | ppl    48.34\n",
            "| epoch  31 |  1600/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.84 | ppl    46.45\n",
            "| epoch  31 |  1800/ 2983 batches | lr 0.000016 | ms/batch 25.27 | loss  3.79 | ppl    44.13\n",
            "| epoch  31 |  2000/ 2983 batches | lr 0.000016 | ms/batch 25.32 | loss  3.79 | ppl    44.19\n",
            "| epoch  31 |  2200/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.73 | ppl    41.51\n",
            "| epoch  31 |  2400/ 2983 batches | lr 0.000016 | ms/batch 25.27 | loss  3.70 | ppl    40.42\n",
            "| epoch  31 |  2600/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.72 | ppl    41.33\n",
            "| epoch  31 |  2800/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.69 | ppl    40.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 77.52s | valid loss  5.20 | valid ppl   182.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  32 |   200/ 2983 batches | lr 0.000016 | ms/batch 25.40 | loss  3.87 | ppl    47.97\n",
            "| epoch  32 |   400/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.92 | ppl    50.48\n",
            "| epoch  32 |   600/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.69 | ppl    40.12\n",
            "| epoch  32 |   800/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.72 | ppl    41.36\n",
            "| epoch  32 |  1000/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.80 | ppl    44.89\n",
            "| epoch  32 |  1200/ 2983 batches | lr 0.000016 | ms/batch 25.27 | loss  3.74 | ppl    41.91\n",
            "| epoch  32 |  1400/ 2983 batches | lr 0.000016 | ms/batch 25.32 | loss  3.87 | ppl    47.84\n",
            "| epoch  32 |  1600/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.82 | ppl    45.41\n",
            "| epoch  32 |  1800/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.77 | ppl    43.47\n",
            "| epoch  32 |  2000/ 2983 batches | lr 0.000016 | ms/batch 25.33 | loss  3.78 | ppl    43.91\n",
            "| epoch  32 |  2200/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.71 | ppl    41.05\n",
            "| epoch  32 |  2400/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.69 | ppl    40.06\n",
            "| epoch  32 |  2600/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.72 | ppl    41.36\n",
            "| epoch  32 |  2800/ 2983 batches | lr 0.000016 | ms/batch 25.33 | loss  3.68 | ppl    39.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 77.55s | valid loss  5.21 | valid ppl   183.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  33 |   200/ 2983 batches | lr 0.000016 | ms/batch 25.45 | loss  3.87 | ppl    47.82\n",
            "| epoch  33 |   400/ 2983 batches | lr 0.000016 | ms/batch 25.34 | loss  3.91 | ppl    49.90\n",
            "| epoch  33 |   600/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.69 | ppl    39.95\n",
            "| epoch  33 |   800/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.72 | ppl    41.09\n",
            "| epoch  33 |  1000/ 2983 batches | lr 0.000016 | ms/batch 25.26 | loss  3.83 | ppl    46.24\n",
            "| epoch  33 |  1200/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.74 | ppl    41.89\n",
            "| epoch  33 |  1400/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.87 | ppl    47.80\n",
            "| epoch  33 |  1600/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.83 | ppl    45.84\n",
            "| epoch  33 |  1800/ 2983 batches | lr 0.000016 | ms/batch 25.27 | loss  3.78 | ppl    43.62\n",
            "| epoch  33 |  2000/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.77 | ppl    43.29\n",
            "| epoch  33 |  2200/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.70 | ppl    40.53\n",
            "| epoch  33 |  2400/ 2983 batches | lr 0.000016 | ms/batch 25.28 | loss  3.69 | ppl    40.06\n",
            "| epoch  33 |  2600/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.71 | ppl    40.81\n",
            "| epoch  33 |  2800/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.68 | ppl    39.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 77.53s | valid loss  5.22 | valid ppl   184.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  34 |   200/ 2983 batches | lr 0.000016 | ms/batch 25.43 | loss  3.86 | ppl    47.31\n",
            "| epoch  34 |   400/ 2983 batches | lr 0.000016 | ms/batch 25.27 | loss  3.90 | ppl    49.57\n",
            "| epoch  34 |   600/ 2983 batches | lr 0.000016 | ms/batch 25.32 | loss  3.69 | ppl    39.95\n",
            "| epoch  34 |   800/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.70 | ppl    40.47\n",
            "| epoch  34 |  1000/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.83 | ppl    45.90\n",
            "| epoch  34 |  1200/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.72 | ppl    41.20\n",
            "| epoch  34 |  1400/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.86 | ppl    47.24\n",
            "| epoch  34 |  1600/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.82 | ppl    45.66\n",
            "| epoch  34 |  1800/ 2983 batches | lr 0.000016 | ms/batch 25.29 | loss  3.78 | ppl    43.62\n",
            "| epoch  34 |  2000/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.77 | ppl    43.33\n",
            "| epoch  34 |  2200/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.70 | ppl    40.41\n",
            "| epoch  34 |  2400/ 2983 batches | lr 0.000016 | ms/batch 25.31 | loss  3.68 | ppl    39.75\n",
            "| epoch  34 |  2600/ 2983 batches | lr 0.000016 | ms/batch 25.32 | loss  3.71 | ppl    40.79\n",
            "| epoch  34 |  2800/ 2983 batches | lr 0.000016 | ms/batch 25.30 | loss  3.67 | ppl    39.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 77.54s | valid loss  5.21 | valid ppl   183.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  35 |   200/ 2983 batches | lr 0.000008 | ms/batch 25.42 | loss  3.88 | ppl    48.50\n",
            "| epoch  35 |   400/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.98 | ppl    53.46\n",
            "| epoch  35 |   600/ 2983 batches | lr 0.000008 | ms/batch 25.27 | loss  3.74 | ppl    42.27\n",
            "| epoch  35 |   800/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.77 | ppl    43.31\n",
            "| epoch  35 |  1000/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.89 | ppl    48.85\n",
            "| epoch  35 |  1200/ 2983 batches | lr 0.000008 | ms/batch 25.27 | loss  3.79 | ppl    44.32\n",
            "| epoch  35 |  1400/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.96 | ppl    52.38\n",
            "| epoch  35 |  1600/ 2983 batches | lr 0.000008 | ms/batch 25.27 | loss  3.93 | ppl    50.84\n",
            "| epoch  35 |  1800/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.83 | ppl    46.01\n",
            "| epoch  35 |  2000/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.84 | ppl    46.45\n",
            "| epoch  35 |  2200/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.73 | ppl    41.85\n",
            "| epoch  35 |  2400/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.73 | ppl    41.57\n",
            "| epoch  35 |  2600/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.73 | ppl    41.87\n",
            "| epoch  35 |  2800/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.72 | ppl    41.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 77.52s | valid loss  5.20 | valid ppl   180.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  36 |   200/ 2983 batches | lr 0.000008 | ms/batch 25.44 | loss  3.98 | ppl    53.56\n",
            "| epoch  36 |   400/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.91 | ppl    49.68\n",
            "| epoch  36 |   600/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.76 | ppl    42.96\n",
            "| epoch  36 |   800/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.78 | ppl    43.63\n",
            "| epoch  36 |  1000/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.91 | ppl    49.99\n",
            "| epoch  36 |  1200/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.77 | ppl    43.45\n",
            "| epoch  36 |  1400/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.95 | ppl    52.14\n",
            "| epoch  36 |  1600/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.92 | ppl    50.56\n",
            "| epoch  36 |  1800/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.81 | ppl    45.29\n",
            "| epoch  36 |  2000/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.83 | ppl    46.12\n",
            "| epoch  36 |  2200/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.73 | ppl    41.74\n",
            "| epoch  36 |  2400/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.72 | ppl    41.13\n",
            "| epoch  36 |  2600/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.75 | ppl    42.51\n",
            "| epoch  36 |  2800/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.72 | ppl    41.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 77.56s | valid loss  5.19 | valid ppl   180.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  37 |   200/ 2983 batches | lr 0.000008 | ms/batch 25.43 | loss  3.96 | ppl    52.72\n",
            "| epoch  37 |   400/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.90 | ppl    49.40\n",
            "| epoch  37 |   600/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.73 | ppl    41.81\n",
            "| epoch  37 |   800/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.78 | ppl    43.69\n",
            "| epoch  37 |  1000/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.91 | ppl    50.04\n",
            "| epoch  37 |  1200/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.76 | ppl    43.07\n",
            "| epoch  37 |  1400/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.96 | ppl    52.56\n",
            "| epoch  37 |  1600/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.91 | ppl    49.90\n",
            "| epoch  37 |  1800/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.82 | ppl    45.68\n",
            "| epoch  37 |  2000/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.83 | ppl    46.13\n",
            "| epoch  37 |  2200/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.72 | ppl    41.08\n",
            "| epoch  37 |  2400/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.71 | ppl    40.82\n",
            "| epoch  37 |  2600/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.74 | ppl    42.27\n",
            "| epoch  37 |  2800/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.70 | ppl    40.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 77.53s | valid loss  5.19 | valid ppl   180.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  38 |   200/ 2983 batches | lr 0.000008 | ms/batch 25.42 | loss  3.95 | ppl    51.98\n",
            "| epoch  38 |   400/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.94 | ppl    51.55\n",
            "| epoch  38 |   600/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.71 | ppl    40.83\n",
            "| epoch  38 |   800/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.76 | ppl    42.93\n",
            "| epoch  38 |  1000/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.91 | ppl    49.80\n",
            "| epoch  38 |  1200/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.76 | ppl    42.83\n",
            "| epoch  38 |  1400/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.95 | ppl    51.77\n",
            "| epoch  38 |  1600/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.90 | ppl    49.18\n",
            "| epoch  38 |  1800/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.80 | ppl    44.74\n",
            "| epoch  38 |  2000/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.83 | ppl    46.06\n",
            "| epoch  38 |  2200/ 2983 batches | lr 0.000008 | ms/batch 25.34 | loss  3.72 | ppl    41.42\n",
            "| epoch  38 |  2400/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.70 | ppl    40.57\n",
            "| epoch  38 |  2600/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.74 | ppl    42.17\n",
            "| epoch  38 |  2800/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.70 | ppl    40.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 77.56s | valid loss  5.19 | valid ppl   178.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  39 |   200/ 2983 batches | lr 0.000008 | ms/batch 25.43 | loss  3.96 | ppl    52.25\n",
            "| epoch  39 |   400/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.91 | ppl    49.98\n",
            "| epoch  39 |   600/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.71 | ppl    40.74\n",
            "| epoch  39 |   800/ 2983 batches | lr 0.000008 | ms/batch 25.27 | loss  3.77 | ppl    43.28\n",
            "| epoch  39 |  1000/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.91 | ppl    49.96\n",
            "| epoch  39 |  1200/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.75 | ppl    42.37\n",
            "| epoch  39 |  1400/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.95 | ppl    51.70\n",
            "| epoch  39 |  1600/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.90 | ppl    49.30\n",
            "| epoch  39 |  1800/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.81 | ppl    44.95\n",
            "| epoch  39 |  2000/ 2983 batches | lr 0.000008 | ms/batch 25.33 | loss  3.84 | ppl    46.33\n",
            "| epoch  39 |  2200/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.71 | ppl    40.87\n",
            "| epoch  39 |  2400/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.69 | ppl    40.17\n",
            "| epoch  39 |  2600/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.74 | ppl    42.09\n",
            "| epoch  39 |  2800/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.68 | ppl    39.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 77.53s | valid loss  5.19 | valid ppl   179.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  40 |   200/ 2983 batches | lr 0.000008 | ms/batch 25.42 | loss  3.94 | ppl    51.20\n",
            "| epoch  40 |   400/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.90 | ppl    49.60\n",
            "| epoch  40 |   600/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.70 | ppl    40.53\n",
            "| epoch  40 |   800/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.76 | ppl    42.74\n",
            "| epoch  40 |  1000/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.90 | ppl    49.63\n",
            "| epoch  40 |  1200/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.75 | ppl    42.32\n",
            "| epoch  40 |  1400/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.94 | ppl    51.33\n",
            "| epoch  40 |  1600/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.89 | ppl    49.03\n",
            "| epoch  40 |  1800/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.81 | ppl    45.23\n",
            "| epoch  40 |  2000/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.83 | ppl    46.01\n",
            "| epoch  40 |  2200/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.71 | ppl    40.87\n",
            "| epoch  40 |  2400/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.70 | ppl    40.37\n",
            "| epoch  40 |  2600/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.74 | ppl    42.04\n",
            "| epoch  40 |  2800/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.69 | ppl    39.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 77.55s | valid loss  5.18 | valid ppl   177.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  41 |   200/ 2983 batches | lr 0.000008 | ms/batch 25.45 | loss  3.96 | ppl    52.40\n",
            "| epoch  41 |   400/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.90 | ppl    49.26\n",
            "| epoch  41 |   600/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.70 | ppl    40.34\n",
            "| epoch  41 |   800/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.75 | ppl    42.48\n",
            "| epoch  41 |  1000/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.89 | ppl    49.09\n",
            "| epoch  41 |  1200/ 2983 batches | lr 0.000008 | ms/batch 25.33 | loss  3.75 | ppl    42.48\n",
            "| epoch  41 |  1400/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.95 | ppl    51.77\n",
            "| epoch  41 |  1600/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.88 | ppl    48.62\n",
            "| epoch  41 |  1800/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.80 | ppl    44.83\n",
            "| epoch  41 |  2000/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.83 | ppl    45.89\n",
            "| epoch  41 |  2200/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.72 | ppl    41.14\n",
            "| epoch  41 |  2400/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.69 | ppl    39.97\n",
            "| epoch  41 |  2600/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.73 | ppl    41.73\n",
            "| epoch  41 |  2800/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.68 | ppl    39.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 77.57s | valid loss  5.18 | valid ppl   178.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  42 |   200/ 2983 batches | lr 0.000008 | ms/batch 25.42 | loss  3.93 | ppl    51.00\n",
            "| epoch  42 |   400/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.90 | ppl    49.26\n",
            "| epoch  42 |   600/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.69 | ppl    39.98\n",
            "| epoch  42 |   800/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.76 | ppl    43.03\n",
            "| epoch  42 |  1000/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.90 | ppl    49.34\n",
            "| epoch  42 |  1200/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.74 | ppl    42.03\n",
            "| epoch  42 |  1400/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.95 | ppl    51.79\n",
            "| epoch  42 |  1600/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.89 | ppl    48.76\n",
            "| epoch  42 |  1800/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.79 | ppl    44.21\n",
            "| epoch  42 |  2000/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.83 | ppl    45.90\n",
            "| epoch  42 |  2200/ 2983 batches | lr 0.000008 | ms/batch 25.32 | loss  3.71 | ppl    40.90\n",
            "| epoch  42 |  2400/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.69 | ppl    39.91\n",
            "| epoch  42 |  2600/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.73 | ppl    41.62\n",
            "| epoch  42 |  2800/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.68 | ppl    39.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 77.53s | valid loss  5.18 | valid ppl   177.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  43 |   200/ 2983 batches | lr 0.000008 | ms/batch 25.42 | loss  3.96 | ppl    52.41\n",
            "| epoch  43 |   400/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.90 | ppl    49.37\n",
            "| epoch  43 |   600/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.69 | ppl    40.22\n",
            "| epoch  43 |   800/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.75 | ppl    42.70\n",
            "| epoch  43 |  1000/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.85 | ppl    46.93\n",
            "| epoch  43 |  1200/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.74 | ppl    42.12\n",
            "| epoch  43 |  1400/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.95 | ppl    52.18\n",
            "| epoch  43 |  1600/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.88 | ppl    48.62\n",
            "| epoch  43 |  1800/ 2983 batches | lr 0.000008 | ms/batch 25.28 | loss  3.80 | ppl    44.54\n",
            "| epoch  43 |  2000/ 2983 batches | lr 0.000008 | ms/batch 25.27 | loss  3.82 | ppl    45.82\n",
            "| epoch  43 |  2200/ 2983 batches | lr 0.000008 | ms/batch 25.29 | loss  3.72 | ppl    41.46\n",
            "| epoch  43 |  2400/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.68 | ppl    39.61\n",
            "| epoch  43 |  2600/ 2983 batches | lr 0.000008 | ms/batch 25.30 | loss  3.73 | ppl    41.72\n",
            "| epoch  43 |  2800/ 2983 batches | lr 0.000008 | ms/batch 25.31 | loss  3.67 | ppl    39.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 77.52s | valid loss  5.19 | valid ppl   179.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000004\n",
            "| epoch  44 |   200/ 2983 batches | lr 0.000004 | ms/batch 25.42 | loss  3.98 | ppl    53.40\n",
            "| epoch  44 |   400/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  4.08 | ppl    59.40\n",
            "| epoch  44 |   600/ 2983 batches | lr 0.000004 | ms/batch 25.31 | loss  3.85 | ppl    46.96\n",
            "| epoch  44 |   800/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  3.82 | ppl    45.44\n",
            "| epoch  44 |  1000/ 2983 batches | lr 0.000004 | ms/batch 25.32 | loss  3.99 | ppl    54.11\n",
            "| epoch  44 |  1200/ 2983 batches | lr 0.000004 | ms/batch 25.28 | loss  3.97 | ppl    52.75\n",
            "| epoch  44 |  1400/ 2983 batches | lr 0.000004 | ms/batch 25.27 | loss  3.90 | ppl    49.55\n",
            "| epoch  44 |  1600/ 2983 batches | lr 0.000004 | ms/batch 25.28 | loss  4.01 | ppl    55.15\n",
            "| epoch  44 |  1800/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  3.92 | ppl    50.21\n",
            "| epoch  44 |  2000/ 2983 batches | lr 0.000004 | ms/batch 25.31 | loss  3.92 | ppl    50.32\n",
            "| epoch  44 |  2200/ 2983 batches | lr 0.000004 | ms/batch 25.30 | loss  3.77 | ppl    43.48\n",
            "| epoch  44 |  2400/ 2983 batches | lr 0.000004 | ms/batch 25.31 | loss  3.72 | ppl    41.08\n",
            "| epoch  44 |  2600/ 2983 batches | lr 0.000004 | ms/batch 25.31 | loss  3.75 | ppl    42.60\n",
            "| epoch  44 |  2800/ 2983 batches | lr 0.000004 | ms/batch 25.30 | loss  3.70 | ppl    40.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 77.54s | valid loss  5.19 | valid ppl   179.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000004\n",
            "| epoch  45 |   200/ 2983 batches | lr 0.000004 | ms/batch 25.43 | loss  4.01 | ppl    55.37\n",
            "| epoch  45 |   400/ 2983 batches | lr 0.000004 | ms/batch 25.31 | loss  4.09 | ppl    59.76\n",
            "| epoch  45 |   600/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  3.89 | ppl    49.14\n",
            "| epoch  45 |   800/ 2983 batches | lr 0.000004 | ms/batch 25.28 | loss  3.84 | ppl    46.63\n",
            "| epoch  45 |  1000/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  3.95 | ppl    52.10\n",
            "| epoch  45 |  1200/ 2983 batches | lr 0.000004 | ms/batch 25.30 | loss  3.85 | ppl    47.14\n",
            "| epoch  45 |  1400/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  3.95 | ppl    52.11\n",
            "| epoch  45 |  1600/ 2983 batches | lr 0.000004 | ms/batch 25.30 | loss  4.02 | ppl    55.58\n",
            "| epoch  45 |  1800/ 2983 batches | lr 0.000004 | ms/batch 25.30 | loss  3.88 | ppl    48.64\n",
            "| epoch  45 |  2000/ 2983 batches | lr 0.000004 | ms/batch 25.32 | loss  3.90 | ppl    49.52\n",
            "| epoch  45 |  2200/ 2983 batches | lr 0.000004 | ms/batch 25.31 | loss  3.76 | ppl    42.86\n",
            "| epoch  45 |  2400/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  3.71 | ppl    40.93\n",
            "| epoch  45 |  2600/ 2983 batches | lr 0.000004 | ms/batch 25.31 | loss  3.75 | ppl    42.40\n",
            "| epoch  45 |  2800/ 2983 batches | lr 0.000004 | ms/batch 25.31 | loss  3.70 | ppl    40.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 77.52s | valid loss  5.19 | valid ppl   179.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000004\n",
            "| epoch  46 |   200/ 2983 batches | lr 0.000004 | ms/batch 25.44 | loss  4.03 | ppl    56.25\n",
            "| epoch  46 |   400/ 2983 batches | lr 0.000004 | ms/batch 25.30 | loss  4.10 | ppl    60.24\n",
            "| epoch  46 |   600/ 2983 batches | lr 0.000004 | ms/batch 25.28 | loss  3.88 | ppl    48.60\n",
            "| epoch  46 |   800/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  3.82 | ppl    45.54\n",
            "| epoch  46 |  1000/ 2983 batches | lr 0.000004 | ms/batch 25.30 | loss  3.97 | ppl    52.93\n",
            "| epoch  46 |  1200/ 2983 batches | lr 0.000004 | ms/batch 25.30 | loss  3.83 | ppl    46.04\n",
            "| epoch  46 |  1400/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  3.94 | ppl    51.56\n",
            "| epoch  46 |  1600/ 2983 batches | lr 0.000004 | ms/batch 25.30 | loss  4.00 | ppl    54.67\n",
            "| epoch  46 |  1800/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  3.86 | ppl    47.26\n",
            "| epoch  46 |  2000/ 2983 batches | lr 0.000004 | ms/batch 25.31 | loss  3.89 | ppl    48.92\n",
            "| epoch  46 |  2200/ 2983 batches | lr 0.000004 | ms/batch 25.31 | loss  3.76 | ppl    43.03\n",
            "| epoch  46 |  2400/ 2983 batches | lr 0.000004 | ms/batch 25.29 | loss  3.71 | ppl    41.03\n",
            "| epoch  46 |  2600/ 2983 batches | lr 0.000004 | ms/batch 25.32 | loss  3.74 | ppl    42.28\n",
            "| epoch  46 |  2800/ 2983 batches | lr 0.000004 | ms/batch 25.32 | loss  3.69 | ppl    40.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 77.55s | valid loss  5.19 | valid ppl   179.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000002\n",
            "| epoch  47 |   200/ 2983 batches | lr 0.000002 | ms/batch 25.41 | loss  4.07 | ppl    58.29\n",
            "| epoch  47 |   400/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  4.16 | ppl    63.90\n",
            "| epoch  47 |   600/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  4.04 | ppl    56.67\n",
            "| epoch  47 |   800/ 2983 batches | lr 0.000002 | ms/batch 25.27 | loss  4.13 | ppl    62.29\n",
            "| epoch  47 |  1000/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  4.10 | ppl    60.27\n",
            "| epoch  47 |  1200/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  3.91 | ppl    50.00\n",
            "| epoch  47 |  1400/ 2983 batches | lr 0.000002 | ms/batch 25.28 | loss  3.94 | ppl    51.24\n",
            "| epoch  47 |  1600/ 2983 batches | lr 0.000002 | ms/batch 25.28 | loss  4.00 | ppl    54.60\n",
            "| epoch  47 |  1800/ 2983 batches | lr 0.000002 | ms/batch 25.28 | loss  3.89 | ppl    48.97\n",
            "| epoch  47 |  2000/ 2983 batches | lr 0.000002 | ms/batch 25.34 | loss  3.89 | ppl    49.06\n",
            "| epoch  47 |  2200/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  3.79 | ppl    44.21\n",
            "| epoch  47 |  2400/ 2983 batches | lr 0.000002 | ms/batch 25.31 | loss  3.80 | ppl    44.80\n",
            "| epoch  47 |  2600/ 2983 batches | lr 0.000002 | ms/batch 25.32 | loss  3.78 | ppl    43.93\n",
            "| epoch  47 |  2800/ 2983 batches | lr 0.000002 | ms/batch 25.31 | loss  3.72 | ppl    41.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 77.54s | valid loss  5.19 | valid ppl   179.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000002\n",
            "| epoch  48 |   200/ 2983 batches | lr 0.000002 | ms/batch 25.41 | loss  4.03 | ppl    56.10\n",
            "| epoch  48 |   400/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  4.09 | ppl    59.98\n",
            "| epoch  48 |   600/ 2983 batches | lr 0.000002 | ms/batch 25.28 | loss  3.95 | ppl    52.16\n",
            "| epoch  48 |   800/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  3.99 | ppl    54.17\n",
            "| epoch  48 |  1000/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  4.01 | ppl    55.31\n",
            "| epoch  48 |  1200/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  3.91 | ppl    50.14\n",
            "| epoch  48 |  1400/ 2983 batches | lr 0.000002 | ms/batch 25.31 | loss  3.93 | ppl    50.79\n",
            "| epoch  48 |  1600/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  4.04 | ppl    56.90\n",
            "| epoch  48 |  1800/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  3.91 | ppl    49.68\n",
            "| epoch  48 |  2000/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  3.91 | ppl    49.74\n",
            "| epoch  48 |  2200/ 2983 batches | lr 0.000002 | ms/batch 25.32 | loss  3.80 | ppl    44.53\n",
            "| epoch  48 |  2400/ 2983 batches | lr 0.000002 | ms/batch 25.31 | loss  3.78 | ppl    43.86\n",
            "| epoch  48 |  2600/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  3.78 | ppl    43.72\n",
            "| epoch  48 |  2800/ 2983 batches | lr 0.000002 | ms/batch 25.31 | loss  3.71 | ppl    40.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 77.53s | valid loss  5.19 | valid ppl   178.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000002\n",
            "| epoch  49 |   200/ 2983 batches | lr 0.000002 | ms/batch 25.47 | loss  4.06 | ppl    57.84\n",
            "| epoch  49 |   400/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  4.10 | ppl    60.38\n",
            "| epoch  49 |   600/ 2983 batches | lr 0.000002 | ms/batch 25.34 | loss  3.96 | ppl    52.59\n",
            "| epoch  49 |   800/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  4.01 | ppl    54.89\n",
            "| epoch  49 |  1000/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  4.01 | ppl    55.31\n",
            "| epoch  49 |  1200/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  3.90 | ppl    49.39\n",
            "| epoch  49 |  1400/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  3.93 | ppl    50.80\n",
            "| epoch  49 |  1600/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  4.05 | ppl    57.43\n",
            "| epoch  49 |  1800/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  3.92 | ppl    50.23\n",
            "| epoch  49 |  2000/ 2983 batches | lr 0.000002 | ms/batch 25.31 | loss  3.91 | ppl    50.02\n",
            "| epoch  49 |  2200/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  3.79 | ppl    44.12\n",
            "| epoch  49 |  2400/ 2983 batches | lr 0.000002 | ms/batch 25.28 | loss  3.77 | ppl    43.59\n",
            "| epoch  49 |  2600/ 2983 batches | lr 0.000002 | ms/batch 25.30 | loss  3.77 | ppl    43.29\n",
            "| epoch  49 |  2800/ 2983 batches | lr 0.000002 | ms/batch 25.29 | loss  3.70 | ppl    40.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 77.54s | valid loss  5.18 | valid ppl   177.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  50 |   200/ 2983 batches | lr 0.000001 | ms/batch 25.43 | loss  4.09 | ppl    59.49\n",
            "| epoch  50 |   400/ 2983 batches | lr 0.000001 | ms/batch 25.31 | loss  4.16 | ppl    64.19\n",
            "| epoch  50 |   600/ 2983 batches | lr 0.000001 | ms/batch 25.32 | loss  4.04 | ppl    56.69\n",
            "| epoch  50 |   800/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  4.13 | ppl    62.25\n",
            "| epoch  50 |  1000/ 2983 batches | lr 0.000001 | ms/batch 25.32 | loss  4.11 | ppl    61.00\n",
            "| epoch  50 |  1200/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  4.06 | ppl    58.03\n",
            "| epoch  50 |  1400/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  4.05 | ppl    57.63\n",
            "| epoch  50 |  1600/ 2983 batches | lr 0.000001 | ms/batch 25.28 | loss  4.04 | ppl    57.03\n",
            "| epoch  50 |  1800/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  3.89 | ppl    48.78\n",
            "| epoch  50 |  2000/ 2983 batches | lr 0.000001 | ms/batch 25.34 | loss  3.88 | ppl    48.40\n",
            "| epoch  50 |  2200/ 2983 batches | lr 0.000001 | ms/batch 25.32 | loss  3.82 | ppl    45.67\n",
            "| epoch  50 |  2400/ 2983 batches | lr 0.000001 | ms/batch 25.34 | loss  3.84 | ppl    46.74\n",
            "| epoch  50 |  2600/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  3.86 | ppl    47.27\n",
            "| epoch  50 |  2800/ 2983 batches | lr 0.000001 | ms/batch 25.31 | loss  3.82 | ppl    45.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 77.57s | valid loss  5.13 | valid ppl   169.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  51 |   200/ 2983 batches | lr 0.000001 | ms/batch 25.43 | loss  4.05 | ppl    57.61\n",
            "| epoch  51 |   400/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  4.09 | ppl    59.86\n",
            "| epoch  51 |   600/ 2983 batches | lr 0.000001 | ms/batch 25.27 | loss  3.96 | ppl    52.21\n",
            "| epoch  51 |   800/ 2983 batches | lr 0.000001 | ms/batch 25.28 | loss  4.03 | ppl    56.16\n",
            "| epoch  51 |  1000/ 2983 batches | lr 0.000001 | ms/batch 25.27 | loss  4.07 | ppl    58.47\n",
            "| epoch  51 |  1200/ 2983 batches | lr 0.000001 | ms/batch 25.32 | loss  4.00 | ppl    54.56\n",
            "| epoch  51 |  1400/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  3.98 | ppl    53.61\n",
            "| epoch  51 |  1600/ 2983 batches | lr 0.000001 | ms/batch 25.31 | loss  4.01 | ppl    54.99\n",
            "| epoch  51 |  1800/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  3.91 | ppl    49.95\n",
            "| epoch  51 |  2000/ 2983 batches | lr 0.000001 | ms/batch 25.28 | loss  3.93 | ppl    50.89\n",
            "| epoch  51 |  2200/ 2983 batches | lr 0.000001 | ms/batch 25.34 | loss  3.85 | ppl    46.87\n",
            "| epoch  51 |  2400/ 2983 batches | lr 0.000001 | ms/batch 25.34 | loss  3.85 | ppl    46.95\n",
            "| epoch  51 |  2600/ 2983 batches | lr 0.000001 | ms/batch 25.33 | loss  3.85 | ppl    46.89\n",
            "| epoch  51 |  2800/ 2983 batches | lr 0.000001 | ms/batch 25.31 | loss  3.78 | ppl    43.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 77.56s | valid loss  5.15 | valid ppl   172.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  52 |   200/ 2983 batches | lr 0.000001 | ms/batch 25.43 | loss  4.02 | ppl    55.55\n",
            "| epoch  52 |   400/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  4.09 | ppl    59.44\n",
            "| epoch  52 |   600/ 2983 batches | lr 0.000001 | ms/batch 25.28 | loss  3.96 | ppl    52.52\n",
            "| epoch  52 |   800/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  4.04 | ppl    56.83\n",
            "| epoch  52 |  1000/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  4.06 | ppl    58.06\n",
            "| epoch  52 |  1200/ 2983 batches | lr 0.000001 | ms/batch 25.28 | loss  4.00 | ppl    54.53\n",
            "| epoch  52 |  1400/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  3.97 | ppl    53.06\n",
            "| epoch  52 |  1600/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  4.02 | ppl    55.71\n",
            "| epoch  52 |  1800/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  3.92 | ppl    50.20\n",
            "| epoch  52 |  2000/ 2983 batches | lr 0.000001 | ms/batch 25.31 | loss  3.94 | ppl    51.48\n",
            "| epoch  52 |  2200/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  3.85 | ppl    47.10\n",
            "| epoch  52 |  2400/ 2983 batches | lr 0.000001 | ms/batch 25.31 | loss  3.85 | ppl    46.95\n",
            "| epoch  52 |  2600/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  3.84 | ppl    46.45\n",
            "| epoch  52 |  2800/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  3.77 | ppl    43.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 77.53s | valid loss  5.16 | valid ppl   173.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  53 |   200/ 2983 batches | lr 0.000001 | ms/batch 25.42 | loss  4.02 | ppl    55.67\n",
            "| epoch  53 |   400/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  4.09 | ppl    59.82\n",
            "| epoch  53 |   600/ 2983 batches | lr 0.000001 | ms/batch 25.27 | loss  3.97 | ppl    53.00\n",
            "| epoch  53 |   800/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  4.05 | ppl    57.13\n",
            "| epoch  53 |  1000/ 2983 batches | lr 0.000001 | ms/batch 25.27 | loss  4.06 | ppl    58.00\n",
            "| epoch  53 |  1200/ 2983 batches | lr 0.000001 | ms/batch 25.29 | loss  4.00 | ppl    54.49\n",
            "| epoch  53 |  1400/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  3.97 | ppl    53.03\n",
            "| epoch  53 |  1600/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  4.02 | ppl    55.60\n",
            "| epoch  53 |  1800/ 2983 batches | lr 0.000001 | ms/batch 25.32 | loss  3.92 | ppl    50.55\n",
            "| epoch  53 |  2000/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  3.94 | ppl    51.56\n",
            "| epoch  53 |  2200/ 2983 batches | lr 0.000001 | ms/batch 25.32 | loss  3.84 | ppl    46.68\n",
            "| epoch  53 |  2400/ 2983 batches | lr 0.000001 | ms/batch 25.32 | loss  3.84 | ppl    46.72\n",
            "| epoch  53 |  2600/ 2983 batches | lr 0.000001 | ms/batch 25.30 | loss  3.84 | ppl    46.54\n",
            "| epoch  53 |  2800/ 2983 batches | lr 0.000001 | ms/batch 25.32 | loss  3.76 | ppl    42.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 77.54s | valid loss  5.16 | valid ppl   174.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  54 |   200/ 2983 batches | lr 0.000000 | ms/batch 25.43 | loss  4.04 | ppl    56.57\n",
            "| epoch  54 |   400/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.13 | ppl    62.31\n",
            "| epoch  54 |   600/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.02 | ppl    55.80\n",
            "| epoch  54 |   800/ 2983 batches | lr 0.000000 | ms/batch 25.27 | loss  4.13 | ppl    61.97\n",
            "| epoch  54 |  1000/ 2983 batches | lr 0.000000 | ms/batch 25.26 | loss  4.13 | ppl    62.40\n",
            "| epoch  54 |  1200/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.08 | ppl    59.00\n",
            "| epoch  54 |  1400/ 2983 batches | lr 0.000000 | ms/batch 25.27 | loss  4.08 | ppl    59.26\n",
            "| epoch  54 |  1600/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.11 | ppl    61.14\n",
            "| epoch  54 |  1800/ 2983 batches | lr 0.000000 | ms/batch 25.28 | loss  3.98 | ppl    53.72\n",
            "| epoch  54 |  2000/ 2983 batches | lr 0.000000 | ms/batch 25.32 | loss  3.94 | ppl    51.65\n",
            "| epoch  54 |  2200/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.84 | ppl    46.50\n",
            "| epoch  54 |  2400/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.83 | ppl    46.23\n",
            "| epoch  54 |  2600/ 2983 batches | lr 0.000000 | ms/batch 25.33 | loss  3.86 | ppl    47.26\n",
            "| epoch  54 |  2800/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.81 | ppl    45.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 77.53s | valid loss  5.12 | valid ppl   167.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  55 |   200/ 2983 batches | lr 0.000000 | ms/batch 25.42 | loss  4.06 | ppl    58.16\n",
            "| epoch  55 |   400/ 2983 batches | lr 0.000000 | ms/batch 25.32 | loss  4.13 | ppl    62.07\n",
            "| epoch  55 |   600/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.00 | ppl    54.82\n",
            "| epoch  55 |   800/ 2983 batches | lr 0.000000 | ms/batch 25.28 | loss  4.09 | ppl    59.93\n",
            "| epoch  55 |  1000/ 2983 batches | lr 0.000000 | ms/batch 25.32 | loss  4.09 | ppl    60.02\n",
            "| epoch  55 |  1200/ 2983 batches | lr 0.000000 | ms/batch 25.28 | loss  4.03 | ppl    56.51\n",
            "| epoch  55 |  1400/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.03 | ppl    56.41\n",
            "| epoch  55 |  1600/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.04 | ppl    56.73\n",
            "| epoch  55 |  1800/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.92 | ppl    50.53\n",
            "| epoch  55 |  2000/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.93 | ppl    50.87\n",
            "| epoch  55 |  2200/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.85 | ppl    46.80\n",
            "| epoch  55 |  2400/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.87 | ppl    48.05\n",
            "| epoch  55 |  2600/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.90 | ppl    49.62\n",
            "| epoch  55 |  2800/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.84 | ppl    46.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 77.54s | valid loss  5.12 | valid ppl   167.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  56 |   200/ 2983 batches | lr 0.000000 | ms/batch 25.44 | loss  4.08 | ppl    59.02\n",
            "| epoch  56 |   400/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.12 | ppl    61.41\n",
            "| epoch  56 |   600/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.98 | ppl    53.32\n",
            "| epoch  56 |   800/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.07 | ppl    58.36\n",
            "| epoch  56 |  1000/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.09 | ppl    59.64\n",
            "| epoch  56 |  1200/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.02 | ppl    55.72\n",
            "| epoch  56 |  1400/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.02 | ppl    55.56\n",
            "| epoch  56 |  1600/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.03 | ppl    56.22\n",
            "| epoch  56 |  1800/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.93 | ppl    51.09\n",
            "| epoch  56 |  2000/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  3.94 | ppl    51.47\n",
            "| epoch  56 |  2200/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.86 | ppl    47.39\n",
            "| epoch  56 |  2400/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  3.89 | ppl    48.72\n",
            "| epoch  56 |  2600/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  3.90 | ppl    49.59\n",
            "| epoch  56 |  2800/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  3.83 | ppl    46.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 77.54s | valid loss  5.12 | valid ppl   167.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  57 |   200/ 2983 batches | lr 0.000000 | ms/batch 25.43 | loss  4.05 | ppl    57.60\n",
            "| epoch  57 |   400/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.10 | ppl    60.51\n",
            "| epoch  57 |   600/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.97 | ppl    52.91\n",
            "| epoch  57 |   800/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.06 | ppl    57.91\n",
            "| epoch  57 |  1000/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.09 | ppl    59.46\n",
            "| epoch  57 |  1200/ 2983 batches | lr 0.000000 | ms/batch 25.28 | loss  4.02 | ppl    55.79\n",
            "| epoch  57 |  1400/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.01 | ppl    55.20\n",
            "| epoch  57 |  1600/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.04 | ppl    56.73\n",
            "| epoch  57 |  1800/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.95 | ppl    52.15\n",
            "| epoch  57 |  2000/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.96 | ppl    52.61\n",
            "| epoch  57 |  2200/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  3.87 | ppl    47.78\n",
            "| epoch  57 |  2400/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.88 | ppl    48.66\n",
            "| epoch  57 |  2600/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  3.90 | ppl    49.40\n",
            "| epoch  57 |  2800/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  3.81 | ppl    45.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 77.52s | valid loss  5.12 | valid ppl   167.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  58 |   200/ 2983 batches | lr 0.000000 | ms/batch 25.40 | loss  4.04 | ppl    56.94\n",
            "| epoch  58 |   400/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.10 | ppl    60.45\n",
            "| epoch  58 |   600/ 2983 batches | lr 0.000000 | ms/batch 25.28 | loss  3.97 | ppl    53.00\n",
            "| epoch  58 |   800/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.06 | ppl    57.91\n",
            "| epoch  58 |  1000/ 2983 batches | lr 0.000000 | ms/batch 25.28 | loss  4.08 | ppl    59.33\n",
            "| epoch  58 |  1200/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.02 | ppl    55.91\n",
            "| epoch  58 |  1400/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.02 | ppl    55.67\n",
            "| epoch  58 |  1600/ 2983 batches | lr 0.000000 | ms/batch 25.28 | loss  4.04 | ppl    56.66\n",
            "| epoch  58 |  1800/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  3.94 | ppl    51.52\n",
            "| epoch  58 |  2000/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  3.96 | ppl    52.59\n",
            "| epoch  58 |  2200/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.86 | ppl    47.45\n",
            "| epoch  58 |  2400/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.88 | ppl    48.41\n",
            "| epoch  58 |  2600/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.89 | ppl    48.96\n",
            "| epoch  58 |  2800/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.81 | ppl    45.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 77.51s | valid loss  5.12 | valid ppl   167.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  59 |   200/ 2983 batches | lr 0.000000 | ms/batch 25.43 | loss  4.05 | ppl    57.40\n",
            "| epoch  59 |   400/ 2983 batches | lr 0.000000 | ms/batch 25.28 | loss  4.10 | ppl    60.44\n",
            "| epoch  59 |   600/ 2983 batches | lr 0.000000 | ms/batch 25.27 | loss  4.00 | ppl    54.36\n",
            "| epoch  59 |   800/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.11 | ppl    61.01\n",
            "| epoch  59 |  1000/ 2983 batches | lr 0.000000 | ms/batch 25.28 | loss  4.12 | ppl    61.67\n",
            "| epoch  59 |  1200/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.07 | ppl    58.34\n",
            "| epoch  59 |  1400/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.08 | ppl    58.98\n",
            "| epoch  59 |  1600/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.11 | ppl    60.83\n",
            "| epoch  59 |  1800/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.01 | ppl    54.95\n",
            "| epoch  59 |  2000/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.99 | ppl    54.19\n",
            "| epoch  59 |  2200/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  3.87 | ppl    47.85\n",
            "| epoch  59 |  2400/ 2983 batches | lr 0.000000 | ms/batch 25.28 | loss  3.87 | ppl    47.94\n",
            "| epoch  59 |  2600/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.88 | ppl    48.31\n",
            "| epoch  59 |  2800/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  3.81 | ppl    45.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 77.52s | valid loss  5.12 | valid ppl   166.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  60 |   200/ 2983 batches | lr 0.000000 | ms/batch 25.41 | loss  4.06 | ppl    58.15\n",
            "| epoch  60 |   400/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.12 | ppl    61.56\n",
            "| epoch  60 |   600/ 2983 batches | lr 0.000000 | ms/batch 25.30 | loss  4.00 | ppl    54.75\n",
            "| epoch  60 |   800/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.11 | ppl    60.69\n",
            "| epoch  60 |  1000/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.11 | ppl    60.96\n",
            "| epoch  60 |  1200/ 2983 batches | lr 0.000000 | ms/batch 25.29 | loss  4.05 | ppl    57.58\n",
            "| epoch  60 |  1400/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.06 | ppl    57.83\n",
            "| epoch  60 |  1600/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  4.09 | ppl    59.79\n",
            "| epoch  60 |  1800/ 2983 batches | lr 0.000000 | ms/batch 25.34 | loss  3.99 | ppl    54.06\n",
            "| epoch  60 |  2000/ 2983 batches | lr 0.000000 | ms/batch 25.38 | loss  3.97 | ppl    52.95\n",
            "| epoch  60 |  2200/ 2983 batches | lr 0.000000 | ms/batch 25.32 | loss  3.86 | ppl    47.40\n",
            "| epoch  60 |  2400/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.87 | ppl    47.75\n",
            "| epoch  60 |  2600/ 2983 batches | lr 0.000000 | ms/batch 25.31 | loss  3.90 | ppl    49.23\n",
            "| epoch  60 |  2800/ 2983 batches | lr 0.000000 | ms/batch 25.34 | loss  3.83 | ppl    46.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 77.58s | valid loss  5.11 | valid ppl   166.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "=========================================================================================\n",
            "| End of training | test loss  5.05 | test ppl   155.26\n",
            "=========================================================================================\n",
            "\n",
            "Generating text...\n",
            "Vocabulary size: 33278\n",
            "Vocabulary size: 33278\n",
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n",
            "to do not incorporate , there was a little written gate in the top seven episode . It usually included\n",
            "as Main 's most direct personalities of My America , but it remained often survives . During the 1970s .\n",
            "Tourism from the 9th century were a leap travel from the music between the Moon , billed as several \"\n",
            "emerging \" and \" <unk> \" <unk> , the historical pressure of the center is of between the modern 80s\n",
            "tours from the ship . Immediately after the race , the University of Southern Bay generated an impressive field of\n",
            "workers in Europe and some of the national   Technical <unk> Well , a long race with its fifth depth\n",
            "rear rate Metro , and how the Bill 's brother are this running past the entry of the Mississippi River\n",
            "which reduces a more height . They respected the music distribution where Douglas was intentionally also successful on the PlayStation\n",
            ", Earth deriving to strips , posters and oak . Moored buoys begin , further in the <unk>   <unk>\n",
            "row and blue liquids , similar to moving ahead of these research that utilizes the event and would interfere with\n",
            "other stages of the game . <eos> Hoover counties that finished the tracks with several <unk> passes , featuring the\n",
            "removal of the kakapo to sell the sequel a large track . <eos> There is a low – approximately three\n",
            "24   foot ( 15 m ) stucco system and with 7   inch ( 6   3 for ×\n",
            "2   9 m2 ) gross water . The midrange was 1   3 billion and an 8   2\n",
            "numerous tons ( 7   0 kg ) number 110 ( 1   2 lb ) per year , and\n",
            "slightly 59 kg ( 250 cm ) long . The most common female supply was darker , while the background\n",
            "appeared again , but these was the open   back airport temperature to up a traverse to match the heightened\n",
            "ocean view was branched . The rivalry was lost at Ceres in 2013 , and they again absorbed the Stanley\n",
            "Park Forest , though it was discovered in the 5000 National 3 / 1 L / <unk> : Motor Directing\n",
            "( 2015 ) for the Leckwith <unk> 's <unk> toy . <eos> <eos> = = = Armament = = =\n",
            "<eos> <eos> The engine also replaced the outer grout power located within their own <unk> . A second male Fish\n",
            "'s barbarian for a diet that includes the Rockefeller represents the factory , and later underwent the <unk> . This\n",
            "featured an active percentage of 20   000 to 10   000 people , a day home although most of\n",
            "the passengers were mobilized partly with its previous lack and stay them across it inside approximately 10 to 50 tenths\n",
            ". Accordingly , the tour still increased a earlier series of 1897 competition before teams have been mounting the name\n",
            "double <unk> . Construction represented by flash computing regulation and fixed <unk> became the longest   held total of 3\n",
            "% . Tech 's architectural advantage was Montagne as a state for six years earlier , but they introduced it\n",
            "in baseball . Another version that lasted for several years , was now held in <unk> by the A Easter\n",
            "<unk> since the late 1970s , but would briefly be green . Many of the continued <unk> , titled \"\n",
            "Songs 's ball the <unk> \" , allows it from an attraction that would be occasionally perfect at the entrance\n",
            "to the cargo people . The <unk> Millennium officially revised today glass ballet , producing radiation leg developer William Flynn\n",
            ", whose father <unk> , Viscount Fusiliers , Sri <unk> , Martin Ray Cunningham , and Shane <unk> <unk> ,\n",
            "who had previously decided to look for charge over events , and rather than assured use of the nature of\n",
            "the inspiration and risked Byung   <unk> followed with direct offenders . According to a revival known as the \"\n",
            "<unk> \" boat , Watkins admitted , \" Ten songs take precedence for the party when the <unk> , or\n",
            "experience to <unk> <unk> you inflict support we could in a standing fan . He finds me out . \"\n",
            "<eos> He was commissioned into the benefit problems of this highly active frequency \" , a contest on red music\n",
            "and a short black memory genre . \" The video was defended by Don Fei and Paul <unk> by <unk>\n",
            "The Dawn Barbara <unk> as a <unk> . <eos> <eos> = = Personal popular literature = = <eos> <eos> Barbarian\n",
            "II is a simple review in New York and instead resulted in three complaints as the third highest day ,\n",
            "a record of three years , a team called remained with a Journal of <unk> , 1   2 and\n",
            "15 % share , using a single seasonal range of blue <unk> and the white bar , a projecting <unk>\n",
            "factory . It also is the expansion of the falsetto object , a three   70 piece with negative rivals\n",
            "tips or each time . <eos> <eos> = = World War II = = <eos> <eos> <unk> since the 15th\n",
            "century , the Tom Salmon Authority Company was discovered in 1985 in the 1980s for the President . The group\n",
            "unveiled 33   8 % of deaths for the shares during the event 's conclusion . Most research applications have\n",
            "made the largest element of the people and sold the James Ball Award , which contributed to the <unk> Register\n",
            "in modern history . The understanding of economic submarines from the Jewish Civil War would Sosa as to destroy the\n",
            "titles of the invasion . Numerous important charitable sites in their new site has been exercised by the British government\n",
            ". In April 1947 , the British Ministry of Mogadiscio was given for two nations : the <unk> Range of\n",
            "\n",
            "Training on custom names dataset...\n"
          ]
        }
      ],
      "source": [
        "# Example 1: Train on WikiText-2\n",
        "print(\"Training LSTM on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type = 'LSTM', # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "    data_path = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize = 400,\n",
        "    nhid = 400,\n",
        "    nlayers = 4,\n",
        "    lr = 0.001,\n",
        "    clip = 0.25,\n",
        "    epochs = 60,\n",
        "    batch_size = 20,\n",
        "    bptt = 35,\n",
        "    dropout = 0.2,\n",
        "    tied = False,\n",
        "    nhead = 4,\n",
        "    log_interval = 200,\n",
        "    save_path = 'model.pt',\n",
        "    onnx_export = '',\n",
        "    dry_run = False,\n",
        "    accel = True,\n",
        "    use_optimizer = True,\n",
        "    optimizer_type = 'AdamW',\n",
        "    weight_decay=1e-5,\n",
        "    use_betas = False,\n",
        "    use_eps = False,\n",
        "    criterion = nn.NLLLoss(),\n",
        "    use_label_smoothing = False,\n",
        "    label_smoothing = 0.1,\n",
        "    use_warmup = False,\n",
        "    warmup_steps = 4000,\n",
        "    min_freq = 5,\n",
        "    seed = 1111,\n",
        "    old_version = True\n",
        ")\n",
        "\n",
        "# Example 2: Generate text\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    words=1000,\n",
        "    temperature=1.0,\n",
        "    old_version=True,\n",
        "    use_top_k=False,\n",
        "    accel = True\n",
        ")\n",
        "\n",
        "!cat generated.txt\n",
        "\n",
        "# Example 3: Train on custom names dataset\n",
        "# First, create the data files (see instructions below)\n",
        "print(\"\\nTraining on custom names dataset...\")\n",
        "# train_model(\n",
        "#     model_type='LSTM',\n",
        "#     data_path='./data/names',\n",
        "#     emsize=128,\n",
        "#     nhid=128,\n",
        "#     nlayers=2,\n",
        "#     epochs=20,\n",
        "#     lr=0.001\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p8DeDRupqMNl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p8DeDRupqMNl",
        "outputId": "3c73516e-4df9-468c-ee60-23344abf2ec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LSTM on WikiText-2...\n",
            "Using device: cuda\n",
            "Vocabulary size: 25251\n",
            "| epoch   1 |   200/ 2983 batches | lr 0.000040 | ms/batch 27.46 | loss  9.72 | ppl 16632.75\n",
            "| epoch   1 |   400/ 2983 batches | lr 0.000079 | ms/batch 22.84 | loss  7.68 | ppl  2170.74\n",
            "| epoch   1 |   600/ 2983 batches | lr 0.000119 | ms/batch 22.83 | loss  7.46 | ppl  1730.95\n",
            "| epoch   1 |   800/ 2983 batches | lr 0.000158 | ms/batch 22.89 | loss  7.36 | ppl  1568.45\n",
            "| epoch   1 |  1000/ 2983 batches | lr 0.000198 | ms/batch 22.87 | loss  7.26 | ppl  1424.63\n",
            "| epoch   1 |  1200/ 2983 batches | lr 0.000237 | ms/batch 22.91 | loss  7.10 | ppl  1209.58\n",
            "| epoch   1 |  1400/ 2983 batches | lr 0.000277 | ms/batch 22.95 | loss  6.97 | ppl  1067.89\n",
            "| epoch   1 |  1600/ 2983 batches | lr 0.000316 | ms/batch 22.94 | loss  6.94 | ppl  1029.98\n",
            "| epoch   1 |  1800/ 2983 batches | lr 0.000356 | ms/batch 22.98 | loss  6.86 | ppl   953.98\n",
            "| epoch   1 |  2000/ 2983 batches | lr 0.000395 | ms/batch 23.01 | loss  6.84 | ppl   932.51\n",
            "| epoch   1 |  2200/ 2983 batches | lr 0.000435 | ms/batch 23.01 | loss  6.76 | ppl   858.89\n",
            "| epoch   1 |  2400/ 2983 batches | lr 0.000475 | ms/batch 23.07 | loss  6.70 | ppl   810.20\n",
            "| epoch   1 |  2600/ 2983 batches | lr 0.000514 | ms/batch 23.05 | loss  6.69 | ppl   800.64\n",
            "| epoch   1 |  2800/ 2983 batches | lr 0.000554 | ms/batch 23.08 | loss  6.61 | ppl   741.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 71.79s | valid loss  6.98 | valid ppl  1080.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000590\n",
            "| epoch   2 |   200/ 2983 batches | lr 0.000629 | ms/batch 23.25 | loss  6.57 | ppl   715.24\n",
            "| epoch   2 |   400/ 2983 batches | lr 0.000669 | ms/batch 23.20 | loss  6.51 | ppl   672.45\n",
            "| epoch   2 |   600/ 2983 batches | lr 0.000709 | ms/batch 23.19 | loss  6.44 | ppl   623.41\n",
            "| epoch   2 |   800/ 2983 batches | lr 0.000748 | ms/batch 23.21 | loss  6.42 | ppl   615.49\n",
            "| epoch   2 |  1000/ 2983 batches | lr 0.000788 | ms/batch 23.19 | loss  6.40 | ppl   602.45\n",
            "| epoch   2 |  1200/ 2983 batches | lr 0.000773 | ms/batch 23.17 | loss  6.38 | ppl   591.25\n",
            "| epoch   2 |  1400/ 2983 batches | lr 0.000755 | ms/batch 23.19 | loss  6.35 | ppl   570.07\n",
            "| epoch   2 |  1600/ 2983 batches | lr 0.000738 | ms/batch 23.12 | loss  6.37 | ppl   581.27\n",
            "| epoch   2 |  1800/ 2983 batches | lr 0.000723 | ms/batch 23.11 | loss  6.28 | ppl   536.08\n",
            "| epoch   2 |  2000/ 2983 batches | lr 0.000708 | ms/batch 23.12 | loss  6.30 | ppl   542.97\n",
            "| epoch   2 |  2200/ 2983 batches | lr 0.000694 | ms/batch 23.09 | loss  6.22 | ppl   502.74\n",
            "| epoch   2 |  2400/ 2983 batches | lr 0.000681 | ms/batch 23.11 | loss  6.22 | ppl   502.58\n",
            "| epoch   2 |  2600/ 2983 batches | lr 0.000669 | ms/batch 23.09 | loss  6.23 | ppl   508.80\n",
            "| epoch   2 |  2800/ 2983 batches | lr 0.000657 | ms/batch 23.04 | loss  6.16 | ppl   474.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 71.41s | valid loss  6.62 | valid ppl   751.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000647\n",
            "| epoch   3 |   200/ 2983 batches | lr 0.000637 | ms/batch 23.20 | loss  6.19 | ppl   486.18\n",
            "| epoch   3 |   400/ 2983 batches | lr 0.000627 | ms/batch 23.13 | loss  6.17 | ppl   477.92\n",
            "| epoch   3 |   600/ 2983 batches | lr 0.000617 | ms/batch 23.08 | loss  6.09 | ppl   440.39\n",
            "| epoch   3 |   800/ 2983 batches | lr 0.000608 | ms/batch 23.11 | loss  6.10 | ppl   445.00\n",
            "| epoch   3 |  1000/ 2983 batches | lr 0.000599 | ms/batch 23.16 | loss  6.10 | ppl   447.74\n",
            "| epoch   3 |  1200/ 2983 batches | lr 0.000591 | ms/batch 23.13 | loss  6.09 | ppl   442.28\n",
            "| epoch   3 |  1400/ 2983 batches | lr 0.000582 | ms/batch 23.18 | loss  6.09 | ppl   442.95\n",
            "| epoch   3 |  1600/ 2983 batches | lr 0.000575 | ms/batch 23.17 | loss  6.14 | ppl   462.78\n",
            "| epoch   3 |  1800/ 2983 batches | lr 0.000567 | ms/batch 23.16 | loss  6.05 | ppl   425.27\n",
            "| epoch   3 |  2000/ 2983 batches | lr 0.000560 | ms/batch 23.20 | loss  6.08 | ppl   435.35\n",
            "| epoch   3 |  2200/ 2983 batches | lr 0.000553 | ms/batch 23.15 | loss  6.01 | ppl   409.10\n",
            "| epoch   3 |  2400/ 2983 batches | lr 0.000547 | ms/batch 23.15 | loss  6.02 | ppl   411.48\n",
            "| epoch   3 |  2600/ 2983 batches | lr 0.000540 | ms/batch 23.16 | loss  6.05 | ppl   424.35\n",
            "| epoch   3 |  2800/ 2983 batches | lr 0.000534 | ms/batch 23.11 | loss  5.99 | ppl   400.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 71.42s | valid loss  6.47 | valid ppl   643.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000528\n",
            "| epoch   4 |   200/ 2983 batches | lr 0.000523 | ms/batch 23.23 | loss  6.04 | ppl   419.58\n",
            "| epoch   4 |   400/ 2983 batches | lr 0.000517 | ms/batch 23.14 | loss  6.03 | ppl   414.43\n",
            "| epoch   4 |   600/ 2983 batches | lr 0.000512 | ms/batch 23.14 | loss  5.95 | ppl   382.78\n",
            "| epoch   4 |   800/ 2983 batches | lr 0.000506 | ms/batch 23.12 | loss  5.97 | ppl   392.52\n",
            "| epoch   4 |  1000/ 2983 batches | lr 0.000501 | ms/batch 23.14 | loss  5.98 | ppl   395.41\n",
            "| epoch   4 |  1200/ 2983 batches | lr 0.000496 | ms/batch 23.12 | loss  5.97 | ppl   389.62\n",
            "| epoch   4 |  1400/ 2983 batches | lr 0.000491 | ms/batch 23.11 | loss  5.98 | ppl   395.99\n",
            "| epoch   4 |  1600/ 2983 batches | lr 0.000487 | ms/batch 23.12 | loss  6.03 | ppl   415.17\n",
            "| epoch   4 |  1800/ 2983 batches | lr 0.000482 | ms/batch 23.09 | loss  5.94 | ppl   381.69\n",
            "| epoch   4 |  2000/ 2983 batches | lr 0.000478 | ms/batch 23.12 | loss  5.97 | ppl   392.06\n",
            "| epoch   4 |  2200/ 2983 batches | lr 0.000473 | ms/batch 23.08 | loss  5.91 | ppl   368.75\n",
            "| epoch   4 |  2400/ 2983 batches | lr 0.000469 | ms/batch 23.09 | loss  5.92 | ppl   373.89\n",
            "| epoch   4 |  2600/ 2983 batches | lr 0.000465 | ms/batch 23.10 | loss  5.95 | ppl   385.51\n",
            "| epoch   4 |  2800/ 2983 batches | lr 0.000461 | ms/batch 23.08 | loss  5.90 | ppl   363.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 71.33s | valid loss  6.40 | valid ppl   599.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000458\n",
            "| epoch   5 |   200/ 2983 batches | lr 0.000454 | ms/batch 23.18 | loss  5.95 | ppl   385.40\n",
            "| epoch   5 |   400/ 2983 batches | lr 0.000450 | ms/batch 23.11 | loss  5.95 | ppl   383.34\n",
            "| epoch   5 |   600/ 2983 batches | lr 0.000447 | ms/batch 23.15 | loss  5.86 | ppl   351.00\n",
            "| epoch   5 |   800/ 2983 batches | lr 0.000443 | ms/batch 23.10 | loss  5.89 | ppl   361.93\n",
            "| epoch   5 |  1000/ 2983 batches | lr 0.000440 | ms/batch 23.16 | loss  5.90 | ppl   364.85\n",
            "| epoch   5 |  1200/ 2983 batches | lr 0.000436 | ms/batch 23.12 | loss  5.89 | ppl   359.97\n",
            "| epoch   5 |  1400/ 2983 batches | lr 0.000433 | ms/batch 23.11 | loss  5.91 | ppl   369.18\n",
            "| epoch   5 |  1600/ 2983 batches | lr 0.000430 | ms/batch 23.16 | loss  5.96 | ppl   387.91\n",
            "| epoch   5 |  1800/ 2983 batches | lr 0.000427 | ms/batch 23.15 | loss  5.87 | ppl   355.45\n",
            "| epoch   5 |  2000/ 2983 batches | lr 0.000424 | ms/batch 23.13 | loss  5.91 | ppl   367.47\n",
            "| epoch   5 |  2200/ 2983 batches | lr 0.000421 | ms/batch 23.14 | loss  5.84 | ppl   345.31\n",
            "| epoch   5 |  2400/ 2983 batches | lr 0.000418 | ms/batch 23.14 | loss  5.86 | ppl   349.98\n",
            "| epoch   5 |  2600/ 2983 batches | lr 0.000415 | ms/batch 23.17 | loss  5.89 | ppl   362.54\n",
            "| epoch   5 |  2800/ 2983 batches | lr 0.000412 | ms/batch 23.09 | loss  5.83 | ppl   341.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 71.39s | valid loss  6.36 | valid ppl   575.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000409\n",
            "| epoch   6 |   200/ 2983 batches | lr 0.000407 | ms/batch 23.23 | loss  5.89 | ppl   361.29\n",
            "| epoch   6 |   400/ 2983 batches | lr 0.000404 | ms/batch 23.12 | loss  5.89 | ppl   360.71\n",
            "| epoch   6 |   600/ 2983 batches | lr 0.000401 | ms/batch 23.13 | loss  5.80 | ppl   329.71\n",
            "| epoch   6 |   800/ 2983 batches | lr 0.000399 | ms/batch 23.12 | loss  5.84 | ppl   342.46\n",
            "| epoch   6 |  1000/ 2983 batches | lr 0.000396 | ms/batch 23.13 | loss  5.85 | ppl   346.23\n",
            "| epoch   6 |  1200/ 2983 batches | lr 0.000394 | ms/batch 23.15 | loss  5.83 | ppl   340.42\n",
            "| epoch   6 |  1400/ 2983 batches | lr 0.000391 | ms/batch 23.10 | loss  5.85 | ppl   347.79\n",
            "| epoch   6 |  1600/ 2983 batches | lr 0.000389 | ms/batch 23.14 | loss  5.91 | ppl   368.07\n",
            "| epoch   6 |  1800/ 2983 batches | lr 0.000387 | ms/batch 23.10 | loss  5.82 | ppl   338.57\n",
            "| epoch   6 |  2000/ 2983 batches | lr 0.000384 | ms/batch 23.09 | loss  5.86 | ppl   351.26\n",
            "| epoch   6 |  2200/ 2983 batches | lr 0.000382 | ms/batch 23.15 | loss  5.79 | ppl   327.69\n",
            "| epoch   6 |  2400/ 2983 batches | lr 0.000380 | ms/batch 23.13 | loss  5.81 | ppl   333.47\n",
            "| epoch   6 |  2600/ 2983 batches | lr 0.000378 | ms/batch 23.15 | loss  5.84 | ppl   344.75\n",
            "| epoch   6 |  2800/ 2983 batches | lr 0.000376 | ms/batch 23.10 | loss  5.78 | ppl   325.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 71.38s | valid loss  6.33 | valid ppl   559.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000374\n",
            "| epoch   7 |   200/ 2983 batches | lr 0.000372 | ms/batch 23.24 | loss  5.84 | ppl   344.92\n",
            "| epoch   7 |   400/ 2983 batches | lr 0.000370 | ms/batch 23.11 | loss  5.84 | ppl   343.38\n",
            "| epoch   7 |   600/ 2983 batches | lr 0.000368 | ms/batch 23.15 | loss  5.75 | ppl   313.51\n",
            "| epoch   7 |   800/ 2983 batches | lr 0.000366 | ms/batch 23.08 | loss  5.79 | ppl   327.15\n",
            "| epoch   7 |  1000/ 2983 batches | lr 0.000364 | ms/batch 23.09 | loss  5.80 | ppl   331.94\n",
            "| epoch   7 |  1200/ 2983 batches | lr 0.000362 | ms/batch 23.13 | loss  5.79 | ppl   327.56\n",
            "| epoch   7 |  1400/ 2983 batches | lr 0.000360 | ms/batch 23.09 | loss  5.81 | ppl   334.20\n",
            "| epoch   7 |  1600/ 2983 batches | lr 0.000358 | ms/batch 23.09 | loss  5.87 | ppl   353.56\n",
            "| epoch   7 |  1800/ 2983 batches | lr 0.000356 | ms/batch 23.09 | loss  5.79 | ppl   325.82\n",
            "| epoch   7 |  2000/ 2983 batches | lr 0.000354 | ms/batch 23.08 | loss  5.82 | ppl   337.53\n",
            "| epoch   7 |  2200/ 2983 batches | lr 0.000353 | ms/batch 23.10 | loss  5.75 | ppl   314.81\n",
            "| epoch   7 |  2400/ 2983 batches | lr 0.000351 | ms/batch 23.10 | loss  5.77 | ppl   320.39\n",
            "| epoch   7 |  2600/ 2983 batches | lr 0.000349 | ms/batch 23.11 | loss  5.81 | ppl   331.96\n",
            "| epoch   7 |  2800/ 2983 batches | lr 0.000347 | ms/batch 23.10 | loss  5.75 | ppl   312.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 71.30s | valid loss  6.31 | valid ppl   550.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000346\n",
            "| epoch   8 |   200/ 2983 batches | lr 0.000344 | ms/batch 23.24 | loss  5.80 | ppl   331.92\n",
            "| epoch   8 |   400/ 2983 batches | lr 0.000343 | ms/batch 23.07 | loss  5.80 | ppl   331.21\n",
            "| epoch   8 |   600/ 2983 batches | lr 0.000341 | ms/batch 23.09 | loss  5.71 | ppl   302.06\n",
            "| epoch   8 |   800/ 2983 batches | lr 0.000340 | ms/batch 23.10 | loss  5.76 | ppl   315.82\n",
            "| epoch   8 |  1000/ 2983 batches | lr 0.000338 | ms/batch 23.09 | loss  5.77 | ppl   320.15\n",
            "| epoch   8 |  1200/ 2983 batches | lr 0.000336 | ms/batch 23.14 | loss  5.75 | ppl   315.62\n",
            "| epoch   8 |  1400/ 2983 batches | lr 0.000335 | ms/batch 23.10 | loss  5.77 | ppl   321.55\n",
            "| epoch   8 |  1600/ 2983 batches | lr 0.000333 | ms/batch 23.11 | loss  5.83 | ppl   341.31\n",
            "| epoch   8 |  1800/ 2983 batches | lr 0.000332 | ms/batch 23.14 | loss  5.75 | ppl   314.84\n",
            "| epoch   8 |  2000/ 2983 batches | lr 0.000330 | ms/batch 23.12 | loss  5.79 | ppl   326.34\n",
            "| epoch   8 |  2200/ 2983 batches | lr 0.000329 | ms/batch 23.17 | loss  5.72 | ppl   303.72\n",
            "| epoch   8 |  2400/ 2983 batches | lr 0.000328 | ms/batch 23.10 | loss  5.74 | ppl   309.98\n",
            "| epoch   8 |  2600/ 2983 batches | lr 0.000326 | ms/batch 23.10 | loss  5.77 | ppl   320.61\n",
            "| epoch   8 |  2800/ 2983 batches | lr 0.000325 | ms/batch 23.13 | loss  5.71 | ppl   302.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 71.33s | valid loss  6.30 | valid ppl   541.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000324\n",
            "| epoch   9 |   200/ 2983 batches | lr 0.000322 | ms/batch 23.22 | loss  5.77 | ppl   321.50\n",
            "| epoch   9 |   400/ 2983 batches | lr 0.000321 | ms/batch 23.09 | loss  5.77 | ppl   320.68\n",
            "| epoch   9 |   600/ 2983 batches | lr 0.000320 | ms/batch 23.09 | loss  5.68 | ppl   293.82\n",
            "| epoch   9 |   800/ 2983 batches | lr 0.000318 | ms/batch 23.11 | loss  5.73 | ppl   306.63\n",
            "| epoch   9 |  1000/ 2983 batches | lr 0.000317 | ms/batch 23.11 | loss  5.74 | ppl   309.92\n",
            "| epoch   9 |  1200/ 2983 batches | lr 0.000316 | ms/batch 23.11 | loss  5.73 | ppl   306.82\n",
            "| epoch   9 |  1400/ 2983 batches | lr 0.000315 | ms/batch 23.08 | loss  5.74 | ppl   312.28\n",
            "| epoch   9 |  1600/ 2983 batches | lr 0.000313 | ms/batch 23.10 | loss  5.80 | ppl   331.04\n",
            "| epoch   9 |  1800/ 2983 batches | lr 0.000312 | ms/batch 23.14 | loss  5.72 | ppl   305.47\n",
            "| epoch   9 |  2000/ 2983 batches | lr 0.000311 | ms/batch 23.13 | loss  5.76 | ppl   316.59\n",
            "| epoch   9 |  2200/ 2983 batches | lr 0.000310 | ms/batch 23.16 | loss  5.68 | ppl   294.17\n",
            "| epoch   9 |  2400/ 2983 batches | lr 0.000308 | ms/batch 23.14 | loss  5.71 | ppl   300.63\n",
            "| epoch   9 |  2600/ 2983 batches | lr 0.000307 | ms/batch 23.11 | loss  5.74 | ppl   311.83\n",
            "| epoch   9 |  2800/ 2983 batches | lr 0.000306 | ms/batch 23.16 | loss  5.68 | ppl   294.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 71.36s | valid loss  6.27 | valid ppl   530.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000305\n",
            "| epoch  10 |   200/ 2983 batches | lr 0.000304 | ms/batch 23.24 | loss  5.74 | ppl   312.23\n",
            "| epoch  10 |   400/ 2983 batches | lr 0.000303 | ms/batch 23.09 | loss  5.75 | ppl   312.78\n",
            "| epoch  10 |   600/ 2983 batches | lr 0.000302 | ms/batch 23.09 | loss  5.65 | ppl   284.93\n",
            "| epoch  10 |   800/ 2983 batches | lr 0.000301 | ms/batch 23.09 | loss  5.70 | ppl   298.07\n",
            "| epoch  10 |  1000/ 2983 batches | lr 0.000300 | ms/batch 23.07 | loss  5.71 | ppl   302.14\n",
            "| epoch  10 |  1200/ 2983 batches | lr 0.000299 | ms/batch 23.08 | loss  5.70 | ppl   297.99\n",
            "| epoch  10 |  1400/ 2983 batches | lr 0.000297 | ms/batch 23.10 | loss  5.72 | ppl   304.14\n",
            "| epoch  10 |  1600/ 2983 batches | lr 0.000296 | ms/batch 23.08 | loss  5.78 | ppl   322.79\n",
            "| epoch  10 |  1800/ 2983 batches | lr 0.000295 | ms/batch 23.12 | loss  5.70 | ppl   297.39\n",
            "| epoch  10 |  2000/ 2983 batches | lr 0.000294 | ms/batch 23.08 | loss  5.73 | ppl   309.14\n",
            "| epoch  10 |  2200/ 2983 batches | lr 0.000293 | ms/batch 23.09 | loss  5.66 | ppl   287.97\n",
            "| epoch  10 |  2400/ 2983 batches | lr 0.000292 | ms/batch 23.14 | loss  5.68 | ppl   293.67\n",
            "| epoch  10 |  2600/ 2983 batches | lr 0.000291 | ms/batch 23.10 | loss  5.72 | ppl   304.72\n",
            "| epoch  10 |  2800/ 2983 batches | lr 0.000290 | ms/batch 23.13 | loss  5.66 | ppl   286.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 71.30s | valid loss  6.26 | valid ppl   525.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000289\n",
            "| epoch  11 |   200/ 2983 batches | lr 0.000288 | ms/batch 23.24 | loss  5.72 | ppl   304.25\n",
            "| epoch  11 |   400/ 2983 batches | lr 0.000288 | ms/batch 23.14 | loss  5.72 | ppl   306.20\n",
            "| epoch  11 |   600/ 2983 batches | lr 0.000287 | ms/batch 23.11 | loss  5.63 | ppl   277.42\n",
            "| epoch  11 |   800/ 2983 batches | lr 0.000286 | ms/batch 23.14 | loss  5.68 | ppl   291.93\n",
            "| epoch  11 |  1000/ 2983 batches | lr 0.000285 | ms/batch 23.10 | loss  5.69 | ppl   294.61\n",
            "| epoch  11 |  1200/ 2983 batches | lr 0.000284 | ms/batch 23.14 | loss  5.68 | ppl   291.57\n",
            "| epoch  11 |  1400/ 2983 batches | lr 0.000283 | ms/batch 23.18 | loss  5.69 | ppl   297.32\n",
            "| epoch  11 |  1600/ 2983 batches | lr 0.000282 | ms/batch 23.12 | loss  5.75 | ppl   315.27\n",
            "| epoch  11 |  1800/ 2983 batches | lr 0.000281 | ms/batch 23.11 | loss  5.67 | ppl   291.47\n",
            "| epoch  11 |  2000/ 2983 batches | lr 0.000280 | ms/batch 23.15 | loss  5.71 | ppl   301.39\n",
            "| epoch  11 |  2200/ 2983 batches | lr 0.000279 | ms/batch 23.09 | loss  5.64 | ppl   281.16\n",
            "| epoch  11 |  2400/ 2983 batches | lr 0.000278 | ms/batch 23.12 | loss  5.66 | ppl   286.28\n",
            "| epoch  11 |  2600/ 2983 batches | lr 0.000278 | ms/batch 23.10 | loss  5.70 | ppl   297.47\n",
            "| epoch  11 |  2800/ 2983 batches | lr 0.000277 | ms/batch 23.10 | loss  5.64 | ppl   281.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 71.37s | valid loss  6.26 | valid ppl   521.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000276\n",
            "| epoch  12 |   200/ 2983 batches | lr 0.000275 | ms/batch 23.21 | loss  5.70 | ppl   297.91\n",
            "| epoch  12 |   400/ 2983 batches | lr 0.000274 | ms/batch 23.13 | loss  5.70 | ppl   299.84\n",
            "| epoch  12 |   600/ 2983 batches | lr 0.000273 | ms/batch 23.08 | loss  5.60 | ppl   271.17\n",
            "| epoch  12 |   800/ 2983 batches | lr 0.000273 | ms/batch 23.14 | loss  5.65 | ppl   284.85\n",
            "| epoch  12 |  1000/ 2983 batches | lr 0.000272 | ms/batch 23.12 | loss  5.66 | ppl   288.30\n",
            "| epoch  12 |  1200/ 2983 batches | lr 0.000271 | ms/batch 23.11 | loss  5.65 | ppl   284.87\n",
            "| epoch  12 |  1400/ 2983 batches | lr 0.000270 | ms/batch 23.14 | loss  5.67 | ppl   291.18\n",
            "| epoch  12 |  1600/ 2983 batches | lr 0.000269 | ms/batch 23.11 | loss  5.73 | ppl   308.27\n",
            "| epoch  12 |  1800/ 2983 batches | lr 0.000269 | ms/batch 23.10 | loss  5.65 | ppl   285.44\n",
            "| epoch  12 |  2000/ 2983 batches | lr 0.000268 | ms/batch 23.15 | loss  5.69 | ppl   295.64\n",
            "| epoch  12 |  2200/ 2983 batches | lr 0.000267 | ms/batch 23.11 | loss  5.62 | ppl   275.01\n",
            "| epoch  12 |  2400/ 2983 batches | lr 0.000266 | ms/batch 23.14 | loss  5.64 | ppl   280.72\n",
            "| epoch  12 |  2600/ 2983 batches | lr 0.000266 | ms/batch 23.10 | loss  5.68 | ppl   291.53\n",
            "| epoch  12 |  2800/ 2983 batches | lr 0.000265 | ms/batch 23.10 | loss  5.62 | ppl   275.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 71.35s | valid loss  6.25 | valid ppl   516.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000264\n",
            "| epoch  13 |   200/ 2983 batches | lr 0.000263 | ms/batch 23.21 | loss  5.67 | ppl   291.26\n",
            "| epoch  13 |   400/ 2983 batches | lr 0.000263 | ms/batch 23.15 | loss  5.68 | ppl   293.08\n",
            "| epoch  13 |   600/ 2983 batches | lr 0.000262 | ms/batch 23.10 | loss  5.58 | ppl   265.64\n",
            "| epoch  13 |   800/ 2983 batches | lr 0.000261 | ms/batch 23.12 | loss  5.63 | ppl   278.76\n",
            "| epoch  13 |  1000/ 2983 batches | lr 0.000261 | ms/batch 23.16 | loss  5.64 | ppl   282.69\n",
            "| epoch  13 |  1200/ 2983 batches | lr 0.000260 | ms/batch 23.13 | loss  5.64 | ppl   280.64\n",
            "| epoch  13 |  1400/ 2983 batches | lr 0.000259 | ms/batch 23.15 | loss  5.66 | ppl   286.69\n",
            "| epoch  13 |  1600/ 2983 batches | lr 0.000259 | ms/batch 23.12 | loss  5.71 | ppl   302.87\n",
            "| epoch  13 |  1800/ 2983 batches | lr 0.000258 | ms/batch 23.11 | loss  5.63 | ppl   280.02\n",
            "| epoch  13 |  2000/ 2983 batches | lr 0.000257 | ms/batch 23.18 | loss  5.67 | ppl   290.71\n",
            "| epoch  13 |  2200/ 2983 batches | lr 0.000256 | ms/batch 23.14 | loss  5.60 | ppl   269.71\n",
            "| epoch  13 |  2400/ 2983 batches | lr 0.000256 | ms/batch 23.14 | loss  5.62 | ppl   274.70\n",
            "| epoch  13 |  2600/ 2983 batches | lr 0.000255 | ms/batch 23.14 | loss  5.66 | ppl   287.02\n",
            "| epoch  13 |  2800/ 2983 batches | lr 0.000254 | ms/batch 23.14 | loss  5.60 | ppl   270.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 71.41s | valid loss  6.24 | valid ppl   511.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000254\n",
            "| epoch  14 |   200/ 2983 batches | lr 0.000253 | ms/batch 23.21 | loss  5.66 | ppl   286.67\n",
            "| epoch  14 |   400/ 2983 batches | lr 0.000253 | ms/batch 23.12 | loss  5.66 | ppl   287.89\n",
            "| epoch  14 |   600/ 2983 batches | lr 0.000252 | ms/batch 23.09 | loss  5.56 | ppl   260.20\n",
            "| epoch  14 |   800/ 2983 batches | lr 0.000251 | ms/batch 23.08 | loss  5.62 | ppl   274.71\n",
            "| epoch  14 |  1000/ 2983 batches | lr 0.000251 | ms/batch 23.12 | loss  5.63 | ppl   277.91\n",
            "| epoch  14 |  1200/ 2983 batches | lr 0.000250 | ms/batch 23.08 | loss  5.62 | ppl   274.71\n",
            "| epoch  14 |  1400/ 2983 batches | lr 0.000249 | ms/batch 23.10 | loss  5.63 | ppl   279.73\n",
            "| epoch  14 |  1600/ 2983 batches | lr 0.000249 | ms/batch 23.10 | loss  5.70 | ppl   297.51\n",
            "| epoch  14 |  1800/ 2983 batches | lr 0.000248 | ms/batch 23.08 | loss  5.62 | ppl   274.90\n",
            "| epoch  14 |  2000/ 2983 batches | lr 0.000248 | ms/batch 23.12 | loss  5.65 | ppl   284.71\n",
            "| epoch  14 |  2200/ 2983 batches | lr 0.000247 | ms/batch 23.10 | loss  5.58 | ppl   265.31\n",
            "| epoch  14 |  2400/ 2983 batches | lr 0.000246 | ms/batch 23.09 | loss  5.60 | ppl   270.94\n",
            "| epoch  14 |  2600/ 2983 batches | lr 0.000246 | ms/batch 23.18 | loss  5.64 | ppl   280.96\n",
            "| epoch  14 |  2800/ 2983 batches | lr 0.000245 | ms/batch 23.10 | loss  5.58 | ppl   266.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 71.31s | valid loss  6.23 | valid ppl   508.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000245\n",
            "| epoch  15 |   200/ 2983 batches | lr 0.000244 | ms/batch 23.20 | loss  5.64 | ppl   281.84\n",
            "| epoch  15 |   400/ 2983 batches | lr 0.000243 | ms/batch 23.09 | loss  5.64 | ppl   282.18\n",
            "| epoch  15 |   600/ 2983 batches | lr 0.000243 | ms/batch 23.13 | loss  5.55 | ppl   256.45\n",
            "| epoch  15 |   800/ 2983 batches | lr 0.000242 | ms/batch 23.07 | loss  5.60 | ppl   269.76\n",
            "| epoch  15 |  1000/ 2983 batches | lr 0.000242 | ms/batch 23.10 | loss  5.61 | ppl   273.52\n",
            "| epoch  15 |  1200/ 2983 batches | lr 0.000241 | ms/batch 23.08 | loss  5.60 | ppl   271.01\n",
            "| epoch  15 |  1400/ 2983 batches | lr 0.000241 | ms/batch 23.07 | loss  5.62 | ppl   276.06\n",
            "| epoch  15 |  1600/ 2983 batches | lr 0.000240 | ms/batch 23.10 | loss  5.68 | ppl   292.70\n",
            "| epoch  15 |  1800/ 2983 batches | lr 0.000240 | ms/batch 23.08 | loss  5.60 | ppl   271.53\n",
            "| epoch  15 |  2000/ 2983 batches | lr 0.000239 | ms/batch 23.17 | loss  5.64 | ppl   280.88\n",
            "| epoch  15 |  2200/ 2983 batches | lr 0.000238 | ms/batch 23.13 | loss  5.56 | ppl   260.79\n",
            "| epoch  15 |  2400/ 2983 batches | lr 0.000238 | ms/batch 23.10 | loss  5.59 | ppl   266.80\n",
            "| epoch  15 |  2600/ 2983 batches | lr 0.000237 | ms/batch 23.15 | loss  5.62 | ppl   276.48\n",
            "| epoch  15 |  2800/ 2983 batches | lr 0.000237 | ms/batch 23.11 | loss  5.57 | ppl   261.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 71.33s | valid loss  6.23 | valid ppl   508.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000236\n",
            "| epoch  16 |   200/ 2983 batches | lr 0.000236 | ms/batch 23.23 | loss  5.63 | ppl   277.28\n",
            "| epoch  16 |   400/ 2983 batches | lr 0.000235 | ms/batch 23.10 | loss  5.63 | ppl   278.25\n",
            "| epoch  16 |   600/ 2983 batches | lr 0.000235 | ms/batch 23.12 | loss  5.53 | ppl   251.44\n",
            "| epoch  16 |   800/ 2983 batches | lr 0.000234 | ms/batch 23.08 | loss  5.58 | ppl   265.93\n",
            "| epoch  16 |  1000/ 2983 batches | lr 0.000234 | ms/batch 23.13 | loss  5.59 | ppl   268.82\n",
            "| epoch  16 |  1200/ 2983 batches | lr 0.000233 | ms/batch 23.12 | loss  5.58 | ppl   265.72\n",
            "| epoch  16 |  1400/ 2983 batches | lr 0.000233 | ms/batch 23.08 | loss  5.61 | ppl   271.84\n",
            "| epoch  16 |  1600/ 2983 batches | lr 0.000232 | ms/batch 23.12 | loss  5.66 | ppl   288.31\n",
            "| epoch  16 |  1800/ 2983 batches | lr 0.000232 | ms/batch 23.08 | loss  5.59 | ppl   267.07\n",
            "| epoch  16 |  2000/ 2983 batches | lr 0.000231 | ms/batch 23.11 | loss  5.62 | ppl   276.76\n",
            "| epoch  16 |  2200/ 2983 batches | lr 0.000231 | ms/batch 23.12 | loss  5.55 | ppl   257.37\n",
            "| epoch  16 |  2400/ 2983 batches | lr 0.000230 | ms/batch 23.11 | loss  5.57 | ppl   262.35\n",
            "| epoch  16 |  2600/ 2983 batches | lr 0.000230 | ms/batch 23.14 | loss  5.61 | ppl   272.64\n",
            "| epoch  16 |  2800/ 2983 batches | lr 0.000229 | ms/batch 23.12 | loss  5.55 | ppl   258.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 71.33s | valid loss  6.23 | valid ppl   509.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000229\n",
            "| epoch  17 |   200/ 2983 batches | lr 0.000228 | ms/batch 23.24 | loss  5.61 | ppl   273.04\n",
            "| epoch  17 |   400/ 2983 batches | lr 0.000228 | ms/batch 23.15 | loss  5.61 | ppl   274.48\n",
            "| epoch  17 |   600/ 2983 batches | lr 0.000227 | ms/batch 23.14 | loss  5.52 | ppl   248.55\n",
            "| epoch  17 |   800/ 2983 batches | lr 0.000227 | ms/batch 23.10 | loss  5.57 | ppl   262.28\n",
            "| epoch  17 |  1000/ 2983 batches | lr 0.000226 | ms/batch 23.09 | loss  5.58 | ppl   265.23\n",
            "| epoch  17 |  1200/ 2983 batches | lr 0.000226 | ms/batch 23.14 | loss  5.57 | ppl   262.46\n",
            "| epoch  17 |  1400/ 2983 batches | lr 0.000226 | ms/batch 23.08 | loss  5.59 | ppl   268.39\n",
            "| epoch  17 |  1600/ 2983 batches | lr 0.000225 | ms/batch 23.10 | loss  5.65 | ppl   284.77\n",
            "| epoch  17 |  1800/ 2983 batches | lr 0.000225 | ms/batch 23.08 | loss  5.58 | ppl   264.06\n",
            "| epoch  17 |  2000/ 2983 batches | lr 0.000224 | ms/batch 23.05 | loss  5.61 | ppl   272.15\n",
            "| epoch  17 |  2200/ 2983 batches | lr 0.000224 | ms/batch 23.12 | loss  5.54 | ppl   253.99\n",
            "| epoch  17 |  2400/ 2983 batches | lr 0.000223 | ms/batch 23.08 | loss  5.56 | ppl   259.35\n",
            "| epoch  17 |  2600/ 2983 batches | lr 0.000223 | ms/batch 23.10 | loss  5.59 | ppl   268.86\n",
            "| epoch  17 |  2800/ 2983 batches | lr 0.000222 | ms/batch 23.12 | loss  5.54 | ppl   255.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 71.32s | valid loss  6.23 | valid ppl   506.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000222\n",
            "| epoch  18 |   200/ 2983 batches | lr 0.000222 | ms/batch 23.22 | loss  5.60 | ppl   269.12\n",
            "| epoch  18 |   400/ 2983 batches | lr 0.000221 | ms/batch 23.15 | loss  5.60 | ppl   271.78\n",
            "| epoch  18 |   600/ 2983 batches | lr 0.000221 | ms/batch 23.11 | loss  5.50 | ppl   245.12\n",
            "| epoch  18 |   800/ 2983 batches | lr 0.000220 | ms/batch 23.12 | loss  5.56 | ppl   258.63\n",
            "| epoch  18 |  1000/ 2983 batches | lr 0.000220 | ms/batch 23.10 | loss  5.57 | ppl   261.89\n",
            "| epoch  18 |  1200/ 2983 batches | lr 0.000219 | ms/batch 23.15 | loss  5.56 | ppl   258.77\n",
            "| epoch  18 |  1400/ 2983 batches | lr 0.000219 | ms/batch 23.11 | loss  5.58 | ppl   265.63\n",
            "| epoch  18 |  1600/ 2983 batches | lr 0.000219 | ms/batch 23.13 | loss  5.64 | ppl   281.46\n",
            "| epoch  18 |  1800/ 2983 batches | lr 0.000218 | ms/batch 23.12 | loss  5.56 | ppl   260.04\n",
            "| epoch  18 |  2000/ 2983 batches | lr 0.000218 | ms/batch 23.07 | loss  5.60 | ppl   270.47\n",
            "| epoch  18 |  2200/ 2983 batches | lr 0.000217 | ms/batch 23.13 | loss  5.53 | ppl   251.46\n",
            "| epoch  18 |  2400/ 2983 batches | lr 0.000217 | ms/batch 23.11 | loss  5.55 | ppl   256.22\n",
            "| epoch  18 |  2600/ 2983 batches | lr 0.000217 | ms/batch 23.08 | loss  5.59 | ppl   266.49\n",
            "| epoch  18 |  2800/ 2983 batches | lr 0.000216 | ms/batch 23.09 | loss  5.53 | ppl   252.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 71.34s | valid loss  6.22 | valid ppl   501.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000216\n",
            "| epoch  19 |   200/ 2983 batches | lr 0.000215 | ms/batch 23.21 | loss  5.59 | ppl   266.41\n",
            "| epoch  19 |   400/ 2983 batches | lr 0.000215 | ms/batch 23.07 | loss  5.59 | ppl   268.70\n",
            "| epoch  19 |   600/ 2983 batches | lr 0.000215 | ms/batch 23.10 | loss  5.49 | ppl   242.52\n",
            "| epoch  19 |   800/ 2983 batches | lr 0.000214 | ms/batch 23.10 | loss  5.54 | ppl   255.75\n",
            "| epoch  19 |  1000/ 2983 batches | lr 0.000214 | ms/batch 23.08 | loss  5.55 | ppl   258.34\n",
            "| epoch  19 |  1200/ 2983 batches | lr 0.000213 | ms/batch 23.12 | loss  5.55 | ppl   256.54\n",
            "| epoch  19 |  1400/ 2983 batches | lr 0.000213 | ms/batch 23.11 | loss  5.57 | ppl   262.00\n",
            "| epoch  19 |  1600/ 2983 batches | lr 0.000213 | ms/batch 23.07 | loss  5.63 | ppl   278.70\n",
            "| epoch  19 |  1800/ 2983 batches | lr 0.000212 | ms/batch 23.08 | loss  5.55 | ppl   257.41\n",
            "| epoch  19 |  2000/ 2983 batches | lr 0.000212 | ms/batch 23.05 | loss  5.59 | ppl   267.00\n",
            "| epoch  19 |  2200/ 2983 batches | lr 0.000211 | ms/batch 23.12 | loss  5.52 | ppl   248.73\n",
            "| epoch  19 |  2400/ 2983 batches | lr 0.000211 | ms/batch 23.07 | loss  5.53 | ppl   252.97\n",
            "| epoch  19 |  2600/ 2983 batches | lr 0.000211 | ms/batch 23.07 | loss  5.57 | ppl   263.43\n",
            "| epoch  19 |  2800/ 2983 batches | lr 0.000210 | ms/batch 23.09 | loss  5.52 | ppl   248.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 71.26s | valid loss  6.22 | valid ppl   504.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000210\n",
            "| epoch  20 |   200/ 2983 batches | lr 0.000210 | ms/batch 23.24 | loss  5.57 | ppl   263.24\n",
            "| epoch  20 |   400/ 2983 batches | lr 0.000209 | ms/batch 23.10 | loss  5.58 | ppl   265.96\n",
            "| epoch  20 |   600/ 2983 batches | lr 0.000209 | ms/batch 23.09 | loss  5.48 | ppl   240.43\n",
            "| epoch  20 |   800/ 2983 batches | lr 0.000209 | ms/batch 23.14 | loss  5.53 | ppl   253.13\n",
            "| epoch  20 |  1000/ 2983 batches | lr 0.000208 | ms/batch 23.15 | loss  5.55 | ppl   256.38\n",
            "| epoch  20 |  1200/ 2983 batches | lr 0.000208 | ms/batch 23.13 | loss  5.53 | ppl   253.30\n",
            "| epoch  20 |  1400/ 2983 batches | lr 0.000207 | ms/batch 23.11 | loss  5.56 | ppl   259.70\n",
            "| epoch  20 |  1600/ 2983 batches | lr 0.000207 | ms/batch 23.12 | loss  5.62 | ppl   275.81\n",
            "| epoch  20 |  1800/ 2983 batches | lr 0.000207 | ms/batch 23.12 | loss  5.54 | ppl   255.36\n",
            "| epoch  20 |  2000/ 2983 batches | lr 0.000206 | ms/batch 23.10 | loss  5.57 | ppl   263.56\n",
            "| epoch  20 |  2200/ 2983 batches | lr 0.000206 | ms/batch 23.12 | loss  5.50 | ppl   245.37\n",
            "| epoch  20 |  2400/ 2983 batches | lr 0.000206 | ms/batch 23.13 | loss  5.52 | ppl   249.66\n",
            "| epoch  20 |  2600/ 2983 batches | lr 0.000205 | ms/batch 23.08 | loss  5.56 | ppl   260.98\n",
            "| epoch  20 |  2800/ 2983 batches | lr 0.000205 | ms/batch 23.13 | loss  5.51 | ppl   247.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 71.35s | valid loss  6.22 | valid ppl   505.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000205\n",
            "| epoch  21 |   200/ 2983 batches | lr 0.000204 | ms/batch 23.20 | loss  5.56 | ppl   260.77\n",
            "| epoch  21 |   400/ 2983 batches | lr 0.000204 | ms/batch 23.13 | loss  5.57 | ppl   262.96\n",
            "| epoch  21 |   600/ 2983 batches | lr 0.000204 | ms/batch 23.09 | loss  5.47 | ppl   237.93\n",
            "| epoch  21 |   800/ 2983 batches | lr 0.000203 | ms/batch 23.11 | loss  5.52 | ppl   250.63\n",
            "| epoch  21 |  1000/ 2983 batches | lr 0.000203 | ms/batch 23.10 | loss  5.53 | ppl   253.26\n",
            "| epoch  21 |  1200/ 2983 batches | lr 0.000203 | ms/batch 23.12 | loss  5.52 | ppl   250.70\n",
            "| epoch  21 |  1400/ 2983 batches | lr 0.000202 | ms/batch 23.15 | loss  5.54 | ppl   255.51\n",
            "| epoch  21 |  1600/ 2983 batches | lr 0.000202 | ms/batch 23.14 | loss  5.61 | ppl   273.05\n",
            "| epoch  21 |  1800/ 2983 batches | lr 0.000202 | ms/batch 23.18 | loss  5.54 | ppl   253.73\n",
            "| epoch  21 |  2000/ 2983 batches | lr 0.000201 | ms/batch 23.14 | loss  5.57 | ppl   261.81\n",
            "| epoch  21 |  2200/ 2983 batches | lr 0.000201 | ms/batch 23.09 | loss  5.49 | ppl   242.79\n",
            "| epoch  21 |  2400/ 2983 batches | lr 0.000201 | ms/batch 23.14 | loss  5.51 | ppl   247.58\n",
            "| epoch  21 |  2600/ 2983 batches | lr 0.000200 | ms/batch 23.09 | loss  5.55 | ppl   258.02\n",
            "| epoch  21 |  2800/ 2983 batches | lr 0.000200 | ms/batch 23.12 | loss  5.50 | ppl   244.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 71.35s | valid loss  6.22 | valid ppl   503.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000200\n",
            "| epoch  22 |   200/ 2983 batches | lr 0.000199 | ms/batch 23.20 | loss  5.55 | ppl   257.91\n",
            "| epoch  22 |   400/ 2983 batches | lr 0.000199 | ms/batch 23.10 | loss  5.56 | ppl   259.40\n",
            "| epoch  22 |   600/ 2983 batches | lr 0.000199 | ms/batch 23.08 | loss  5.46 | ppl   235.71\n",
            "| epoch  22 |   800/ 2983 batches | lr 0.000198 | ms/batch 23.10 | loss  5.51 | ppl   248.19\n",
            "| epoch  22 |  1000/ 2983 batches | lr 0.000198 | ms/batch 23.16 | loss  5.53 | ppl   251.38\n",
            "| epoch  22 |  1200/ 2983 batches | lr 0.000198 | ms/batch 23.12 | loss  5.51 | ppl   247.70\n",
            "| epoch  22 |  1400/ 2983 batches | lr 0.000198 | ms/batch 23.15 | loss  5.54 | ppl   253.85\n",
            "| epoch  22 |  1600/ 2983 batches | lr 0.000197 | ms/batch 23.13 | loss  5.60 | ppl   270.27\n",
            "| epoch  22 |  1800/ 2983 batches | lr 0.000197 | ms/batch 23.14 | loss  5.52 | ppl   250.77\n",
            "| epoch  22 |  2000/ 2983 batches | lr 0.000197 | ms/batch 23.12 | loss  5.56 | ppl   259.31\n",
            "| epoch  22 |  2200/ 2983 batches | lr 0.000196 | ms/batch 23.11 | loss  5.48 | ppl   240.44\n",
            "| epoch  22 |  2400/ 2983 batches | lr 0.000196 | ms/batch 23.14 | loss  5.50 | ppl   245.46\n",
            "| epoch  22 |  2600/ 2983 batches | lr 0.000196 | ms/batch 23.12 | loss  5.54 | ppl   255.32\n",
            "| epoch  22 |  2800/ 2983 batches | lr 0.000195 | ms/batch 23.12 | loss  5.49 | ppl   241.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 71.36s | valid loss  6.23 | valid ppl   506.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000195\n",
            "| epoch  23 |   200/ 2983 batches | lr 0.000195 | ms/batch 23.24 | loss  5.55 | ppl   256.01\n",
            "| epoch  23 |   400/ 2983 batches | lr 0.000195 | ms/batch 23.11 | loss  5.55 | ppl   256.54\n",
            "| epoch  23 |   600/ 2983 batches | lr 0.000194 | ms/batch 23.09 | loss  5.45 | ppl   233.24\n",
            "| epoch  23 |   800/ 2983 batches | lr 0.000194 | ms/batch 23.10 | loss  5.50 | ppl   245.92\n",
            "| epoch  23 |  1000/ 2983 batches | lr 0.000194 | ms/batch 23.09 | loss  5.52 | ppl   248.54\n",
            "| epoch  23 |  1200/ 2983 batches | lr 0.000193 | ms/batch 23.09 | loss  5.51 | ppl   245.93\n",
            "| epoch  23 |  1400/ 2983 batches | lr 0.000193 | ms/batch 23.12 | loss  5.53 | ppl   251.54\n",
            "| epoch  23 |  1600/ 2983 batches | lr 0.000193 | ms/batch 23.11 | loss  5.59 | ppl   267.36\n",
            "| epoch  23 |  1800/ 2983 batches | lr 0.000193 | ms/batch 23.10 | loss  5.51 | ppl   247.45\n",
            "| epoch  23 |  2000/ 2983 batches | lr 0.000192 | ms/batch 23.14 | loss  5.55 | ppl   256.46\n",
            "| epoch  23 |  2200/ 2983 batches | lr 0.000192 | ms/batch 23.12 | loss  5.47 | ppl   238.48\n",
            "| epoch  23 |  2400/ 2983 batches | lr 0.000192 | ms/batch 23.12 | loss  5.49 | ppl   242.33\n",
            "| epoch  23 |  2600/ 2983 batches | lr 0.000191 | ms/batch 23.14 | loss  5.53 | ppl   252.96\n",
            "| epoch  23 |  2800/ 2983 batches | lr 0.000191 | ms/batch 23.14 | loss  5.48 | ppl   239.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 71.34s | valid loss  6.23 | valid ppl   505.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000191\n",
            "| epoch  24 |   200/ 2983 batches | lr 0.000191 | ms/batch 23.22 | loss  5.54 | ppl   253.79\n",
            "| epoch  24 |   400/ 2983 batches | lr 0.000190 | ms/batch 23.11 | loss  5.54 | ppl   254.67\n",
            "| epoch  24 |   600/ 2983 batches | lr 0.000190 | ms/batch 23.10 | loss  5.44 | ppl   231.33\n",
            "| epoch  24 |   800/ 2983 batches | lr 0.000190 | ms/batch 23.08 | loss  5.50 | ppl   243.61\n",
            "| epoch  24 |  1000/ 2983 batches | lr 0.000189 | ms/batch 23.12 | loss  5.51 | ppl   246.45\n",
            "| epoch  24 |  1200/ 2983 batches | lr 0.000189 | ms/batch 23.08 | loss  5.50 | ppl   243.49\n",
            "| epoch  24 |  1400/ 2983 batches | lr 0.000189 | ms/batch 23.09 | loss  5.52 | ppl   250.00\n",
            "| epoch  24 |  1600/ 2983 batches | lr 0.000189 | ms/batch 23.10 | loss  5.58 | ppl   265.86\n",
            "| epoch  24 |  1800/ 2983 batches | lr 0.000188 | ms/batch 23.09 | loss  5.50 | ppl   245.44\n",
            "| epoch  24 |  2000/ 2983 batches | lr 0.000188 | ms/batch 23.11 | loss  5.54 | ppl   254.99\n",
            "| epoch  24 |  2200/ 2983 batches | lr 0.000188 | ms/batch 23.09 | loss  5.46 | ppl   236.20\n",
            "| epoch  24 |  2400/ 2983 batches | lr 0.000188 | ms/batch 23.13 | loss  5.48 | ppl   240.36\n",
            "| epoch  24 |  2600/ 2983 batches | lr 0.000187 | ms/batch 23.09 | loss  5.53 | ppl   252.47\n",
            "| epoch  24 |  2800/ 2983 batches | lr 0.000187 | ms/batch 23.11 | loss  5.47 | ppl   237.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 71.30s | valid loss  6.23 | valid ppl   506.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000187\n",
            "| epoch  25 |   200/ 2983 batches | lr 0.000187 | ms/batch 23.23 | loss  5.53 | ppl   251.92\n",
            "| epoch  25 |   400/ 2983 batches | lr 0.000186 | ms/batch 23.12 | loss  5.53 | ppl   252.77\n",
            "| epoch  25 |   600/ 2983 batches | lr 0.000186 | ms/batch 23.11 | loss  5.43 | ppl   228.75\n",
            "| epoch  25 |   800/ 2983 batches | lr 0.000186 | ms/batch 23.07 | loss  5.49 | ppl   241.54\n",
            "| epoch  25 |  1000/ 2983 batches | lr 0.000186 | ms/batch 23.09 | loss  5.50 | ppl   244.81\n",
            "| epoch  25 |  1200/ 2983 batches | lr 0.000185 | ms/batch 23.08 | loss  5.48 | ppl   240.90\n",
            "| epoch  25 |  1400/ 2983 batches | lr 0.000185 | ms/batch 23.07 | loss  5.51 | ppl   247.19\n",
            "| epoch  25 |  1600/ 2983 batches | lr 0.000185 | ms/batch 23.09 | loss  5.57 | ppl   263.19\n",
            "| epoch  25 |  1800/ 2983 batches | lr 0.000185 | ms/batch 23.08 | loss  5.50 | ppl   244.29\n",
            "| epoch  25 |  2000/ 2983 batches | lr 0.000184 | ms/batch 23.08 | loss  5.53 | ppl   252.31\n",
            "| epoch  25 |  2200/ 2983 batches | lr 0.000184 | ms/batch 23.10 | loss  5.46 | ppl   234.65\n",
            "| epoch  25 |  2400/ 2983 batches | lr 0.000184 | ms/batch 23.09 | loss  5.47 | ppl   238.17\n",
            "| epoch  25 |  2600/ 2983 batches | lr 0.000184 | ms/batch 23.17 | loss  5.52 | ppl   249.48\n",
            "| epoch  25 |  2800/ 2983 batches | lr 0.000183 | ms/batch 23.11 | loss  5.46 | ppl   236.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 71.30s | valid loss  6.22 | valid ppl   503.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000183\n",
            "| epoch  26 |   200/ 2983 batches | lr 0.000183 | ms/batch 23.21 | loss  5.52 | ppl   249.85\n",
            "| epoch  26 |   400/ 2983 batches | lr 0.000183 | ms/batch 23.09 | loss  5.52 | ppl   250.38\n",
            "| epoch  26 |   600/ 2983 batches | lr 0.000182 | ms/batch 23.12 | loss  5.42 | ppl   226.94\n",
            "| epoch  26 |   800/ 2983 batches | lr 0.000182 | ms/batch 23.11 | loss  5.48 | ppl   240.28\n",
            "| epoch  26 |  1000/ 2983 batches | lr 0.000182 | ms/batch 23.11 | loss  5.49 | ppl   242.22\n",
            "| epoch  26 |  1200/ 2983 batches | lr 0.000182 | ms/batch 23.11 | loss  5.48 | ppl   239.41\n",
            "| epoch  26 |  1400/ 2983 batches | lr 0.000181 | ms/batch 23.09 | loss  5.50 | ppl   245.62\n",
            "| epoch  26 |  1600/ 2983 batches | lr 0.000181 | ms/batch 23.13 | loss  5.57 | ppl   261.94\n",
            "| epoch  26 |  1800/ 2983 batches | lr 0.000181 | ms/batch 23.09 | loss  5.49 | ppl   241.84\n",
            "| epoch  26 |  2000/ 2983 batches | lr 0.000181 | ms/batch 23.07 | loss  5.52 | ppl   250.58\n",
            "| epoch  26 |  2200/ 2983 batches | lr 0.000180 | ms/batch 23.07 | loss  5.45 | ppl   232.27\n",
            "| epoch  26 |  2400/ 2983 batches | lr 0.000180 | ms/batch 23.07 | loss  5.47 | ppl   236.59\n",
            "| epoch  26 |  2600/ 2983 batches | lr 0.000180 | ms/batch 23.07 | loss  5.51 | ppl   247.18\n",
            "| epoch  26 |  2800/ 2983 batches | lr 0.000180 | ms/batch 23.09 | loss  5.46 | ppl   234.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 71.27s | valid loss  6.22 | valid ppl   503.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000180\n",
            "| epoch  27 |   200/ 2983 batches | lr 0.000179 | ms/batch 23.25 | loss  5.51 | ppl   246.99\n",
            "| epoch  27 |   400/ 2983 batches | lr 0.000179 | ms/batch 23.10 | loss  5.52 | ppl   249.00\n",
            "| epoch  27 |   600/ 2983 batches | lr 0.000179 | ms/batch 23.10 | loss  5.42 | ppl   225.86\n",
            "| epoch  27 |   800/ 2983 batches | lr 0.000179 | ms/batch 23.09 | loss  5.47 | ppl   237.84\n",
            "| epoch  27 |  1000/ 2983 batches | lr 0.000178 | ms/batch 23.09 | loss  5.48 | ppl   240.66\n",
            "| epoch  27 |  1200/ 2983 batches | lr 0.000178 | ms/batch 23.09 | loss  5.47 | ppl   237.14\n",
            "| epoch  27 |  1400/ 2983 batches | lr 0.000178 | ms/batch 23.11 | loss  5.50 | ppl   243.62\n",
            "| epoch  27 |  1600/ 2983 batches | lr 0.000178 | ms/batch 23.08 | loss  5.56 | ppl   259.32\n",
            "| epoch  27 |  1800/ 2983 batches | lr 0.000177 | ms/batch 23.10 | loss  5.48 | ppl   239.58\n",
            "| epoch  27 |  2000/ 2983 batches | lr 0.000177 | ms/batch 23.08 | loss  5.51 | ppl   248.29\n",
            "| epoch  27 |  2200/ 2983 batches | lr 0.000177 | ms/batch 23.11 | loss  5.44 | ppl   230.50\n",
            "| epoch  27 |  2400/ 2983 batches | lr 0.000177 | ms/batch 23.07 | loss  5.46 | ppl   234.53\n",
            "| epoch  27 |  2600/ 2983 batches | lr 0.000177 | ms/batch 23.07 | loss  5.50 | ppl   244.42\n",
            "| epoch  27 |  2800/ 2983 batches | lr 0.000176 | ms/batch 23.09 | loss  5.45 | ppl   232.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 71.29s | valid loss  6.22 | valid ppl   503.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000176\n",
            "| epoch  28 |   200/ 2983 batches | lr 0.000176 | ms/batch 23.21 | loss  5.50 | ppl   245.15\n",
            "| epoch  28 |   400/ 2983 batches | lr 0.000176 | ms/batch 23.07 | loss  5.51 | ppl   246.70\n",
            "| epoch  28 |   600/ 2983 batches | lr 0.000175 | ms/batch 23.10 | loss  5.41 | ppl   223.52\n",
            "| epoch  28 |   800/ 2983 batches | lr 0.000175 | ms/batch 23.10 | loss  5.46 | ppl   235.61\n",
            "| epoch  28 |  1000/ 2983 batches | lr 0.000175 | ms/batch 23.08 | loss  5.48 | ppl   239.01\n",
            "| epoch  28 |  1200/ 2983 batches | lr 0.000175 | ms/batch 23.10 | loss  5.46 | ppl   235.36\n",
            "| epoch  28 |  1400/ 2983 batches | lr 0.000175 | ms/batch 23.14 | loss  5.49 | ppl   241.17\n",
            "| epoch  28 |  1600/ 2983 batches | lr 0.000174 | ms/batch 23.12 | loss  5.55 | ppl   257.46\n",
            "| epoch  28 |  1800/ 2983 batches | lr 0.000174 | ms/batch 23.09 | loss  5.47 | ppl   237.78\n",
            "| epoch  28 |  2000/ 2983 batches | lr 0.000174 | ms/batch 23.06 | loss  5.51 | ppl   246.79\n",
            "| epoch  28 |  2200/ 2983 batches | lr 0.000174 | ms/batch 23.12 | loss  5.43 | ppl   229.00\n",
            "| epoch  28 |  2400/ 2983 batches | lr 0.000174 | ms/batch 23.11 | loss  5.45 | ppl   233.23\n",
            "| epoch  28 |  2600/ 2983 batches | lr 0.000173 | ms/batch 23.12 | loss  5.49 | ppl   242.91\n",
            "| epoch  28 |  2800/ 2983 batches | lr 0.000173 | ms/batch 23.13 | loss  5.44 | ppl   231.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 71.31s | valid loss  6.23 | valid ppl   505.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000173\n",
            "| epoch  29 |   200/ 2983 batches | lr 0.000173 | ms/batch 23.22 | loss  5.49 | ppl   242.92\n",
            "| epoch  29 |   400/ 2983 batches | lr 0.000173 | ms/batch 23.11 | loss  5.50 | ppl   245.31\n",
            "| epoch  29 |   600/ 2983 batches | lr 0.000172 | ms/batch 23.08 | loss  5.40 | ppl   221.95\n",
            "| epoch  29 |   800/ 2983 batches | lr 0.000172 | ms/batch 23.05 | loss  5.45 | ppl   233.91\n",
            "| epoch  29 |  1000/ 2983 batches | lr 0.000172 | ms/batch 23.04 | loss  5.47 | ppl   236.57\n",
            "| epoch  29 |  1200/ 2983 batches | lr 0.000172 | ms/batch 23.08 | loss  5.45 | ppl   233.53\n",
            "| epoch  29 |  1400/ 2983 batches | lr 0.000172 | ms/batch 23.14 | loss  5.48 | ppl   238.98\n",
            "| epoch  29 |  1600/ 2983 batches | lr 0.000171 | ms/batch 23.08 | loss  5.54 | ppl   254.97\n",
            "| epoch  29 |  1800/ 2983 batches | lr 0.000171 | ms/batch 23.10 | loss  5.47 | ppl   236.99\n",
            "| epoch  29 |  2000/ 2983 batches | lr 0.000171 | ms/batch 23.12 | loss  5.50 | ppl   245.11\n",
            "| epoch  29 |  2200/ 2983 batches | lr 0.000171 | ms/batch 23.12 | loss  5.43 | ppl   227.66\n",
            "| epoch  29 |  2400/ 2983 batches | lr 0.000171 | ms/batch 23.09 | loss  5.45 | ppl   232.13\n",
            "| epoch  29 |  2600/ 2983 batches | lr 0.000170 | ms/batch 23.09 | loss  5.49 | ppl   241.08\n",
            "| epoch  29 |  2800/ 2983 batches | lr 0.000170 | ms/batch 23.13 | loss  5.43 | ppl   228.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 71.29s | valid loss  6.22 | valid ppl   504.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000170\n",
            "| epoch  30 |   200/ 2983 batches | lr 0.000170 | ms/batch 23.24 | loss  5.49 | ppl   241.81\n",
            "| epoch  30 |   400/ 2983 batches | lr 0.000170 | ms/batch 23.11 | loss  5.49 | ppl   242.44\n",
            "| epoch  30 |   600/ 2983 batches | lr 0.000169 | ms/batch 23.09 | loss  5.39 | ppl   219.75\n",
            "| epoch  30 |   800/ 2983 batches | lr 0.000169 | ms/batch 23.11 | loss  5.45 | ppl   231.78\n",
            "| epoch  30 |  1000/ 2983 batches | lr 0.000169 | ms/batch 23.10 | loss  5.46 | ppl   234.63\n",
            "| epoch  30 |  1200/ 2983 batches | lr 0.000169 | ms/batch 23.14 | loss  5.45 | ppl   231.74\n",
            "| epoch  30 |  1400/ 2983 batches | lr 0.000169 | ms/batch 23.08 | loss  5.47 | ppl   237.46\n",
            "| epoch  30 |  1600/ 2983 batches | lr 0.000168 | ms/batch 23.08 | loss  5.54 | ppl   253.61\n",
            "| epoch  30 |  1800/ 2983 batches | lr 0.000168 | ms/batch 23.12 | loss  5.46 | ppl   233.99\n",
            "| epoch  30 |  2000/ 2983 batches | lr 0.000168 | ms/batch 23.07 | loss  5.50 | ppl   243.57\n",
            "| epoch  30 |  2200/ 2983 batches | lr 0.000168 | ms/batch 23.08 | loss  5.42 | ppl   225.67\n",
            "| epoch  30 |  2400/ 2983 batches | lr 0.000168 | ms/batch 23.09 | loss  5.44 | ppl   229.89\n",
            "| epoch  30 |  2600/ 2983 batches | lr 0.000167 | ms/batch 23.08 | loss  5.48 | ppl   239.97\n",
            "| epoch  30 |  2800/ 2983 batches | lr 0.000167 | ms/batch 23.10 | loss  5.43 | ppl   227.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 71.29s | valid loss  6.23 | valid ppl   506.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000167\n",
            "| epoch  31 |   200/ 2983 batches | lr 0.000167 | ms/batch 23.23 | loss  5.48 | ppl   239.92\n",
            "| epoch  31 |   400/ 2983 batches | lr 0.000167 | ms/batch 23.11 | loss  5.49 | ppl   242.09\n",
            "| epoch  31 |   600/ 2983 batches | lr 0.000167 | ms/batch 23.12 | loss  5.38 | ppl   217.86\n",
            "| epoch  31 |   800/ 2983 batches | lr 0.000166 | ms/batch 23.13 | loss  5.44 | ppl   230.35\n",
            "| epoch  31 |  1000/ 2983 batches | lr 0.000166 | ms/batch 23.15 | loss  5.45 | ppl   232.98\n",
            "| epoch  31 |  1200/ 2983 batches | lr 0.000166 | ms/batch 23.16 | loss  5.44 | ppl   229.41\n",
            "| epoch  31 |  1400/ 2983 batches | lr 0.000166 | ms/batch 23.12 | loss  5.46 | ppl   235.96\n",
            "| epoch  31 |  1600/ 2983 batches | lr 0.000166 | ms/batch 23.11 | loss  5.53 | ppl   251.74\n",
            "| epoch  31 |  1800/ 2983 batches | lr 0.000165 | ms/batch 23.13 | loss  5.45 | ppl   232.91\n",
            "| epoch  31 |  2000/ 2983 batches | lr 0.000165 | ms/batch 23.11 | loss  5.49 | ppl   241.52\n",
            "| epoch  31 |  2200/ 2983 batches | lr 0.000165 | ms/batch 23.05 | loss  5.41 | ppl   224.61\n",
            "| epoch  31 |  2400/ 2983 batches | lr 0.000165 | ms/batch 23.07 | loss  5.43 | ppl   228.73\n",
            "| epoch  31 |  2600/ 2983 batches | lr 0.000165 | ms/batch 23.07 | loss  5.47 | ppl   237.80\n",
            "| epoch  31 |  2800/ 2983 batches | lr 0.000165 | ms/batch 23.09 | loss  5.42 | ppl   225.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 71.32s | valid loss  6.23 | valid ppl   506.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000164\n",
            "| epoch  32 |   200/ 2983 batches | lr 0.000164 | ms/batch 23.24 | loss  5.47 | ppl   238.34\n",
            "| epoch  32 |   400/ 2983 batches | lr 0.000164 | ms/batch 23.10 | loss  5.48 | ppl   239.65\n",
            "| epoch  32 |   600/ 2983 batches | lr 0.000164 | ms/batch 23.12 | loss  5.38 | ppl   216.28\n",
            "| epoch  32 |   800/ 2983 batches | lr 0.000164 | ms/batch 23.12 | loss  5.43 | ppl   228.75\n",
            "| epoch  32 |  1000/ 2983 batches | lr 0.000164 | ms/batch 23.13 | loss  5.44 | ppl   231.44\n",
            "| epoch  32 |  1200/ 2983 batches | lr 0.000163 | ms/batch 23.10 | loss  5.43 | ppl   228.78\n",
            "| epoch  32 |  1400/ 2983 batches | lr 0.000163 | ms/batch 23.11 | loss  5.46 | ppl   234.16\n",
            "| epoch  32 |  1600/ 2983 batches | lr 0.000163 | ms/batch 23.09 | loss  5.52 | ppl   250.18\n",
            "| epoch  32 |  1800/ 2983 batches | lr 0.000163 | ms/batch 23.08 | loss  5.44 | ppl   230.62\n",
            "| epoch  32 |  2000/ 2983 batches | lr 0.000163 | ms/batch 23.12 | loss  5.48 | ppl   239.71\n",
            "| epoch  32 |  2200/ 2983 batches | lr 0.000162 | ms/batch 23.18 | loss  5.41 | ppl   222.71\n",
            "| epoch  32 |  2400/ 2983 batches | lr 0.000162 | ms/batch 23.10 | loss  5.42 | ppl   226.30\n",
            "| epoch  32 |  2600/ 2983 batches | lr 0.000162 | ms/batch 23.11 | loss  5.47 | ppl   236.33\n",
            "| epoch  32 |  2800/ 2983 batches | lr 0.000162 | ms/batch 23.08 | loss  5.42 | ppl   224.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 71.33s | valid loss  6.23 | valid ppl   505.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000162\n",
            "| epoch  33 |   200/ 2983 batches | lr 0.000162 | ms/batch 23.21 | loss  5.47 | ppl   237.75\n",
            "| epoch  33 |   400/ 2983 batches | lr 0.000161 | ms/batch 23.09 | loss  5.47 | ppl   238.13\n",
            "| epoch  33 |   600/ 2983 batches | lr 0.000161 | ms/batch 23.08 | loss  5.37 | ppl   215.25\n",
            "| epoch  33 |   800/ 2983 batches | lr 0.000161 | ms/batch 23.10 | loss  5.43 | ppl   227.22\n",
            "| epoch  33 |  1000/ 2983 batches | lr 0.000161 | ms/batch 23.09 | loss  5.44 | ppl   229.89\n",
            "| epoch  33 |  1200/ 2983 batches | lr 0.000161 | ms/batch 23.08 | loss  5.42 | ppl   226.61\n",
            "| epoch  33 |  1400/ 2983 batches | lr 0.000161 | ms/batch 23.10 | loss  5.45 | ppl   232.17\n",
            "| epoch  33 |  1600/ 2983 batches | lr 0.000160 | ms/batch 23.10 | loss  5.52 | ppl   248.86\n",
            "| epoch  33 |  1800/ 2983 batches | lr 0.000160 | ms/batch 23.09 | loss  5.44 | ppl   230.24\n",
            "| epoch  33 |  2000/ 2983 batches | lr 0.000160 | ms/batch 23.09 | loss  5.48 | ppl   238.97\n",
            "| epoch  33 |  2200/ 2983 batches | lr 0.000160 | ms/batch 23.11 | loss  5.40 | ppl   220.84\n",
            "| epoch  33 |  2400/ 2983 batches | lr 0.000160 | ms/batch 23.14 | loss  5.42 | ppl   225.17\n",
            "| epoch  33 |  2600/ 2983 batches | lr 0.000160 | ms/batch 23.11 | loss  5.46 | ppl   234.80\n",
            "| epoch  33 |  2800/ 2983 batches | lr 0.000159 | ms/batch 23.09 | loss  5.41 | ppl   223.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 71.30s | valid loss  6.23 | valid ppl   508.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000159\n",
            "| epoch  34 |   200/ 2983 batches | lr 0.000159 | ms/batch 23.22 | loss  5.46 | ppl   235.73\n",
            "| epoch  34 |   400/ 2983 batches | lr 0.000159 | ms/batch 23.12 | loss  5.47 | ppl   237.36\n",
            "| epoch  34 |   600/ 2983 batches | lr 0.000159 | ms/batch 23.11 | loss  5.36 | ppl   213.68\n",
            "| epoch  34 |   800/ 2983 batches | lr 0.000159 | ms/batch 23.11 | loss  5.42 | ppl   225.47\n",
            "| epoch  34 |  1000/ 2983 batches | lr 0.000159 | ms/batch 23.08 | loss  5.43 | ppl   229.01\n",
            "| epoch  34 |  1200/ 2983 batches | lr 0.000158 | ms/batch 23.10 | loss  5.42 | ppl   225.13\n",
            "| epoch  34 |  1400/ 2983 batches | lr 0.000158 | ms/batch 23.10 | loss  5.44 | ppl   231.20\n",
            "| epoch  34 |  1600/ 2983 batches | lr 0.000158 | ms/batch 23.15 | loss  5.51 | ppl   247.17\n",
            "| epoch  34 |  1800/ 2983 batches | lr 0.000158 | ms/batch 23.11 | loss  5.43 | ppl   228.83\n",
            "| epoch  34 |  2000/ 2983 batches | lr 0.000158 | ms/batch 23.11 | loss  5.47 | ppl   236.98\n",
            "| epoch  34 |  2200/ 2983 batches | lr 0.000158 | ms/batch 23.11 | loss  5.39 | ppl   219.30\n",
            "| epoch  34 |  2400/ 2983 batches | lr 0.000157 | ms/batch 23.11 | loss  5.41 | ppl   222.91\n",
            "| epoch  34 |  2600/ 2983 batches | lr 0.000157 | ms/batch 23.11 | loss  5.45 | ppl   233.39\n",
            "| epoch  34 |  2800/ 2983 batches | lr 0.000157 | ms/batch 23.11 | loss  5.40 | ppl   221.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 71.33s | valid loss  6.23 | valid ppl   508.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000157\n",
            "| epoch  35 |   200/ 2983 batches | lr 0.000157 | ms/batch 23.26 | loss  5.45 | ppl   233.26\n",
            "| epoch  35 |   400/ 2983 batches | lr 0.000157 | ms/batch 23.12 | loss  5.46 | ppl   235.19\n",
            "| epoch  35 |   600/ 2983 batches | lr 0.000157 | ms/batch 23.09 | loss  5.36 | ppl   212.92\n",
            "| epoch  35 |   800/ 2983 batches | lr 0.000156 | ms/batch 23.09 | loss  5.41 | ppl   224.52\n",
            "| epoch  35 |  1000/ 2983 batches | lr 0.000156 | ms/batch 23.06 | loss  5.43 | ppl   227.77\n",
            "| epoch  35 |  1200/ 2983 batches | lr 0.000156 | ms/batch 23.07 | loss  5.41 | ppl   222.92\n",
            "| epoch  35 |  1400/ 2983 batches | lr 0.000156 | ms/batch 23.10 | loss  5.43 | ppl   229.21\n",
            "| epoch  35 |  1600/ 2983 batches | lr 0.000156 | ms/batch 23.11 | loss  5.50 | ppl   245.50\n",
            "| epoch  35 |  1800/ 2983 batches | lr 0.000156 | ms/batch 23.09 | loss  5.42 | ppl   226.94\n",
            "| epoch  35 |  2000/ 2983 batches | lr 0.000155 | ms/batch 23.09 | loss  5.46 | ppl   235.91\n",
            "| epoch  35 |  2200/ 2983 batches | lr 0.000155 | ms/batch 23.10 | loss  5.39 | ppl   218.36\n",
            "| epoch  35 |  2400/ 2983 batches | lr 0.000155 | ms/batch 23.10 | loss  5.40 | ppl   221.98\n",
            "| epoch  35 |  2600/ 2983 batches | lr 0.000155 | ms/batch 23.11 | loss  5.45 | ppl   232.26\n",
            "| epoch  35 |  2800/ 2983 batches | lr 0.000155 | ms/batch 23.11 | loss  5.39 | ppl   220.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 71.32s | valid loss  6.23 | valid ppl   508.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000155\n",
            "| epoch  36 |   200/ 2983 batches | lr 0.000155 | ms/batch 23.23 | loss  5.45 | ppl   232.59\n",
            "| epoch  36 |   400/ 2983 batches | lr 0.000154 | ms/batch 23.12 | loss  5.45 | ppl   233.21\n",
            "| epoch  36 |   600/ 2983 batches | lr 0.000154 | ms/batch 23.10 | loss  5.35 | ppl   211.64\n",
            "| epoch  36 |   800/ 2983 batches | lr 0.000154 | ms/batch 23.10 | loss  5.41 | ppl   222.82\n",
            "| epoch  36 |  1000/ 2983 batches | lr 0.000154 | ms/batch 23.11 | loss  5.42 | ppl   226.54\n",
            "| epoch  36 |  1200/ 2983 batches | lr 0.000154 | ms/batch 23.09 | loss  5.40 | ppl   222.23\n",
            "| epoch  36 |  1400/ 2983 batches | lr 0.000154 | ms/batch 23.07 | loss  5.43 | ppl   228.48\n",
            "| epoch  36 |  1600/ 2983 batches | lr 0.000154 | ms/batch 23.07 | loss  5.50 | ppl   243.94\n",
            "| epoch  36 |  1800/ 2983 batches | lr 0.000153 | ms/batch 23.08 | loss  5.42 | ppl   225.59\n",
            "| epoch  36 |  2000/ 2983 batches | lr 0.000153 | ms/batch 23.08 | loss  5.46 | ppl   234.26\n",
            "| epoch  36 |  2200/ 2983 batches | lr 0.000153 | ms/batch 23.07 | loss  5.38 | ppl   217.18\n",
            "| epoch  36 |  2400/ 2983 batches | lr 0.000153 | ms/batch 23.10 | loss  5.40 | ppl   220.83\n",
            "| epoch  36 |  2600/ 2983 batches | lr 0.000153 | ms/batch 23.10 | loss  5.44 | ppl   231.08\n",
            "| epoch  36 |  2800/ 2983 batches | lr 0.000153 | ms/batch 23.08 | loss  5.39 | ppl   219.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 71.29s | valid loss  6.23 | valid ppl   508.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000153\n",
            "| epoch  37 |   200/ 2983 batches | lr 0.000152 | ms/batch 23.20 | loss  5.44 | ppl   230.69\n",
            "| epoch  37 |   400/ 2983 batches | lr 0.000152 | ms/batch 23.08 | loss  5.45 | ppl   232.20\n",
            "| epoch  37 |   600/ 2983 batches | lr 0.000152 | ms/batch 23.08 | loss  5.34 | ppl   209.45\n",
            "| epoch  37 |   800/ 2983 batches | lr 0.000152 | ms/batch 23.08 | loss  5.40 | ppl   222.13\n",
            "| epoch  37 |  1000/ 2983 batches | lr 0.000152 | ms/batch 23.11 | loss  5.42 | ppl   225.62\n",
            "| epoch  37 |  1200/ 2983 batches | lr 0.000152 | ms/batch 23.06 | loss  5.40 | ppl   221.28\n",
            "| epoch  37 |  1400/ 2983 batches | lr 0.000152 | ms/batch 23.09 | loss  5.43 | ppl   228.15\n",
            "| epoch  37 |  1600/ 2983 batches | lr 0.000151 | ms/batch 23.10 | loss  5.49 | ppl   242.08\n",
            "| epoch  37 |  1800/ 2983 batches | lr 0.000151 | ms/batch 23.09 | loss  5.41 | ppl   223.86\n",
            "| epoch  37 |  2000/ 2983 batches | lr 0.000151 | ms/batch 23.10 | loss  5.45 | ppl   233.11\n",
            "| epoch  37 |  2200/ 2983 batches | lr 0.000151 | ms/batch 23.11 | loss  5.38 | ppl   215.95\n",
            "| epoch  37 |  2400/ 2983 batches | lr 0.000151 | ms/batch 23.09 | loss  5.39 | ppl   220.24\n",
            "| epoch  37 |  2600/ 2983 batches | lr 0.000151 | ms/batch 23.12 | loss  5.44 | ppl   229.54\n",
            "| epoch  37 |  2800/ 2983 batches | lr 0.000151 | ms/batch 23.12 | loss  5.38 | ppl   217.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 71.29s | valid loss  6.23 | valid ppl   509.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000150\n",
            "| epoch  38 |   200/ 2983 batches | lr 0.000150 | ms/batch 23.20 | loss  5.43 | ppl   229.24\n",
            "| epoch  38 |   400/ 2983 batches | lr 0.000150 | ms/batch 23.09 | loss  5.44 | ppl   231.48\n",
            "| epoch  38 |   600/ 2983 batches | lr 0.000150 | ms/batch 23.08 | loss  5.34 | ppl   208.36\n",
            "| epoch  38 |   800/ 2983 batches | lr 0.000150 | ms/batch 23.05 | loss  5.40 | ppl   220.59\n",
            "| epoch  38 |  1000/ 2983 batches | lr 0.000150 | ms/batch 23.07 | loss  5.41 | ppl   223.45\n",
            "| epoch  38 |  1200/ 2983 batches | lr 0.000150 | ms/batch 23.08 | loss  5.39 | ppl   219.60\n",
            "| epoch  38 |  1400/ 2983 batches | lr 0.000150 | ms/batch 23.09 | loss  5.42 | ppl   226.72\n",
            "| epoch  38 |  1600/ 2983 batches | lr 0.000149 | ms/batch 23.09 | loss  5.48 | ppl   240.92\n",
            "| epoch  38 |  1800/ 2983 batches | lr 0.000149 | ms/batch 23.08 | loss  5.41 | ppl   223.30\n",
            "| epoch  38 |  2000/ 2983 batches | lr 0.000149 | ms/batch 23.07 | loss  5.45 | ppl   231.60\n",
            "| epoch  38 |  2200/ 2983 batches | lr 0.000149 | ms/batch 23.18 | loss  5.37 | ppl   214.42\n",
            "| epoch  38 |  2400/ 2983 batches | lr 0.000149 | ms/batch 23.11 | loss  5.39 | ppl   218.73\n",
            "| epoch  38 |  2600/ 2983 batches | lr 0.000149 | ms/batch 23.12 | loss  5.43 | ppl   227.84\n",
            "| epoch  38 |  2800/ 2983 batches | lr 0.000149 | ms/batch 23.14 | loss  5.38 | ppl   216.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 71.30s | valid loss  6.24 | valid ppl   511.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000148\n",
            "| epoch  39 |   200/ 2983 batches | lr 0.000148 | ms/batch 23.26 | loss  5.43 | ppl   227.84\n",
            "| epoch  39 |   400/ 2983 batches | lr 0.000148 | ms/batch 23.11 | loss  5.44 | ppl   229.50\n",
            "| epoch  39 |   600/ 2983 batches | lr 0.000148 | ms/batch 23.11 | loss  5.34 | ppl   207.66\n",
            "| epoch  39 |   800/ 2983 batches | lr 0.000148 | ms/batch 23.09 | loss  5.39 | ppl   218.91\n",
            "| epoch  39 |  1000/ 2983 batches | lr 0.000148 | ms/batch 23.09 | loss  5.40 | ppl   222.23\n",
            "| epoch  39 |  1200/ 2983 batches | lr 0.000148 | ms/batch 23.11 | loss  5.39 | ppl   218.33\n",
            "| epoch  39 |  1400/ 2983 batches | lr 0.000148 | ms/batch 23.09 | loss  5.41 | ppl   224.62\n",
            "| epoch  39 |  1600/ 2983 batches | lr 0.000147 | ms/batch 23.10 | loss  5.48 | ppl   239.23\n",
            "| epoch  39 |  1800/ 2983 batches | lr 0.000147 | ms/batch 23.09 | loss  5.40 | ppl   222.32\n",
            "| epoch  39 |  2000/ 2983 batches | lr 0.000147 | ms/batch 23.09 | loss  5.44 | ppl   229.99\n",
            "| epoch  39 |  2200/ 2983 batches | lr 0.000147 | ms/batch 23.15 | loss  5.36 | ppl   213.00\n",
            "| epoch  39 |  2400/ 2983 batches | lr 0.000147 | ms/batch 23.08 | loss  5.38 | ppl   217.88\n",
            "| epoch  39 |  2600/ 2983 batches | lr 0.000147 | ms/batch 23.08 | loss  5.42 | ppl   226.42\n",
            "| epoch  39 |  2800/ 2983 batches | lr 0.000147 | ms/batch 23.11 | loss  5.37 | ppl   214.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 71.30s | valid loss  6.24 | valid ppl   512.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000147\n",
            "| epoch  40 |   200/ 2983 batches | lr 0.000146 | ms/batch 23.26 | loss  5.42 | ppl   226.89\n",
            "| epoch  40 |   400/ 2983 batches | lr 0.000146 | ms/batch 23.12 | loss  5.43 | ppl   228.26\n",
            "| epoch  40 |   600/ 2983 batches | lr 0.000146 | ms/batch 23.13 | loss  5.33 | ppl   205.84\n",
            "| epoch  40 |   800/ 2983 batches | lr 0.000146 | ms/batch 23.12 | loss  5.38 | ppl   217.95\n",
            "| epoch  40 |  1000/ 2983 batches | lr 0.000146 | ms/batch 23.11 | loss  5.40 | ppl   220.95\n",
            "| epoch  40 |  1200/ 2983 batches | lr 0.000146 | ms/batch 23.11 | loss  5.38 | ppl   217.70\n",
            "| epoch  40 |  1400/ 2983 batches | lr 0.000146 | ms/batch 23.15 | loss  5.41 | ppl   224.40\n",
            "| epoch  40 |  1600/ 2983 batches | lr 0.000146 | ms/batch 23.12 | loss  5.48 | ppl   238.95\n",
            "| epoch  40 |  1800/ 2983 batches | lr 0.000145 | ms/batch 23.11 | loss  5.40 | ppl   220.80\n",
            "| epoch  40 |  2000/ 2983 batches | lr 0.000145 | ms/batch 23.11 | loss  5.43 | ppl   228.83\n",
            "| epoch  40 |  2200/ 2983 batches | lr 0.000145 | ms/batch 23.12 | loss  5.36 | ppl   212.75\n",
            "| epoch  40 |  2400/ 2983 batches | lr 0.000145 | ms/batch 23.09 | loss  5.38 | ppl   216.77\n",
            "| epoch  40 |  2600/ 2983 batches | lr 0.000145 | ms/batch 23.09 | loss  5.42 | ppl   225.42\n",
            "| epoch  40 |  2800/ 2983 batches | lr 0.000145 | ms/batch 23.12 | loss  5.36 | ppl   213.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 71.34s | valid loss  6.23 | valid ppl   509.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000145\n",
            "| epoch  41 |   200/ 2983 batches | lr 0.000145 | ms/batch 23.24 | loss  5.42 | ppl   225.57\n",
            "| epoch  41 |   400/ 2983 batches | lr 0.000144 | ms/batch 23.08 | loss  5.43 | ppl   227.47\n",
            "| epoch  41 |   600/ 2983 batches | lr 0.000144 | ms/batch 23.09 | loss  5.33 | ppl   205.53\n",
            "| epoch  41 |   800/ 2983 batches | lr 0.000144 | ms/batch 23.10 | loss  5.38 | ppl   217.14\n",
            "| epoch  41 |  1000/ 2983 batches | lr 0.000144 | ms/batch 23.08 | loss  5.40 | ppl   220.31\n",
            "| epoch  41 |  1200/ 2983 batches | lr 0.000144 | ms/batch 23.10 | loss  5.38 | ppl   216.62\n",
            "| epoch  41 |  1400/ 2983 batches | lr 0.000144 | ms/batch 23.08 | loss  5.41 | ppl   222.89\n",
            "| epoch  41 |  1600/ 2983 batches | lr 0.000144 | ms/batch 23.10 | loss  5.47 | ppl   237.22\n",
            "| epoch  41 |  1800/ 2983 batches | lr 0.000144 | ms/batch 23.09 | loss  5.39 | ppl   219.59\n",
            "| epoch  41 |  2000/ 2983 batches | lr 0.000144 | ms/batch 23.08 | loss  5.43 | ppl   227.39\n",
            "| epoch  41 |  2200/ 2983 batches | lr 0.000143 | ms/batch 23.17 | loss  5.35 | ppl   210.49\n",
            "| epoch  41 |  2400/ 2983 batches | lr 0.000143 | ms/batch 23.12 | loss  5.37 | ppl   215.12\n",
            "| epoch  41 |  2600/ 2983 batches | lr 0.000143 | ms/batch 23.09 | loss  5.41 | ppl   224.50\n",
            "| epoch  41 |  2800/ 2983 batches | lr 0.000143 | ms/batch 23.12 | loss  5.36 | ppl   213.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 71.30s | valid loss  6.23 | valid ppl   509.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000143\n",
            "| epoch  42 |   200/ 2983 batches | lr 0.000143 | ms/batch 23.22 | loss  5.41 | ppl   224.50\n",
            "| epoch  42 |   400/ 2983 batches | lr 0.000143 | ms/batch 23.09 | loss  5.42 | ppl   225.88\n",
            "| epoch  42 |   600/ 2983 batches | lr 0.000143 | ms/batch 23.10 | loss  5.32 | ppl   203.99\n",
            "| epoch  42 |   800/ 2983 batches | lr 0.000142 | ms/batch 23.11 | loss  5.37 | ppl   215.54\n",
            "| epoch  42 |  1000/ 2983 batches | lr 0.000142 | ms/batch 23.09 | loss  5.39 | ppl   218.32\n",
            "| epoch  42 |  1200/ 2983 batches | lr 0.000142 | ms/batch 23.12 | loss  5.37 | ppl   215.13\n",
            "| epoch  42 |  1400/ 2983 batches | lr 0.000142 | ms/batch 23.07 | loss  5.40 | ppl   221.33\n",
            "| epoch  42 |  1600/ 2983 batches | lr 0.000142 | ms/batch 23.07 | loss  5.46 | ppl   235.38\n",
            "| epoch  42 |  1800/ 2983 batches | lr 0.000142 | ms/batch 23.15 | loss  5.39 | ppl   218.29\n",
            "| epoch  42 |  2000/ 2983 batches | lr 0.000142 | ms/batch 23.09 | loss  5.42 | ppl   226.41\n",
            "| epoch  42 |  2200/ 2983 batches | lr 0.000142 | ms/batch 23.09 | loss  5.34 | ppl   209.55\n",
            "| epoch  42 |  2400/ 2983 batches | lr 0.000142 | ms/batch 23.13 | loss  5.37 | ppl   214.06\n",
            "| epoch  42 |  2600/ 2983 batches | lr 0.000141 | ms/batch 23.07 | loss  5.41 | ppl   223.42\n",
            "| epoch  42 |  2800/ 2983 batches | lr 0.000141 | ms/batch 23.09 | loss  5.36 | ppl   212.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 71.30s | valid loss  6.24 | valid ppl   511.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000141\n",
            "| epoch  43 |   200/ 2983 batches | lr 0.000141 | ms/batch 23.22 | loss  5.41 | ppl   223.28\n",
            "| epoch  43 |   400/ 2983 batches | lr 0.000141 | ms/batch 23.09 | loss  5.42 | ppl   225.09\n",
            "| epoch  43 |   600/ 2983 batches | lr 0.000141 | ms/batch 23.08 | loss  5.31 | ppl   203.02\n",
            "| epoch  43 |   800/ 2983 batches | lr 0.000141 | ms/batch 23.10 | loss  5.37 | ppl   214.75\n",
            "| epoch  43 |  1000/ 2983 batches | lr 0.000141 | ms/batch 23.09 | loss  5.38 | ppl   217.37\n",
            "| epoch  43 |  1200/ 2983 batches | lr 0.000141 | ms/batch 23.08 | loss  5.37 | ppl   214.39\n",
            "| epoch  43 |  1400/ 2983 batches | lr 0.000140 | ms/batch 23.09 | loss  5.40 | ppl   220.65\n",
            "| epoch  43 |  1600/ 2983 batches | lr 0.000140 | ms/batch 23.08 | loss  5.46 | ppl   234.17\n",
            "| epoch  43 |  1800/ 2983 batches | lr 0.000140 | ms/batch 23.16 | loss  5.38 | ppl   217.12\n",
            "| epoch  43 |  2000/ 2983 batches | lr 0.000140 | ms/batch 23.10 | loss  5.42 | ppl   225.81\n",
            "| epoch  43 |  2200/ 2983 batches | lr 0.000140 | ms/batch 23.09 | loss  5.34 | ppl   209.35\n",
            "| epoch  43 |  2400/ 2983 batches | lr 0.000140 | ms/batch 23.11 | loss  5.36 | ppl   212.41\n",
            "| epoch  43 |  2600/ 2983 batches | lr 0.000140 | ms/batch 23.08 | loss  5.40 | ppl   221.88\n",
            "| epoch  43 |  2800/ 2983 batches | lr 0.000140 | ms/batch 23.11 | loss  5.36 | ppl   211.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 71.29s | valid loss  6.23 | valid ppl   509.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000140\n",
            "| epoch  44 |   200/ 2983 batches | lr 0.000139 | ms/batch 23.20 | loss  5.40 | ppl   222.27\n",
            "| epoch  44 |   400/ 2983 batches | lr 0.000139 | ms/batch 23.11 | loss  5.41 | ppl   223.22\n",
            "| epoch  44 |   600/ 2983 batches | lr 0.000139 | ms/batch 23.09 | loss  5.31 | ppl   201.71\n",
            "| epoch  44 |   800/ 2983 batches | lr 0.000139 | ms/batch 23.12 | loss  5.36 | ppl   212.88\n",
            "| epoch  44 |  1000/ 2983 batches | lr 0.000139 | ms/batch 23.10 | loss  5.38 | ppl   216.11\n",
            "| epoch  44 |  1200/ 2983 batches | lr 0.000139 | ms/batch 23.08 | loss  5.36 | ppl   213.30\n",
            "| epoch  44 |  1400/ 2983 batches | lr 0.000139 | ms/batch 23.12 | loss  5.39 | ppl   219.16\n",
            "| epoch  44 |  1600/ 2983 batches | lr 0.000139 | ms/batch 23.10 | loss  5.45 | ppl   233.27\n",
            "| epoch  44 |  1800/ 2983 batches | lr 0.000139 | ms/batch 23.09 | loss  5.38 | ppl   216.45\n",
            "| epoch  44 |  2000/ 2983 batches | lr 0.000139 | ms/batch 23.13 | loss  5.41 | ppl   224.42\n",
            "| epoch  44 |  2200/ 2983 batches | lr 0.000138 | ms/batch 23.10 | loss  5.34 | ppl   207.53\n",
            "| epoch  44 |  2400/ 2983 batches | lr 0.000138 | ms/batch 23.09 | loss  5.35 | ppl   211.55\n",
            "| epoch  44 |  2600/ 2983 batches | lr 0.000138 | ms/batch 23.09 | loss  5.40 | ppl   220.64\n",
            "| epoch  44 |  2800/ 2983 batches | lr 0.000138 | ms/batch 23.18 | loss  5.35 | ppl   210.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 71.32s | valid loss  6.24 | valid ppl   513.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000138\n",
            "| epoch  45 |   200/ 2983 batches | lr 0.000138 | ms/batch 23.20 | loss  5.40 | ppl   220.80\n",
            "| epoch  45 |   400/ 2983 batches | lr 0.000138 | ms/batch 23.13 | loss  5.41 | ppl   222.80\n",
            "| epoch  45 |   600/ 2983 batches | lr 0.000138 | ms/batch 23.10 | loss  5.30 | ppl   200.91\n",
            "| epoch  45 |   800/ 2983 batches | lr 0.000138 | ms/batch 23.10 | loss  5.36 | ppl   211.95\n",
            "| epoch  45 |  1000/ 2983 batches | lr 0.000137 | ms/batch 23.12 | loss  5.37 | ppl   214.96\n",
            "| epoch  45 |  1200/ 2983 batches | lr 0.000137 | ms/batch 23.08 | loss  5.35 | ppl   211.38\n",
            "| epoch  45 |  1400/ 2983 batches | lr 0.000137 | ms/batch 23.11 | loss  5.39 | ppl   218.12\n",
            "| epoch  45 |  1600/ 2983 batches | lr 0.000137 | ms/batch 23.10 | loss  5.44 | ppl   231.40\n",
            "| epoch  45 |  1800/ 2983 batches | lr 0.000137 | ms/batch 23.09 | loss  5.37 | ppl   214.79\n",
            "| epoch  45 |  2000/ 2983 batches | lr 0.000137 | ms/batch 23.18 | loss  5.41 | ppl   223.29\n",
            "| epoch  45 |  2200/ 2983 batches | lr 0.000137 | ms/batch 23.14 | loss  5.34 | ppl   207.56\n",
            "| epoch  45 |  2400/ 2983 batches | lr 0.000137 | ms/batch 23.17 | loss  5.35 | ppl   210.18\n",
            "| epoch  45 |  2600/ 2983 batches | lr 0.000137 | ms/batch 23.13 | loss  5.39 | ppl   219.50\n",
            "| epoch  45 |  2800/ 2983 batches | lr 0.000137 | ms/batch 23.12 | loss  5.35 | ppl   209.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 71.35s | valid loss  6.23 | valid ppl   509.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000136\n",
            "| epoch  46 |   200/ 2983 batches | lr 0.000136 | ms/batch 23.22 | loss  5.39 | ppl   219.96\n",
            "| epoch  46 |   400/ 2983 batches | lr 0.000136 | ms/batch 23.14 | loss  5.40 | ppl   221.33\n",
            "| epoch  46 |   600/ 2983 batches | lr 0.000136 | ms/batch 23.13 | loss  5.30 | ppl   199.64\n",
            "| epoch  46 |   800/ 2983 batches | lr 0.000136 | ms/batch 23.08 | loss  5.35 | ppl   210.97\n",
            "| epoch  46 |  1000/ 2983 batches | lr 0.000136 | ms/batch 23.18 | loss  5.37 | ppl   214.36\n",
            "| epoch  46 |  1200/ 2983 batches | lr 0.000136 | ms/batch 23.12 | loss  5.35 | ppl   211.41\n",
            "| epoch  46 |  1400/ 2983 batches | lr 0.000136 | ms/batch 23.10 | loss  5.38 | ppl   217.01\n",
            "| epoch  46 |  1600/ 2983 batches | lr 0.000136 | ms/batch 23.11 | loss  5.44 | ppl   230.88\n",
            "| epoch  46 |  1800/ 2983 batches | lr 0.000136 | ms/batch 23.08 | loss  5.36 | ppl   213.79\n",
            "| epoch  46 |  2000/ 2983 batches | lr 0.000135 | ms/batch 23.12 | loss  5.40 | ppl   221.61\n",
            "| epoch  46 |  2200/ 2983 batches | lr 0.000135 | ms/batch 23.19 | loss  5.33 | ppl   206.08\n",
            "| epoch  46 |  2400/ 2983 batches | lr 0.000135 | ms/batch 23.20 | loss  5.35 | ppl   210.58\n",
            "| epoch  46 |  2600/ 2983 batches | lr 0.000135 | ms/batch 23.16 | loss  5.39 | ppl   218.79\n",
            "| epoch  46 |  2800/ 2983 batches | lr 0.000135 | ms/batch 23.10 | loss  5.34 | ppl   207.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 71.39s | valid loss  6.23 | valid ppl   510.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000135\n",
            "| epoch  47 |   200/ 2983 batches | lr 0.000135 | ms/batch 23.19 | loss  5.39 | ppl   218.13\n",
            "| epoch  47 |   400/ 2983 batches | lr 0.000135 | ms/batch 23.11 | loss  5.39 | ppl   219.89\n",
            "| epoch  47 |   600/ 2983 batches | lr 0.000135 | ms/batch 23.09 | loss  5.29 | ppl   198.68\n",
            "| epoch  47 |   800/ 2983 batches | lr 0.000135 | ms/batch 23.08 | loss  5.35 | ppl   210.00\n",
            "| epoch  47 |  1000/ 2983 batches | lr 0.000134 | ms/batch 23.11 | loss  5.36 | ppl   213.21\n",
            "| epoch  47 |  1200/ 2983 batches | lr 0.000134 | ms/batch 23.07 | loss  5.35 | ppl   209.92\n",
            "| epoch  47 |  1400/ 2983 batches | lr 0.000134 | ms/batch 23.08 | loss  5.38 | ppl   216.33\n",
            "| epoch  47 |  1600/ 2983 batches | lr 0.000134 | ms/batch 23.12 | loss  5.43 | ppl   229.21\n",
            "| epoch  47 |  1800/ 2983 batches | lr 0.000134 | ms/batch 23.09 | loss  5.36 | ppl   212.73\n",
            "| epoch  47 |  2000/ 2983 batches | lr 0.000134 | ms/batch 23.09 | loss  5.40 | ppl   220.63\n",
            "| epoch  47 |  2200/ 2983 batches | lr 0.000134 | ms/batch 23.13 | loss  5.32 | ppl   204.19\n",
            "| epoch  47 |  2400/ 2983 batches | lr 0.000134 | ms/batch 23.18 | loss  5.34 | ppl   209.01\n",
            "| epoch  47 |  2600/ 2983 batches | lr 0.000134 | ms/batch 23.23 | loss  5.38 | ppl   217.74\n",
            "| epoch  47 |  2800/ 2983 batches | lr 0.000134 | ms/batch 23.14 | loss  5.33 | ppl   206.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 71.37s | valid loss  6.23 | valid ppl   510.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000134\n",
            "| epoch  48 |   200/ 2983 batches | lr 0.000133 | ms/batch 23.33 | loss  5.38 | ppl   217.20\n",
            "| epoch  48 |   400/ 2983 batches | lr 0.000133 | ms/batch 23.11 | loss  5.39 | ppl   219.16\n",
            "| epoch  48 |   600/ 2983 batches | lr 0.000133 | ms/batch 23.16 | loss  5.29 | ppl   197.41\n",
            "| epoch  48 |   800/ 2983 batches | lr 0.000133 | ms/batch 23.09 | loss  5.34 | ppl   208.25\n",
            "| epoch  48 |  1000/ 2983 batches | lr 0.000133 | ms/batch 23.08 | loss  5.36 | ppl   211.86\n",
            "| epoch  48 |  1200/ 2983 batches | lr 0.000133 | ms/batch 23.13 | loss  5.34 | ppl   208.92\n",
            "| epoch  48 |  1400/ 2983 batches | lr 0.000133 | ms/batch 23.10 | loss  5.37 | ppl   215.46\n",
            "| epoch  48 |  1600/ 2983 batches | lr 0.000133 | ms/batch 23.11 | loss  5.43 | ppl   228.54\n",
            "| epoch  48 |  1800/ 2983 batches | lr 0.000133 | ms/batch 23.07 | loss  5.35 | ppl   211.30\n",
            "| epoch  48 |  2000/ 2983 batches | lr 0.000133 | ms/batch 23.11 | loss  5.39 | ppl   219.75\n",
            "| epoch  48 |  2200/ 2983 batches | lr 0.000132 | ms/batch 23.11 | loss  5.32 | ppl   203.90\n",
            "| epoch  48 |  2400/ 2983 batches | lr 0.000132 | ms/batch 23.06 | loss  5.34 | ppl   207.52\n",
            "| epoch  48 |  2600/ 2983 batches | lr 0.000132 | ms/batch 23.13 | loss  5.38 | ppl   216.47\n",
            "| epoch  48 |  2800/ 2983 batches | lr 0.000132 | ms/batch 23.21 | loss  5.33 | ppl   206.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 71.36s | valid loss  6.24 | valid ppl   513.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000132\n",
            "| epoch  49 |   200/ 2983 batches | lr 0.000132 | ms/batch 23.28 | loss  5.38 | ppl   216.96\n",
            "| epoch  49 |   400/ 2983 batches | lr 0.000132 | ms/batch 23.11 | loss  5.39 | ppl   218.46\n",
            "| epoch  49 |   600/ 2983 batches | lr 0.000132 | ms/batch 23.14 | loss  5.28 | ppl   196.63\n",
            "| epoch  49 |   800/ 2983 batches | lr 0.000132 | ms/batch 23.13 | loss  5.34 | ppl   208.43\n",
            "| epoch  49 |  1000/ 2983 batches | lr 0.000132 | ms/batch 23.11 | loss  5.35 | ppl   211.10\n",
            "| epoch  49 |  1200/ 2983 batches | lr 0.000132 | ms/batch 23.16 | loss  5.34 | ppl   207.51\n",
            "| epoch  49 |  1400/ 2983 batches | lr 0.000131 | ms/batch 23.11 | loss  5.37 | ppl   214.35\n",
            "| epoch  49 |  1600/ 2983 batches | lr 0.000131 | ms/batch 23.11 | loss  5.43 | ppl   227.35\n",
            "| epoch  49 |  1800/ 2983 batches | lr 0.000131 | ms/batch 23.08 | loss  5.34 | ppl   209.55\n",
            "| epoch  49 |  2000/ 2983 batches | lr 0.000131 | ms/batch 23.08 | loss  5.39 | ppl   218.99\n",
            "| epoch  49 |  2200/ 2983 batches | lr 0.000131 | ms/batch 23.10 | loss  5.31 | ppl   203.03\n",
            "| epoch  49 |  2400/ 2983 batches | lr 0.000131 | ms/batch 23.09 | loss  5.33 | ppl   206.79\n",
            "| epoch  49 |  2600/ 2983 batches | lr 0.000131 | ms/batch 23.12 | loss  5.37 | ppl   215.27\n",
            "| epoch  49 |  2800/ 2983 batches | lr 0.000131 | ms/batch 23.12 | loss  5.32 | ppl   205.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 71.35s | valid loss  6.24 | valid ppl   512.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000131\n",
            "| epoch  50 |   200/ 2983 batches | lr 0.000131 | ms/batch 23.25 | loss  5.38 | ppl   216.00\n",
            "| epoch  50 |   400/ 2983 batches | lr 0.000131 | ms/batch 23.11 | loss  5.38 | ppl   217.54\n",
            "| epoch  50 |   600/ 2983 batches | lr 0.000130 | ms/batch 23.13 | loss  5.28 | ppl   195.80\n",
            "| epoch  50 |   800/ 2983 batches | lr 0.000130 | ms/batch 23.11 | loss  5.33 | ppl   206.78\n",
            "| epoch  50 |  1000/ 2983 batches | lr 0.000130 | ms/batch 23.09 | loss  5.35 | ppl   210.01\n",
            "| epoch  50 |  1200/ 2983 batches | lr 0.000130 | ms/batch 23.13 | loss  5.33 | ppl   206.59\n",
            "| epoch  50 |  1400/ 2983 batches | lr 0.000130 | ms/batch 23.08 | loss  5.36 | ppl   212.58\n",
            "| epoch  50 |  1600/ 2983 batches | lr 0.000130 | ms/batch 23.10 | loss  5.42 | ppl   226.22\n",
            "| epoch  50 |  1800/ 2983 batches | lr 0.000130 | ms/batch 23.10 | loss  5.35 | ppl   209.98\n",
            "| epoch  50 |  2000/ 2983 batches | lr 0.000130 | ms/batch 23.09 | loss  5.38 | ppl   217.71\n",
            "| epoch  50 |  2200/ 2983 batches | lr 0.000130 | ms/batch 23.13 | loss  5.31 | ppl   202.12\n",
            "| epoch  50 |  2400/ 2983 batches | lr 0.000130 | ms/batch 23.11 | loss  5.33 | ppl   205.65\n",
            "| epoch  50 |  2600/ 2983 batches | lr 0.000130 | ms/batch 23.10 | loss  5.37 | ppl   214.76\n",
            "| epoch  50 |  2800/ 2983 batches | lr 0.000130 | ms/batch 23.12 | loss  5.32 | ppl   203.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 71.32s | valid loss  6.23 | valid ppl   510.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000129\n",
            "| epoch  51 |   200/ 2983 batches | lr 0.000129 | ms/batch 23.25 | loss  5.37 | ppl   214.83\n",
            "| epoch  51 |   400/ 2983 batches | lr 0.000129 | ms/batch 23.11 | loss  5.38 | ppl   216.29\n",
            "| epoch  51 |   600/ 2983 batches | lr 0.000129 | ms/batch 23.08 | loss  5.27 | ppl   194.11\n",
            "| epoch  51 |   800/ 2983 batches | lr 0.000129 | ms/batch 23.17 | loss  5.33 | ppl   205.72\n",
            "| epoch  51 |  1000/ 2983 batches | lr 0.000129 | ms/batch 23.09 | loss  5.35 | ppl   209.74\n",
            "| epoch  51 |  1200/ 2983 batches | lr 0.000129 | ms/batch 23.10 | loss  5.33 | ppl   206.54\n",
            "| epoch  51 |  1400/ 2983 batches | lr 0.000129 | ms/batch 23.09 | loss  5.35 | ppl   211.40\n",
            "| epoch  51 |  1600/ 2983 batches | lr 0.000129 | ms/batch 23.09 | loss  5.42 | ppl   225.61\n",
            "| epoch  51 |  1800/ 2983 batches | lr 0.000129 | ms/batch 23.12 | loss  5.34 | ppl   208.33\n",
            "| epoch  51 |  2000/ 2983 batches | lr 0.000129 | ms/batch 23.08 | loss  5.38 | ppl   216.84\n",
            "| epoch  51 |  2200/ 2983 batches | lr 0.000129 | ms/batch 23.10 | loss  5.31 | ppl   201.67\n",
            "| epoch  51 |  2400/ 2983 batches | lr 0.000128 | ms/batch 23.13 | loss  5.32 | ppl   203.77\n",
            "| epoch  51 |  2600/ 2983 batches | lr 0.000128 | ms/batch 23.10 | loss  5.37 | ppl   214.00\n",
            "| epoch  51 |  2800/ 2983 batches | lr 0.000128 | ms/batch 23.11 | loss  5.31 | ppl   203.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 71.32s | valid loss  6.24 | valid ppl   515.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000128\n",
            "| epoch  52 |   200/ 2983 batches | lr 0.000128 | ms/batch 23.24 | loss  5.36 | ppl   213.66\n",
            "| epoch  52 |   400/ 2983 batches | lr 0.000128 | ms/batch 23.11 | loss  5.37 | ppl   215.10\n",
            "| epoch  52 |   600/ 2983 batches | lr 0.000128 | ms/batch 23.08 | loss  5.26 | ppl   193.08\n",
            "| epoch  52 |   800/ 2983 batches | lr 0.000128 | ms/batch 23.11 | loss  5.32 | ppl   204.74\n",
            "| epoch  52 |  1000/ 2983 batches | lr 0.000128 | ms/batch 23.10 | loss  5.34 | ppl   208.12\n",
            "| epoch  52 |  1200/ 2983 batches | lr 0.000128 | ms/batch 23.11 | loss  5.32 | ppl   204.74\n",
            "| epoch  52 |  1400/ 2983 batches | lr 0.000128 | ms/batch 23.09 | loss  5.35 | ppl   211.12\n",
            "| epoch  52 |  1600/ 2983 batches | lr 0.000128 | ms/batch 23.06 | loss  5.41 | ppl   224.33\n",
            "| epoch  52 |  1800/ 2983 batches | lr 0.000127 | ms/batch 23.08 | loss  5.34 | ppl   208.54\n",
            "| epoch  52 |  2000/ 2983 batches | lr 0.000127 | ms/batch 23.10 | loss  5.38 | ppl   216.20\n",
            "| epoch  52 |  2200/ 2983 batches | lr 0.000127 | ms/batch 23.10 | loss  5.30 | ppl   200.63\n",
            "| epoch  52 |  2400/ 2983 batches | lr 0.000127 | ms/batch 23.11 | loss  5.32 | ppl   204.45\n",
            "| epoch  52 |  2600/ 2983 batches | lr 0.000127 | ms/batch 23.08 | loss  5.36 | ppl   212.57\n",
            "| epoch  52 |  2800/ 2983 batches | lr 0.000127 | ms/batch 23.11 | loss  5.31 | ppl   201.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 71.30s | valid loss  6.24 | valid ppl   512.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000127\n",
            "| epoch  53 |   200/ 2983 batches | lr 0.000127 | ms/batch 23.23 | loss  5.36 | ppl   212.59\n",
            "| epoch  53 |   400/ 2983 batches | lr 0.000127 | ms/batch 23.14 | loss  5.37 | ppl   214.22\n",
            "| epoch  53 |   600/ 2983 batches | lr 0.000127 | ms/batch 23.10 | loss  5.26 | ppl   193.12\n",
            "| epoch  53 |   800/ 2983 batches | lr 0.000127 | ms/batch 23.13 | loss  5.31 | ppl   203.35\n",
            "| epoch  53 |  1000/ 2983 batches | lr 0.000127 | ms/batch 23.09 | loss  5.33 | ppl   207.13\n",
            "| epoch  53 |  1200/ 2983 batches | lr 0.000126 | ms/batch 23.08 | loss  5.32 | ppl   204.14\n",
            "| epoch  53 |  1400/ 2983 batches | lr 0.000126 | ms/batch 23.10 | loss  5.35 | ppl   210.24\n",
            "| epoch  53 |  1600/ 2983 batches | lr 0.000126 | ms/batch 23.09 | loss  5.41 | ppl   223.66\n",
            "| epoch  53 |  1800/ 2983 batches | lr 0.000126 | ms/batch 23.11 | loss  5.33 | ppl   206.33\n",
            "| epoch  53 |  2000/ 2983 batches | lr 0.000126 | ms/batch 23.11 | loss  5.37 | ppl   214.83\n",
            "| epoch  53 |  2200/ 2983 batches | lr 0.000126 | ms/batch 23.08 | loss  5.29 | ppl   199.25\n",
            "| epoch  53 |  2400/ 2983 batches | lr 0.000126 | ms/batch 23.12 | loss  5.31 | ppl   202.62\n",
            "| epoch  53 |  2600/ 2983 batches | lr 0.000126 | ms/batch 23.10 | loss  5.36 | ppl   211.86\n",
            "| epoch  53 |  2800/ 2983 batches | lr 0.000126 | ms/batch 23.10 | loss  5.31 | ppl   201.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 71.32s | valid loss  6.25 | valid ppl   516.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000126\n",
            "| epoch  54 |   200/ 2983 batches | lr 0.000126 | ms/batch 23.22 | loss  5.35 | ppl   211.29\n",
            "| epoch  54 |   400/ 2983 batches | lr 0.000126 | ms/batch 23.11 | loss  5.36 | ppl   213.71\n",
            "| epoch  54 |   600/ 2983 batches | lr 0.000125 | ms/batch 23.11 | loss  5.26 | ppl   191.74\n",
            "| epoch  54 |   800/ 2983 batches | lr 0.000125 | ms/batch 23.11 | loss  5.31 | ppl   202.53\n",
            "| epoch  54 |  1000/ 2983 batches | lr 0.000125 | ms/batch 23.10 | loss  5.33 | ppl   206.20\n",
            "| epoch  54 |  1200/ 2983 batches | lr 0.000125 | ms/batch 23.10 | loss  5.32 | ppl   203.84\n",
            "| epoch  54 |  1400/ 2983 batches | lr 0.000125 | ms/batch 23.11 | loss  5.34 | ppl   209.42\n",
            "| epoch  54 |  1600/ 2983 batches | lr 0.000125 | ms/batch 23.07 | loss  5.40 | ppl   221.83\n",
            "| epoch  54 |  1800/ 2983 batches | lr 0.000125 | ms/batch 23.09 | loss  5.33 | ppl   206.04\n",
            "| epoch  54 |  2000/ 2983 batches | lr 0.000125 | ms/batch 23.11 | loss  5.37 | ppl   214.56\n",
            "| epoch  54 |  2200/ 2983 batches | lr 0.000125 | ms/batch 23.09 | loss  5.29 | ppl   198.17\n",
            "| epoch  54 |  2400/ 2983 batches | lr 0.000125 | ms/batch 23.09 | loss  5.31 | ppl   201.73\n",
            "| epoch  54 |  2600/ 2983 batches | lr 0.000125 | ms/batch 23.10 | loss  5.35 | ppl   211.37\n",
            "| epoch  54 |  2800/ 2983 batches | lr 0.000125 | ms/batch 23.10 | loss  5.30 | ppl   200.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 71.30s | valid loss  6.24 | valid ppl   514.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  55 |   200/ 2983 batches | lr 0.000124 | ms/batch 23.21 | loss  5.35 | ppl   211.27\n",
            "| epoch  55 |   400/ 2983 batches | lr 0.000124 | ms/batch 23.10 | loss  5.36 | ppl   212.94\n",
            "| epoch  55 |   600/ 2983 batches | lr 0.000124 | ms/batch 23.09 | loss  5.25 | ppl   191.10\n",
            "| epoch  55 |   800/ 2983 batches | lr 0.000124 | ms/batch 23.11 | loss  5.31 | ppl   202.76\n",
            "| epoch  55 |  1000/ 2983 batches | lr 0.000124 | ms/batch 23.10 | loss  5.33 | ppl   205.80\n",
            "| epoch  55 |  1200/ 2983 batches | lr 0.000124 | ms/batch 23.05 | loss  5.31 | ppl   202.59\n",
            "| epoch  55 |  1400/ 2983 batches | lr 0.000124 | ms/batch 23.09 | loss  5.34 | ppl   208.48\n",
            "| epoch  55 |  1600/ 2983 batches | lr 0.000124 | ms/batch 23.08 | loss  5.40 | ppl   221.26\n",
            "| epoch  55 |  1800/ 2983 batches | lr 0.000124 | ms/batch 23.05 | loss  5.33 | ppl   205.90\n",
            "| epoch  55 |  2000/ 2983 batches | lr 0.000124 | ms/batch 23.07 | loss  5.37 | ppl   214.05\n",
            "| epoch  55 |  2200/ 2983 batches | lr 0.000124 | ms/batch 23.09 | loss  5.29 | ppl   197.55\n",
            "| epoch  55 |  2400/ 2983 batches | lr 0.000124 | ms/batch 23.10 | loss  5.30 | ppl   200.69\n",
            "| epoch  55 |  2600/ 2983 batches | lr 0.000124 | ms/batch 23.12 | loss  5.35 | ppl   210.39\n",
            "| epoch  55 |  2800/ 2983 batches | lr 0.000123 | ms/batch 23.09 | loss  5.30 | ppl   199.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 71.26s | valid loss  6.25 | valid ppl   515.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000123\n",
            "| epoch  56 |   200/ 2983 batches | lr 0.000123 | ms/batch 23.23 | loss  5.35 | ppl   209.92\n",
            "| epoch  56 |   400/ 2983 batches | lr 0.000123 | ms/batch 23.12 | loss  5.35 | ppl   211.49\n",
            "| epoch  56 |   600/ 2983 batches | lr 0.000123 | ms/batch 23.11 | loss  5.25 | ppl   191.42\n",
            "| epoch  56 |   800/ 2983 batches | lr 0.000123 | ms/batch 23.09 | loss  5.31 | ppl   201.79\n",
            "| epoch  56 |  1000/ 2983 batches | lr 0.000123 | ms/batch 23.12 | loss  5.32 | ppl   205.13\n",
            "| epoch  56 |  1200/ 2983 batches | lr 0.000123 | ms/batch 23.10 | loss  5.31 | ppl   202.05\n",
            "| epoch  56 |  1400/ 2983 batches | lr 0.000123 | ms/batch 23.10 | loss  5.34 | ppl   207.71\n",
            "| epoch  56 |  1600/ 2983 batches | lr 0.000123 | ms/batch 23.10 | loss  5.40 | ppl   221.24\n",
            "| epoch  56 |  1800/ 2983 batches | lr 0.000123 | ms/batch 23.09 | loss  5.32 | ppl   204.46\n",
            "| epoch  56 |  2000/ 2983 batches | lr 0.000123 | ms/batch 23.09 | loss  5.36 | ppl   213.34\n",
            "| epoch  56 |  2200/ 2983 batches | lr 0.000123 | ms/batch 23.08 | loss  5.28 | ppl   196.97\n",
            "| epoch  56 |  2400/ 2983 batches | lr 0.000123 | ms/batch 23.06 | loss  5.30 | ppl   200.29\n",
            "| epoch  56 |  2600/ 2983 batches | lr 0.000122 | ms/batch 23.09 | loss  5.34 | ppl   208.64\n",
            "| epoch  56 |  2800/ 2983 batches | lr 0.000122 | ms/batch 23.07 | loss  5.30 | ppl   199.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 71.28s | valid loss  6.25 | valid ppl   517.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000122\n",
            "| epoch  57 |   200/ 2983 batches | lr 0.000122 | ms/batch 23.19 | loss  5.34 | ppl   209.29\n",
            "| epoch  57 |   400/ 2983 batches | lr 0.000122 | ms/batch 23.05 | loss  5.35 | ppl   211.12\n",
            "| epoch  57 |   600/ 2983 batches | lr 0.000122 | ms/batch 23.09 | loss  5.25 | ppl   190.19\n",
            "| epoch  57 |   800/ 2983 batches | lr 0.000122 | ms/batch 23.09 | loss  5.30 | ppl   201.04\n",
            "| epoch  57 |  1000/ 2983 batches | lr 0.000122 | ms/batch 23.13 | loss  5.32 | ppl   204.25\n",
            "| epoch  57 |  1200/ 2983 batches | lr 0.000122 | ms/batch 23.08 | loss  5.31 | ppl   201.36\n",
            "| epoch  57 |  1400/ 2983 batches | lr 0.000122 | ms/batch 23.07 | loss  5.33 | ppl   206.83\n",
            "| epoch  57 |  1600/ 2983 batches | lr 0.000122 | ms/batch 23.14 | loss  5.39 | ppl   219.77\n",
            "| epoch  57 |  1800/ 2983 batches | lr 0.000122 | ms/batch 23.11 | loss  5.32 | ppl   204.17\n",
            "| epoch  57 |  2000/ 2983 batches | lr 0.000122 | ms/batch 23.11 | loss  5.36 | ppl   212.06\n",
            "| epoch  57 |  2200/ 2983 batches | lr 0.000122 | ms/batch 23.12 | loss  5.28 | ppl   196.46\n",
            "| epoch  57 |  2400/ 2983 batches | lr 0.000121 | ms/batch 23.09 | loss  5.30 | ppl   200.34\n",
            "| epoch  57 |  2600/ 2983 batches | lr 0.000121 | ms/batch 23.11 | loss  5.34 | ppl   208.40\n",
            "| epoch  57 |  2800/ 2983 batches | lr 0.000121 | ms/batch 23.12 | loss  5.29 | ppl   198.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 71.30s | valid loss  6.25 | valid ppl   516.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000121\n",
            "| epoch  58 |   200/ 2983 batches | lr 0.000121 | ms/batch 23.21 | loss  5.34 | ppl   208.64\n",
            "| epoch  58 |   400/ 2983 batches | lr 0.000121 | ms/batch 23.07 | loss  5.35 | ppl   210.36\n",
            "| epoch  58 |   600/ 2983 batches | lr 0.000121 | ms/batch 23.07 | loss  5.24 | ppl   188.90\n",
            "| epoch  58 |   800/ 2983 batches | lr 0.000121 | ms/batch 23.08 | loss  5.30 | ppl   199.69\n",
            "| epoch  58 |  1000/ 2983 batches | lr 0.000121 | ms/batch 23.06 | loss  5.32 | ppl   203.91\n",
            "| epoch  58 |  1200/ 2983 batches | lr 0.000121 | ms/batch 23.05 | loss  5.30 | ppl   200.81\n",
            "| epoch  58 |  1400/ 2983 batches | lr 0.000121 | ms/batch 23.05 | loss  5.33 | ppl   206.30\n",
            "| epoch  58 |  1600/ 2983 batches | lr 0.000121 | ms/batch 23.05 | loss  5.39 | ppl   218.99\n",
            "| epoch  58 |  1800/ 2983 batches | lr 0.000121 | ms/batch 23.07 | loss  5.31 | ppl   202.99\n",
            "| epoch  58 |  2000/ 2983 batches | lr 0.000121 | ms/batch 23.08 | loss  5.35 | ppl   211.32\n",
            "| epoch  58 |  2200/ 2983 batches | lr 0.000120 | ms/batch 23.09 | loss  5.28 | ppl   196.05\n",
            "| epoch  58 |  2400/ 2983 batches | lr 0.000120 | ms/batch 23.13 | loss  5.30 | ppl   199.54\n",
            "| epoch  58 |  2600/ 2983 batches | lr 0.000120 | ms/batch 23.10 | loss  5.34 | ppl   207.80\n",
            "| epoch  58 |  2800/ 2983 batches | lr 0.000120 | ms/batch 23.22 | loss  5.29 | ppl   197.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 71.27s | valid loss  6.25 | valid ppl   518.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000120\n",
            "| epoch  59 |   200/ 2983 batches | lr 0.000120 | ms/batch 23.25 | loss  5.34 | ppl   207.81\n",
            "| epoch  59 |   400/ 2983 batches | lr 0.000120 | ms/batch 23.14 | loss  5.34 | ppl   209.53\n",
            "| epoch  59 |   600/ 2983 batches | lr 0.000120 | ms/batch 23.12 | loss  5.24 | ppl   188.49\n",
            "| epoch  59 |   800/ 2983 batches | lr 0.000120 | ms/batch 23.09 | loss  5.30 | ppl   199.47\n",
            "| epoch  59 |  1000/ 2983 batches | lr 0.000120 | ms/batch 23.10 | loss  5.31 | ppl   202.65\n",
            "| epoch  59 |  1200/ 2983 batches | lr 0.000120 | ms/batch 23.09 | loss  5.30 | ppl   199.71\n",
            "| epoch  59 |  1400/ 2983 batches | lr 0.000120 | ms/batch 23.10 | loss  5.33 | ppl   205.52\n",
            "| epoch  59 |  1600/ 2983 batches | lr 0.000120 | ms/batch 23.09 | loss  5.39 | ppl   218.54\n",
            "| epoch  59 |  1800/ 2983 batches | lr 0.000120 | ms/batch 23.07 | loss  5.31 | ppl   202.53\n",
            "| epoch  59 |  2000/ 2983 batches | lr 0.000119 | ms/batch 23.14 | loss  5.35 | ppl   210.92\n",
            "| epoch  59 |  2200/ 2983 batches | lr 0.000119 | ms/batch 23.18 | loss  5.27 | ppl   194.81\n",
            "| epoch  59 |  2400/ 2983 batches | lr 0.000119 | ms/batch 23.11 | loss  5.29 | ppl   198.24\n",
            "| epoch  59 |  2600/ 2983 batches | lr 0.000119 | ms/batch 23.12 | loss  5.33 | ppl   206.16\n",
            "| epoch  59 |  2800/ 2983 batches | lr 0.000119 | ms/batch 23.09 | loss  5.29 | ppl   197.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 71.33s | valid loss  6.25 | valid ppl   517.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000119\n",
            "| epoch  60 |   200/ 2983 batches | lr 0.000119 | ms/batch 23.20 | loss  5.33 | ppl   207.39\n",
            "| epoch  60 |   400/ 2983 batches | lr 0.000119 | ms/batch 23.08 | loss  5.34 | ppl   209.09\n",
            "| epoch  60 |   600/ 2983 batches | lr 0.000119 | ms/batch 23.10 | loss  5.24 | ppl   188.50\n",
            "| epoch  60 |   800/ 2983 batches | lr 0.000119 | ms/batch 23.06 | loss  5.29 | ppl   198.20\n",
            "| epoch  60 |  1000/ 2983 batches | lr 0.000119 | ms/batch 23.09 | loss  5.31 | ppl   202.46\n",
            "| epoch  60 |  1200/ 2983 batches | lr 0.000119 | ms/batch 23.12 | loss  5.29 | ppl   198.70\n",
            "| epoch  60 |  1400/ 2983 batches | lr 0.000119 | ms/batch 23.09 | loss  5.32 | ppl   205.25\n",
            "| epoch  60 |  1600/ 2983 batches | lr 0.000119 | ms/batch 23.09 | loss  5.38 | ppl   218.09\n",
            "| epoch  60 |  1800/ 2983 batches | lr 0.000119 | ms/batch 23.08 | loss  5.31 | ppl   202.47\n",
            "| epoch  60 |  2000/ 2983 batches | lr 0.000118 | ms/batch 23.09 | loss  5.35 | ppl   210.54\n",
            "| epoch  60 |  2200/ 2983 batches | lr 0.000118 | ms/batch 23.06 | loss  5.27 | ppl   193.75\n",
            "| epoch  60 |  2400/ 2983 batches | lr 0.000118 | ms/batch 23.08 | loss  5.29 | ppl   197.89\n",
            "| epoch  60 |  2600/ 2983 batches | lr 0.000118 | ms/batch 23.09 | loss  5.33 | ppl   206.26\n",
            "| epoch  60 |  2800/ 2983 batches | lr 0.000118 | ms/batch 23.12 | loss  5.28 | ppl   196.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 71.27s | valid loss  6.25 | valid ppl   520.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000118\n",
            "=========================================================================================\n",
            "| End of training | test loss  6.20 | test ppl   491.70\n",
            "=========================================================================================\n",
            "\n",
            "Generating text...\n"
          ]
        },
        {
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'corpus' where it is not associated with a value",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1735378377.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Example 2: Generate text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nGenerating text...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m generate_text(\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model_2.pt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/data_word_train/wikitext-2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3925097961.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(checkpoint, data_path, outf, words, temperature, top_k, seed, log_interval, accel, min_freq, use_top_k, old_version)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maccel\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Vocabulary size mismatch: {len(corpus.dictionary)} vs {model.ntoken}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'corpus' where it is not associated with a value"
          ]
        }
      ],
      "source": [
        "print(\"Training LSTM on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type = 'LSTM', # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "    data_path = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize = 400,\n",
        "    nhid = 400,\n",
        "    nlayers = 4,\n",
        "    lr = 0.001,\n",
        "    clip = 0.25,\n",
        "    epochs = 60,\n",
        "    batch_size = 20,\n",
        "    bptt = 35,\n",
        "    dropout = 0.2,\n",
        "    tied = False,\n",
        "    nhead = 4,\n",
        "    log_interval = 200,\n",
        "    save_path='model_2.pt',\n",
        "    dry_run=False,\n",
        "    accel=True,\n",
        "    use_optimizer=True,\n",
        "    optimizer_type='AdamW',\n",
        "    weight_decay=1e-4,\n",
        "    use_betas=True,\n",
        "    betas=(0.9, 0.98),\n",
        "    use_eps=True,\n",
        "    eps=1e-9,\n",
        "    criterion=None,\n",
        "    use_label_smoothing=True,\n",
        "    label_smoothing=0.1,\n",
        "    use_warmup=True,\n",
        "    warmup_steps=4000,\n",
        "    min_freq=5,\n",
        "    seed=1111,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "# Example 2: Generate text\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model_2.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf='generated_2.txt',\n",
        "    words=1000,\n",
        "    temperature=1.0,\n",
        "    top_k=40,\n",
        "    seed=1111,\n",
        "    log_interval=100,\n",
        "    use_top_k=True,  # Use top-k sampling\n",
        "    min_freq=5,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "!cat generated_2.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XrWGgKkwD51e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrWGgKkwD51e",
        "outputId": "b54847cd-29e2-4152-f02d-0838c0818a0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generating text...\n",
            "Vocabulary size: 25251\n",
            "Vocabulary size: 25251\n",
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n",
            "<unk> \" of the <unk> essay celebrated later that year . In addition to the photo account of characters ,\n",
            "the England records him choice every way . <eos> 1906 being less than an area in rich titles . The\n",
            "plan was not a cuts by summit chickens but its lies Yankovic yet entitled expeditions under the loss of damage\n",
            "at him took place . <eos> Although that stories was vote by this year , — posits made Gyakuten Charles\n",
            "W. <unk> , from and there were an annual army written by divorce crime and Jerry [ favor ] ,\n",
            "Zimmerman . Around this time , the unnamed Southern left coast reefs , breaking the in two Stansfield Fisher and\n",
            "the U.S. . 89 Reality were have to have largely accepted the role no allows by diverged for the first\n",
            "part of English critics ' uncut Moses tests . For each operation , bishops during the incoming final and rails\n",
            "would Cautionary America , positive , and 43 lb old . <eos> wetlands Maximum times mink , led by financial\n",
            "Post <unk> and plans Cavalry   155 aging and was already credited on offerings . election was the most beautiful\n",
            "primaries early in the season , and the official which accepts a Scenes on the front theater was its unusual\n",
            "<unk> . <eos> Many in the mountains 33rd Buddhist Murdoch facility towers have been recorded . About one loose ,\n",
            "along basis , was Poll . In the third sophisticated station stretched on 1 September . The mature area of\n",
            "the reign knit Trinidad in its upper drive were a recording period . At the leader of the gradient ,\n",
            "two aboard <unk> who treat a smaller ruined belt , caused a <unk> to Holloway the River NLDS Sennacherib in\n",
            "the drums . <eos> converted with 1923 , bodies were Archdiocese . <eos> Only 50 to 8 % of the\n",
            "lay population was <unk> and nurse   fired France divergence had pectoral <unk> class is Little <unk> ( rocky .\n",
            "<unk> ) . <eos> Other other regions followed anti   <unk> to the south of particularly those audience categorised for\n",
            "the region . In contrast to typesetting also could telescope from <unk> identification Amy Bryan outline , partnership of gamma\n",
            "intent , keyboards and Sun along the living press . <unk> , along with the a M. <unk> <unk> ,\n",
            "also <unk> the 8th new <unk> 1920s , while the Late ( east ) is discussed and has not done\n",
            "to Reports ; the protagonists are usually star . On 15 January , demolition <unk> , <unk> and to 1860\n",
            "absorbed the rapidly <unk>   shaped Romania light state of <unk> . <eos> At the time of utilize cries in\n",
            "the <unk> , with a very spirit unfamiliar — the <unk> crex weakest votes with <unk> <unk> in the which\n",
            "that , as the rarely become the species 's faith . Toy is very common upon annual 34 education ,\n",
            "although no attacks can Dakotas the estimate of those generals Stephanie eastern 29 and have been said to be the\n",
            "25 It originated from helpful . fresh Patrick , hoping yard to accept this , became editor and permit <unk>\n",
            ". This night is used as a essence of <unk> with great self Lakshmi . The China Africa is no\n",
            "longer much developed . <eos> rank gained backyard Eduardo 250 , 1631 , rams , and parents at number 12\n",
            ". The not is <unk> men as overall ( 8 request Carolina ) are donation of their <unk> . <eos>\n",
            "<eos> = = = 1917 = = = <eos> <eos> Instead of 24 September 52 , a peak of French\n",
            "abbot from 1808 in small <unk> was <unk> . After a 30   member death , construction was taken NLF\n",
            "in Britain — this pleaded to much 40   000 large B   word Grand , but now made an\n",
            "suburbs time . It was later perhaps Butetown to an climate of so lived and became how peoples of March\n",
            "2010 would urban 32 years . Only one beans died at the centre where it expressions is ill   wild\n",
            ". On the operations works <unk> of <unk> , <unk> , and In heritage . <eos> website clarity ( 1\n",
            "  <unk> ) pest global accounts of the assumption that the royal 1796 can build digital methods by and related\n",
            ". props are their customer Bahamas from its own # 16 loss , where Duat 61 ( also more issues\n",
            ") are ashore expands . slowing the mass and the our psychology of the path increased <unk>   east ,\n",
            "his largest battery could not include adorn . class at the status of any block occurs in around the AC\n",
            "Trail . The advisory features SNL information periods with <unk> notes . <eos> <eos> figure Mesopotamia in at least 1\n",
            "faction Directed there as a <unk> attack . In 1988 her region remained place at 3 : Later and <unk>\n",
            "at the <unk> <unk> signed with <unk> artists such surrounded Majesty 40 Wood and This strategically so , about father\n",
            "away . The that year <unk> nurse got in the hymn of the take to every 90 mi ( <unk>\n",
            "pages ) compromises , Newark Italians . <eos> <eos> = western <unk> = <eos> <unk> cast and <unk> cruisers are\n",
            "consisting of <unk> <unk> / <unk> <unk> Kennedy ( 3   5 time rituals ) . Such underlying or intensity\n",
            "Near spines away into classes . These only <unk> Aerosmith are run and where no birds lay in each other\n",
            "header for election . The plan is illumination , 66 – small <unk> , Advent countries „ parasites and <unk>\n",
            "head before food . The Canadian Matthews also residual <unk> Gabrielle H. <unk> ( <unk> liner <unk> ) within the\n",
            "Azores ( comedy debuting on either diameter ) , upon hairstyles , while the least category are greatly otherwise fruits\n"
          ]
        }
      ],
      "source": [
        "# Example 2: Generate text\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model_2.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf='generated_2.txt',\n",
        "    words=1000,\n",
        "    temperature=1.0,\n",
        "    top_k=40,\n",
        "    seed=1111,\n",
        "    log_interval=100,\n",
        "    use_top_k=True,  # Use top-k sampling\n",
        "    min_freq=5,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "!cat generated_2.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Su3MvoQapn9Z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Su3MvoQapn9Z",
        "outputId": "8ddca927-16cd-4bfd-cddc-e78f880ac82e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM on WikiText-2...\n",
            "Using device: cuda\n",
            "Vocabulary size: 25251\n",
            "| epoch   1 |   200/ 1305 batches | lr 0.000035 | ms/batch 1002.80 | loss  8.72 | ppl  6112.29\n",
            "| epoch   1 |   400/ 1305 batches | lr 0.000070 | ms/batch 1019.28 | loss  7.42 | ppl  1663.20\n",
            "| epoch   1 |   600/ 1305 batches | lr 0.000105 | ms/batch 1019.77 | loss  7.32 | ppl  1516.69\n",
            "| epoch   1 |   800/ 1305 batches | lr 0.000140 | ms/batch 1024.47 | loss  7.04 | ppl  1144.46\n"
          ]
        }
      ],
      "source": [
        "print(\"Training LSTM on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type = 'LSTM', # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "    data_path = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize=512,\n",
        "    nhid=2048,\n",
        "    nlayers=6,\n",
        "    lr=0.0001,\n",
        "    clip=0.25,\n",
        "    epochs=60,\n",
        "    batch_size=32,\n",
        "    bptt=50,\n",
        "    dropout=0.1,\n",
        "    tied=False,\n",
        "    nhead=8,\n",
        "    log_interval=200,\n",
        "    save_path='model_3.pt',\n",
        "    dry_run=False,\n",
        "    accel=True,\n",
        "    use_optimizer=True,\n",
        "    optimizer_type='AdamW',\n",
        "    weight_decay=1e-4,\n",
        "    use_betas=True,\n",
        "    betas=(0.9, 0.98),\n",
        "    use_eps=True,\n",
        "    eps=1e-9,\n",
        "    criterion=None,\n",
        "    use_label_smoothing=True,\n",
        "    label_smoothing=0.1,\n",
        "    use_warmup=True,\n",
        "    warmup_steps=4000,\n",
        "    min_freq=5,\n",
        "    seed=1111,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "# Example 2: Generate text\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model_3.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf='generated_3.txt',\n",
        "    words=1000,\n",
        "    temperature=1.0,\n",
        "    top_k=40,\n",
        "    seed=1111,\n",
        "    log_interval=100,\n",
        "    use_top_k=True,  # Use top-k sampling\n",
        "    min_freq=5,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "!cat generated_3.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E7NP6F_Vqq5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7NP6F_Vqq5f",
        "outputId": "bdcfe52a-90f1-4152-a366-5dac694309bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LSTM on WikiText-2...\n",
            "Using device: cuda\n",
            "Vocabulary size: 33278\n",
            "| epoch   1 |   200/ 2983 batches | lr 0.001000 | ms/batch 26.54 | loss  7.47 | ppl  1761.28\n",
            "| epoch   1 |   400/ 2983 batches | lr 0.001000 | ms/batch 22.23 | loss  7.29 | ppl  1472.76\n",
            "| epoch   1 |   600/ 2983 batches | lr 0.001000 | ms/batch 22.28 | loss  7.26 | ppl  1422.94\n",
            "| epoch   1 |   800/ 2983 batches | lr 0.001000 | ms/batch 22.24 | loss  7.25 | ppl  1413.46\n",
            "| epoch   1 |  1000/ 2983 batches | lr 0.001000 | ms/batch 22.23 | loss  7.25 | ppl  1408.35\n",
            "| epoch   1 |  1200/ 2983 batches | lr 0.001000 | ms/batch 22.28 | loss  7.26 | ppl  1425.64\n",
            "| epoch   1 |  1400/ 2983 batches | lr 0.001000 | ms/batch 22.26 | loss  7.25 | ppl  1401.29\n",
            "| epoch   1 |  1600/ 2983 batches | lr 0.001000 | ms/batch 22.32 | loss  7.26 | ppl  1419.39\n",
            "| epoch   1 |  1800/ 2983 batches | lr 0.001000 | ms/batch 22.32 | loss  7.23 | ppl  1376.58\n",
            "| epoch   1 |  2000/ 2983 batches | lr 0.001000 | ms/batch 22.33 | loss  7.25 | ppl  1408.13\n",
            "| epoch   1 |  2200/ 2983 batches | lr 0.001000 | ms/batch 22.36 | loss  7.25 | ppl  1409.23\n",
            "| epoch   1 |  2400/ 2983 batches | lr 0.001000 | ms/batch 22.39 | loss  7.23 | ppl  1377.26\n",
            "| epoch   1 |  2600/ 2983 batches | lr 0.001000 | ms/batch 22.40 | loss  7.25 | ppl  1406.25\n",
            "| epoch   1 |  2800/ 2983 batches | lr 0.001000 | ms/batch 22.43 | loss  7.21 | ppl  1347.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 69.55s | valid loss  7.04 | valid ppl  1146.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   2 |   200/ 2983 batches | lr 0.001000 | ms/batch 22.58 | loss  7.06 | ppl  1163.36\n",
            "| epoch   2 |   400/ 2983 batches | lr 0.001000 | ms/batch 22.50 | loss  7.08 | ppl  1183.40\n",
            "| epoch   2 |   600/ 2983 batches | lr 0.001000 | ms/batch 22.50 | loss  7.07 | ppl  1179.97\n",
            "| epoch   2 |   800/ 2983 batches | lr 0.001000 | ms/batch 22.53 | loss  7.11 | ppl  1223.38\n",
            "| epoch   2 |  1000/ 2983 batches | lr 0.001000 | ms/batch 22.53 | loss  7.12 | ppl  1240.93\n",
            "| epoch   2 |  1200/ 2983 batches | lr 0.001000 | ms/batch 22.52 | loss  7.17 | ppl  1296.01\n",
            "| epoch   2 |  1400/ 2983 batches | lr 0.001000 | ms/batch 22.54 | loss  7.14 | ppl  1260.84\n",
            "| epoch   2 |  1600/ 2983 batches | lr 0.001000 | ms/batch 22.50 | loss  7.15 | ppl  1276.12\n",
            "| epoch   2 |  1800/ 2983 batches | lr 0.001000 | ms/batch 22.48 | loss  7.12 | ppl  1239.42\n",
            "| epoch   2 |  2000/ 2983 batches | lr 0.001000 | ms/batch 22.47 | loss  7.16 | ppl  1282.19\n",
            "| epoch   2 |  2200/ 2983 batches | lr 0.001000 | ms/batch 22.44 | loss  7.14 | ppl  1266.05\n",
            "| epoch   2 |  2400/ 2983 batches | lr 0.001000 | ms/batch 22.45 | loss  7.10 | ppl  1216.07\n",
            "| epoch   2 |  2600/ 2983 batches | lr 0.001000 | ms/batch 22.43 | loss  7.14 | ppl  1267.51\n",
            "| epoch   2 |  2800/ 2983 batches | lr 0.001000 | ms/batch 22.43 | loss  7.10 | ppl  1217.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 69.20s | valid loss  7.26 | valid ppl  1416.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   3 |   200/ 2983 batches | lr 0.001000 | ms/batch 22.53 | loss  7.04 | ppl  1144.53\n",
            "| epoch   3 |   400/ 2983 batches | lr 0.001000 | ms/batch 22.41 | loss  7.03 | ppl  1125.59\n",
            "| epoch   3 |   600/ 2983 batches | lr 0.001000 | ms/batch 22.39 | loss  7.04 | ppl  1139.83\n",
            "| epoch   3 |   800/ 2983 batches | lr 0.001000 | ms/batch 22.43 | loss  7.05 | ppl  1148.45\n",
            "| epoch   3 |  1000/ 2983 batches | lr 0.001000 | ms/batch 22.45 | loss  7.09 | ppl  1195.25\n",
            "| epoch   3 |  1200/ 2983 batches | lr 0.001000 | ms/batch 22.43 | loss  7.11 | ppl  1220.61\n",
            "| epoch   3 |  1400/ 2983 batches | lr 0.001000 | ms/batch 22.45 | loss  7.12 | ppl  1237.27\n",
            "| epoch   3 |  1600/ 2983 batches | lr 0.001000 | ms/batch 22.45 | loss  7.13 | ppl  1246.91\n",
            "| epoch   3 |  1800/ 2983 batches | lr 0.001000 | ms/batch 22.45 | loss  7.11 | ppl  1223.95\n",
            "| epoch   3 |  2000/ 2983 batches | lr 0.001000 | ms/batch 22.44 | loss  7.13 | ppl  1255.01\n",
            "| epoch   3 |  2200/ 2983 batches | lr 0.001000 | ms/batch 22.44 | loss  7.14 | ppl  1257.11\n",
            "| epoch   3 |  2400/ 2983 batches | lr 0.001000 | ms/batch 22.44 | loss  7.10 | ppl  1212.26\n",
            "| epoch   3 |  2600/ 2983 batches | lr 0.001000 | ms/batch 22.46 | loss  7.14 | ppl  1259.58\n",
            "| epoch   3 |  2800/ 2983 batches | lr 0.001000 | ms/batch 22.44 | loss  7.11 | ppl  1226.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 69.07s | valid loss  7.32 | valid ppl  1508.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   4 |   200/ 2983 batches | lr 0.001000 | ms/batch 22.54 | loss  7.12 | ppl  1230.37\n",
            "| epoch   4 |   400/ 2983 batches | lr 0.001000 | ms/batch 22.41 | loss  7.14 | ppl  1263.02\n",
            "| epoch   4 |   600/ 2983 batches | lr 0.001000 | ms/batch 22.43 | loss  7.17 | ppl  1296.87\n",
            "| epoch   4 |   800/ 2983 batches | lr 0.001000 | ms/batch 22.40 | loss  7.19 | ppl  1329.73\n",
            "| epoch   4 |  1000/ 2983 batches | lr 0.001000 | ms/batch 22.40 | loss  7.23 | ppl  1373.65\n",
            "| epoch   4 |  1200/ 2983 batches | lr 0.001000 | ms/batch 22.40 | loss  7.27 | ppl  1433.35\n",
            "| epoch   4 |  1400/ 2983 batches | lr 0.001000 | ms/batch 22.40 | loss  7.28 | ppl  1453.53\n",
            "| epoch   4 |  1600/ 2983 batches | lr 0.001000 | ms/batch 22.39 | loss  7.34 | ppl  1547.79\n",
            "| epoch   4 |  1800/ 2983 batches | lr 0.001000 | ms/batch 22.40 | loss  7.32 | ppl  1504.71\n",
            "| epoch   4 |  2000/ 2983 batches | lr 0.001000 | ms/batch 22.40 | loss  7.34 | ppl  1546.82\n",
            "| epoch   4 |  2200/ 2983 batches | lr 0.001000 | ms/batch 22.45 | loss  7.35 | ppl  1559.74\n",
            "| epoch   4 |  2400/ 2983 batches | lr 0.001000 | ms/batch 22.39 | loss  7.32 | ppl  1510.69\n",
            "| epoch   4 |  2600/ 2983 batches | lr 0.001000 | ms/batch 22.40 | loss  7.39 | ppl  1613.05\n",
            "| epoch   4 |  2800/ 2983 batches | lr 0.001000 | ms/batch 22.39 | loss  7.35 | ppl  1555.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 68.95s | valid loss  7.37 | valid ppl  1590.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch   5 |   200/ 2983 batches | lr 0.000500 | ms/batch 22.53 | loss  7.26 | ppl  1429.00\n",
            "| epoch   5 |   400/ 2983 batches | lr 0.000500 | ms/batch 22.39 | loss  7.22 | ppl  1368.21\n",
            "| epoch   5 |   600/ 2983 batches | lr 0.000500 | ms/batch 22.37 | loss  7.21 | ppl  1352.90\n",
            "| epoch   5 |   800/ 2983 batches | lr 0.000500 | ms/batch 22.38 | loss  7.23 | ppl  1387.11\n",
            "| epoch   5 |  1000/ 2983 batches | lr 0.000500 | ms/batch 22.38 | loss  7.24 | ppl  1398.33\n",
            "| epoch   5 |  1200/ 2983 batches | lr 0.000500 | ms/batch 22.40 | loss  7.26 | ppl  1423.58\n",
            "| epoch   5 |  1400/ 2983 batches | lr 0.000500 | ms/batch 22.38 | loss  7.23 | ppl  1377.96\n",
            "| epoch   5 |  1600/ 2983 batches | lr 0.000500 | ms/batch 22.37 | loss  7.25 | ppl  1404.25\n",
            "| epoch   5 |  1800/ 2983 batches | lr 0.000500 | ms/batch 22.42 | loss  7.21 | ppl  1352.42\n",
            "| epoch   5 |  2000/ 2983 batches | lr 0.000500 | ms/batch 22.39 | loss  7.24 | ppl  1398.01\n",
            "| epoch   5 |  2200/ 2983 batches | lr 0.000500 | ms/batch 22.40 | loss  7.23 | ppl  1373.95\n",
            "| epoch   5 |  2400/ 2983 batches | lr 0.000500 | ms/batch 22.40 | loss  7.20 | ppl  1338.80\n",
            "| epoch   5 |  2600/ 2983 batches | lr 0.000500 | ms/batch 22.39 | loss  7.24 | ppl  1389.98\n",
            "| epoch   5 |  2800/ 2983 batches | lr 0.000500 | ms/batch 22.37 | loss  7.20 | ppl  1340.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 68.90s | valid loss  7.19 | valid ppl  1330.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch   6 |   200/ 2983 batches | lr 0.000500 | ms/batch 22.48 | loss  7.24 | ppl  1392.61\n",
            "| epoch   6 |   400/ 2983 batches | lr 0.000500 | ms/batch 22.38 | loss  7.21 | ppl  1358.03\n",
            "| epoch   6 |   600/ 2983 batches | lr 0.000500 | ms/batch 22.36 | loss  7.19 | ppl  1331.01\n",
            "| epoch   6 |   800/ 2983 batches | lr 0.000500 | ms/batch 22.36 | loss  7.21 | ppl  1358.12\n",
            "| epoch   6 |  1000/ 2983 batches | lr 0.000500 | ms/batch 22.36 | loss  7.22 | ppl  1368.64\n",
            "| epoch   6 |  1200/ 2983 batches | lr 0.000500 | ms/batch 22.37 | loss  7.26 | ppl  1428.78\n",
            "| epoch   6 |  1400/ 2983 batches | lr 0.000500 | ms/batch 22.38 | loss  7.26 | ppl  1415.91\n",
            "| epoch   6 |  1600/ 2983 batches | lr 0.000500 | ms/batch 22.36 | loss  7.25 | ppl  1405.46\n",
            "| epoch   6 |  1800/ 2983 batches | lr 0.000500 | ms/batch 22.36 | loss  7.20 | ppl  1340.55\n",
            "| epoch   6 |  2000/ 2983 batches | lr 0.000500 | ms/batch 22.39 | loss  7.23 | ppl  1385.91\n",
            "| epoch   6 |  2200/ 2983 batches | lr 0.000500 | ms/batch 22.36 | loss  7.23 | ppl  1387.02\n",
            "| epoch   6 |  2400/ 2983 batches | lr 0.000500 | ms/batch 22.37 | loss  7.20 | ppl  1337.69\n",
            "| epoch   6 |  2600/ 2983 batches | lr 0.000500 | ms/batch 22.37 | loss  7.24 | ppl  1387.29\n",
            "| epoch   6 |  2800/ 2983 batches | lr 0.000500 | ms/batch 22.34 | loss  7.20 | ppl  1335.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 68.83s | valid loss  7.18 | valid ppl  1308.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch   7 |   200/ 2983 batches | lr 0.000500 | ms/batch 22.46 | loss  7.23 | ppl  1380.55\n",
            "| epoch   7 |   400/ 2983 batches | lr 0.000500 | ms/batch 22.37 | loss  7.20 | ppl  1336.91\n",
            "| epoch   7 |   600/ 2983 batches | lr 0.000500 | ms/batch 22.38 | loss  7.19 | ppl  1320.54\n",
            "| epoch   7 |   800/ 2983 batches | lr 0.000500 | ms/batch 22.37 | loss  7.21 | ppl  1353.62\n",
            "| epoch   7 |  1000/ 2983 batches | lr 0.000500 | ms/batch 22.36 | loss  7.21 | ppl  1349.12\n",
            "| epoch   7 |  1200/ 2983 batches | lr 0.000500 | ms/batch 22.36 | loss  7.24 | ppl  1392.03\n",
            "| epoch   7 |  1400/ 2983 batches | lr 0.000500 | ms/batch 22.35 | loss  7.22 | ppl  1368.07\n",
            "| epoch   7 |  1600/ 2983 batches | lr 0.000500 | ms/batch 22.37 | loss  7.24 | ppl  1397.35\n",
            "| epoch   7 |  1800/ 2983 batches | lr 0.000500 | ms/batch 22.34 | loss  7.20 | ppl  1337.96\n",
            "| epoch   7 |  2000/ 2983 batches | lr 0.000500 | ms/batch 22.36 | loss  7.22 | ppl  1373.05\n",
            "| epoch   7 |  2200/ 2983 batches | lr 0.000500 | ms/batch 22.38 | loss  7.23 | ppl  1375.70\n",
            "| epoch   7 |  2400/ 2983 batches | lr 0.000500 | ms/batch 22.35 | loss  7.20 | ppl  1342.02\n",
            "| epoch   7 |  2600/ 2983 batches | lr 0.000500 | ms/batch 22.34 | loss  7.23 | ppl  1373.49\n",
            "| epoch   7 |  2800/ 2983 batches | lr 0.000500 | ms/batch 22.35 | loss  7.18 | ppl  1314.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 68.81s | valid loss  7.18 | valid ppl  1316.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch   8 |   200/ 2983 batches | lr 0.000250 | ms/batch 22.47 | loss  7.20 | ppl  1337.51\n",
            "| epoch   8 |   400/ 2983 batches | lr 0.000250 | ms/batch 22.36 | loss  7.17 | ppl  1299.19\n",
            "| epoch   8 |   600/ 2983 batches | lr 0.000250 | ms/batch 22.39 | loss  7.15 | ppl  1274.04\n",
            "| epoch   8 |   800/ 2983 batches | lr 0.000250 | ms/batch 22.36 | loss  7.16 | ppl  1291.79\n",
            "| epoch   8 |  1000/ 2983 batches | lr 0.000250 | ms/batch 22.35 | loss  7.17 | ppl  1302.09\n",
            "| epoch   8 |  1200/ 2983 batches | lr 0.000250 | ms/batch 22.39 | loss  7.19 | ppl  1326.92\n",
            "| epoch   8 |  1400/ 2983 batches | lr 0.000250 | ms/batch 22.34 | loss  7.17 | ppl  1303.23\n",
            "| epoch   8 |  1600/ 2983 batches | lr 0.000250 | ms/batch 22.36 | loss  7.18 | ppl  1317.20\n",
            "| epoch   8 |  1800/ 2983 batches | lr 0.000250 | ms/batch 22.37 | loss  7.14 | ppl  1262.54\n",
            "| epoch   8 |  2000/ 2983 batches | lr 0.000250 | ms/batch 22.36 | loss  7.18 | ppl  1314.71\n",
            "| epoch   8 |  2200/ 2983 batches | lr 0.000250 | ms/batch 22.38 | loss  7.19 | ppl  1320.23\n",
            "| epoch   8 |  2400/ 2983 batches | lr 0.000250 | ms/batch 22.36 | loss  7.15 | ppl  1277.99\n",
            "| epoch   8 |  2600/ 2983 batches | lr 0.000250 | ms/batch 22.34 | loss  7.18 | ppl  1311.83\n",
            "| epoch   8 |  2800/ 2983 batches | lr 0.000250 | ms/batch 22.35 | loss  7.16 | ppl  1281.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 68.83s | valid loss  9.02 | valid ppl  8285.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch   9 |   200/ 2983 batches | lr 0.000250 | ms/batch 22.49 | loss  7.51 | ppl  1818.90\n",
            "| epoch   9 |   400/ 2983 batches | lr 0.000250 | ms/batch 22.43 | loss  7.16 | ppl  1292.30\n",
            "| epoch   9 |   600/ 2983 batches | lr 0.000250 | ms/batch 22.40 | loss  7.08 | ppl  1193.40\n",
            "| epoch   9 |   800/ 2983 batches | lr 0.000250 | ms/batch 22.41 | loss  7.07 | ppl  1175.09\n",
            "| epoch   9 |  1000/ 2983 batches | lr 0.000250 | ms/batch 22.44 | loss  7.09 | ppl  1203.59\n",
            "| epoch   9 |  1200/ 2983 batches | lr 0.000250 | ms/batch 22.40 | loss  7.13 | ppl  1244.19\n",
            "| epoch   9 |  1400/ 2983 batches | lr 0.000250 | ms/batch 22.42 | loss  7.09 | ppl  1202.36\n",
            "| epoch   9 |  1600/ 2983 batches | lr 0.000250 | ms/batch 22.42 | loss  7.11 | ppl  1221.63\n",
            "| epoch   9 |  1800/ 2983 batches | lr 0.000250 | ms/batch 22.43 | loss  7.06 | ppl  1162.25\n",
            "| epoch   9 |  2000/ 2983 batches | lr 0.000250 | ms/batch 22.44 | loss  7.08 | ppl  1191.17\n",
            "| epoch   9 |  2200/ 2983 batches | lr 0.000250 | ms/batch 22.42 | loss  7.07 | ppl  1180.75\n",
            "| epoch   9 |  2400/ 2983 batches | lr 0.000250 | ms/batch 22.42 | loss  7.03 | ppl  1130.98\n",
            "| epoch   9 |  2600/ 2983 batches | lr 0.000250 | ms/batch 22.43 | loss  7.06 | ppl  1165.23\n",
            "| epoch   9 |  2800/ 2983 batches | lr 0.000250 | ms/batch 22.45 | loss  7.05 | ppl  1149.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 69.02s | valid loss  7.16 | valid ppl  1290.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  10 |   200/ 2983 batches | lr 0.000250 | ms/batch 22.53 | loss  7.05 | ppl  1157.99\n",
            "| epoch  10 |   400/ 2983 batches | lr 0.000250 | ms/batch 22.42 | loss  7.02 | ppl  1116.36\n",
            "| epoch  10 |   600/ 2983 batches | lr 0.000250 | ms/batch 22.45 | loss  7.01 | ppl  1107.10\n",
            "| epoch  10 |   800/ 2983 batches | lr 0.000250 | ms/batch 22.42 | loss  7.01 | ppl  1108.84\n",
            "| epoch  10 |  1000/ 2983 batches | lr 0.000250 | ms/batch 22.44 | loss  7.04 | ppl  1142.09\n",
            "| epoch  10 |  1200/ 2983 batches | lr 0.000250 | ms/batch 22.43 | loss  7.07 | ppl  1177.29\n",
            "| epoch  10 |  1400/ 2983 batches | lr 0.000250 | ms/batch 22.42 | loss  7.04 | ppl  1144.24\n",
            "| epoch  10 |  1600/ 2983 batches | lr 0.000250 | ms/batch 22.44 | loss  7.08 | ppl  1186.81\n",
            "| epoch  10 |  1800/ 2983 batches | lr 0.000250 | ms/batch 22.44 | loss  7.03 | ppl  1133.14\n",
            "| epoch  10 |  2000/ 2983 batches | lr 0.000250 | ms/batch 22.45 | loss  7.06 | ppl  1159.72\n",
            "| epoch  10 |  2200/ 2983 batches | lr 0.000250 | ms/batch 22.45 | loss  7.05 | ppl  1153.42\n",
            "| epoch  10 |  2400/ 2983 batches | lr 0.000250 | ms/batch 22.45 | loss  7.00 | ppl  1092.42\n",
            "| epoch  10 |  2600/ 2983 batches | lr 0.000250 | ms/batch 22.43 | loss  7.06 | ppl  1158.88\n",
            "| epoch  10 |  2800/ 2983 batches | lr 0.000250 | ms/batch 22.43 | loss  7.03 | ppl  1129.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 69.06s | valid loss  7.15 | valid ppl  1274.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  11 |   200/ 2983 batches | lr 0.000125 | ms/batch 22.56 | loss  7.06 | ppl  1160.59\n",
            "| epoch  11 |   400/ 2983 batches | lr 0.000125 | ms/batch 22.44 | loss  7.03 | ppl  1134.75\n",
            "| epoch  11 |   600/ 2983 batches | lr 0.000125 | ms/batch 22.47 | loss  7.01 | ppl  1106.77\n",
            "| epoch  11 |   800/ 2983 batches | lr 0.000125 | ms/batch 22.47 | loss  7.02 | ppl  1114.59\n",
            "| epoch  11 |  1000/ 2983 batches | lr 0.000125 | ms/batch 22.46 | loss  7.04 | ppl  1135.88\n",
            "| epoch  11 |  1200/ 2983 batches | lr 0.000125 | ms/batch 22.47 | loss  7.07 | ppl  1176.26\n",
            "| epoch  11 |  1400/ 2983 batches | lr 0.000125 | ms/batch 22.43 | loss  7.06 | ppl  1159.31\n",
            "| epoch  11 |  1600/ 2983 batches | lr 0.000125 | ms/batch 22.46 | loss  7.09 | ppl  1196.53\n",
            "| epoch  11 |  1800/ 2983 batches | lr 0.000125 | ms/batch 22.50 | loss  7.02 | ppl  1121.08\n",
            "| epoch  11 |  2000/ 2983 batches | lr 0.000125 | ms/batch 22.51 | loss  7.05 | ppl  1150.20\n",
            "| epoch  11 |  2200/ 2983 batches | lr 0.000125 | ms/batch 22.50 | loss  7.04 | ppl  1142.73\n",
            "| epoch  11 |  2400/ 2983 batches | lr 0.000125 | ms/batch 22.47 | loss  7.00 | ppl  1098.37\n",
            "| epoch  11 |  2600/ 2983 batches | lr 0.000125 | ms/batch 22.47 | loss  7.03 | ppl  1129.90\n",
            "| epoch  11 |  2800/ 2983 batches | lr 0.000125 | ms/batch 22.44 | loss  7.02 | ppl  1122.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 69.17s | valid loss  7.12 | valid ppl  1231.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  12 |   200/ 2983 batches | lr 0.000125 | ms/batch 22.58 | loss  7.04 | ppl  1142.51\n",
            "| epoch  12 |   400/ 2983 batches | lr 0.000125 | ms/batch 22.46 | loss  6.99 | ppl  1089.59\n",
            "| epoch  12 |   600/ 2983 batches | lr 0.000125 | ms/batch 22.43 | loss  6.97 | ppl  1066.57\n",
            "| epoch  12 |   800/ 2983 batches | lr 0.000125 | ms/batch 22.44 | loss  6.97 | ppl  1068.29\n",
            "| epoch  12 |  1000/ 2983 batches | lr 0.000125 | ms/batch 22.43 | loss  7.00 | ppl  1094.38\n",
            "| epoch  12 |  1200/ 2983 batches | lr 0.000125 | ms/batch 22.50 | loss  7.03 | ppl  1134.13\n",
            "| epoch  12 |  1400/ 2983 batches | lr 0.000125 | ms/batch 22.51 | loss  7.02 | ppl  1118.35\n",
            "| epoch  12 |  1600/ 2983 batches | lr 0.000125 | ms/batch 22.52 | loss  7.04 | ppl  1142.96\n",
            "| epoch  12 |  1800/ 2983 batches | lr 0.000125 | ms/batch 22.52 | loss  7.00 | ppl  1095.00\n",
            "| epoch  12 |  2000/ 2983 batches | lr 0.000125 | ms/batch 22.50 | loss  7.01 | ppl  1112.35\n",
            "| epoch  12 |  2200/ 2983 batches | lr 0.000125 | ms/batch 22.48 | loss  7.02 | ppl  1114.62\n",
            "| epoch  12 |  2400/ 2983 batches | lr 0.000125 | ms/batch 22.48 | loss  6.97 | ppl  1067.59\n",
            "| epoch  12 |  2600/ 2983 batches | lr 0.000125 | ms/batch 22.48 | loss  7.00 | ppl  1099.86\n",
            "| epoch  12 |  2800/ 2983 batches | lr 0.000125 | ms/batch 22.47 | loss  6.99 | ppl  1081.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 69.21s | valid loss  7.10 | valid ppl  1217.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  13 |   200/ 2983 batches | lr 0.000125 | ms/batch 22.65 | loss  7.01 | ppl  1109.99\n",
            "| epoch  13 |   400/ 2983 batches | lr 0.000125 | ms/batch 22.51 | loss  6.98 | ppl  1070.50\n",
            "| epoch  13 |   600/ 2983 batches | lr 0.000125 | ms/batch 22.48 | loss  6.95 | ppl  1042.22\n",
            "| epoch  13 |   800/ 2983 batches | lr 0.000125 | ms/batch 22.51 | loss  6.95 | ppl  1045.20\n",
            "| epoch  13 |  1000/ 2983 batches | lr 0.000125 | ms/batch 22.53 | loss  6.97 | ppl  1069.47\n",
            "| epoch  13 |  1200/ 2983 batches | lr 0.000125 | ms/batch 22.54 | loss  7.01 | ppl  1109.20\n",
            "| epoch  13 |  1400/ 2983 batches | lr 0.000125 | ms/batch 22.50 | loss  7.00 | ppl  1095.33\n",
            "| epoch  13 |  1600/ 2983 batches | lr 0.000125 | ms/batch 22.53 | loss  7.02 | ppl  1113.54\n",
            "| epoch  13 |  1800/ 2983 batches | lr 0.000125 | ms/batch 22.53 | loss  6.98 | ppl  1072.25\n",
            "| epoch  13 |  2000/ 2983 batches | lr 0.000125 | ms/batch 22.54 | loss  6.98 | ppl  1079.86\n",
            "| epoch  13 |  2200/ 2983 batches | lr 0.000125 | ms/batch 22.53 | loss  6.99 | ppl  1086.25\n",
            "| epoch  13 |  2400/ 2983 batches | lr 0.000125 | ms/batch 22.51 | loss  6.96 | ppl  1049.55\n",
            "| epoch  13 |  2600/ 2983 batches | lr 0.000125 | ms/batch 22.53 | loss  6.98 | ppl  1077.52\n",
            "| epoch  13 |  2800/ 2983 batches | lr 0.000125 | ms/batch 22.50 | loss  6.96 | ppl  1050.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 69.34s | valid loss  7.11 | valid ppl  1229.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  14 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.58 | loss  7.03 | ppl  1130.02\n",
            "| epoch  14 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  7.00 | ppl  1093.98\n",
            "| epoch  14 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  6.99 | ppl  1085.55\n",
            "| epoch  14 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.50 | loss  6.96 | ppl  1055.60\n",
            "| epoch  14 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.50 | loss  6.98 | ppl  1073.04\n",
            "| epoch  14 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  7.02 | ppl  1117.76\n",
            "| epoch  14 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  7.01 | ppl  1110.39\n",
            "| epoch  14 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.51 | loss  7.03 | ppl  1134.54\n",
            "| epoch  14 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  6.98 | ppl  1076.61\n",
            "| epoch  14 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.50 | loss  6.99 | ppl  1088.32\n",
            "| epoch  14 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.52 | loss  7.01 | ppl  1104.31\n",
            "| epoch  14 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.51 | loss  6.99 | ppl  1089.60\n",
            "| epoch  14 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.50 | loss  6.98 | ppl  1078.94\n",
            "| epoch  14 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.50 | loss  6.97 | ppl  1065.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 69.24s | valid loss  7.09 | valid ppl  1201.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  15 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.61 | loss  7.01 | ppl  1103.36\n",
            "| epoch  15 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.50 | loss  6.97 | ppl  1063.95\n",
            "| epoch  15 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.52 | loss  6.93 | ppl  1023.33\n",
            "| epoch  15 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.52 | loss  6.90 | ppl   994.31\n",
            "| epoch  15 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  6.88 | ppl   974.53\n",
            "| epoch  15 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.54 | loss  6.88 | ppl   974.43\n",
            "| epoch  15 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.50 | loss  6.85 | ppl   943.12\n",
            "| epoch  15 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  6.85 | ppl   940.83\n",
            "| epoch  15 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  6.76 | ppl   864.95\n",
            "| epoch  15 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  6.76 | ppl   859.80\n",
            "| epoch  15 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  6.73 | ppl   836.19\n",
            "| epoch  15 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.50 | loss  6.67 | ppl   789.79\n",
            "| epoch  15 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  6.66 | ppl   783.45\n",
            "| epoch  15 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.51 | loss  6.62 | ppl   749.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 69.26s | valid loss  6.67 | valid ppl   784.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  16 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.59 | loss  6.65 | ppl   771.84\n",
            "| epoch  16 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.50 | loss  6.59 | ppl   727.59\n",
            "| epoch  16 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  6.52 | ppl   676.32\n",
            "| epoch  16 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.50 | loss  6.52 | ppl   678.90\n",
            "| epoch  16 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  6.49 | ppl   660.23\n",
            "| epoch  16 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.50 | loss  6.53 | ppl   685.33\n",
            "| epoch  16 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.51 | loss  6.53 | ppl   682.59\n",
            "| epoch  16 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  6.54 | ppl   695.66\n",
            "| epoch  16 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  6.48 | ppl   652.30\n",
            "| epoch  16 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  6.50 | ppl   666.58\n",
            "| epoch  16 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  6.47 | ppl   643.39\n",
            "| epoch  16 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  6.43 | ppl   620.19\n",
            "| epoch  16 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.50 | loss  6.45 | ppl   634.85\n",
            "| epoch  16 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.42 | ppl   614.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 69.21s | valid loss  6.51 | valid ppl   675.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  17 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.56 | loss  6.46 | ppl   639.92\n",
            "| epoch  17 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  6.42 | ppl   611.22\n",
            "| epoch  17 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.34 | ppl   567.65\n",
            "| epoch  17 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  6.37 | ppl   581.51\n",
            "| epoch  17 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  6.35 | ppl   573.02\n",
            "| epoch  17 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  6.40 | ppl   599.42\n",
            "| epoch  17 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.51 | loss  6.40 | ppl   602.09\n",
            "| epoch  17 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  6.42 | ppl   615.61\n",
            "| epoch  17 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.36 | ppl   580.96\n",
            "| epoch  17 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  6.38 | ppl   592.43\n",
            "| epoch  17 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.50 | loss  6.35 | ppl   572.42\n",
            "| epoch  17 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  6.32 | ppl   555.23\n",
            "| epoch  17 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.50 | loss  6.35 | ppl   571.42\n",
            "| epoch  17 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.32 | ppl   555.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 69.19s | valid loss  6.45 | valid ppl   632.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  18 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.57 | loss  6.37 | ppl   583.30\n",
            "| epoch  18 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.32 | ppl   555.60\n",
            "| epoch  18 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  6.25 | ppl   516.78\n",
            "| epoch  18 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.28 | ppl   533.50\n",
            "| epoch  18 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.27 | ppl   528.89\n",
            "| epoch  18 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  6.32 | ppl   553.82\n",
            "| epoch  18 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.32 | ppl   557.63\n",
            "| epoch  18 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  6.34 | ppl   568.43\n",
            "| epoch  18 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.50 | loss  6.29 | ppl   538.81\n",
            "| epoch  18 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  6.32 | ppl   553.52\n",
            "| epoch  18 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  6.27 | ppl   528.05\n",
            "| epoch  18 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  6.24 | ppl   514.30\n",
            "| epoch  18 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  6.28 | ppl   532.93\n",
            "| epoch  18 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  6.26 | ppl   521.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 69.18s | valid loss  6.40 | valid ppl   602.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  19 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.59 | loss  6.30 | ppl   544.27\n",
            "| epoch  19 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.26 | ppl   521.99\n",
            "| epoch  19 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  6.18 | ppl   484.35\n",
            "| epoch  19 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  6.22 | ppl   502.80\n",
            "| epoch  19 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  6.21 | ppl   497.98\n",
            "| epoch  19 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  6.26 | ppl   524.21\n",
            "| epoch  19 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  6.27 | ppl   526.72\n",
            "| epoch  19 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.28 | ppl   535.89\n",
            "| epoch  19 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.24 | ppl   511.81\n",
            "| epoch  19 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  6.26 | ppl   524.78\n",
            "| epoch  19 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  6.21 | ppl   496.91\n",
            "| epoch  19 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  6.19 | ppl   486.00\n",
            "| epoch  19 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  6.22 | ppl   501.67\n",
            "| epoch  19 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.20 | ppl   492.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 69.14s | valid loss  6.37 | valid ppl   584.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  20 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.53 | loss  6.24 | ppl   512.73\n",
            "| epoch  20 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.20 | ppl   494.76\n",
            "| epoch  20 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  6.12 | ppl   456.95\n",
            "| epoch  20 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  6.17 | ppl   476.30\n",
            "| epoch  20 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  6.16 | ppl   473.32\n",
            "| epoch  20 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  6.21 | ppl   498.17\n",
            "| epoch  20 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  6.22 | ppl   500.36\n",
            "| epoch  20 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  6.24 | ppl   510.98\n",
            "| epoch  20 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  6.19 | ppl   487.57\n",
            "| epoch  20 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  6.21 | ppl   499.72\n",
            "| epoch  20 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  6.15 | ppl   470.93\n",
            "| epoch  20 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  6.13 | ppl   461.08\n",
            "| epoch  20 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  6.16 | ppl   475.73\n",
            "| epoch  20 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  6.15 | ppl   466.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 69.07s | valid loss  6.34 | valid ppl   566.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  21 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.55 | loss  6.19 | ppl   487.89\n",
            "| epoch  21 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  6.15 | ppl   470.53\n",
            "| epoch  21 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  6.08 | ppl   436.02\n",
            "| epoch  21 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  6.12 | ppl   454.97\n",
            "| epoch  21 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.12 | ppl   452.77\n",
            "| epoch  21 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  6.17 | ppl   476.34\n",
            "| epoch  21 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  6.17 | ppl   478.44\n",
            "| epoch  21 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  6.19 | ppl   488.09\n",
            "| epoch  21 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  6.14 | ppl   465.40\n",
            "| epoch  21 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.16 | ppl   475.52\n",
            "| epoch  21 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.10 | ppl   447.87\n",
            "| epoch  21 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  6.09 | ppl   439.29\n",
            "| epoch  21 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  6.12 | ppl   454.16\n",
            "| epoch  21 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  6.09 | ppl   442.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 69.10s | valid loss  6.30 | valid ppl   545.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  22 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.57 | loss  6.13 | ppl   461.53\n",
            "| epoch  22 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.11 | ppl   448.27\n",
            "| epoch  22 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  6.02 | ppl   413.41\n",
            "| epoch  22 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  6.07 | ppl   432.91\n",
            "| epoch  22 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  6.06 | ppl   430.38\n",
            "| epoch  22 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.11 | ppl   449.98\n",
            "| epoch  22 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  6.12 | ppl   454.19\n",
            "| epoch  22 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  6.14 | ppl   462.83\n",
            "| epoch  22 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  6.09 | ppl   443.06\n",
            "| epoch  22 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.12 | ppl   454.91\n",
            "| epoch  22 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  6.05 | ppl   422.41\n",
            "| epoch  22 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  6.03 | ppl   415.04\n",
            "| epoch  22 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  6.07 | ppl   430.58\n",
            "| epoch  22 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.54 | loss  6.04 | ppl   418.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 69.13s | valid loss  6.26 | valid ppl   525.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  23 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.56 | loss  6.08 | ppl   435.80\n",
            "| epoch  23 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.05 | ppl   425.75\n",
            "| epoch  23 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.97 | ppl   393.42\n",
            "| epoch  23 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.02 | ppl   412.18\n",
            "| epoch  23 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.01 | ppl   407.72\n",
            "| epoch  23 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.06 | ppl   429.06\n",
            "| epoch  23 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  6.07 | ppl   433.01\n",
            "| epoch  23 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.08 | ppl   439.09\n",
            "| epoch  23 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.04 | ppl   421.82\n",
            "| epoch  23 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  6.07 | ppl   431.93\n",
            "| epoch  23 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.99 | ppl   401.15\n",
            "| epoch  23 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.51 | loss  5.98 | ppl   395.20\n",
            "| epoch  23 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  6.01 | ppl   409.32\n",
            "| epoch  23 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.98 | ppl   396.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 69.14s | valid loss  6.23 | valid ppl   508.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  24 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.54 | loss  6.02 | ppl   412.98\n",
            "| epoch  24 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  6.00 | ppl   403.84\n",
            "| epoch  24 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.92 | ppl   372.07\n",
            "| epoch  24 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.97 | ppl   390.61\n",
            "| epoch  24 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.96 | ppl   387.54\n",
            "| epoch  24 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  6.01 | ppl   406.58\n",
            "| epoch  24 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  6.01 | ppl   409.18\n",
            "| epoch  24 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.42 | loss  6.03 | ppl   417.32\n",
            "| epoch  24 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.41 | loss  6.00 | ppl   402.63\n",
            "| epoch  24 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  6.03 | ppl   413.81\n",
            "| epoch  24 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.94 | ppl   380.60\n",
            "| epoch  24 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.93 | ppl   376.15\n",
            "| epoch  24 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.97 | ppl   390.49\n",
            "| epoch  24 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.94 | ppl   378.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 69.09s | valid loss  6.19 | valid ppl   486.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  25 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.55 | loss  5.97 | ppl   392.96\n",
            "| epoch  25 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.42 | loss  5.96 | ppl   386.37\n",
            "| epoch  25 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.87 | ppl   354.15\n",
            "| epoch  25 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.92 | ppl   373.00\n",
            "| epoch  25 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.91 | ppl   368.32\n",
            "| epoch  25 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.96 | ppl   388.51\n",
            "| epoch  25 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.97 | ppl   389.93\n",
            "| epoch  25 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  5.99 | ppl   398.04\n",
            "| epoch  25 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.42 | loss  5.95 | ppl   382.61\n",
            "| epoch  25 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.98 | ppl   395.04\n",
            "| epoch  25 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.89 | ppl   362.22\n",
            "| epoch  25 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.88 | ppl   359.28\n",
            "| epoch  25 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.92 | ppl   372.48\n",
            "| epoch  25 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.89 | ppl   360.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 69.09s | valid loss  6.16 | valid ppl   474.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  26 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.56 | loss  5.93 | ppl   375.54\n",
            "| epoch  26 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.91 | ppl   366.98\n",
            "| epoch  26 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.82 | ppl   338.21\n",
            "| epoch  26 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.87 | ppl   355.98\n",
            "| epoch  26 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.86 | ppl   351.44\n",
            "| epoch  26 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.91 | ppl   368.71\n",
            "| epoch  26 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.92 | ppl   371.68\n",
            "| epoch  26 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.94 | ppl   379.82\n",
            "| epoch  26 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.91 | ppl   367.21\n",
            "| epoch  26 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.93 | ppl   376.95\n",
            "| epoch  26 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.84 | ppl   345.15\n",
            "| epoch  26 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.84 | ppl   342.27\n",
            "| epoch  26 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.88 | ppl   356.19\n",
            "| epoch  26 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.84 | ppl   344.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 69.08s | valid loss  6.12 | valid ppl   456.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  27 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.54 | loss  5.88 | ppl   357.41\n",
            "| epoch  27 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.86 | ppl   350.84\n",
            "| epoch  27 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.77 | ppl   321.56\n",
            "| epoch  27 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.83 | ppl   340.15\n",
            "| epoch  27 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.42 | loss  5.81 | ppl   334.30\n",
            "| epoch  27 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.86 | ppl   351.51\n",
            "| epoch  27 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.87 | ppl   354.48\n",
            "| epoch  27 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.90 | ppl   363.45\n",
            "| epoch  27 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.85 | ppl   348.43\n",
            "| epoch  27 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.88 | ppl   359.44\n",
            "| epoch  27 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.80 | ppl   328.79\n",
            "| epoch  27 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  5.79 | ppl   326.90\n",
            "| epoch  27 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.83 | ppl   340.38\n",
            "| epoch  27 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.79 | ppl   328.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 69.06s | valid loss  6.10 | valid ppl   446.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  28 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.55 | loss  5.83 | ppl   341.34\n",
            "| epoch  28 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.81 | ppl   333.65\n",
            "| epoch  28 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.73 | ppl   308.22\n",
            "| epoch  28 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.42 | loss  5.78 | ppl   324.85\n",
            "| epoch  28 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.77 | ppl   319.55\n",
            "| epoch  28 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.82 | ppl   335.84\n",
            "| epoch  28 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.82 | ppl   338.02\n",
            "| epoch  28 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.85 | ppl   345.76\n",
            "| epoch  28 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.81 | ppl   334.02\n",
            "| epoch  28 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.84 | ppl   343.55\n",
            "| epoch  28 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.75 | ppl   314.52\n",
            "| epoch  28 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.75 | ppl   313.27\n",
            "| epoch  28 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.78 | ppl   325.17\n",
            "| epoch  28 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.75 | ppl   314.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 69.08s | valid loss  6.06 | valid ppl   429.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  29 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.56 | loss  5.79 | ppl   325.88\n",
            "| epoch  29 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.77 | ppl   319.06\n",
            "| epoch  29 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.69 | ppl   295.29\n",
            "| epoch  29 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.74 | ppl   310.13\n",
            "| epoch  29 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.72 | ppl   306.17\n",
            "| epoch  29 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.78 | ppl   322.31\n",
            "| epoch  29 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.78 | ppl   324.20\n",
            "| epoch  29 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.81 | ppl   332.55\n",
            "| epoch  29 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.77 | ppl   320.14\n",
            "| epoch  29 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.80 | ppl   331.53\n",
            "| epoch  29 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.71 | ppl   300.94\n",
            "| epoch  29 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.70 | ppl   300.27\n",
            "| epoch  29 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.74 | ppl   311.10\n",
            "| epoch  29 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.71 | ppl   302.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 69.09s | valid loss  6.03 | valid ppl   417.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  30 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.56 | loss  5.76 | ppl   318.70\n",
            "| epoch  30 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.73 | ppl   308.17\n",
            "| epoch  30 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.66 | ppl   287.84\n",
            "| epoch  30 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.72 | ppl   304.59\n",
            "| epoch  30 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.71 | ppl   300.91\n",
            "| epoch  30 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.76 | ppl   316.66\n",
            "| epoch  30 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.76 | ppl   316.54\n",
            "| epoch  30 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.79 | ppl   325.69\n",
            "| epoch  30 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.74 | ppl   311.84\n",
            "| epoch  30 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.78 | ppl   322.23\n",
            "| epoch  30 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.69 | ppl   296.81\n",
            "| epoch  30 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.70 | ppl   297.57\n",
            "| epoch  30 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.73 | ppl   307.30\n",
            "| epoch  30 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.69 | ppl   295.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 69.09s | valid loss  5.99 | valid ppl   401.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  31 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.56 | loss  5.71 | ppl   302.38\n",
            "| epoch  31 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.69 | ppl   295.21\n",
            "| epoch  31 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.61 | ppl   272.52\n",
            "| epoch  31 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.66 | ppl   287.12\n",
            "| epoch  31 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.65 | ppl   283.49\n",
            "| epoch  31 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.70 | ppl   297.66\n",
            "| epoch  31 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.70 | ppl   300.04\n",
            "| epoch  31 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.73 | ppl   308.82\n",
            "| epoch  31 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.69 | ppl   297.18\n",
            "| epoch  31 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.42 | loss  5.73 | ppl   306.81\n",
            "| epoch  31 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.63 | ppl   279.00\n",
            "| epoch  31 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.63 | ppl   278.97\n",
            "| epoch  31 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.67 | ppl   289.49\n",
            "| epoch  31 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.63 | ppl   280.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 69.08s | valid loss  5.98 | valid ppl   394.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  32 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.55 | loss  5.67 | ppl   290.13\n",
            "| epoch  32 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.65 | ppl   284.03\n",
            "| epoch  32 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.57 | ppl   261.62\n",
            "| epoch  32 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.62 | ppl   276.98\n",
            "| epoch  32 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.61 | ppl   272.66\n",
            "| epoch  32 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.66 | ppl   287.27\n",
            "| epoch  32 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.67 | ppl   288.70\n",
            "| epoch  32 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.69 | ppl   297.23\n",
            "| epoch  32 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.66 | ppl   286.73\n",
            "| epoch  32 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.69 | ppl   295.84\n",
            "| epoch  32 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.60 | ppl   269.11\n",
            "| epoch  32 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.60 | ppl   269.51\n",
            "| epoch  32 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.63 | ppl   279.18\n",
            "| epoch  32 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.60 | ppl   270.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 69.09s | valid loss  5.96 | valid ppl   386.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  33 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.54 | loss  5.64 | ppl   280.55\n",
            "| epoch  33 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.62 | ppl   275.17\n",
            "| epoch  33 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.53 | ppl   252.64\n",
            "| epoch  33 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.59 | ppl   266.97\n",
            "| epoch  33 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.57 | ppl   262.83\n",
            "| epoch  33 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.62 | ppl   277.10\n",
            "| epoch  33 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.63 | ppl   279.96\n",
            "| epoch  33 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.66 | ppl   287.23\n",
            "| epoch  33 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.63 | ppl   277.44\n",
            "| epoch  33 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.66 | ppl   285.93\n",
            "| epoch  33 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.56 | ppl   260.16\n",
            "| epoch  33 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.56 | ppl   260.29\n",
            "| epoch  33 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.60 | ppl   269.69\n",
            "| epoch  33 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.57 | ppl   261.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 69.09s | valid loss  5.94 | valid ppl   380.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  34 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.56 | loss  5.60 | ppl   270.32\n",
            "| epoch  34 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.58 | ppl   266.31\n",
            "| epoch  34 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.50 | ppl   245.59\n",
            "| epoch  34 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.55 | ppl   257.61\n",
            "| epoch  34 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.54 | ppl   255.00\n",
            "| epoch  34 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.60 | ppl   269.40\n",
            "| epoch  34 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.60 | ppl   269.93\n",
            "| epoch  34 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.63 | ppl   278.07\n",
            "| epoch  34 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.59 | ppl   268.86\n",
            "| epoch  34 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  5.62 | ppl   277.10\n",
            "| epoch  34 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.53 | ppl   251.32\n",
            "| epoch  34 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.53 | ppl   251.52\n",
            "| epoch  34 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.57 | ppl   262.12\n",
            "| epoch  34 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.53 | ppl   252.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 69.10s | valid loss  5.92 | valid ppl   372.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  35 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.55 | loss  5.57 | ppl   262.52\n",
            "| epoch  35 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.55 | ppl   258.24\n",
            "| epoch  35 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.47 | ppl   238.30\n",
            "| epoch  35 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.42 | loss  5.52 | ppl   250.28\n",
            "| epoch  35 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.51 | ppl   247.38\n",
            "| epoch  35 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.57 | ppl   261.29\n",
            "| epoch  35 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.57 | ppl   262.39\n",
            "| epoch  35 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.60 | ppl   271.10\n",
            "| epoch  35 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.57 | ppl   261.21\n",
            "| epoch  35 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.60 | ppl   269.09\n",
            "| epoch  35 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.50 | ppl   244.64\n",
            "| epoch  35 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.50 | ppl   244.19\n",
            "| epoch  35 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.54 | ppl   254.35\n",
            "| epoch  35 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.50 | ppl   245.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 69.09s | valid loss  5.90 | valid ppl   364.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  36 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.54 | loss  5.54 | ppl   255.48\n",
            "| epoch  36 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.52 | ppl   250.67\n",
            "| epoch  36 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.44 | ppl   231.60\n",
            "| epoch  36 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.49 | ppl   243.10\n",
            "| epoch  36 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.48 | ppl   239.68\n",
            "| epoch  36 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.54 | ppl   253.84\n",
            "| epoch  36 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.54 | ppl   255.01\n",
            "| epoch  36 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  5.57 | ppl   262.94\n",
            "| epoch  36 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.54 | ppl   253.96\n",
            "| epoch  36 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.57 | ppl   261.51\n",
            "| epoch  36 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.47 | ppl   237.05\n",
            "| epoch  36 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.47 | ppl   238.18\n",
            "| epoch  36 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.51 | ppl   247.50\n",
            "| epoch  36 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.47 | ppl   238.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 69.10s | valid loss  5.87 | valid ppl   353.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  37 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.57 | loss  5.51 | ppl   247.53\n",
            "| epoch  37 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.50 | ppl   243.70\n",
            "| epoch  37 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.42 | ppl   225.95\n",
            "| epoch  37 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.47 | ppl   237.28\n",
            "| epoch  37 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.45 | ppl   233.68\n",
            "| epoch  37 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.51 | ppl   246.63\n",
            "| epoch  37 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.52 | ppl   248.42\n",
            "| epoch  37 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.54 | ppl   255.64\n",
            "| epoch  37 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.51 | ppl   247.06\n",
            "| epoch  37 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.54 | ppl   254.57\n",
            "| epoch  37 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.44 | ppl   230.76\n",
            "| epoch  37 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.45 | ppl   231.90\n",
            "| epoch  37 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.48 | ppl   240.29\n",
            "| epoch  37 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.45 | ppl   232.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 69.11s | valid loss  5.86 | valid ppl   350.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  38 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.57 | loss  5.49 | ppl   241.52\n",
            "| epoch  38 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.47 | ppl   237.71\n",
            "| epoch  38 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.39 | ppl   220.01\n",
            "| epoch  38 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.44 | ppl   230.56\n",
            "| epoch  38 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.43 | ppl   227.98\n",
            "| epoch  38 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.48 | ppl   240.53\n",
            "| epoch  38 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.49 | ppl   242.27\n",
            "| epoch  38 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.52 | ppl   249.36\n",
            "| epoch  38 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.49 | ppl   241.59\n",
            "| epoch  38 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.51 | ppl   248.37\n",
            "| epoch  38 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.41 | ppl   224.58\n",
            "| epoch  38 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.42 | ppl   226.47\n",
            "| epoch  38 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.46 | ppl   234.76\n",
            "| epoch  38 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.42 | ppl   225.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 69.11s | valid loss  5.85 | valid ppl   347.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  39 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.55 | loss  5.46 | ppl   235.21\n",
            "| epoch  39 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.45 | ppl   231.68\n",
            "| epoch  39 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.37 | ppl   214.42\n",
            "| epoch  39 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.42 | ppl   225.29\n",
            "| epoch  39 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.40 | ppl   222.11\n",
            "| epoch  39 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.46 | ppl   233.94\n",
            "| epoch  39 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  5.46 | ppl   235.80\n",
            "| epoch  39 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.49 | ppl   242.78\n",
            "| epoch  39 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.46 | ppl   234.34\n",
            "| epoch  39 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  5.49 | ppl   242.67\n",
            "| epoch  39 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.39 | ppl   218.81\n",
            "| epoch  39 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.40 | ppl   220.88\n",
            "| epoch  39 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.43 | ppl   229.23\n",
            "| epoch  39 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.39 | ppl   220.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 69.15s | valid loss  5.82 | valid ppl   337.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  40 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.56 | loss  5.44 | ppl   229.85\n",
            "| epoch  40 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.42 | ppl   226.33\n",
            "| epoch  40 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.35 | ppl   209.92\n",
            "| epoch  40 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.39 | ppl   219.07\n",
            "| epoch  40 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.38 | ppl   216.92\n",
            "| epoch  40 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.44 | ppl   229.94\n",
            "| epoch  40 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.44 | ppl   230.34\n",
            "| epoch  40 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.47 | ppl   237.39\n",
            "| epoch  40 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.44 | ppl   229.51\n",
            "| epoch  40 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.47 | ppl   236.32\n",
            "| epoch  40 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.37 | ppl   214.27\n",
            "| epoch  40 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.37 | ppl   215.54\n",
            "| epoch  40 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.41 | ppl   223.67\n",
            "| epoch  40 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.37 | ppl   215.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 69.11s | valid loss  5.83 | valid ppl   339.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  41 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.56 | loss  5.42 | ppl   225.37\n",
            "| epoch  41 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.40 | ppl   221.66\n",
            "| epoch  41 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  5.32 | ppl   205.26\n",
            "| epoch  41 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.37 | ppl   213.95\n",
            "| epoch  41 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.36 | ppl   212.31\n",
            "| epoch  41 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.41 | ppl   223.82\n",
            "| epoch  41 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.42 | ppl   224.85\n",
            "| epoch  41 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.45 | ppl   231.89\n",
            "| epoch  41 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.41 | ppl   224.12\n",
            "| epoch  41 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.44 | ppl   231.41\n",
            "| epoch  41 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.34 | ppl   209.39\n",
            "| epoch  41 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.35 | ppl   209.76\n",
            "| epoch  41 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.39 | ppl   219.83\n",
            "| epoch  41 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.35 | ppl   210.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 69.10s | valid loss  5.80 | valid ppl   330.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  42 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.57 | loss  5.39 | ppl   220.28\n",
            "| epoch  42 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.38 | ppl   217.14\n",
            "| epoch  42 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.30 | ppl   200.62\n",
            "| epoch  42 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.34 | ppl   208.87\n",
            "| epoch  42 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.34 | ppl   207.78\n",
            "| epoch  42 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.39 | ppl   219.07\n",
            "| epoch  42 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  5.39 | ppl   220.11\n",
            "| epoch  42 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  5.43 | ppl   227.92\n",
            "| epoch  42 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.39 | ppl   218.83\n",
            "| epoch  42 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.42 | ppl   226.44\n",
            "| epoch  42 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.32 | ppl   204.91\n",
            "| epoch  42 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  5.33 | ppl   206.10\n",
            "| epoch  42 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.37 | ppl   214.06\n",
            "| epoch  42 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.32 | ppl   204.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 69.14s | valid loss  5.79 | valid ppl   325.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  43 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.55 | loss  5.37 | ppl   215.69\n",
            "| epoch  43 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.36 | ppl   212.54\n",
            "| epoch  43 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.28 | ppl   197.32\n",
            "| epoch  43 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.32 | ppl   205.19\n",
            "| epoch  43 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.32 | ppl   204.19\n",
            "| epoch  43 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.37 | ppl   214.28\n",
            "| epoch  43 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.38 | ppl   216.16\n",
            "| epoch  43 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.41 | ppl   222.83\n",
            "| epoch  43 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.37 | ppl   214.88\n",
            "| epoch  43 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.40 | ppl   221.78\n",
            "| epoch  43 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.30 | ppl   200.82\n",
            "| epoch  43 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.31 | ppl   202.03\n",
            "| epoch  43 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  5.35 | ppl   209.90\n",
            "| epoch  43 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.30 | ppl   200.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 69.14s | valid loss  5.77 | valid ppl   321.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  44 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.56 | loss  5.36 | ppl   212.17\n",
            "| epoch  44 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.34 | ppl   207.90\n",
            "| epoch  44 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.26 | ppl   193.01\n",
            "| epoch  44 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.30 | ppl   200.20\n",
            "| epoch  44 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.30 | ppl   199.77\n",
            "| epoch  44 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.35 | ppl   210.26\n",
            "| epoch  44 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.35 | ppl   211.11\n",
            "| epoch  44 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  5.38 | ppl   217.86\n",
            "| epoch  44 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.35 | ppl   210.90\n",
            "| epoch  44 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.38 | ppl   216.89\n",
            "| epoch  44 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.28 | ppl   197.12\n",
            "| epoch  44 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.29 | ppl   197.77\n",
            "| epoch  44 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.33 | ppl   205.59\n",
            "| epoch  44 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.28 | ppl   196.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 69.14s | valid loss  5.77 | valid ppl   320.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  45 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.55 | loss  5.33 | ppl   206.71\n",
            "| epoch  45 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.32 | ppl   204.44\n",
            "| epoch  45 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.24 | ppl   189.16\n",
            "| epoch  45 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.28 | ppl   197.10\n",
            "| epoch  45 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.28 | ppl   195.69\n",
            "| epoch  45 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.33 | ppl   206.51\n",
            "| epoch  45 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.33 | ppl   207.13\n",
            "| epoch  45 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.37 | ppl   214.33\n",
            "| epoch  45 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.33 | ppl   207.30\n",
            "| epoch  45 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.36 | ppl   212.58\n",
            "| epoch  45 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.42 | loss  5.26 | ppl   192.87\n",
            "| epoch  45 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.27 | ppl   194.89\n",
            "| epoch  45 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.31 | ppl   202.54\n",
            "| epoch  45 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.26 | ppl   192.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 69.07s | valid loss  5.76 | valid ppl   315.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  46 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.54 | loss  5.32 | ppl   204.99\n",
            "| epoch  46 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.31 | ppl   201.76\n",
            "| epoch  46 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.22 | ppl   185.62\n",
            "| epoch  46 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  5.26 | ppl   193.30\n",
            "| epoch  46 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.26 | ppl   192.54\n",
            "| epoch  46 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.31 | ppl   202.14\n",
            "| epoch  46 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.32 | ppl   203.96\n",
            "| epoch  46 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.35 | ppl   210.93\n",
            "| epoch  46 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.32 | ppl   203.87\n",
            "| epoch  46 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.34 | ppl   209.10\n",
            "| epoch  46 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.24 | ppl   189.36\n",
            "| epoch  46 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.25 | ppl   190.65\n",
            "| epoch  46 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.29 | ppl   198.87\n",
            "| epoch  46 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.25 | ppl   189.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 69.10s | valid loss  5.76 | valid ppl   316.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  47 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.55 | loss  5.30 | ppl   200.75\n",
            "| epoch  47 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  5.29 | ppl   198.38\n",
            "| epoch  47 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.20 | ppl   181.85\n",
            "| epoch  47 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.24 | ppl   189.61\n",
            "| epoch  47 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.24 | ppl   189.13\n",
            "| epoch  47 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.29 | ppl   199.32\n",
            "| epoch  47 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.30 | ppl   200.32\n",
            "| epoch  47 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.33 | ppl   206.84\n",
            "| epoch  47 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.30 | ppl   199.88\n",
            "| epoch  47 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.32 | ppl   204.46\n",
            "| epoch  47 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.23 | ppl   185.89\n",
            "| epoch  47 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.23 | ppl   187.11\n",
            "| epoch  47 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.28 | ppl   195.50\n",
            "| epoch  47 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.23 | ppl   186.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 69.10s | valid loss  5.75 | valid ppl   314.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  48 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.55 | loss  5.31 | ppl   202.65\n",
            "| epoch  48 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.29 | ppl   197.97\n",
            "| epoch  48 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.21 | ppl   182.41\n",
            "| epoch  48 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.26 | ppl   191.83\n",
            "| epoch  48 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.26 | ppl   191.71\n",
            "| epoch  48 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.31 | ppl   201.48\n",
            "| epoch  48 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.31 | ppl   202.71\n",
            "| epoch  48 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.34 | ppl   208.72\n",
            "| epoch  48 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.31 | ppl   202.06\n",
            "| epoch  48 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.32 | ppl   205.19\n",
            "| epoch  48 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.50 | loss  5.24 | ppl   188.89\n",
            "| epoch  48 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.25 | ppl   190.67\n",
            "| epoch  48 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  5.28 | ppl   197.05\n",
            "| epoch  48 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.53 | loss  5.24 | ppl   188.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 69.13s | valid loss  5.74 | valid ppl   310.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  49 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.57 | loss  5.27 | ppl   194.91\n",
            "| epoch  49 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.25 | ppl   191.51\n",
            "| epoch  49 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.17 | ppl   175.44\n",
            "| epoch  49 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.21 | ppl   183.17\n",
            "| epoch  49 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.21 | ppl   183.39\n",
            "| epoch  49 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.26 | ppl   192.60\n",
            "| epoch  49 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.27 | ppl   193.53\n",
            "| epoch  49 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.30 | ppl   200.18\n",
            "| epoch  49 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  5.27 | ppl   195.23\n",
            "| epoch  49 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.29 | ppl   199.20\n",
            "| epoch  49 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.19 | ppl   180.08\n",
            "| epoch  49 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  5.20 | ppl   181.88\n",
            "| epoch  49 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.24 | ppl   188.52\n",
            "| epoch  49 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.52 | loss  5.20 | ppl   181.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 69.16s | valid loss  5.74 | valid ppl   311.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  50 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.62 | loss  5.27 | ppl   193.84\n",
            "| epoch  50 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.51 | loss  5.25 | ppl   190.35\n",
            "| epoch  50 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  5.17 | ppl   175.45\n",
            "| epoch  50 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.52 | loss  5.22 | ppl   185.25\n",
            "| epoch  50 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.51 | loss  5.22 | ppl   184.96\n",
            "| epoch  50 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.27 | ppl   193.62\n",
            "| epoch  50 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  5.27 | ppl   195.13\n",
            "| epoch  50 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.31 | ppl   201.64\n",
            "| epoch  50 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  5.27 | ppl   194.55\n",
            "| epoch  50 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.51 | loss  5.29 | ppl   198.01\n",
            "| epoch  50 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.21 | ppl   182.25\n",
            "| epoch  50 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.49 | loss  5.21 | ppl   183.05\n",
            "| epoch  50 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.24 | ppl   189.39\n",
            "| epoch  50 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.20 | ppl   181.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 69.21s | valid loss  5.72 | valid ppl   304.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  51 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.56 | loss  5.24 | ppl   188.19\n",
            "| epoch  51 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  5.23 | ppl   186.09\n",
            "| epoch  51 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.13 | ppl   169.32\n",
            "| epoch  51 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.18 | ppl   177.57\n",
            "| epoch  51 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.18 | ppl   177.35\n",
            "| epoch  51 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.23 | ppl   186.59\n",
            "| epoch  51 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.24 | ppl   188.18\n",
            "| epoch  51 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  5.27 | ppl   194.30\n",
            "| epoch  51 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.24 | ppl   188.82\n",
            "| epoch  51 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.26 | ppl   192.14\n",
            "| epoch  51 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.16 | ppl   173.92\n",
            "| epoch  51 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.17 | ppl   175.89\n",
            "| epoch  51 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  5.22 | ppl   184.47\n",
            "| epoch  51 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.16 | ppl   174.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 69.16s | valid loss  5.73 | valid ppl   306.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  52 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.57 | loss  5.24 | ppl   188.31\n",
            "| epoch  52 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.22 | ppl   185.12\n",
            "| epoch  52 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.13 | ppl   169.59\n",
            "| epoch  52 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.19 | ppl   178.80\n",
            "| epoch  52 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.19 | ppl   178.99\n",
            "| epoch  52 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.23 | ppl   186.62\n",
            "| epoch  52 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.24 | ppl   188.52\n",
            "| epoch  52 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.27 | ppl   194.25\n",
            "| epoch  52 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.24 | ppl   189.00\n",
            "| epoch  52 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.25 | ppl   190.12\n",
            "| epoch  52 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.17 | ppl   175.65\n",
            "| epoch  52 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.18 | ppl   176.88\n",
            "| epoch  52 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.22 | ppl   184.10\n",
            "| epoch  52 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.17 | ppl   176.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 69.10s | valid loss  5.72 | valid ppl   305.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  53 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.56 | loss  5.21 | ppl   183.25\n",
            "| epoch  53 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.20 | ppl   180.47\n",
            "| epoch  53 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.10 | ppl   164.71\n",
            "| epoch  53 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.15 | ppl   171.90\n",
            "| epoch  53 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.15 | ppl   172.74\n",
            "| epoch  53 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.20 | ppl   180.63\n",
            "| epoch  53 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.20 | ppl   181.89\n",
            "| epoch  53 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.23 | ppl   187.52\n",
            "| epoch  53 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.20 | ppl   181.89\n",
            "| epoch  53 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.22 | ppl   185.64\n",
            "| epoch  53 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.13 | ppl   169.12\n",
            "| epoch  53 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.14 | ppl   170.89\n",
            "| epoch  53 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.18 | ppl   178.39\n",
            "| epoch  53 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.14 | ppl   170.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 69.11s | valid loss  5.71 | valid ppl   303.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  54 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.57 | loss  5.19 | ppl   179.03\n",
            "| epoch  54 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.18 | ppl   177.73\n",
            "| epoch  54 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.09 | ppl   161.79\n",
            "| epoch  54 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.13 | ppl   169.25\n",
            "| epoch  54 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.42 | loss  5.14 | ppl   170.70\n",
            "| epoch  54 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.18 | ppl   177.60\n",
            "| epoch  54 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.19 | ppl   178.91\n",
            "| epoch  54 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.22 | ppl   185.14\n",
            "| epoch  54 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.19 | ppl   179.66\n",
            "| epoch  54 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.21 | ppl   183.13\n",
            "| epoch  54 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.11 | ppl   166.07\n",
            "| epoch  54 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.12 | ppl   167.92\n",
            "| epoch  54 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.16 | ppl   174.56\n",
            "| epoch  54 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.12 | ppl   167.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 69.09s | valid loss  5.72 | valid ppl   303.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  55 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.56 | loss  5.17 | ppl   176.25\n",
            "| epoch  55 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.17 | ppl   175.11\n",
            "| epoch  55 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.07 | ppl   159.00\n",
            "| epoch  55 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.12 | ppl   167.39\n",
            "| epoch  55 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.12 | ppl   168.16\n",
            "| epoch  55 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.16 | ppl   174.76\n",
            "| epoch  55 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.18 | ppl   176.84\n",
            "| epoch  55 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.21 | ppl   182.89\n",
            "| epoch  55 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.17 | ppl   176.46\n",
            "| epoch  55 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.19 | ppl   179.95\n",
            "| epoch  55 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.10 | ppl   163.33\n",
            "| epoch  55 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.11 | ppl   165.13\n",
            "| epoch  55 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.15 | ppl   172.97\n",
            "| epoch  55 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.10 | ppl   164.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 69.12s | valid loss  5.70 | valid ppl   300.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  56 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.59 | loss  5.16 | ppl   175.03\n",
            "| epoch  56 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.15 | ppl   171.68\n",
            "| epoch  56 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.05 | ppl   156.44\n",
            "| epoch  56 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.11 | ppl   165.11\n",
            "| epoch  56 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.10 | ppl   164.65\n",
            "| epoch  56 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.15 | ppl   172.02\n",
            "| epoch  56 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.16 | ppl   173.95\n",
            "| epoch  56 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.19 | ppl   179.93\n",
            "| epoch  56 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.16 | ppl   173.93\n",
            "| epoch  56 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.18 | ppl   177.83\n",
            "| epoch  56 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.08 | ppl   161.35\n",
            "| epoch  56 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.10 | ppl   163.61\n",
            "| epoch  56 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.42 | loss  5.14 | ppl   170.28\n",
            "| epoch  56 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.09 | ppl   162.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 69.10s | valid loss  5.70 | valid ppl   297.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  57 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.56 | loss  5.15 | ppl   172.87\n",
            "| epoch  57 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.14 | ppl   170.30\n",
            "| epoch  57 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.04 | ppl   154.42\n",
            "| epoch  57 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.09 | ppl   161.60\n",
            "| epoch  57 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.09 | ppl   162.33\n",
            "| epoch  57 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.14 | ppl   170.28\n",
            "| epoch  57 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.15 | ppl   172.56\n",
            "| epoch  57 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.18 | ppl   176.95\n",
            "| epoch  57 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.14 | ppl   170.93\n",
            "| epoch  57 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.17 | ppl   175.53\n",
            "| epoch  57 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.06 | ppl   158.27\n",
            "| epoch  57 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.08 | ppl   160.34\n",
            "| epoch  57 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.12 | ppl   167.95\n",
            "| epoch  57 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.07 | ppl   159.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 69.10s | valid loss  5.70 | valid ppl   297.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  58 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.57 | loss  5.14 | ppl   169.90\n",
            "| epoch  58 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.12 | ppl   167.52\n",
            "| epoch  58 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.02 | ppl   151.69\n",
            "| epoch  58 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.08 | ppl   160.03\n",
            "| epoch  58 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.08 | ppl   159.99\n",
            "| epoch  58 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.12 | ppl   167.79\n",
            "| epoch  58 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.13 | ppl   169.74\n",
            "| epoch  58 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.17 | ppl   175.22\n",
            "| epoch  58 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.13 | ppl   169.05\n",
            "| epoch  58 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.15 | ppl   172.59\n",
            "| epoch  58 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.06 | ppl   156.95\n",
            "| epoch  58 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.07 | ppl   158.41\n",
            "| epoch  58 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.11 | ppl   165.50\n",
            "| epoch  58 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.06 | ppl   157.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 69.11s | valid loss  5.68 | valid ppl   293.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  59 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.53 | loss  5.12 | ppl   167.48\n",
            "| epoch  59 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.11 | ppl   165.67\n",
            "| epoch  59 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.01 | ppl   150.35\n",
            "| epoch  59 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.07 | ppl   158.48\n",
            "| epoch  59 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.06 | ppl   157.46\n",
            "| epoch  59 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.11 | ppl   165.83\n",
            "| epoch  59 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.13 | ppl   168.59\n",
            "| epoch  59 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.16 | ppl   173.52\n",
            "| epoch  59 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.12 | ppl   166.78\n",
            "| epoch  59 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.14 | ppl   171.17\n",
            "| epoch  59 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.04 | ppl   154.32\n",
            "| epoch  59 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.05 | ppl   156.11\n",
            "| epoch  59 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.09 | ppl   162.83\n",
            "| epoch  59 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.05 | ppl   156.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 69.09s | valid loss  5.68 | valid ppl   292.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  60 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.56 | loss  5.11 | ppl   165.53\n",
            "| epoch  60 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.46 | loss  5.10 | ppl   163.72\n",
            "| epoch  60 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.00 | ppl   148.79\n",
            "| epoch  60 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.05 | ppl   156.48\n",
            "| epoch  60 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.05 | ppl   156.27\n",
            "| epoch  60 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.10 | ppl   163.70\n",
            "| epoch  60 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.11 | ppl   166.31\n",
            "| epoch  60 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.14 | ppl   170.57\n",
            "| epoch  60 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.11 | ppl   165.36\n",
            "| epoch  60 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.45 | loss  5.13 | ppl   168.84\n",
            "| epoch  60 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.03 | ppl   152.20\n",
            "| epoch  60 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.47 | loss  5.05 | ppl   155.38\n",
            "| epoch  60 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.43 | loss  5.08 | ppl   161.19\n",
            "| epoch  60 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  5.03 | ppl   153.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 69.11s | valid loss  5.67 | valid ppl   290.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "=========================================================================================\n",
            "| End of training | test loss  5.56 | test ppl   260.08\n",
            "=========================================================================================\n",
            "\n",
            "Generating text...\n",
            "Vocabulary size: 33278\n",
            "Vocabulary size: 33278\n",
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n",
            "Since every restoration of Ireland of the series birds have been certainly restricted by the ROK criminal   touchdown Commissioner\n",
            "as known as order to sharp of more under the entire purpose of screen for a part of just .\n",
            "Hornung 's work records indicates as a Indo   story \" music that Rick , but he begins on \"\n",
            "the Week for the thief , <unk> , the water . He working by right of between Dublin \" ,\n",
            "encouraging from writings at this Star writer , this primary novels hard , pretty teaches the \" , a bit\n",
            "too assimilated to find his friend , but time , Rome identifies about his long race with its fifth depth\n",
            "of once Metro since Hornung 's son Bill , searching with this forces \" , who was deeply obtained by\n",
            "\" grasslands by which it is left the evil . \" The Magdalen was classified with Gofraid 's project .\n",
            "\" <eos> In 2004 , the season arises in multiple players , simple , <unk> mac 'Malley <unk>   Sun\n",
            "games , where it was invited to shut down civilian and Omar that according to their reign of the Calendar\n",
            "arenas , but reveals his series , he sometimes unable to be under minor seconds . But in the book\n",
            "'s reincarnation , Moment character of the name a collection of fascists , very compares in this languages and considerable\n",
            "blood species by his character , and , we 've feel legislators , both him . Benedict resort for the\n",
            "very entries of individual which gross would be permitted in the size of the traditional and rival . <eos> On\n",
            "numerous forest , and publishing 20 cm ( 10 km / h ) , which was completed , completing many\n",
            "environments . <eos> Oxford 's link held by the Eastern Gaelic River , no large transit of Ceres , the\n",
            "first cross   campus post   yard open goal areas , and died up himself refuge to Winnipeg . This\n",
            "sexes was fertile throughout air in taking £ 10 ° F ( 2013 , Lawrence – one of Kingdom )\n",
            "and did now be Cererian conferences , she went   regular run back and nearby rights on attaining the first\n",
            "time . March 10 yards ( 28 September 3 – 10 – 6 in <unk> available for No. 6  \n",
            "200 mi ) on the east of the north and shape . Facing peace <unk> , despite less down the\n",
            "range of for the game that he is leaked to Assi , and her projection and the <unk> was forced\n",
            "in an money to modernize him , Sharif , Cambridge subtle <unk> ( Golden African <unk> ) although most successful\n",
            "<unk> in Italy , he was on the Pulse and was one across it inside all to to clear him\n",
            "as the city of Wilde 's kingship a earlier series catches as spoken and teams have been native . While\n",
            "a result that learns that gameplay I must quit his own <unk> juice Peshkin , despite the name 's son\n",
            ". It began to take advantage of him as possible in <unk> . I 'm still run ; the phenomenon\n",
            "] that may batted Walpole , \" for a man , but greyhound , and <unk> by this A gift\n",
            "<unk> . \" Atwood continued to have evident it by the song on a <unk> for Tessa , \" Cox\n",
            "that 's ball the mortar range of his net between jazz and some song with a perfect government on which\n",
            "allowing out to make him to <unk> possibilities up common today glass and unoriginal ... even rather prominent that some\n",
            "\" <unk> \" when rarely did 't find a \" we don 't immediate certainly of the same play that\n",
            "\" Us mammals . \" Blitt hesitates still , this of the immortality on the end of the 19th century\n",
            ". In 1969 , Byung points <unk> followed with first to meanwhile run after a resupply known as Little Russell\n",
            "\" . Mosley was typically under other <unk> <unk> . <eos> <eos> = = Release Finals Online <unk> = =\n",
            "<eos> <eos> Greg <unk> included his support against the Rogue ( and Palace ) , finds the counterpart . <eos>\n",
            "<eos> = = = Des 'Malley = = = <eos> <eos> On the town of Ímar to the islands ,\n",
            "Turkey , the Wind Devils ; northeastern Korean consecutive hours , along with white and 50 angels ; the search\n",
            "are went to long as twenty   spored , tough in the sport they easily tied a compact flesh .\n",
            "<eos> <eos> = = Development = = <eos> <eos> In an Honorary   supported as moderately of this occasions ,\n",
            "a composition of the margins of a team called remained with a Lites and <unk> him to thrive in the\n",
            "Red Diego ( 1970 ) shut to select crime on the 2010 and the white barometric based in order up\n",
            "to some album forever and <unk> with public   falsetto object \" as the PlayStation 70 suitable charts negative as\n",
            "slightly named King ' ! \" . A text of this their novels 's ability to dealt since the Gaboon\n",
            "Port Republican Man \" , \" Well , Always see its name and <unk> for the <unk> . The group\n",
            "is entitled Eaton \" ; \" <unk> <unk> \" that he contained to see I evidence work and of the\n",
            "20th century and the left prey , and sold the American court . <eos> <eos> = = = <unk> =\n",
            "= = <eos> <eos> NASA State Day , [ which was little positive in passing   overall lover . <eos>\n",
            "<eos> = = Life = = <eos> <eos> In the University of creating the lead rebellion is the first important\n",
            "Empire line . At 9 % for the public flows before 13   000   yard apartment 's production annually\n"
          ]
        }
      ],
      "source": [
        "print(\"Training LSTM on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type = 'RNN_TANH', # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "    data_path = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize = 400,\n",
        "    nhid = 400,\n",
        "    nlayers = 4,\n",
        "    lr = 0.001,\n",
        "    clip = 0.25,\n",
        "    epochs = 60,\n",
        "    batch_size = 20,\n",
        "    bptt = 35,\n",
        "    dropout = 0.2,\n",
        "    tied = False,\n",
        "    nhead = 4,\n",
        "    log_interval = 200,\n",
        "    save_path = 'model_4.pt',\n",
        "    onnx_export = '',\n",
        "    dry_run = False,\n",
        "    accel = True,\n",
        "    use_optimizer = True,\n",
        "    optimizer_type = 'AdamW',\n",
        "    weight_decay=1e-5,\n",
        "    use_betas = False,\n",
        "    use_eps = False,\n",
        "    criterion = nn.NLLLoss(),\n",
        "    use_label_smoothing = False,\n",
        "    label_smoothing = 0.1,\n",
        "    use_warmup = False,\n",
        "    warmup_steps = 4000,\n",
        "    min_freq = 5,\n",
        "    seed = 1111,\n",
        "    old_version = True\n",
        ")\n",
        "\n",
        "# Example 2: Generate text\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model_4.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf='generated_4.txt',\n",
        "    words=1000,\n",
        "    temperature=1.0,\n",
        "    old_version=True,\n",
        "    use_top_k=False,\n",
        "    accel = True\n",
        ")\n",
        "\n",
        "!cat generated_4.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RvSaSWoNrBXG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvSaSWoNrBXG",
        "outputId": "36de2e6a-2fdd-4e05-af06-463bae56f0f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LSTM on WikiText-2...\n",
            "Using device: cuda\n",
            "Vocabulary size: 25251\n",
            "| epoch   1 |   200/ 2983 batches | lr 0.000040 | ms/batch 20.91 | loss  9.50 | ppl 13319.58\n",
            "| epoch   1 |   400/ 2983 batches | lr 0.000079 | ms/batch 20.49 | loss  7.70 | ppl  2206.33\n",
            "| epoch   1 |   600/ 2983 batches | lr 0.000119 | ms/batch 20.54 | loss  7.35 | ppl  1558.14\n",
            "| epoch   1 |   800/ 2983 batches | lr 0.000158 | ms/batch 20.54 | loss  7.12 | ppl  1231.50\n",
            "| epoch   1 |  1000/ 2983 batches | lr 0.000198 | ms/batch 20.57 | loss  7.03 | ppl  1127.30\n",
            "| epoch   1 |  1200/ 2983 batches | lr 0.000237 | ms/batch 20.60 | loss  6.93 | ppl  1018.17\n",
            "| epoch   1 |  1400/ 2983 batches | lr 0.000277 | ms/batch 20.62 | loss  6.81 | ppl   909.53\n",
            "| epoch   1 |  1600/ 2983 batches | lr 0.000316 | ms/batch 20.60 | loss  6.78 | ppl   880.76\n",
            "| epoch   1 |  1800/ 2983 batches | lr 0.000356 | ms/batch 20.60 | loss  6.70 | ppl   813.87\n",
            "| epoch   1 |  2000/ 2983 batches | lr 0.000395 | ms/batch 20.57 | loss  6.69 | ppl   802.77\n",
            "| epoch   1 |  2200/ 2983 batches | lr 0.000435 | ms/batch 20.56 | loss  6.60 | ppl   738.39\n",
            "| epoch   1 |  2400/ 2983 batches | lr 0.000475 | ms/batch 20.56 | loss  6.58 | ppl   718.26\n",
            "| epoch   1 |  2600/ 2983 batches | lr 0.000514 | ms/batch 20.54 | loss  6.57 | ppl   714.15\n",
            "| epoch   1 |  2800/ 2983 batches | lr 0.000554 | ms/batch 20.53 | loss  6.51 | ppl   675.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 63.88s | valid loss  6.91 | valid ppl  1005.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000590\n",
            "| epoch   2 |   200/ 2983 batches | lr 0.000629 | ms/batch 20.62 | loss  6.53 | ppl   685.02\n",
            "| epoch   2 |   400/ 2983 batches | lr 0.000669 | ms/batch 20.55 | loss  6.51 | ppl   671.21\n",
            "| epoch   2 |   600/ 2983 batches | lr 0.000709 | ms/batch 20.52 | loss  6.43 | ppl   620.35\n",
            "| epoch   2 |   800/ 2983 batches | lr 0.000748 | ms/batch 20.51 | loss  6.44 | ppl   628.58\n",
            "| epoch   2 |  1000/ 2983 batches | lr 0.000788 | ms/batch 20.53 | loss  6.45 | ppl   631.02\n",
            "| epoch   2 |  1200/ 2983 batches | lr 0.000773 | ms/batch 20.53 | loss  6.44 | ppl   626.87\n",
            "| epoch   2 |  1400/ 2983 batches | lr 0.000755 | ms/batch 20.53 | loss  6.42 | ppl   616.20\n",
            "| epoch   2 |  1600/ 2983 batches | lr 0.000738 | ms/batch 20.55 | loss  6.46 | ppl   636.87\n",
            "| epoch   2 |  1800/ 2983 batches | lr 0.000723 | ms/batch 20.53 | loss  6.37 | ppl   581.76\n",
            "| epoch   2 |  2000/ 2983 batches | lr 0.000708 | ms/batch 20.53 | loss  6.39 | ppl   592.92\n",
            "| epoch   2 |  2200/ 2983 batches | lr 0.000694 | ms/batch 20.55 | loss  6.32 | ppl   555.79\n",
            "| epoch   2 |  2400/ 2983 batches | lr 0.000681 | ms/batch 20.53 | loss  6.32 | ppl   558.29\n",
            "| epoch   2 |  2600/ 2983 batches | lr 0.000669 | ms/batch 20.53 | loss  6.35 | ppl   571.47\n",
            "| epoch   2 |  2800/ 2983 batches | lr 0.000657 | ms/batch 20.54 | loss  6.29 | ppl   538.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 63.75s | valid loss  6.73 | valid ppl   835.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000647\n",
            "| epoch   3 |   200/ 2983 batches | lr 0.000637 | ms/batch 20.67 | loss  6.33 | ppl   559.23\n",
            "| epoch   3 |   400/ 2983 batches | lr 0.000627 | ms/batch 20.53 | loss  6.32 | ppl   555.22\n",
            "| epoch   3 |   600/ 2983 batches | lr 0.000617 | ms/batch 20.53 | loss  6.22 | ppl   503.60\n",
            "| epoch   3 |   800/ 2983 batches | lr 0.000608 | ms/batch 20.54 | loss  6.24 | ppl   513.92\n",
            "| epoch   3 |  1000/ 2983 batches | lr 0.000599 | ms/batch 20.55 | loss  6.25 | ppl   518.27\n",
            "| epoch   3 |  1200/ 2983 batches | lr 0.000591 | ms/batch 20.53 | loss  6.25 | ppl   517.17\n",
            "| epoch   3 |  1400/ 2983 batches | lr 0.000582 | ms/batch 20.57 | loss  6.25 | ppl   519.78\n",
            "| epoch   3 |  1600/ 2983 batches | lr 0.000575 | ms/batch 20.55 | loss  6.28 | ppl   534.46\n",
            "| epoch   3 |  1800/ 2983 batches | lr 0.000567 | ms/batch 20.53 | loss  6.20 | ppl   494.48\n",
            "| epoch   3 |  2000/ 2983 batches | lr 0.000560 | ms/batch 20.56 | loss  6.23 | ppl   507.25\n",
            "| epoch   3 |  2200/ 2983 batches | lr 0.000553 | ms/batch 20.53 | loss  6.16 | ppl   475.00\n",
            "| epoch   3 |  2400/ 2983 batches | lr 0.000547 | ms/batch 20.52 | loss  6.17 | ppl   479.24\n",
            "| epoch   3 |  2600/ 2983 batches | lr 0.000540 | ms/batch 20.54 | loss  6.21 | ppl   495.27\n",
            "| epoch   3 |  2800/ 2983 batches | lr 0.000534 | ms/batch 20.54 | loss  6.14 | ppl   465.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 63.79s | valid loss  6.67 | valid ppl   785.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000528\n",
            "| epoch   4 |   200/ 2983 batches | lr 0.000523 | ms/batch 20.64 | loss  6.20 | ppl   492.54\n",
            "| epoch   4 |   400/ 2983 batches | lr 0.000517 | ms/batch 20.54 | loss  6.19 | ppl   488.05\n",
            "| epoch   4 |   600/ 2983 batches | lr 0.000512 | ms/batch 20.59 | loss  6.10 | ppl   446.68\n",
            "| epoch   4 |   800/ 2983 batches | lr 0.000506 | ms/batch 20.54 | loss  6.13 | ppl   458.33\n",
            "| epoch   4 |  1000/ 2983 batches | lr 0.000501 | ms/batch 20.53 | loss  6.14 | ppl   463.50\n",
            "| epoch   4 |  1200/ 2983 batches | lr 0.000496 | ms/batch 20.54 | loss  6.14 | ppl   464.77\n",
            "| epoch   4 |  1400/ 2983 batches | lr 0.000491 | ms/batch 20.52 | loss  6.15 | ppl   467.33\n",
            "| epoch   4 |  1600/ 2983 batches | lr 0.000487 | ms/batch 20.53 | loss  6.19 | ppl   485.79\n",
            "| epoch   4 |  1800/ 2983 batches | lr 0.000482 | ms/batch 20.53 | loss  6.10 | ppl   446.52\n",
            "| epoch   4 |  2000/ 2983 batches | lr 0.000478 | ms/batch 20.54 | loss  6.13 | ppl   460.28\n",
            "| epoch   4 |  2200/ 2983 batches | lr 0.000473 | ms/batch 20.51 | loss  6.06 | ppl   430.13\n",
            "| epoch   4 |  2400/ 2983 batches | lr 0.000469 | ms/batch 20.52 | loss  6.08 | ppl   437.70\n",
            "| epoch   4 |  2600/ 2983 batches | lr 0.000465 | ms/batch 20.53 | loss  6.11 | ppl   452.59\n",
            "| epoch   4 |  2800/ 2983 batches | lr 0.000461 | ms/batch 20.52 | loss  6.05 | ppl   425.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 63.74s | valid loss  6.59 | valid ppl   729.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000458\n",
            "| epoch   5 |   200/ 2983 batches | lr 0.000454 | ms/batch 20.62 | loss  6.12 | ppl   452.71\n",
            "| epoch   5 |   400/ 2983 batches | lr 0.000450 | ms/batch 20.52 | loss  6.10 | ppl   447.18\n",
            "| epoch   5 |   600/ 2983 batches | lr 0.000447 | ms/batch 20.54 | loss  6.01 | ppl   409.20\n",
            "| epoch   5 |   800/ 2983 batches | lr 0.000443 | ms/batch 20.52 | loss  6.04 | ppl   421.91\n",
            "| epoch   5 |  1000/ 2983 batches | lr 0.000440 | ms/batch 20.52 | loss  6.06 | ppl   427.03\n",
            "| epoch   5 |  1200/ 2983 batches | lr 0.000436 | ms/batch 20.51 | loss  6.06 | ppl   426.60\n",
            "| epoch   5 |  1400/ 2983 batches | lr 0.000433 | ms/batch 20.51 | loss  6.07 | ppl   432.29\n",
            "| epoch   5 |  1600/ 2983 batches | lr 0.000430 | ms/batch 20.53 | loss  6.11 | ppl   450.28\n",
            "| epoch   5 |  1800/ 2983 batches | lr 0.000427 | ms/batch 20.54 | loss  6.03 | ppl   413.81\n",
            "| epoch   5 |  2000/ 2983 batches | lr 0.000424 | ms/batch 20.52 | loss  6.05 | ppl   425.05\n",
            "| epoch   5 |  2200/ 2983 batches | lr 0.000421 | ms/batch 20.53 | loss  5.99 | ppl   399.46\n",
            "| epoch   5 |  2400/ 2983 batches | lr 0.000418 | ms/batch 20.53 | loss  6.01 | ppl   405.66\n",
            "| epoch   5 |  2600/ 2983 batches | lr 0.000415 | ms/batch 20.52 | loss  6.04 | ppl   421.05\n",
            "| epoch   5 |  2800/ 2983 batches | lr 0.000412 | ms/batch 20.53 | loss  5.98 | ppl   397.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 63.72s | valid loss  6.55 | valid ppl   697.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000409\n",
            "| epoch   6 |   200/ 2983 batches | lr 0.000407 | ms/batch 20.63 | loss  6.05 | ppl   422.16\n",
            "| epoch   6 |   400/ 2983 batches | lr 0.000404 | ms/batch 20.55 | loss  6.04 | ppl   418.49\n",
            "| epoch   6 |   600/ 2983 batches | lr 0.000401 | ms/batch 20.53 | loss  5.95 | ppl   383.62\n",
            "| epoch   6 |   800/ 2983 batches | lr 0.000399 | ms/batch 20.52 | loss  5.98 | ppl   397.10\n",
            "| epoch   6 |  1000/ 2983 batches | lr 0.000396 | ms/batch 20.54 | loss  5.99 | ppl   400.92\n",
            "| epoch   6 |  1200/ 2983 batches | lr 0.000394 | ms/batch 20.53 | loss  5.99 | ppl   400.20\n",
            "| epoch   6 |  1400/ 2983 batches | lr 0.000391 | ms/batch 20.51 | loss  6.01 | ppl   406.00\n",
            "| epoch   6 |  1600/ 2983 batches | lr 0.000389 | ms/batch 20.52 | loss  6.06 | ppl   426.44\n",
            "| epoch   6 |  1800/ 2983 batches | lr 0.000387 | ms/batch 20.50 | loss  5.97 | ppl   390.89\n",
            "| epoch   6 |  2000/ 2983 batches | lr 0.000384 | ms/batch 20.53 | loss  6.00 | ppl   401.47\n",
            "| epoch   6 |  2200/ 2983 batches | lr 0.000382 | ms/batch 20.55 | loss  5.94 | ppl   378.07\n",
            "| epoch   6 |  2400/ 2983 batches | lr 0.000380 | ms/batch 20.54 | loss  5.95 | ppl   385.53\n",
            "| epoch   6 |  2600/ 2983 batches | lr 0.000378 | ms/batch 20.55 | loss  5.99 | ppl   400.05\n",
            "| epoch   6 |  2800/ 2983 batches | lr 0.000376 | ms/batch 20.54 | loss  5.93 | ppl   375.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 63.74s | valid loss  6.51 | valid ppl   672.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000374\n",
            "| epoch   7 |   200/ 2983 batches | lr 0.000372 | ms/batch 20.67 | loss  5.99 | ppl   400.89\n",
            "| epoch   7 |   400/ 2983 batches | lr 0.000370 | ms/batch 20.52 | loss  5.99 | ppl   399.37\n",
            "| epoch   7 |   600/ 2983 batches | lr 0.000368 | ms/batch 20.54 | loss  5.90 | ppl   366.03\n",
            "| epoch   7 |   800/ 2983 batches | lr 0.000366 | ms/batch 20.54 | loss  5.94 | ppl   378.10\n",
            "| epoch   7 |  1000/ 2983 batches | lr 0.000364 | ms/batch 20.52 | loss  5.95 | ppl   382.99\n",
            "| epoch   7 |  1200/ 2983 batches | lr 0.000362 | ms/batch 20.53 | loss  5.94 | ppl   381.50\n",
            "| epoch   7 |  1400/ 2983 batches | lr 0.000360 | ms/batch 20.56 | loss  5.96 | ppl   388.54\n",
            "| epoch   7 |  1600/ 2983 batches | lr 0.000358 | ms/batch 20.53 | loss  6.01 | ppl   408.28\n",
            "| epoch   7 |  1800/ 2983 batches | lr 0.000356 | ms/batch 20.53 | loss  5.92 | ppl   372.69\n",
            "| epoch   7 |  2000/ 2983 batches | lr 0.000354 | ms/batch 20.54 | loss  5.95 | ppl   384.41\n",
            "| epoch   7 |  2200/ 2983 batches | lr 0.000353 | ms/batch 20.52 | loss  5.89 | ppl   362.42\n",
            "| epoch   7 |  2400/ 2983 batches | lr 0.000351 | ms/batch 20.53 | loss  5.91 | ppl   368.95\n",
            "| epoch   7 |  2600/ 2983 batches | lr 0.000349 | ms/batch 20.56 | loss  5.95 | ppl   383.78\n",
            "| epoch   7 |  2800/ 2983 batches | lr 0.000347 | ms/batch 20.53 | loss  5.89 | ppl   361.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 63.76s | valid loss  6.49 | valid ppl   657.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000346\n",
            "| epoch   8 |   200/ 2983 batches | lr 0.000344 | ms/batch 20.64 | loss  5.96 | ppl   386.54\n",
            "| epoch   8 |   400/ 2983 batches | lr 0.000343 | ms/batch 20.54 | loss  5.95 | ppl   385.07\n",
            "| epoch   8 |   600/ 2983 batches | lr 0.000341 | ms/batch 20.53 | loss  5.86 | ppl   352.31\n",
            "| epoch   8 |   800/ 2983 batches | lr 0.000340 | ms/batch 20.52 | loss  5.90 | ppl   365.41\n",
            "| epoch   8 |  1000/ 2983 batches | lr 0.000338 | ms/batch 20.53 | loss  5.92 | ppl   370.79\n",
            "| epoch   8 |  1200/ 2983 batches | lr 0.000336 | ms/batch 20.54 | loss  5.91 | ppl   367.82\n",
            "| epoch   8 |  1400/ 2983 batches | lr 0.000335 | ms/batch 20.52 | loss  5.93 | ppl   375.00\n",
            "| epoch   8 |  1600/ 2983 batches | lr 0.000333 | ms/batch 20.52 | loss  5.98 | ppl   395.07\n",
            "| epoch   8 |  1800/ 2983 batches | lr 0.000332 | ms/batch 20.54 | loss  5.89 | ppl   361.80\n",
            "| epoch   8 |  2000/ 2983 batches | lr 0.000330 | ms/batch 20.53 | loss  5.92 | ppl   372.94\n",
            "| epoch   8 |  2200/ 2983 batches | lr 0.000329 | ms/batch 20.55 | loss  5.86 | ppl   350.75\n",
            "| epoch   8 |  2400/ 2983 batches | lr 0.000328 | ms/batch 20.53 | loss  5.88 | ppl   357.19\n",
            "| epoch   8 |  2600/ 2983 batches | lr 0.000326 | ms/batch 20.54 | loss  5.92 | ppl   372.78\n",
            "| epoch   8 |  2800/ 2983 batches | lr 0.000325 | ms/batch 20.54 | loss  5.86 | ppl   350.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 63.76s | valid loss  6.46 | valid ppl   640.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000324\n",
            "| epoch   9 |   200/ 2983 batches | lr 0.000322 | ms/batch 20.68 | loss  5.93 | ppl   375.01\n",
            "| epoch   9 |   400/ 2983 batches | lr 0.000321 | ms/batch 20.58 | loss  5.92 | ppl   373.93\n",
            "| epoch   9 |   600/ 2983 batches | lr 0.000320 | ms/batch 20.56 | loss  5.83 | ppl   342.04\n",
            "| epoch   9 |   800/ 2983 batches | lr 0.000318 | ms/batch 20.55 | loss  5.87 | ppl   353.97\n",
            "| epoch   9 |  1000/ 2983 batches | lr 0.000317 | ms/batch 20.60 | loss  5.89 | ppl   360.28\n",
            "| epoch   9 |  1200/ 2983 batches | lr 0.000316 | ms/batch 20.58 | loss  5.88 | ppl   357.36\n",
            "| epoch   9 |  1400/ 2983 batches | lr 0.000315 | ms/batch 20.55 | loss  5.90 | ppl   363.93\n",
            "| epoch   9 |  1600/ 2983 batches | lr 0.000313 | ms/batch 20.55 | loss  5.95 | ppl   384.43\n",
            "| epoch   9 |  1800/ 2983 batches | lr 0.000312 | ms/batch 20.54 | loss  5.86 | ppl   352.18\n",
            "| epoch   9 |  2000/ 2983 batches | lr 0.000311 | ms/batch 20.53 | loss  5.90 | ppl   363.41\n",
            "| epoch   9 |  2200/ 2983 batches | lr 0.000310 | ms/batch 20.57 | loss  5.84 | ppl   342.13\n",
            "| epoch   9 |  2400/ 2983 batches | lr 0.000308 | ms/batch 20.50 | loss  5.85 | ppl   346.75\n",
            "| epoch   9 |  2600/ 2983 batches | lr 0.000307 | ms/batch 20.54 | loss  5.90 | ppl   363.37\n",
            "| epoch   9 |  2800/ 2983 batches | lr 0.000306 | ms/batch 20.56 | loss  5.84 | ppl   342.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 63.80s | valid loss  6.44 | valid ppl   627.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000305\n",
            "| epoch  10 |   200/ 2983 batches | lr 0.000304 | ms/batch 20.63 | loss  5.90 | ppl   364.25\n",
            "| epoch  10 |   400/ 2983 batches | lr 0.000303 | ms/batch 20.52 | loss  5.90 | ppl   364.76\n",
            "| epoch  10 |   600/ 2983 batches | lr 0.000302 | ms/batch 20.52 | loss  5.81 | ppl   333.08\n",
            "| epoch  10 |   800/ 2983 batches | lr 0.000301 | ms/batch 20.53 | loss  5.85 | ppl   346.75\n",
            "| epoch  10 |  1000/ 2983 batches | lr 0.000300 | ms/batch 20.52 | loss  5.86 | ppl   350.18\n",
            "| epoch  10 |  1200/ 2983 batches | lr 0.000299 | ms/batch 20.52 | loss  5.85 | ppl   347.51\n",
            "| epoch  10 |  1400/ 2983 batches | lr 0.000297 | ms/batch 20.53 | loss  5.88 | ppl   356.50\n",
            "| epoch  10 |  1600/ 2983 batches | lr 0.000296 | ms/batch 20.53 | loss  5.93 | ppl   375.71\n",
            "| epoch  10 |  1800/ 2983 batches | lr 0.000295 | ms/batch 20.52 | loss  5.84 | ppl   344.53\n",
            "| epoch  10 |  2000/ 2983 batches | lr 0.000294 | ms/batch 20.57 | loss  5.87 | ppl   355.98\n",
            "| epoch  10 |  2200/ 2983 batches | lr 0.000293 | ms/batch 20.51 | loss  5.81 | ppl   334.78\n",
            "| epoch  10 |  2400/ 2983 batches | lr 0.000292 | ms/batch 20.54 | loss  5.83 | ppl   341.71\n",
            "| epoch  10 |  2600/ 2983 batches | lr 0.000291 | ms/batch 20.57 | loss  5.88 | ppl   356.44\n",
            "| epoch  10 |  2800/ 2983 batches | lr 0.000290 | ms/batch 20.52 | loss  5.81 | ppl   334.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 63.74s | valid loss  6.43 | valid ppl   621.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000289\n",
            "| epoch  11 |   200/ 2983 batches | lr 0.000288 | ms/batch 20.64 | loss  5.88 | ppl   357.54\n",
            "| epoch  11 |   400/ 2983 batches | lr 0.000288 | ms/batch 20.53 | loss  5.88 | ppl   357.36\n",
            "| epoch  11 |   600/ 2983 batches | lr 0.000287 | ms/batch 20.54 | loss  5.79 | ppl   327.66\n",
            "| epoch  11 |   800/ 2983 batches | lr 0.000286 | ms/batch 20.53 | loss  5.83 | ppl   340.08\n",
            "| epoch  11 |  1000/ 2983 batches | lr 0.000285 | ms/batch 20.52 | loss  5.84 | ppl   345.03\n",
            "| epoch  11 |  1200/ 2983 batches | lr 0.000284 | ms/batch 20.54 | loss  5.84 | ppl   342.65\n",
            "| epoch  11 |  1400/ 2983 batches | lr 0.000283 | ms/batch 20.52 | loss  5.86 | ppl   350.69\n",
            "| epoch  11 |  1600/ 2983 batches | lr 0.000282 | ms/batch 20.52 | loss  5.91 | ppl   369.49\n",
            "| epoch  11 |  1800/ 2983 batches | lr 0.000281 | ms/batch 20.56 | loss  5.83 | ppl   339.12\n",
            "| epoch  11 |  2000/ 2983 batches | lr 0.000280 | ms/batch 20.53 | loss  5.86 | ppl   349.62\n",
            "| epoch  11 |  2200/ 2983 batches | lr 0.000279 | ms/batch 20.52 | loss  5.80 | ppl   329.30\n",
            "| epoch  11 |  2400/ 2983 batches | lr 0.000278 | ms/batch 20.58 | loss  5.82 | ppl   335.81\n",
            "| epoch  11 |  2600/ 2983 batches | lr 0.000278 | ms/batch 20.53 | loss  5.86 | ppl   351.15\n",
            "| epoch  11 |  2800/ 2983 batches | lr 0.000277 | ms/batch 20.51 | loss  5.79 | ppl   328.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 63.75s | valid loss  6.42 | valid ppl   612.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000276\n",
            "| epoch  12 |   200/ 2983 batches | lr 0.000275 | ms/batch 20.63 | loss  5.86 | ppl   351.49\n",
            "| epoch  12 |   400/ 2983 batches | lr 0.000274 | ms/batch 20.52 | loss  5.86 | ppl   350.27\n",
            "| epoch  12 |   600/ 2983 batches | lr 0.000273 | ms/batch 20.52 | loss  5.77 | ppl   321.73\n",
            "| epoch  12 |   800/ 2983 batches | lr 0.000273 | ms/batch 20.50 | loss  5.82 | ppl   335.57\n",
            "| epoch  12 |  1000/ 2983 batches | lr 0.000272 | ms/batch 20.52 | loss  5.83 | ppl   339.88\n",
            "| epoch  12 |  1200/ 2983 batches | lr 0.000271 | ms/batch 20.52 | loss  5.82 | ppl   337.22\n",
            "| epoch  12 |  1400/ 2983 batches | lr 0.000270 | ms/batch 20.51 | loss  5.85 | ppl   345.66\n",
            "| epoch  12 |  1600/ 2983 batches | lr 0.000269 | ms/batch 20.52 | loss  5.89 | ppl   363.12\n",
            "| epoch  12 |  1800/ 2983 batches | lr 0.000269 | ms/batch 20.53 | loss  5.81 | ppl   334.43\n",
            "| epoch  12 |  2000/ 2983 batches | lr 0.000268 | ms/batch 20.51 | loss  5.84 | ppl   344.83\n",
            "| epoch  12 |  2200/ 2983 batches | lr 0.000267 | ms/batch 20.56 | loss  5.78 | ppl   325.26\n",
            "| epoch  12 |  2400/ 2983 batches | lr 0.000266 | ms/batch 20.53 | loss  5.80 | ppl   331.38\n",
            "| epoch  12 |  2600/ 2983 batches | lr 0.000266 | ms/batch 20.50 | loss  5.84 | ppl   345.40\n",
            "| epoch  12 |  2800/ 2983 batches | lr 0.000265 | ms/batch 20.54 | loss  5.78 | ppl   323.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 63.72s | valid loss  6.41 | valid ppl   608.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000264\n",
            "| epoch  13 |   200/ 2983 batches | lr 0.000263 | ms/batch 20.66 | loss  5.85 | ppl   347.08\n",
            "| epoch  13 |   400/ 2983 batches | lr 0.000263 | ms/batch 20.56 | loss  5.85 | ppl   346.74\n",
            "| epoch  13 |   600/ 2983 batches | lr 0.000262 | ms/batch 20.54 | loss  5.76 | ppl   317.92\n",
            "| epoch  13 |   800/ 2983 batches | lr 0.000261 | ms/batch 20.53 | loss  5.80 | ppl   330.13\n",
            "| epoch  13 |  1000/ 2983 batches | lr 0.000261 | ms/batch 20.53 | loss  5.82 | ppl   335.51\n",
            "| epoch  13 |  1200/ 2983 batches | lr 0.000260 | ms/batch 20.51 | loss  5.81 | ppl   333.32\n",
            "| epoch  13 |  1400/ 2983 batches | lr 0.000259 | ms/batch 20.55 | loss  5.83 | ppl   341.68\n",
            "| epoch  13 |  1600/ 2983 batches | lr 0.000259 | ms/batch 20.51 | loss  5.88 | ppl   358.38\n",
            "| epoch  13 |  1800/ 2983 batches | lr 0.000258 | ms/batch 20.53 | loss  5.80 | ppl   330.43\n",
            "| epoch  13 |  2000/ 2983 batches | lr 0.000257 | ms/batch 20.55 | loss  5.84 | ppl   342.09\n",
            "| epoch  13 |  2200/ 2983 batches | lr 0.000256 | ms/batch 20.55 | loss  5.77 | ppl   320.99\n",
            "| epoch  13 |  2400/ 2983 batches | lr 0.000256 | ms/batch 20.53 | loss  5.79 | ppl   326.52\n",
            "| epoch  13 |  2600/ 2983 batches | lr 0.000255 | ms/batch 20.54 | loss  5.83 | ppl   340.88\n",
            "| epoch  13 |  2800/ 2983 batches | lr 0.000254 | ms/batch 20.52 | loss  5.77 | ppl   320.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 63.76s | valid loss  6.40 | valid ppl   603.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000254\n",
            "| epoch  14 |   200/ 2983 batches | lr 0.000253 | ms/batch 20.64 | loss  5.84 | ppl   342.50\n",
            "| epoch  14 |   400/ 2983 batches | lr 0.000253 | ms/batch 20.52 | loss  5.84 | ppl   343.26\n",
            "| epoch  14 |   600/ 2983 batches | lr 0.000252 | ms/batch 20.51 | loss  5.75 | ppl   313.59\n",
            "| epoch  14 |   800/ 2983 batches | lr 0.000251 | ms/batch 20.55 | loss  5.79 | ppl   326.70\n",
            "| epoch  14 |  1000/ 2983 batches | lr 0.000251 | ms/batch 20.54 | loss  5.81 | ppl   332.54\n",
            "| epoch  14 |  1200/ 2983 batches | lr 0.000250 | ms/batch 20.54 | loss  5.80 | ppl   329.58\n",
            "| epoch  14 |  1400/ 2983 batches | lr 0.000249 | ms/batch 20.54 | loss  5.82 | ppl   337.37\n",
            "| epoch  14 |  1600/ 2983 batches | lr 0.000249 | ms/batch 20.52 | loss  5.87 | ppl   354.30\n",
            "| epoch  14 |  1800/ 2983 batches | lr 0.000248 | ms/batch 20.53 | loss  5.79 | ppl   326.22\n",
            "| epoch  14 |  2000/ 2983 batches | lr 0.000248 | ms/batch 20.55 | loss  5.82 | ppl   337.88\n",
            "| epoch  14 |  2200/ 2983 batches | lr 0.000247 | ms/batch 20.51 | loss  5.76 | ppl   317.90\n",
            "| epoch  14 |  2400/ 2983 batches | lr 0.000246 | ms/batch 20.56 | loss  5.78 | ppl   322.75\n",
            "| epoch  14 |  2600/ 2983 batches | lr 0.000246 | ms/batch 20.57 | loss  5.82 | ppl   338.17\n",
            "| epoch  14 |  2800/ 2983 batches | lr 0.000245 | ms/batch 20.53 | loss  5.76 | ppl   317.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 63.77s | valid loss  6.40 | valid ppl   600.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000245\n",
            "| epoch  15 |   200/ 2983 batches | lr 0.000244 | ms/batch 20.63 | loss  5.83 | ppl   338.70\n",
            "| epoch  15 |   400/ 2983 batches | lr 0.000243 | ms/batch 20.55 | loss  5.82 | ppl   338.44\n",
            "| epoch  15 |   600/ 2983 batches | lr 0.000243 | ms/batch 20.58 | loss  5.74 | ppl   310.21\n",
            "| epoch  15 |   800/ 2983 batches | lr 0.000242 | ms/batch 20.54 | loss  5.78 | ppl   323.23\n",
            "| epoch  15 |  1000/ 2983 batches | lr 0.000242 | ms/batch 20.54 | loss  5.79 | ppl   327.36\n",
            "| epoch  15 |  1200/ 2983 batches | lr 0.000241 | ms/batch 20.54 | loss  5.79 | ppl   325.66\n",
            "| epoch  15 |  1400/ 2983 batches | lr 0.000241 | ms/batch 20.52 | loss  5.81 | ppl   333.35\n",
            "| epoch  15 |  1600/ 2983 batches | lr 0.000240 | ms/batch 20.57 | loss  5.86 | ppl   351.22\n",
            "| epoch  15 |  1800/ 2983 batches | lr 0.000240 | ms/batch 20.57 | loss  5.78 | ppl   323.18\n",
            "| epoch  15 |  2000/ 2983 batches | lr 0.000239 | ms/batch 20.52 | loss  5.81 | ppl   334.07\n",
            "| epoch  15 |  2200/ 2983 batches | lr 0.000238 | ms/batch 20.52 | loss  5.75 | ppl   314.30\n",
            "| epoch  15 |  2400/ 2983 batches | lr 0.000238 | ms/batch 20.54 | loss  5.77 | ppl   320.02\n",
            "| epoch  15 |  2600/ 2983 batches | lr 0.000237 | ms/batch 20.52 | loss  5.81 | ppl   334.97\n",
            "| epoch  15 |  2800/ 2983 batches | lr 0.000237 | ms/batch 20.51 | loss  5.75 | ppl   314.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 63.76s | valid loss  6.39 | valid ppl   598.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000236\n",
            "| epoch  16 |   200/ 2983 batches | lr 0.000236 | ms/batch 20.63 | loss  5.81 | ppl   335.17\n",
            "| epoch  16 |   400/ 2983 batches | lr 0.000235 | ms/batch 20.54 | loss  5.81 | ppl   334.60\n",
            "| epoch  16 |   600/ 2983 batches | lr 0.000235 | ms/batch 20.52 | loss  5.73 | ppl   308.12\n",
            "| epoch  16 |   800/ 2983 batches | lr 0.000234 | ms/batch 20.52 | loss  5.77 | ppl   320.67\n",
            "| epoch  16 |  1000/ 2983 batches | lr 0.000234 | ms/batch 20.52 | loss  5.78 | ppl   325.23\n",
            "| epoch  16 |  1200/ 2983 batches | lr 0.000233 | ms/batch 20.52 | loss  5.78 | ppl   322.18\n",
            "| epoch  16 |  1400/ 2983 batches | lr 0.000233 | ms/batch 20.53 | loss  5.80 | ppl   330.39\n",
            "| epoch  16 |  1600/ 2983 batches | lr 0.000232 | ms/batch 20.56 | loss  5.85 | ppl   347.10\n",
            "| epoch  16 |  1800/ 2983 batches | lr 0.000232 | ms/batch 20.52 | loss  5.77 | ppl   320.10\n",
            "| epoch  16 |  2000/ 2983 batches | lr 0.000231 | ms/batch 20.50 | loss  5.80 | ppl   331.94\n",
            "| epoch  16 |  2200/ 2983 batches | lr 0.000231 | ms/batch 20.52 | loss  5.74 | ppl   311.02\n",
            "| epoch  16 |  2400/ 2983 batches | lr 0.000230 | ms/batch 20.52 | loss  5.76 | ppl   317.08\n",
            "| epoch  16 |  2600/ 2983 batches | lr 0.000230 | ms/batch 20.52 | loss  5.81 | ppl   333.05\n",
            "| epoch  16 |  2800/ 2983 batches | lr 0.000229 | ms/batch 20.53 | loss  5.74 | ppl   311.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 63.72s | valid loss  6.39 | valid ppl   595.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000229\n",
            "| epoch  17 |   200/ 2983 batches | lr 0.000228 | ms/batch 20.65 | loss  5.81 | ppl   333.14\n",
            "| epoch  17 |   400/ 2983 batches | lr 0.000228 | ms/batch 20.50 | loss  5.81 | ppl   333.41\n",
            "| epoch  17 |   600/ 2983 batches | lr 0.000227 | ms/batch 20.51 | loss  5.72 | ppl   304.57\n",
            "| epoch  17 |   800/ 2983 batches | lr 0.000227 | ms/batch 20.52 | loss  5.76 | ppl   318.50\n",
            "| epoch  17 |  1000/ 2983 batches | lr 0.000226 | ms/batch 20.53 | loss  5.77 | ppl   321.58\n",
            "| epoch  17 |  1200/ 2983 batches | lr 0.000226 | ms/batch 20.53 | loss  5.77 | ppl   319.47\n",
            "| epoch  17 |  1400/ 2983 batches | lr 0.000226 | ms/batch 20.54 | loss  5.79 | ppl   327.83\n",
            "| epoch  17 |  1600/ 2983 batches | lr 0.000225 | ms/batch 20.52 | loss  5.85 | ppl   345.66\n",
            "| epoch  17 |  1800/ 2983 batches | lr 0.000225 | ms/batch 20.52 | loss  5.76 | ppl   317.44\n",
            "| epoch  17 |  2000/ 2983 batches | lr 0.000224 | ms/batch 20.54 | loss  5.80 | ppl   328.79\n",
            "| epoch  17 |  2200/ 2983 batches | lr 0.000224 | ms/batch 20.52 | loss  5.73 | ppl   308.71\n",
            "| epoch  17 |  2400/ 2983 batches | lr 0.000223 | ms/batch 20.52 | loss  5.75 | ppl   314.70\n",
            "| epoch  17 |  2600/ 2983 batches | lr 0.000223 | ms/batch 20.54 | loss  5.80 | ppl   328.90\n",
            "| epoch  17 |  2800/ 2983 batches | lr 0.000222 | ms/batch 20.51 | loss  5.73 | ppl   309.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 63.73s | valid loss  6.39 | valid ppl   593.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000222\n",
            "| epoch  18 |   200/ 2983 batches | lr 0.000222 | ms/batch 20.61 | loss  5.80 | ppl   330.99\n",
            "| epoch  18 |   400/ 2983 batches | lr 0.000221 | ms/batch 20.52 | loss  5.80 | ppl   330.31\n",
            "| epoch  18 |   600/ 2983 batches | lr 0.000221 | ms/batch 20.54 | loss  5.71 | ppl   302.03\n",
            "| epoch  18 |   800/ 2983 batches | lr 0.000220 | ms/batch 20.51 | loss  5.75 | ppl   315.63\n",
            "| epoch  18 |  1000/ 2983 batches | lr 0.000220 | ms/batch 20.52 | loss  5.77 | ppl   319.40\n",
            "| epoch  18 |  1200/ 2983 batches | lr 0.000219 | ms/batch 20.54 | loss  5.76 | ppl   317.08\n",
            "| epoch  18 |  1400/ 2983 batches | lr 0.000219 | ms/batch 20.54 | loss  5.78 | ppl   324.88\n",
            "| epoch  18 |  1600/ 2983 batches | lr 0.000219 | ms/batch 20.53 | loss  5.84 | ppl   342.50\n",
            "| epoch  18 |  1800/ 2983 batches | lr 0.000218 | ms/batch 20.54 | loss  5.75 | ppl   315.31\n",
            "| epoch  18 |  2000/ 2983 batches | lr 0.000218 | ms/batch 20.54 | loss  5.79 | ppl   327.77\n",
            "| epoch  18 |  2200/ 2983 batches | lr 0.000217 | ms/batch 20.53 | loss  5.73 | ppl   306.85\n",
            "| epoch  18 |  2400/ 2983 batches | lr 0.000217 | ms/batch 20.52 | loss  5.74 | ppl   312.43\n",
            "| epoch  18 |  2600/ 2983 batches | lr 0.000217 | ms/batch 20.51 | loss  5.79 | ppl   327.59\n",
            "| epoch  18 |  2800/ 2983 batches | lr 0.000216 | ms/batch 20.52 | loss  5.73 | ppl   307.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 63.73s | valid loss  6.39 | valid ppl   595.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000216\n",
            "| epoch  19 |   200/ 2983 batches | lr 0.000215 | ms/batch 20.61 | loss  5.79 | ppl   327.57\n",
            "| epoch  19 |   400/ 2983 batches | lr 0.000215 | ms/batch 20.53 | loss  5.79 | ppl   327.97\n",
            "| epoch  19 |   600/ 2983 batches | lr 0.000215 | ms/batch 20.52 | loss  5.70 | ppl   300.33\n",
            "| epoch  19 |   800/ 2983 batches | lr 0.000214 | ms/batch 20.52 | loss  5.75 | ppl   313.36\n",
            "| epoch  19 |  1000/ 2983 batches | lr 0.000214 | ms/batch 20.56 | loss  5.76 | ppl   317.54\n",
            "| epoch  19 |  1200/ 2983 batches | lr 0.000213 | ms/batch 20.53 | loss  5.75 | ppl   315.07\n",
            "| epoch  19 |  1400/ 2983 batches | lr 0.000213 | ms/batch 20.53 | loss  5.78 | ppl   323.72\n",
            "| epoch  19 |  1600/ 2983 batches | lr 0.000213 | ms/batch 20.54 | loss  5.83 | ppl   339.36\n",
            "| epoch  19 |  1800/ 2983 batches | lr 0.000212 | ms/batch 20.51 | loss  5.74 | ppl   312.45\n",
            "| epoch  19 |  2000/ 2983 batches | lr 0.000212 | ms/batch 20.51 | loss  5.78 | ppl   324.30\n",
            "| epoch  19 |  2200/ 2983 batches | lr 0.000211 | ms/batch 20.54 | loss  5.72 | ppl   304.14\n",
            "| epoch  19 |  2400/ 2983 batches | lr 0.000211 | ms/batch 20.52 | loss  5.74 | ppl   310.31\n",
            "| epoch  19 |  2600/ 2983 batches | lr 0.000211 | ms/batch 20.52 | loss  5.78 | ppl   325.10\n",
            "| epoch  19 |  2800/ 2983 batches | lr 0.000210 | ms/batch 20.53 | loss  5.72 | ppl   305.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 63.72s | valid loss  6.38 | valid ppl   589.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000210\n",
            "| epoch  20 |   200/ 2983 batches | lr 0.000210 | ms/batch 20.62 | loss  5.78 | ppl   324.87\n",
            "| epoch  20 |   400/ 2983 batches | lr 0.000209 | ms/batch 20.51 | loss  5.78 | ppl   325.38\n",
            "| epoch  20 |   600/ 2983 batches | lr 0.000209 | ms/batch 20.51 | loss  5.70 | ppl   297.57\n",
            "| epoch  20 |   800/ 2983 batches | lr 0.000209 | ms/batch 20.52 | loss  5.74 | ppl   311.82\n",
            "| epoch  20 |  1000/ 2983 batches | lr 0.000208 | ms/batch 20.55 | loss  5.75 | ppl   315.72\n",
            "| epoch  20 |  1200/ 2983 batches | lr 0.000208 | ms/batch 20.52 | loss  5.75 | ppl   312.67\n",
            "| epoch  20 |  1400/ 2983 batches | lr 0.000207 | ms/batch 20.52 | loss  5.77 | ppl   321.55\n",
            "| epoch  20 |  1600/ 2983 batches | lr 0.000207 | ms/batch 20.54 | loss  5.82 | ppl   337.32\n",
            "| epoch  20 |  1800/ 2983 batches | lr 0.000207 | ms/batch 20.52 | loss  5.74 | ppl   311.17\n",
            "| epoch  20 |  2000/ 2983 batches | lr 0.000206 | ms/batch 20.54 | loss  5.77 | ppl   321.70\n",
            "| epoch  20 |  2200/ 2983 batches | lr 0.000206 | ms/batch 20.53 | loss  5.71 | ppl   302.39\n",
            "| epoch  20 |  2400/ 2983 batches | lr 0.000206 | ms/batch 20.50 | loss  5.73 | ppl   308.62\n",
            "| epoch  20 |  2600/ 2983 batches | lr 0.000205 | ms/batch 20.54 | loss  5.78 | ppl   323.55\n",
            "| epoch  20 |  2800/ 2983 batches | lr 0.000205 | ms/batch 20.53 | loss  5.71 | ppl   302.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 63.73s | valid loss  6.37 | valid ppl   585.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000205\n",
            "| epoch  21 |   200/ 2983 batches | lr 0.000204 | ms/batch 20.65 | loss  5.78 | ppl   323.18\n",
            "| epoch  21 |   400/ 2983 batches | lr 0.000204 | ms/batch 20.52 | loss  5.78 | ppl   323.68\n",
            "| epoch  21 |   600/ 2983 batches | lr 0.000204 | ms/batch 20.53 | loss  5.69 | ppl   295.61\n",
            "| epoch  21 |   800/ 2983 batches | lr 0.000203 | ms/batch 20.54 | loss  5.74 | ppl   309.91\n",
            "| epoch  21 |  1000/ 2983 batches | lr 0.000203 | ms/batch 20.52 | loss  5.75 | ppl   314.31\n",
            "| epoch  21 |  1200/ 2983 batches | lr 0.000203 | ms/batch 20.54 | loss  5.74 | ppl   311.58\n",
            "| epoch  21 |  1400/ 2983 batches | lr 0.000202 | ms/batch 20.57 | loss  5.77 | ppl   319.42\n",
            "| epoch  21 |  1600/ 2983 batches | lr 0.000202 | ms/batch 20.53 | loss  5.82 | ppl   335.32\n",
            "| epoch  21 |  1800/ 2983 batches | lr 0.000202 | ms/batch 20.54 | loss  5.73 | ppl   308.64\n",
            "| epoch  21 |  2000/ 2983 batches | lr 0.000201 | ms/batch 20.55 | loss  5.77 | ppl   319.87\n",
            "| epoch  21 |  2200/ 2983 batches | lr 0.000201 | ms/batch 20.53 | loss  5.71 | ppl   301.20\n",
            "| epoch  21 |  2400/ 2983 batches | lr 0.000201 | ms/batch 20.54 | loss  5.73 | ppl   307.36\n",
            "| epoch  21 |  2600/ 2983 batches | lr 0.000200 | ms/batch 20.54 | loss  5.77 | ppl   321.80\n",
            "| epoch  21 |  2800/ 2983 batches | lr 0.000200 | ms/batch 20.53 | loss  5.71 | ppl   301.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 63.76s | valid loss  6.37 | valid ppl   585.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000200\n",
            "| epoch  22 |   200/ 2983 batches | lr 0.000199 | ms/batch 20.62 | loss  5.77 | ppl   320.87\n",
            "| epoch  22 |   400/ 2983 batches | lr 0.000199 | ms/batch 20.53 | loss  5.77 | ppl   321.76\n",
            "| epoch  22 |   600/ 2983 batches | lr 0.000199 | ms/batch 20.54 | loss  5.68 | ppl   293.68\n",
            "| epoch  22 |   800/ 2983 batches | lr 0.000198 | ms/batch 20.51 | loss  5.73 | ppl   308.32\n",
            "| epoch  22 |  1000/ 2983 batches | lr 0.000198 | ms/batch 20.53 | loss  5.74 | ppl   312.02\n",
            "| epoch  22 |  1200/ 2983 batches | lr 0.000198 | ms/batch 20.54 | loss  5.74 | ppl   309.59\n",
            "| epoch  22 |  1400/ 2983 batches | lr 0.000198 | ms/batch 20.51 | loss  5.76 | ppl   317.53\n",
            "| epoch  22 |  1600/ 2983 batches | lr 0.000197 | ms/batch 20.51 | loss  5.81 | ppl   333.03\n",
            "| epoch  22 |  1800/ 2983 batches | lr 0.000197 | ms/batch 20.52 | loss  5.73 | ppl   307.58\n",
            "| epoch  22 |  2000/ 2983 batches | lr 0.000197 | ms/batch 20.53 | loss  5.76 | ppl   318.75\n",
            "| epoch  22 |  2200/ 2983 batches | lr 0.000196 | ms/batch 20.53 | loss  5.70 | ppl   299.82\n",
            "| epoch  22 |  2400/ 2983 batches | lr 0.000196 | ms/batch 20.54 | loss  5.72 | ppl   304.75\n",
            "| epoch  22 |  2600/ 2983 batches | lr 0.000196 | ms/batch 20.53 | loss  5.77 | ppl   319.23\n",
            "| epoch  22 |  2800/ 2983 batches | lr 0.000195 | ms/batch 20.54 | loss  5.70 | ppl   298.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 63.73s | valid loss  6.37 | valid ppl   581.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000195\n",
            "| epoch  23 |   200/ 2983 batches | lr 0.000195 | ms/batch 20.63 | loss  5.76 | ppl   318.93\n",
            "| epoch  23 |   400/ 2983 batches | lr 0.000195 | ms/batch 20.53 | loss  5.77 | ppl   319.46\n",
            "| epoch  23 |   600/ 2983 batches | lr 0.000194 | ms/batch 20.53 | loss  5.68 | ppl   293.45\n",
            "| epoch  23 |   800/ 2983 batches | lr 0.000194 | ms/batch 20.52 | loss  5.73 | ppl   307.23\n",
            "| epoch  23 |  1000/ 2983 batches | lr 0.000194 | ms/batch 20.55 | loss  5.74 | ppl   310.79\n",
            "| epoch  23 |  1200/ 2983 batches | lr 0.000193 | ms/batch 20.52 | loss  5.73 | ppl   308.56\n",
            "| epoch  23 |  1400/ 2983 batches | lr 0.000193 | ms/batch 20.52 | loss  5.76 | ppl   316.30\n",
            "| epoch  23 |  1600/ 2983 batches | lr 0.000193 | ms/batch 20.53 | loss  5.80 | ppl   331.31\n",
            "| epoch  23 |  1800/ 2983 batches | lr 0.000193 | ms/batch 20.49 | loss  5.72 | ppl   305.60\n",
            "| epoch  23 |  2000/ 2983 batches | lr 0.000192 | ms/batch 20.50 | loss  5.76 | ppl   316.85\n",
            "| epoch  23 |  2200/ 2983 batches | lr 0.000192 | ms/batch 20.50 | loss  5.69 | ppl   297.33\n",
            "| epoch  23 |  2400/ 2983 batches | lr 0.000192 | ms/batch 20.51 | loss  5.71 | ppl   303.10\n",
            "| epoch  23 |  2600/ 2983 batches | lr 0.000191 | ms/batch 20.51 | loss  5.76 | ppl   318.84\n",
            "| epoch  23 |  2800/ 2983 batches | lr 0.000191 | ms/batch 20.54 | loss  5.70 | ppl   298.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 63.73s | valid loss  6.37 | valid ppl   583.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000191\n",
            "| epoch  24 |   200/ 2983 batches | lr 0.000191 | ms/batch 20.63 | loss  5.76 | ppl   317.55\n",
            "| epoch  24 |   400/ 2983 batches | lr 0.000190 | ms/batch 20.52 | loss  5.76 | ppl   318.90\n",
            "| epoch  24 |   600/ 2983 batches | lr 0.000190 | ms/batch 20.50 | loss  5.68 | ppl   291.93\n",
            "| epoch  24 |   800/ 2983 batches | lr 0.000190 | ms/batch 20.55 | loss  5.72 | ppl   305.19\n",
            "| epoch  24 |  1000/ 2983 batches | lr 0.000189 | ms/batch 20.53 | loss  5.73 | ppl   308.66\n",
            "| epoch  24 |  1200/ 2983 batches | lr 0.000189 | ms/batch 20.53 | loss  5.72 | ppl   305.88\n",
            "| epoch  24 |  1400/ 2983 batches | lr 0.000189 | ms/batch 20.53 | loss  5.75 | ppl   315.09\n",
            "| epoch  24 |  1600/ 2983 batches | lr 0.000189 | ms/batch 20.53 | loss  5.80 | ppl   330.91\n",
            "| epoch  24 |  1800/ 2983 batches | lr 0.000188 | ms/batch 20.54 | loss  5.72 | ppl   304.74\n",
            "| epoch  24 |  2000/ 2983 batches | lr 0.000188 | ms/batch 20.55 | loss  5.75 | ppl   315.32\n",
            "| epoch  24 |  2200/ 2983 batches | lr 0.000188 | ms/batch 20.51 | loss  5.69 | ppl   296.71\n",
            "| epoch  24 |  2400/ 2983 batches | lr 0.000188 | ms/batch 20.52 | loss  5.71 | ppl   302.35\n",
            "| epoch  24 |  2600/ 2983 batches | lr 0.000187 | ms/batch 20.53 | loss  5.76 | ppl   317.40\n",
            "| epoch  24 |  2800/ 2983 batches | lr 0.000187 | ms/batch 20.52 | loss  5.69 | ppl   296.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 63.75s | valid loss  6.37 | valid ppl   581.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000187\n",
            "| epoch  25 |   200/ 2983 batches | lr 0.000187 | ms/batch 20.61 | loss  5.75 | ppl   315.14\n",
            "| epoch  25 |   400/ 2983 batches | lr 0.000186 | ms/batch 20.52 | loss  5.76 | ppl   316.36\n",
            "| epoch  25 |   600/ 2983 batches | lr 0.000186 | ms/batch 20.54 | loss  5.67 | ppl   290.23\n",
            "| epoch  25 |   800/ 2983 batches | lr 0.000186 | ms/batch 20.51 | loss  5.72 | ppl   304.07\n",
            "| epoch  25 |  1000/ 2983 batches | lr 0.000186 | ms/batch 20.52 | loss  5.73 | ppl   307.52\n",
            "| epoch  25 |  1200/ 2983 batches | lr 0.000185 | ms/batch 20.52 | loss  5.72 | ppl   304.31\n",
            "| epoch  25 |  1400/ 2983 batches | lr 0.000185 | ms/batch 20.51 | loss  5.75 | ppl   313.10\n",
            "| epoch  25 |  1600/ 2983 batches | lr 0.000185 | ms/batch 20.49 | loss  5.80 | ppl   328.91\n",
            "| epoch  25 |  1800/ 2983 batches | lr 0.000185 | ms/batch 20.52 | loss  5.71 | ppl   302.96\n",
            "| epoch  25 |  2000/ 2983 batches | lr 0.000184 | ms/batch 20.52 | loss  5.75 | ppl   313.90\n",
            "| epoch  25 |  2200/ 2983 batches | lr 0.000184 | ms/batch 20.51 | loss  5.69 | ppl   294.89\n",
            "| epoch  25 |  2400/ 2983 batches | lr 0.000184 | ms/batch 20.53 | loss  5.70 | ppl   300.20\n",
            "| epoch  25 |  2600/ 2983 batches | lr 0.000184 | ms/batch 20.51 | loss  5.75 | ppl   315.39\n",
            "| epoch  25 |  2800/ 2983 batches | lr 0.000183 | ms/batch 20.50 | loss  5.69 | ppl   296.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 63.68s | valid loss  6.36 | valid ppl   579.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000183\n",
            "| epoch  26 |   200/ 2983 batches | lr 0.000183 | ms/batch 20.61 | loss  5.75 | ppl   314.65\n",
            "| epoch  26 |   400/ 2983 batches | lr 0.000183 | ms/batch 20.51 | loss  5.75 | ppl   315.27\n",
            "| epoch  26 |   600/ 2983 batches | lr 0.000182 | ms/batch 20.52 | loss  5.67 | ppl   289.78\n",
            "| epoch  26 |   800/ 2983 batches | lr 0.000182 | ms/batch 20.52 | loss  5.71 | ppl   303.26\n",
            "| epoch  26 |  1000/ 2983 batches | lr 0.000182 | ms/batch 20.52 | loss  5.73 | ppl   307.13\n",
            "| epoch  26 |  1200/ 2983 batches | lr 0.000182 | ms/batch 20.50 | loss  5.72 | ppl   303.69\n",
            "| epoch  26 |  1400/ 2983 batches | lr 0.000181 | ms/batch 20.51 | loss  5.74 | ppl   311.53\n",
            "| epoch  26 |  1600/ 2983 batches | lr 0.000181 | ms/batch 20.52 | loss  5.79 | ppl   327.98\n",
            "| epoch  26 |  1800/ 2983 batches | lr 0.000181 | ms/batch 20.52 | loss  5.71 | ppl   301.48\n",
            "| epoch  26 |  2000/ 2983 batches | lr 0.000181 | ms/batch 20.52 | loss  5.75 | ppl   312.88\n",
            "| epoch  26 |  2200/ 2983 batches | lr 0.000180 | ms/batch 20.53 | loss  5.68 | ppl   293.11\n",
            "| epoch  26 |  2400/ 2983 batches | lr 0.000180 | ms/batch 20.52 | loss  5.70 | ppl   298.89\n",
            "| epoch  26 |  2600/ 2983 batches | lr 0.000180 | ms/batch 20.54 | loss  5.75 | ppl   314.77\n",
            "| epoch  26 |  2800/ 2983 batches | lr 0.000180 | ms/batch 20.54 | loss  5.69 | ppl   294.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 63.71s | valid loss  6.36 | valid ppl   579.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000180\n",
            "| epoch  27 |   200/ 2983 batches | lr 0.000179 | ms/batch 20.61 | loss  5.75 | ppl   312.84\n",
            "| epoch  27 |   400/ 2983 batches | lr 0.000179 | ms/batch 20.55 | loss  5.75 | ppl   314.09\n",
            "| epoch  27 |   600/ 2983 batches | lr 0.000179 | ms/batch 20.52 | loss  5.66 | ppl   288.34\n",
            "| epoch  27 |   800/ 2983 batches | lr 0.000179 | ms/batch 20.51 | loss  5.71 | ppl   300.86\n",
            "| epoch  27 |  1000/ 2983 batches | lr 0.000178 | ms/batch 20.53 | loss  5.72 | ppl   304.91\n",
            "| epoch  27 |  1200/ 2983 batches | lr 0.000178 | ms/batch 20.51 | loss  5.71 | ppl   302.16\n",
            "| epoch  27 |  1400/ 2983 batches | lr 0.000178 | ms/batch 20.52 | loss  5.74 | ppl   310.89\n",
            "| epoch  27 |  1600/ 2983 batches | lr 0.000178 | ms/batch 20.51 | loss  5.79 | ppl   326.53\n",
            "| epoch  27 |  1800/ 2983 batches | lr 0.000177 | ms/batch 20.50 | loss  5.70 | ppl   300.15\n",
            "| epoch  27 |  2000/ 2983 batches | lr 0.000177 | ms/batch 20.52 | loss  5.74 | ppl   312.26\n",
            "| epoch  27 |  2200/ 2983 batches | lr 0.000177 | ms/batch 20.51 | loss  5.68 | ppl   292.56\n",
            "| epoch  27 |  2400/ 2983 batches | lr 0.000177 | ms/batch 20.51 | loss  5.70 | ppl   298.41\n",
            "| epoch  27 |  2600/ 2983 batches | lr 0.000177 | ms/batch 20.52 | loss  5.74 | ppl   312.61\n",
            "| epoch  27 |  2800/ 2983 batches | lr 0.000176 | ms/batch 20.52 | loss  5.68 | ppl   293.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 63.70s | valid loss  6.36 | valid ppl   577.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000176\n",
            "| epoch  28 |   200/ 2983 batches | lr 0.000176 | ms/batch 20.61 | loss  5.74 | ppl   311.90\n",
            "| epoch  28 |   400/ 2983 batches | lr 0.000176 | ms/batch 20.51 | loss  5.74 | ppl   311.52\n",
            "| epoch  28 |   600/ 2983 batches | lr 0.000175 | ms/batch 20.50 | loss  5.66 | ppl   286.67\n",
            "| epoch  28 |   800/ 2983 batches | lr 0.000175 | ms/batch 20.53 | loss  5.71 | ppl   300.47\n",
            "| epoch  28 |  1000/ 2983 batches | lr 0.000175 | ms/batch 20.50 | loss  5.72 | ppl   304.30\n",
            "| epoch  28 |  1200/ 2983 batches | lr 0.000175 | ms/batch 20.54 | loss  5.71 | ppl   301.15\n",
            "| epoch  28 |  1400/ 2983 batches | lr 0.000175 | ms/batch 20.54 | loss  5.73 | ppl   308.79\n",
            "| epoch  28 |  1600/ 2983 batches | lr 0.000174 | ms/batch 20.51 | loss  5.79 | ppl   325.72\n",
            "| epoch  28 |  1800/ 2983 batches | lr 0.000174 | ms/batch 20.51 | loss  5.70 | ppl   298.73\n",
            "| epoch  28 |  2000/ 2983 batches | lr 0.000174 | ms/batch 20.50 | loss  5.74 | ppl   310.61\n",
            "| epoch  28 |  2200/ 2983 batches | lr 0.000174 | ms/batch 20.51 | loss  5.67 | ppl   291.08\n",
            "| epoch  28 |  2400/ 2983 batches | lr 0.000174 | ms/batch 20.51 | loss  5.69 | ppl   296.91\n",
            "| epoch  28 |  2600/ 2983 batches | lr 0.000173 | ms/batch 20.52 | loss  5.74 | ppl   311.32\n",
            "| epoch  28 |  2800/ 2983 batches | lr 0.000173 | ms/batch 20.50 | loss  5.68 | ppl   291.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 63.69s | valid loss  6.36 | valid ppl   577.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000173\n",
            "| epoch  29 |   200/ 2983 batches | lr 0.000173 | ms/batch 20.63 | loss  5.74 | ppl   310.69\n",
            "| epoch  29 |   400/ 2983 batches | lr 0.000173 | ms/batch 20.51 | loss  5.74 | ppl   310.93\n",
            "| epoch  29 |   600/ 2983 batches | lr 0.000172 | ms/batch 20.52 | loss  5.66 | ppl   286.16\n",
            "| epoch  29 |   800/ 2983 batches | lr 0.000172 | ms/batch 20.51 | loss  5.70 | ppl   298.99\n",
            "| epoch  29 |  1000/ 2983 batches | lr 0.000172 | ms/batch 20.51 | loss  5.71 | ppl   302.93\n",
            "| epoch  29 |  1200/ 2983 batches | lr 0.000172 | ms/batch 20.53 | loss  5.70 | ppl   298.93\n",
            "| epoch  29 |  1400/ 2983 batches | lr 0.000172 | ms/batch 20.52 | loss  5.73 | ppl   308.37\n",
            "| epoch  29 |  1600/ 2983 batches | lr 0.000171 | ms/batch 20.53 | loss  5.78 | ppl   324.42\n",
            "| epoch  29 |  1800/ 2983 batches | lr 0.000171 | ms/batch 20.53 | loss  5.70 | ppl   297.87\n",
            "| epoch  29 |  2000/ 2983 batches | lr 0.000171 | ms/batch 20.53 | loss  5.74 | ppl   309.82\n",
            "| epoch  29 |  2200/ 2983 batches | lr 0.000171 | ms/batch 20.52 | loss  5.67 | ppl   290.35\n",
            "| epoch  29 |  2400/ 2983 batches | lr 0.000171 | ms/batch 20.55 | loss  5.69 | ppl   295.93\n",
            "| epoch  29 |  2600/ 2983 batches | lr 0.000170 | ms/batch 20.53 | loss  5.74 | ppl   310.88\n",
            "| epoch  29 |  2800/ 2983 batches | lr 0.000170 | ms/batch 20.54 | loss  5.67 | ppl   290.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 63.75s | valid loss  6.36 | valid ppl   577.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000170\n",
            "| epoch  30 |   200/ 2983 batches | lr 0.000170 | ms/batch 20.65 | loss  5.73 | ppl   309.33\n",
            "| epoch  30 |   400/ 2983 batches | lr 0.000170 | ms/batch 20.54 | loss  5.74 | ppl   310.02\n",
            "| epoch  30 |   600/ 2983 batches | lr 0.000169 | ms/batch 20.50 | loss  5.65 | ppl   284.58\n",
            "| epoch  30 |   800/ 2983 batches | lr 0.000169 | ms/batch 20.53 | loss  5.70 | ppl   298.19\n",
            "| epoch  30 |  1000/ 2983 batches | lr 0.000169 | ms/batch 20.55 | loss  5.71 | ppl   301.83\n",
            "| epoch  30 |  1200/ 2983 batches | lr 0.000169 | ms/batch 20.56 | loss  5.70 | ppl   298.74\n",
            "| epoch  30 |  1400/ 2983 batches | lr 0.000169 | ms/batch 20.56 | loss  5.73 | ppl   306.58\n",
            "| epoch  30 |  1600/ 2983 batches | lr 0.000168 | ms/batch 20.55 | loss  5.77 | ppl   321.86\n",
            "| epoch  30 |  1800/ 2983 batches | lr 0.000168 | ms/batch 20.57 | loss  5.69 | ppl   297.10\n",
            "| epoch  30 |  2000/ 2983 batches | lr 0.000168 | ms/batch 20.55 | loss  5.73 | ppl   308.58\n",
            "| epoch  30 |  2200/ 2983 batches | lr 0.000168 | ms/batch 20.54 | loss  5.67 | ppl   289.65\n",
            "| epoch  30 |  2400/ 2983 batches | lr 0.000168 | ms/batch 20.52 | loss  5.69 | ppl   295.16\n",
            "| epoch  30 |  2600/ 2983 batches | lr 0.000167 | ms/batch 20.54 | loss  5.74 | ppl   310.13\n",
            "| epoch  30 |  2800/ 2983 batches | lr 0.000167 | ms/batch 20.54 | loss  5.67 | ppl   288.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 63.77s | valid loss  6.35 | valid ppl   574.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000167\n",
            "| epoch  31 |   200/ 2983 batches | lr 0.000167 | ms/batch 20.64 | loss  5.73 | ppl   308.45\n",
            "| epoch  31 |   400/ 2983 batches | lr 0.000167 | ms/batch 20.52 | loss  5.73 | ppl   308.47\n",
            "| epoch  31 |   600/ 2983 batches | lr 0.000167 | ms/batch 20.51 | loss  5.65 | ppl   283.60\n",
            "| epoch  31 |   800/ 2983 batches | lr 0.000166 | ms/batch 20.52 | loss  5.69 | ppl   296.85\n",
            "| epoch  31 |  1000/ 2983 batches | lr 0.000166 | ms/batch 20.51 | loss  5.71 | ppl   300.72\n",
            "| epoch  31 |  1200/ 2983 batches | lr 0.000166 | ms/batch 20.51 | loss  5.70 | ppl   297.50\n",
            "| epoch  31 |  1400/ 2983 batches | lr 0.000166 | ms/batch 20.54 | loss  5.72 | ppl   305.19\n",
            "| epoch  31 |  1600/ 2983 batches | lr 0.000166 | ms/batch 20.54 | loss  5.77 | ppl   321.45\n",
            "| epoch  31 |  1800/ 2983 batches | lr 0.000165 | ms/batch 20.51 | loss  5.69 | ppl   295.47\n",
            "| epoch  31 |  2000/ 2983 batches | lr 0.000165 | ms/batch 20.52 | loss  5.73 | ppl   308.42\n",
            "| epoch  31 |  2200/ 2983 batches | lr 0.000165 | ms/batch 20.51 | loss  5.67 | ppl   288.95\n",
            "| epoch  31 |  2400/ 2983 batches | lr 0.000165 | ms/batch 20.50 | loss  5.68 | ppl   293.35\n",
            "| epoch  31 |  2600/ 2983 batches | lr 0.000165 | ms/batch 20.54 | loss  5.73 | ppl   309.36\n",
            "| epoch  31 |  2800/ 2983 batches | lr 0.000165 | ms/batch 20.52 | loss  5.67 | ppl   288.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 63.72s | valid loss  6.35 | valid ppl   574.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000164\n",
            "| epoch  32 |   200/ 2983 batches | lr 0.000164 | ms/batch 20.63 | loss  5.73 | ppl   307.21\n",
            "| epoch  32 |   400/ 2983 batches | lr 0.000164 | ms/batch 20.53 | loss  5.73 | ppl   307.90\n",
            "| epoch  32 |   600/ 2983 batches | lr 0.000164 | ms/batch 20.53 | loss  5.64 | ppl   282.82\n",
            "| epoch  32 |   800/ 2983 batches | lr 0.000164 | ms/batch 20.51 | loss  5.69 | ppl   296.71\n",
            "| epoch  32 |  1000/ 2983 batches | lr 0.000164 | ms/batch 20.48 | loss  5.70 | ppl   300.24\n",
            "| epoch  32 |  1200/ 2983 batches | lr 0.000163 | ms/batch 20.51 | loss  5.69 | ppl   296.59\n",
            "| epoch  32 |  1400/ 2983 batches | lr 0.000163 | ms/batch 20.54 | loss  5.72 | ppl   305.38\n",
            "| epoch  32 |  1600/ 2983 batches | lr 0.000163 | ms/batch 20.52 | loss  5.77 | ppl   320.48\n",
            "| epoch  32 |  1800/ 2983 batches | lr 0.000163 | ms/batch 20.53 | loss  5.69 | ppl   295.28\n",
            "| epoch  32 |  2000/ 2983 batches | lr 0.000163 | ms/batch 20.52 | loss  5.72 | ppl   306.28\n",
            "| epoch  32 |  2200/ 2983 batches | lr 0.000162 | ms/batch 20.52 | loss  5.66 | ppl   287.59\n",
            "| epoch  32 |  2400/ 2983 batches | lr 0.000162 | ms/batch 20.53 | loss  5.68 | ppl   292.69\n",
            "| epoch  32 |  2600/ 2983 batches | lr 0.000162 | ms/batch 20.52 | loss  5.73 | ppl   308.40\n",
            "| epoch  32 |  2800/ 2983 batches | lr 0.000162 | ms/batch 20.52 | loss  5.66 | ppl   287.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 63.71s | valid loss  6.35 | valid ppl   572.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000162\n",
            "| epoch  33 |   200/ 2983 batches | lr 0.000162 | ms/batch 20.62 | loss  5.73 | ppl   306.46\n",
            "| epoch  33 |   400/ 2983 batches | lr 0.000161 | ms/batch 20.53 | loss  5.73 | ppl   306.61\n",
            "| epoch  33 |   600/ 2983 batches | lr 0.000161 | ms/batch 20.53 | loss  5.64 | ppl   280.72\n",
            "| epoch  33 |   800/ 2983 batches | lr 0.000161 | ms/batch 20.52 | loss  5.69 | ppl   295.37\n",
            "| epoch  33 |  1000/ 2983 batches | lr 0.000161 | ms/batch 20.52 | loss  5.70 | ppl   298.99\n",
            "| epoch  33 |  1200/ 2983 batches | lr 0.000161 | ms/batch 20.53 | loss  5.69 | ppl   296.86\n",
            "| epoch  33 |  1400/ 2983 batches | lr 0.000161 | ms/batch 20.53 | loss  5.72 | ppl   303.42\n",
            "| epoch  33 |  1600/ 2983 batches | lr 0.000160 | ms/batch 20.52 | loss  5.77 | ppl   320.47\n",
            "| epoch  33 |  1800/ 2983 batches | lr 0.000160 | ms/batch 20.52 | loss  5.68 | ppl   293.15\n",
            "| epoch  33 |  2000/ 2983 batches | lr 0.000160 | ms/batch 20.52 | loss  5.73 | ppl   306.66\n",
            "| epoch  33 |  2200/ 2983 batches | lr 0.000160 | ms/batch 20.56 | loss  5.66 | ppl   287.09\n",
            "| epoch  33 |  2400/ 2983 batches | lr 0.000160 | ms/batch 20.54 | loss  5.67 | ppl   291.10\n",
            "| epoch  33 |  2600/ 2983 batches | lr 0.000160 | ms/batch 20.51 | loss  5.73 | ppl   307.38\n",
            "| epoch  33 |  2800/ 2983 batches | lr 0.000159 | ms/batch 20.55 | loss  5.66 | ppl   286.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 63.74s | valid loss  6.35 | valid ppl   573.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000159\n",
            "| epoch  34 |   200/ 2983 batches | lr 0.000159 | ms/batch 20.65 | loss  5.72 | ppl   306.15\n",
            "| epoch  34 |   400/ 2983 batches | lr 0.000159 | ms/batch 20.55 | loss  5.72 | ppl   306.11\n",
            "| epoch  34 |   600/ 2983 batches | lr 0.000159 | ms/batch 20.55 | loss  5.64 | ppl   281.42\n",
            "| epoch  34 |   800/ 2983 batches | lr 0.000159 | ms/batch 20.55 | loss  5.69 | ppl   295.45\n",
            "| epoch  34 |  1000/ 2983 batches | lr 0.000159 | ms/batch 20.56 | loss  5.70 | ppl   298.22\n",
            "| epoch  34 |  1200/ 2983 batches | lr 0.000158 | ms/batch 20.54 | loss  5.69 | ppl   295.63\n",
            "| epoch  34 |  1400/ 2983 batches | lr 0.000158 | ms/batch 20.54 | loss  5.71 | ppl   302.73\n",
            "| epoch  34 |  1600/ 2983 batches | lr 0.000158 | ms/batch 20.53 | loss  5.77 | ppl   319.34\n",
            "| epoch  34 |  1800/ 2983 batches | lr 0.000158 | ms/batch 20.50 | loss  5.68 | ppl   293.64\n",
            "| epoch  34 |  2000/ 2983 batches | lr 0.000158 | ms/batch 20.52 | loss  5.72 | ppl   305.50\n",
            "| epoch  34 |  2200/ 2983 batches | lr 0.000158 | ms/batch 20.52 | loss  5.66 | ppl   286.10\n",
            "| epoch  34 |  2400/ 2983 batches | lr 0.000157 | ms/batch 20.51 | loss  5.67 | ppl   290.56\n",
            "| epoch  34 |  2600/ 2983 batches | lr 0.000157 | ms/batch 20.56 | loss  5.73 | ppl   307.10\n",
            "| epoch  34 |  2800/ 2983 batches | lr 0.000157 | ms/batch 20.51 | loss  5.66 | ppl   286.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 63.75s | valid loss  6.35 | valid ppl   574.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000157\n",
            "| epoch  35 |   200/ 2983 batches | lr 0.000157 | ms/batch 20.62 | loss  5.72 | ppl   304.64\n",
            "| epoch  35 |   400/ 2983 batches | lr 0.000157 | ms/batch 20.51 | loss  5.72 | ppl   305.21\n",
            "| epoch  35 |   600/ 2983 batches | lr 0.000157 | ms/batch 20.52 | loss  5.63 | ppl   279.49\n",
            "| epoch  35 |   800/ 2983 batches | lr 0.000156 | ms/batch 20.56 | loss  5.69 | ppl   294.90\n",
            "| epoch  35 |  1000/ 2983 batches | lr 0.000156 | ms/batch 20.52 | loss  5.70 | ppl   297.84\n",
            "| epoch  35 |  1200/ 2983 batches | lr 0.000156 | ms/batch 20.52 | loss  5.68 | ppl   294.39\n",
            "| epoch  35 |  1400/ 2983 batches | lr 0.000156 | ms/batch 20.53 | loss  5.71 | ppl   301.88\n",
            "| epoch  35 |  1600/ 2983 batches | lr 0.000156 | ms/batch 20.52 | loss  5.76 | ppl   318.48\n",
            "| epoch  35 |  1800/ 2983 batches | lr 0.000156 | ms/batch 20.53 | loss  5.68 | ppl   292.83\n",
            "| epoch  35 |  2000/ 2983 batches | lr 0.000155 | ms/batch 20.51 | loss  5.72 | ppl   305.22\n",
            "| epoch  35 |  2200/ 2983 batches | lr 0.000155 | ms/batch 20.51 | loss  5.66 | ppl   285.98\n",
            "| epoch  35 |  2400/ 2983 batches | lr 0.000155 | ms/batch 20.54 | loss  5.67 | ppl   289.43\n",
            "| epoch  35 |  2600/ 2983 batches | lr 0.000155 | ms/batch 20.53 | loss  5.72 | ppl   306.38\n",
            "| epoch  35 |  2800/ 2983 batches | lr 0.000155 | ms/batch 20.52 | loss  5.65 | ppl   285.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 63.72s | valid loss  6.35 | valid ppl   572.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000155\n",
            "| epoch  36 |   200/ 2983 batches | lr 0.000155 | ms/batch 20.62 | loss  5.72 | ppl   304.33\n",
            "| epoch  36 |   400/ 2983 batches | lr 0.000154 | ms/batch 20.52 | loss  5.72 | ppl   304.70\n",
            "| epoch  36 |   600/ 2983 batches | lr 0.000154 | ms/batch 20.52 | loss  5.63 | ppl   278.90\n",
            "| epoch  36 |   800/ 2983 batches | lr 0.000154 | ms/batch 20.52 | loss  5.68 | ppl   293.36\n",
            "| epoch  36 |  1000/ 2983 batches | lr 0.000154 | ms/batch 20.51 | loss  5.69 | ppl   296.42\n",
            "| epoch  36 |  1200/ 2983 batches | lr 0.000154 | ms/batch 20.51 | loss  5.68 | ppl   293.92\n",
            "| epoch  36 |  1400/ 2983 batches | lr 0.000154 | ms/batch 20.52 | loss  5.71 | ppl   301.69\n",
            "| epoch  36 |  1600/ 2983 batches | lr 0.000154 | ms/batch 20.49 | loss  5.76 | ppl   317.09\n",
            "| epoch  36 |  1800/ 2983 batches | lr 0.000153 | ms/batch 20.53 | loss  5.68 | ppl   292.16\n",
            "| epoch  36 |  2000/ 2983 batches | lr 0.000153 | ms/batch 20.49 | loss  5.72 | ppl   303.84\n",
            "| epoch  36 |  2200/ 2983 batches | lr 0.000153 | ms/batch 20.51 | loss  5.65 | ppl   284.78\n",
            "| epoch  36 |  2400/ 2983 batches | lr 0.000153 | ms/batch 20.53 | loss  5.67 | ppl   288.81\n",
            "| epoch  36 |  2600/ 2983 batches | lr 0.000153 | ms/batch 20.52 | loss  5.72 | ppl   305.39\n",
            "| epoch  36 |  2800/ 2983 batches | lr 0.000153 | ms/batch 20.51 | loss  5.65 | ppl   284.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 63.69s | valid loss  6.35 | valid ppl   573.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000153\n",
            "| epoch  37 |   200/ 2983 batches | lr 0.000152 | ms/batch 20.61 | loss  5.72 | ppl   303.93\n",
            "| epoch  37 |   400/ 2983 batches | lr 0.000152 | ms/batch 20.54 | loss  5.72 | ppl   303.44\n",
            "| epoch  37 |   600/ 2983 batches | lr 0.000152 | ms/batch 20.51 | loss  5.63 | ppl   278.46\n",
            "| epoch  37 |   800/ 2983 batches | lr 0.000152 | ms/batch 20.51 | loss  5.68 | ppl   292.56\n",
            "| epoch  37 |  1000/ 2983 batches | lr 0.000152 | ms/batch 20.57 | loss  5.69 | ppl   296.00\n",
            "| epoch  37 |  1200/ 2983 batches | lr 0.000152 | ms/batch 20.51 | loss  5.68 | ppl   292.83\n",
            "| epoch  37 |  1400/ 2983 batches | lr 0.000152 | ms/batch 20.53 | loss  5.70 | ppl   299.49\n",
            "| epoch  37 |  1600/ 2983 batches | lr 0.000151 | ms/batch 20.60 | loss  5.76 | ppl   316.84\n",
            "| epoch  37 |  1800/ 2983 batches | lr 0.000151 | ms/batch 20.57 | loss  5.67 | ppl   291.40\n",
            "| epoch  37 |  2000/ 2983 batches | lr 0.000151 | ms/batch 20.56 | loss  5.71 | ppl   302.40\n",
            "| epoch  37 |  2200/ 2983 batches | lr 0.000151 | ms/batch 20.62 | loss  5.65 | ppl   283.59\n",
            "| epoch  37 |  2400/ 2983 batches | lr 0.000151 | ms/batch 20.58 | loss  5.67 | ppl   288.98\n",
            "| epoch  37 |  2600/ 2983 batches | lr 0.000151 | ms/batch 20.58 | loss  5.72 | ppl   304.90\n",
            "| epoch  37 |  2800/ 2983 batches | lr 0.000151 | ms/batch 20.58 | loss  5.65 | ppl   283.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 63.81s | valid loss  6.35 | valid ppl   572.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000150\n",
            "| epoch  38 |   200/ 2983 batches | lr 0.000150 | ms/batch 20.72 | loss  5.71 | ppl   302.79\n",
            "| epoch  38 |   400/ 2983 batches | lr 0.000150 | ms/batch 20.60 | loss  5.71 | ppl   302.63\n",
            "| epoch  38 |   600/ 2983 batches | lr 0.000150 | ms/batch 20.56 | loss  5.62 | ppl   277.16\n",
            "| epoch  38 |   800/ 2983 batches | lr 0.000150 | ms/batch 20.60 | loss  5.68 | ppl   292.12\n",
            "| epoch  38 |  1000/ 2983 batches | lr 0.000150 | ms/batch 20.59 | loss  5.69 | ppl   295.81\n",
            "| epoch  38 |  1200/ 2983 batches | lr 0.000150 | ms/batch 20.56 | loss  5.68 | ppl   292.30\n",
            "| epoch  38 |  1400/ 2983 batches | lr 0.000150 | ms/batch 20.60 | loss  5.70 | ppl   299.94\n",
            "| epoch  38 |  1600/ 2983 batches | lr 0.000149 | ms/batch 20.57 | loss  5.76 | ppl   315.88\n",
            "| epoch  38 |  1800/ 2983 batches | lr 0.000149 | ms/batch 20.56 | loss  5.67 | ppl   290.44\n",
            "| epoch  38 |  2000/ 2983 batches | lr 0.000149 | ms/batch 20.56 | loss  5.71 | ppl   302.29\n",
            "| epoch  38 |  2200/ 2983 batches | lr 0.000149 | ms/batch 20.56 | loss  5.64 | ppl   282.36\n",
            "| epoch  38 |  2400/ 2983 batches | lr 0.000149 | ms/batch 20.52 | loss  5.66 | ppl   286.87\n",
            "| epoch  38 |  2600/ 2983 batches | lr 0.000149 | ms/batch 20.54 | loss  5.71 | ppl   303.31\n",
            "| epoch  38 |  2800/ 2983 batches | lr 0.000149 | ms/batch 20.52 | loss  5.65 | ppl   283.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 63.85s | valid loss  6.35 | valid ppl   572.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000148\n",
            "| epoch  39 |   200/ 2983 batches | lr 0.000148 | ms/batch 20.64 | loss  5.71 | ppl   301.92\n",
            "| epoch  39 |   400/ 2983 batches | lr 0.000148 | ms/batch 20.53 | loss  5.71 | ppl   302.33\n",
            "| epoch  39 |   600/ 2983 batches | lr 0.000148 | ms/batch 20.52 | loss  5.62 | ppl   276.16\n",
            "| epoch  39 |   800/ 2983 batches | lr 0.000148 | ms/batch 20.52 | loss  5.68 | ppl   291.95\n",
            "| epoch  39 |  1000/ 2983 batches | lr 0.000148 | ms/batch 20.52 | loss  5.69 | ppl   294.53\n",
            "| epoch  39 |  1200/ 2983 batches | lr 0.000148 | ms/batch 20.53 | loss  5.68 | ppl   292.01\n",
            "| epoch  39 |  1400/ 2983 batches | lr 0.000148 | ms/batch 20.54 | loss  5.70 | ppl   299.20\n",
            "| epoch  39 |  1600/ 2983 batches | lr 0.000147 | ms/batch 20.53 | loss  5.75 | ppl   314.70\n",
            "| epoch  39 |  1800/ 2983 batches | lr 0.000147 | ms/batch 20.52 | loss  5.67 | ppl   290.15\n",
            "| epoch  39 |  2000/ 2983 batches | lr 0.000147 | ms/batch 20.53 | loss  5.71 | ppl   301.19\n",
            "| epoch  39 |  2200/ 2983 batches | lr 0.000147 | ms/batch 20.52 | loss  5.64 | ppl   281.50\n",
            "| epoch  39 |  2400/ 2983 batches | lr 0.000147 | ms/batch 20.53 | loss  5.66 | ppl   287.37\n",
            "| epoch  39 |  2600/ 2983 batches | lr 0.000147 | ms/batch 20.53 | loss  5.71 | ppl   302.54\n",
            "| epoch  39 |  2800/ 2983 batches | lr 0.000147 | ms/batch 20.53 | loss  5.64 | ppl   282.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 63.73s | valid loss  6.35 | valid ppl   572.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000147\n",
            "| epoch  40 |   200/ 2983 batches | lr 0.000146 | ms/batch 20.63 | loss  5.70 | ppl   300.29\n",
            "| epoch  40 |   400/ 2983 batches | lr 0.000146 | ms/batch 20.53 | loss  5.71 | ppl   301.81\n",
            "| epoch  40 |   600/ 2983 batches | lr 0.000146 | ms/batch 20.54 | loss  5.62 | ppl   275.76\n",
            "| epoch  40 |   800/ 2983 batches | lr 0.000146 | ms/batch 20.52 | loss  5.67 | ppl   290.99\n",
            "| epoch  40 |  1000/ 2983 batches | lr 0.000146 | ms/batch 20.53 | loss  5.69 | ppl   294.65\n",
            "| epoch  40 |  1200/ 2983 batches | lr 0.000146 | ms/batch 20.56 | loss  5.68 | ppl   291.67\n",
            "| epoch  40 |  1400/ 2983 batches | lr 0.000146 | ms/batch 20.50 | loss  5.70 | ppl   297.99\n",
            "| epoch  40 |  1600/ 2983 batches | lr 0.000146 | ms/batch 20.52 | loss  5.75 | ppl   313.07\n",
            "| epoch  40 |  1800/ 2983 batches | lr 0.000145 | ms/batch 20.51 | loss  5.66 | ppl   288.44\n",
            "| epoch  40 |  2000/ 2983 batches | lr 0.000145 | ms/batch 20.52 | loss  5.71 | ppl   302.20\n",
            "| epoch  40 |  2200/ 2983 batches | lr 0.000145 | ms/batch 20.51 | loss  5.64 | ppl   281.65\n",
            "| epoch  40 |  2400/ 2983 batches | lr 0.000145 | ms/batch 20.52 | loss  5.66 | ppl   286.61\n",
            "| epoch  40 |  2600/ 2983 batches | lr 0.000145 | ms/batch 20.52 | loss  5.71 | ppl   302.36\n",
            "| epoch  40 |  2800/ 2983 batches | lr 0.000145 | ms/batch 20.53 | loss  5.64 | ppl   281.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 63.72s | valid loss  6.35 | valid ppl   570.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000145\n",
            "| epoch  41 |   200/ 2983 batches | lr 0.000145 | ms/batch 20.62 | loss  5.70 | ppl   299.11\n",
            "| epoch  41 |   400/ 2983 batches | lr 0.000144 | ms/batch 20.50 | loss  5.71 | ppl   300.76\n",
            "| epoch  41 |   600/ 2983 batches | lr 0.000144 | ms/batch 20.51 | loss  5.62 | ppl   274.73\n",
            "| epoch  41 |   800/ 2983 batches | lr 0.000144 | ms/batch 20.51 | loss  5.67 | ppl   290.22\n",
            "| epoch  41 |  1000/ 2983 batches | lr 0.000144 | ms/batch 20.51 | loss  5.68 | ppl   292.73\n",
            "| epoch  41 |  1200/ 2983 batches | lr 0.000144 | ms/batch 20.52 | loss  5.67 | ppl   290.36\n",
            "| epoch  41 |  1400/ 2983 batches | lr 0.000144 | ms/batch 20.53 | loss  5.70 | ppl   297.47\n",
            "| epoch  41 |  1600/ 2983 batches | lr 0.000144 | ms/batch 20.53 | loss  5.74 | ppl   312.60\n",
            "| epoch  41 |  1800/ 2983 batches | lr 0.000144 | ms/batch 20.53 | loss  5.66 | ppl   288.58\n",
            "| epoch  41 |  2000/ 2983 batches | lr 0.000144 | ms/batch 20.50 | loss  5.70 | ppl   299.86\n",
            "| epoch  41 |  2200/ 2983 batches | lr 0.000143 | ms/batch 20.54 | loss  5.64 | ppl   281.11\n",
            "| epoch  41 |  2400/ 2983 batches | lr 0.000143 | ms/batch 20.52 | loss  5.66 | ppl   286.26\n",
            "| epoch  41 |  2600/ 2983 batches | lr 0.000143 | ms/batch 20.52 | loss  5.71 | ppl   301.61\n",
            "| epoch  41 |  2800/ 2983 batches | lr 0.000143 | ms/batch 20.53 | loss  5.64 | ppl   281.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 63.71s | valid loss  6.35 | valid ppl   573.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000143\n",
            "| epoch  42 |   200/ 2983 batches | lr 0.000143 | ms/batch 20.65 | loss  5.70 | ppl   299.95\n",
            "| epoch  42 |   400/ 2983 batches | lr 0.000143 | ms/batch 20.53 | loss  5.70 | ppl   300.15\n",
            "| epoch  42 |   600/ 2983 batches | lr 0.000143 | ms/batch 20.54 | loss  5.61 | ppl   274.49\n",
            "| epoch  42 |   800/ 2983 batches | lr 0.000142 | ms/batch 20.58 | loss  5.67 | ppl   289.26\n",
            "| epoch  42 |  1000/ 2983 batches | lr 0.000142 | ms/batch 20.56 | loss  5.68 | ppl   292.86\n",
            "| epoch  42 |  1200/ 2983 batches | lr 0.000142 | ms/batch 20.57 | loss  5.67 | ppl   290.25\n",
            "| epoch  42 |  1400/ 2983 batches | lr 0.000142 | ms/batch 20.54 | loss  5.69 | ppl   296.85\n",
            "| epoch  42 |  1600/ 2983 batches | lr 0.000142 | ms/batch 20.53 | loss  5.74 | ppl   312.52\n",
            "| epoch  42 |  1800/ 2983 batches | lr 0.000142 | ms/batch 20.50 | loss  5.66 | ppl   287.85\n",
            "| epoch  42 |  2000/ 2983 batches | lr 0.000142 | ms/batch 20.54 | loss  5.70 | ppl   299.77\n",
            "| epoch  42 |  2200/ 2983 batches | lr 0.000142 | ms/batch 20.52 | loss  5.64 | ppl   280.57\n",
            "| epoch  42 |  2400/ 2983 batches | lr 0.000142 | ms/batch 20.55 | loss  5.65 | ppl   285.12\n",
            "| epoch  42 |  2600/ 2983 batches | lr 0.000141 | ms/batch 20.60 | loss  5.70 | ppl   300.31\n",
            "| epoch  42 |  2800/ 2983 batches | lr 0.000141 | ms/batch 20.53 | loss  5.64 | ppl   280.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 63.79s | valid loss  6.35 | valid ppl   570.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000141\n",
            "| epoch  43 |   200/ 2983 batches | lr 0.000141 | ms/batch 20.62 | loss  5.70 | ppl   299.25\n",
            "| epoch  43 |   400/ 2983 batches | lr 0.000141 | ms/batch 20.52 | loss  5.70 | ppl   299.05\n",
            "| epoch  43 |   600/ 2983 batches | lr 0.000141 | ms/batch 20.52 | loss  5.61 | ppl   273.08\n",
            "| epoch  43 |   800/ 2983 batches | lr 0.000141 | ms/batch 20.54 | loss  5.67 | ppl   289.24\n",
            "| epoch  43 |  1000/ 2983 batches | lr 0.000141 | ms/batch 20.53 | loss  5.68 | ppl   292.03\n",
            "| epoch  43 |  1200/ 2983 batches | lr 0.000141 | ms/batch 20.54 | loss  5.67 | ppl   289.66\n",
            "| epoch  43 |  1400/ 2983 batches | lr 0.000140 | ms/batch 20.55 | loss  5.69 | ppl   295.70\n",
            "| epoch  43 |  1600/ 2983 batches | lr 0.000140 | ms/batch 20.56 | loss  5.74 | ppl   312.54\n",
            "| epoch  43 |  1800/ 2983 batches | lr 0.000140 | ms/batch 20.58 | loss  5.66 | ppl   288.16\n",
            "| epoch  43 |  2000/ 2983 batches | lr 0.000140 | ms/batch 20.55 | loss  5.70 | ppl   299.37\n",
            "| epoch  43 |  2200/ 2983 batches | lr 0.000140 | ms/batch 20.65 | loss  5.63 | ppl   279.78\n",
            "| epoch  43 |  2400/ 2983 batches | lr 0.000140 | ms/batch 20.62 | loss  5.65 | ppl   284.44\n",
            "| epoch  43 |  2600/ 2983 batches | lr 0.000140 | ms/batch 20.53 | loss  5.70 | ppl   299.90\n",
            "| epoch  43 |  2800/ 2983 batches | lr 0.000140 | ms/batch 20.56 | loss  5.63 | ppl   279.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 63.83s | valid loss  6.34 | valid ppl   569.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000140\n",
            "| epoch  44 |   200/ 2983 batches | lr 0.000139 | ms/batch 20.69 | loss  5.69 | ppl   297.29\n",
            "| epoch  44 |   400/ 2983 batches | lr 0.000139 | ms/batch 20.59 | loss  5.70 | ppl   299.19\n",
            "| epoch  44 |   600/ 2983 batches | lr 0.000139 | ms/batch 20.51 | loss  5.61 | ppl   273.17\n",
            "| epoch  44 |   800/ 2983 batches | lr 0.000139 | ms/batch 20.51 | loss  5.66 | ppl   288.40\n",
            "| epoch  44 |  1000/ 2983 batches | lr 0.000139 | ms/batch 20.52 | loss  5.67 | ppl   291.43\n",
            "| epoch  44 |  1200/ 2983 batches | lr 0.000139 | ms/batch 20.52 | loss  5.67 | ppl   289.10\n",
            "| epoch  44 |  1400/ 2983 batches | lr 0.000139 | ms/batch 20.52 | loss  5.69 | ppl   295.22\n",
            "| epoch  44 |  1600/ 2983 batches | lr 0.000139 | ms/batch 20.53 | loss  5.74 | ppl   310.96\n",
            "| epoch  44 |  1800/ 2983 batches | lr 0.000139 | ms/batch 20.52 | loss  5.66 | ppl   286.45\n",
            "| epoch  44 |  2000/ 2983 batches | lr 0.000139 | ms/batch 20.52 | loss  5.70 | ppl   298.82\n",
            "| epoch  44 |  2200/ 2983 batches | lr 0.000138 | ms/batch 20.52 | loss  5.63 | ppl   279.88\n",
            "| epoch  44 |  2400/ 2983 batches | lr 0.000138 | ms/batch 20.50 | loss  5.65 | ppl   284.22\n",
            "| epoch  44 |  2600/ 2983 batches | lr 0.000138 | ms/batch 20.51 | loss  5.70 | ppl   298.48\n",
            "| epoch  44 |  2800/ 2983 batches | lr 0.000138 | ms/batch 20.51 | loss  5.63 | ppl   278.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 63.72s | valid loss  6.34 | valid ppl   568.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000138\n",
            "| epoch  45 |   200/ 2983 batches | lr 0.000138 | ms/batch 20.61 | loss  5.69 | ppl   296.64\n",
            "| epoch  45 |   400/ 2983 batches | lr 0.000138 | ms/batch 20.52 | loss  5.70 | ppl   298.19\n",
            "| epoch  45 |   600/ 2983 batches | lr 0.000138 | ms/batch 20.51 | loss  5.61 | ppl   272.46\n",
            "| epoch  45 |   800/ 2983 batches | lr 0.000138 | ms/batch 20.53 | loss  5.66 | ppl   288.06\n",
            "| epoch  45 |  1000/ 2983 batches | lr 0.000137 | ms/batch 20.58 | loss  5.67 | ppl   290.89\n",
            "| epoch  45 |  1200/ 2983 batches | lr 0.000137 | ms/batch 20.60 | loss  5.66 | ppl   287.50\n",
            "| epoch  45 |  1400/ 2983 batches | lr 0.000137 | ms/batch 20.64 | loss  5.69 | ppl   295.10\n",
            "| epoch  45 |  1600/ 2983 batches | lr 0.000137 | ms/batch 20.64 | loss  5.74 | ppl   310.95\n",
            "| epoch  45 |  1800/ 2983 batches | lr 0.000137 | ms/batch 20.69 | loss  5.66 | ppl   286.18\n",
            "| epoch  45 |  2000/ 2983 batches | lr 0.000137 | ms/batch 20.59 | loss  5.70 | ppl   297.42\n",
            "| epoch  45 |  2200/ 2983 batches | lr 0.000137 | ms/batch 20.57 | loss  5.63 | ppl   278.31\n",
            "| epoch  45 |  2400/ 2983 batches | lr 0.000137 | ms/batch 20.60 | loss  5.65 | ppl   283.02\n",
            "| epoch  45 |  2600/ 2983 batches | lr 0.000137 | ms/batch 20.66 | loss  5.70 | ppl   298.44\n",
            "| epoch  45 |  2800/ 2983 batches | lr 0.000137 | ms/batch 20.57 | loss  5.63 | ppl   279.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 63.95s | valid loss  6.34 | valid ppl   569.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000136\n",
            "| epoch  46 |   200/ 2983 batches | lr 0.000136 | ms/batch 20.69 | loss  5.69 | ppl   296.87\n",
            "| epoch  46 |   400/ 2983 batches | lr 0.000136 | ms/batch 20.56 | loss  5.69 | ppl   297.20\n",
            "| epoch  46 |   600/ 2983 batches | lr 0.000136 | ms/batch 20.57 | loss  5.60 | ppl   271.70\n",
            "| epoch  46 |   800/ 2983 batches | lr 0.000136 | ms/batch 20.59 | loss  5.66 | ppl   287.49\n",
            "| epoch  46 |  1000/ 2983 batches | lr 0.000136 | ms/batch 20.57 | loss  5.67 | ppl   290.50\n",
            "| epoch  46 |  1200/ 2983 batches | lr 0.000136 | ms/batch 20.59 | loss  5.66 | ppl   287.92\n",
            "| epoch  46 |  1400/ 2983 batches | lr 0.000136 | ms/batch 20.57 | loss  5.68 | ppl   294.40\n",
            "| epoch  46 |  1600/ 2983 batches | lr 0.000136 | ms/batch 20.59 | loss  5.74 | ppl   310.77\n",
            "| epoch  46 |  1800/ 2983 batches | lr 0.000136 | ms/batch 20.59 | loss  5.65 | ppl   285.71\n",
            "| epoch  46 |  2000/ 2983 batches | lr 0.000135 | ms/batch 20.57 | loss  5.70 | ppl   298.69\n",
            "| epoch  46 |  2200/ 2983 batches | lr 0.000135 | ms/batch 20.55 | loss  5.63 | ppl   278.66\n",
            "| epoch  46 |  2400/ 2983 batches | lr 0.000135 | ms/batch 20.57 | loss  5.64 | ppl   282.45\n",
            "| epoch  46 |  2600/ 2983 batches | lr 0.000135 | ms/batch 20.64 | loss  5.70 | ppl   299.30\n",
            "| epoch  46 |  2800/ 2983 batches | lr 0.000135 | ms/batch 20.56 | loss  5.63 | ppl   277.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 63.93s | valid loss  6.34 | valid ppl   569.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000135\n",
            "| epoch  47 |   200/ 2983 batches | lr 0.000135 | ms/batch 20.66 | loss  5.69 | ppl   296.01\n",
            "| epoch  47 |   400/ 2983 batches | lr 0.000135 | ms/batch 20.58 | loss  5.69 | ppl   296.48\n",
            "| epoch  47 |   600/ 2983 batches | lr 0.000135 | ms/batch 20.65 | loss  5.60 | ppl   271.71\n",
            "| epoch  47 |   800/ 2983 batches | lr 0.000135 | ms/batch 20.56 | loss  5.66 | ppl   286.95\n",
            "| epoch  47 |  1000/ 2983 batches | lr 0.000134 | ms/batch 20.65 | loss  5.67 | ppl   289.33\n",
            "| epoch  47 |  1200/ 2983 batches | lr 0.000134 | ms/batch 20.61 | loss  5.66 | ppl   286.93\n",
            "| epoch  47 |  1400/ 2983 batches | lr 0.000134 | ms/batch 20.59 | loss  5.68 | ppl   294.32\n",
            "| epoch  47 |  1600/ 2983 batches | lr 0.000134 | ms/batch 20.57 | loss  5.73 | ppl   309.24\n",
            "| epoch  47 |  1800/ 2983 batches | lr 0.000134 | ms/batch 20.59 | loss  5.65 | ppl   285.53\n",
            "| epoch  47 |  2000/ 2983 batches | lr 0.000134 | ms/batch 20.61 | loss  5.69 | ppl   297.11\n",
            "| epoch  47 |  2200/ 2983 batches | lr 0.000134 | ms/batch 20.61 | loss  5.63 | ppl   278.04\n",
            "| epoch  47 |  2400/ 2983 batches | lr 0.000134 | ms/batch 20.64 | loss  5.64 | ppl   281.93\n",
            "| epoch  47 |  2600/ 2983 batches | lr 0.000134 | ms/batch 20.58 | loss  5.70 | ppl   297.62\n",
            "| epoch  47 |  2800/ 2983 batches | lr 0.000134 | ms/batch 20.56 | loss  5.63 | ppl   277.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 63.96s | valid loss  6.34 | valid ppl   569.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000134\n",
            "| epoch  48 |   200/ 2983 batches | lr 0.000133 | ms/batch 20.67 | loss  5.69 | ppl   295.81\n",
            "| epoch  48 |   400/ 2983 batches | lr 0.000133 | ms/batch 20.63 | loss  5.69 | ppl   296.21\n",
            "| epoch  48 |   600/ 2983 batches | lr 0.000133 | ms/batch 20.62 | loss  5.60 | ppl   270.83\n",
            "| epoch  48 |   800/ 2983 batches | lr 0.000133 | ms/batch 20.57 | loss  5.66 | ppl   286.64\n",
            "| epoch  48 |  1000/ 2983 batches | lr 0.000133 | ms/batch 20.63 | loss  5.67 | ppl   290.01\n",
            "| epoch  48 |  1200/ 2983 batches | lr 0.000133 | ms/batch 20.59 | loss  5.66 | ppl   286.74\n",
            "| epoch  48 |  1400/ 2983 batches | lr 0.000133 | ms/batch 20.62 | loss  5.68 | ppl   293.66\n",
            "| epoch  48 |  1600/ 2983 batches | lr 0.000133 | ms/batch 20.62 | loss  5.73 | ppl   309.41\n",
            "| epoch  48 |  1800/ 2983 batches | lr 0.000133 | ms/batch 20.59 | loss  5.65 | ppl   284.35\n",
            "| epoch  48 |  2000/ 2983 batches | lr 0.000133 | ms/batch 20.61 | loss  5.69 | ppl   296.93\n",
            "| epoch  48 |  2200/ 2983 batches | lr 0.000132 | ms/batch 20.62 | loss  5.63 | ppl   277.54\n",
            "| epoch  48 |  2400/ 2983 batches | lr 0.000132 | ms/batch 20.60 | loss  5.64 | ppl   281.43\n",
            "| epoch  48 |  2600/ 2983 batches | lr 0.000132 | ms/batch 20.65 | loss  5.69 | ppl   297.26\n",
            "| epoch  48 |  2800/ 2983 batches | lr 0.000132 | ms/batch 20.60 | loss  5.62 | ppl   277.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 64.01s | valid loss  6.35 | valid ppl   570.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000132\n",
            "| epoch  49 |   200/ 2983 batches | lr 0.000132 | ms/batch 20.72 | loss  5.69 | ppl   295.19\n",
            "| epoch  49 |   400/ 2983 batches | lr 0.000132 | ms/batch 20.56 | loss  5.69 | ppl   296.22\n",
            "| epoch  49 |   600/ 2983 batches | lr 0.000132 | ms/batch 20.58 | loss  5.60 | ppl   270.99\n",
            "| epoch  49 |   800/ 2983 batches | lr 0.000132 | ms/batch 20.66 | loss  5.66 | ppl   286.16\n",
            "| epoch  49 |  1000/ 2983 batches | lr 0.000132 | ms/batch 20.62 | loss  5.66 | ppl   288.43\n",
            "| epoch  49 |  1200/ 2983 batches | lr 0.000132 | ms/batch 20.58 | loss  5.66 | ppl   285.89\n",
            "| epoch  49 |  1400/ 2983 batches | lr 0.000131 | ms/batch 20.58 | loss  5.68 | ppl   292.28\n",
            "| epoch  49 |  1600/ 2983 batches | lr 0.000131 | ms/batch 20.58 | loss  5.73 | ppl   307.94\n",
            "| epoch  49 |  1800/ 2983 batches | lr 0.000131 | ms/batch 20.57 | loss  5.65 | ppl   284.66\n",
            "| epoch  49 |  2000/ 2983 batches | lr 0.000131 | ms/batch 20.59 | loss  5.69 | ppl   296.73\n",
            "| epoch  49 |  2200/ 2983 batches | lr 0.000131 | ms/batch 20.58 | loss  5.62 | ppl   276.27\n",
            "| epoch  49 |  2400/ 2983 batches | lr 0.000131 | ms/batch 20.64 | loss  5.64 | ppl   280.65\n",
            "| epoch  49 |  2600/ 2983 batches | lr 0.000131 | ms/batch 20.62 | loss  5.69 | ppl   296.11\n",
            "| epoch  49 |  2800/ 2983 batches | lr 0.000131 | ms/batch 20.58 | loss  5.62 | ppl   276.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 64.00s | valid loss  6.34 | valid ppl   567.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000131\n",
            "| epoch  50 |   200/ 2983 batches | lr 0.000131 | ms/batch 20.70 | loss  5.68 | ppl   294.19\n",
            "| epoch  50 |   400/ 2983 batches | lr 0.000131 | ms/batch 20.63 | loss  5.69 | ppl   295.58\n",
            "| epoch  50 |   600/ 2983 batches | lr 0.000130 | ms/batch 20.65 | loss  5.60 | ppl   270.31\n",
            "| epoch  50 |   800/ 2983 batches | lr 0.000130 | ms/batch 20.61 | loss  5.65 | ppl   285.53\n",
            "| epoch  50 |  1000/ 2983 batches | lr 0.000130 | ms/batch 20.58 | loss  5.66 | ppl   288.43\n",
            "| epoch  50 |  1200/ 2983 batches | lr 0.000130 | ms/batch 20.59 | loss  5.65 | ppl   285.68\n",
            "| epoch  50 |  1400/ 2983 batches | lr 0.000130 | ms/batch 20.59 | loss  5.68 | ppl   291.66\n",
            "| epoch  50 |  1600/ 2983 batches | lr 0.000130 | ms/batch 20.57 | loss  5.73 | ppl   306.76\n",
            "| epoch  50 |  1800/ 2983 batches | lr 0.000130 | ms/batch 20.59 | loss  5.65 | ppl   283.73\n",
            "| epoch  50 |  2000/ 2983 batches | lr 0.000130 | ms/batch 20.61 | loss  5.69 | ppl   295.63\n",
            "| epoch  50 |  2200/ 2983 batches | lr 0.000130 | ms/batch 20.58 | loss  5.62 | ppl   275.83\n",
            "| epoch  50 |  2400/ 2983 batches | lr 0.000130 | ms/batch 20.60 | loss  5.64 | ppl   281.31\n",
            "| epoch  50 |  2600/ 2983 batches | lr 0.000130 | ms/batch 20.56 | loss  5.69 | ppl   295.58\n",
            "| epoch  50 |  2800/ 2983 batches | lr 0.000130 | ms/batch 20.59 | loss  5.62 | ppl   276.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 63.96s | valid loss  6.34 | valid ppl   567.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000129\n",
            "| epoch  51 |   200/ 2983 batches | lr 0.000129 | ms/batch 20.70 | loss  5.68 | ppl   293.16\n",
            "| epoch  51 |   400/ 2983 batches | lr 0.000129 | ms/batch 20.60 | loss  5.69 | ppl   295.38\n",
            "| epoch  51 |   600/ 2983 batches | lr 0.000129 | ms/batch 20.64 | loss  5.60 | ppl   270.25\n",
            "| epoch  51 |   800/ 2983 batches | lr 0.000129 | ms/batch 20.66 | loss  5.65 | ppl   284.90\n",
            "| epoch  51 |  1000/ 2983 batches | lr 0.000129 | ms/batch 20.58 | loss  5.66 | ppl   287.76\n",
            "| epoch  51 |  1200/ 2983 batches | lr 0.000129 | ms/batch 20.55 | loss  5.65 | ppl   284.36\n",
            "| epoch  51 |  1400/ 2983 batches | lr 0.000129 | ms/batch 20.55 | loss  5.68 | ppl   291.79\n",
            "| epoch  51 |  1600/ 2983 batches | lr 0.000129 | ms/batch 20.56 | loss  5.73 | ppl   306.55\n",
            "| epoch  51 |  1800/ 2983 batches | lr 0.000129 | ms/batch 20.58 | loss  5.65 | ppl   283.28\n",
            "| epoch  51 |  2000/ 2983 batches | lr 0.000129 | ms/batch 20.56 | loss  5.69 | ppl   295.52\n",
            "| epoch  51 |  2200/ 2983 batches | lr 0.000129 | ms/batch 20.56 | loss  5.62 | ppl   275.52\n",
            "| epoch  51 |  2400/ 2983 batches | lr 0.000128 | ms/batch 20.59 | loss  5.63 | ppl   279.58\n",
            "| epoch  51 |  2600/ 2983 batches | lr 0.000128 | ms/batch 20.55 | loss  5.69 | ppl   295.46\n",
            "| epoch  51 |  2800/ 2983 batches | lr 0.000128 | ms/batch 20.58 | loss  5.62 | ppl   275.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 63.91s | valid loss  6.34 | valid ppl   567.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000128\n",
            "| epoch  52 |   200/ 2983 batches | lr 0.000128 | ms/batch 20.73 | loss  5.68 | ppl   293.79\n",
            "| epoch  52 |   400/ 2983 batches | lr 0.000128 | ms/batch 20.62 | loss  5.68 | ppl   294.02\n",
            "| epoch  52 |   600/ 2983 batches | lr 0.000128 | ms/batch 20.61 | loss  5.60 | ppl   269.38\n",
            "| epoch  52 |   800/ 2983 batches | lr 0.000128 | ms/batch 20.67 | loss  5.65 | ppl   285.11\n",
            "| epoch  52 |  1000/ 2983 batches | lr 0.000128 | ms/batch 20.59 | loss  5.66 | ppl   287.41\n",
            "| epoch  52 |  1200/ 2983 batches | lr 0.000128 | ms/batch 20.57 | loss  5.65 | ppl   283.72\n",
            "| epoch  52 |  1400/ 2983 batches | lr 0.000128 | ms/batch 20.58 | loss  5.67 | ppl   291.00\n",
            "| epoch  52 |  1600/ 2983 batches | lr 0.000128 | ms/batch 20.60 | loss  5.73 | ppl   306.48\n",
            "| epoch  52 |  1800/ 2983 batches | lr 0.000127 | ms/batch 20.64 | loss  5.64 | ppl   282.52\n",
            "| epoch  52 |  2000/ 2983 batches | lr 0.000127 | ms/batch 20.60 | loss  5.69 | ppl   294.71\n",
            "| epoch  52 |  2200/ 2983 batches | lr 0.000127 | ms/batch 20.61 | loss  5.62 | ppl   275.43\n",
            "| epoch  52 |  2400/ 2983 batches | lr 0.000127 | ms/batch 20.60 | loss  5.63 | ppl   279.31\n",
            "| epoch  52 |  2600/ 2983 batches | lr 0.000127 | ms/batch 20.60 | loss  5.69 | ppl   294.59\n",
            "| epoch  52 |  2800/ 2983 batches | lr 0.000127 | ms/batch 20.59 | loss  5.62 | ppl   275.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 64.05s | valid loss  6.34 | valid ppl   567.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000127\n",
            "| epoch  53 |   200/ 2983 batches | lr 0.000127 | ms/batch 20.74 | loss  5.68 | ppl   293.09\n",
            "| epoch  53 |   400/ 2983 batches | lr 0.000127 | ms/batch 20.64 | loss  5.68 | ppl   293.61\n",
            "| epoch  53 |   600/ 2983 batches | lr 0.000127 | ms/batch 20.62 | loss  5.59 | ppl   268.39\n",
            "| epoch  53 |   800/ 2983 batches | lr 0.000127 | ms/batch 20.62 | loss  5.65 | ppl   285.01\n",
            "| epoch  53 |  1000/ 2983 batches | lr 0.000127 | ms/batch 20.57 | loss  5.66 | ppl   286.93\n",
            "| epoch  53 |  1200/ 2983 batches | lr 0.000126 | ms/batch 20.64 | loss  5.65 | ppl   283.69\n",
            "| epoch  53 |  1400/ 2983 batches | lr 0.000126 | ms/batch 20.58 | loss  5.67 | ppl   290.79\n",
            "| epoch  53 |  1600/ 2983 batches | lr 0.000126 | ms/batch 20.58 | loss  5.72 | ppl   306.38\n",
            "| epoch  53 |  1800/ 2983 batches | lr 0.000126 | ms/batch 20.61 | loss  5.64 | ppl   282.30\n",
            "| epoch  53 |  2000/ 2983 batches | lr 0.000126 | ms/batch 20.67 | loss  5.68 | ppl   293.99\n",
            "| epoch  53 |  2200/ 2983 batches | lr 0.000126 | ms/batch 20.60 | loss  5.61 | ppl   274.42\n",
            "| epoch  53 |  2400/ 2983 batches | lr 0.000126 | ms/batch 20.60 | loss  5.63 | ppl   278.81\n",
            "| epoch  53 |  2600/ 2983 batches | lr 0.000126 | ms/batch 20.59 | loss  5.68 | ppl   294.18\n",
            "| epoch  53 |  2800/ 2983 batches | lr 0.000126 | ms/batch 20.58 | loss  5.62 | ppl   275.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 64.03s | valid loss  6.34 | valid ppl   567.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000126\n",
            "| epoch  54 |   200/ 2983 batches | lr 0.000126 | ms/batch 20.75 | loss  5.68 | ppl   291.88\n",
            "| epoch  54 |   400/ 2983 batches | lr 0.000126 | ms/batch 20.57 | loss  5.68 | ppl   292.47\n",
            "| epoch  54 |   600/ 2983 batches | lr 0.000125 | ms/batch 20.61 | loss  5.59 | ppl   268.14\n",
            "| epoch  54 |   800/ 2983 batches | lr 0.000125 | ms/batch 20.61 | loss  5.65 | ppl   283.91\n",
            "| epoch  54 |  1000/ 2983 batches | lr 0.000125 | ms/batch 20.62 | loss  5.65 | ppl   285.67\n",
            "| epoch  54 |  1200/ 2983 batches | lr 0.000125 | ms/batch 20.62 | loss  5.65 | ppl   283.45\n",
            "| epoch  54 |  1400/ 2983 batches | lr 0.000125 | ms/batch 20.56 | loss  5.67 | ppl   289.75\n",
            "| epoch  54 |  1600/ 2983 batches | lr 0.000125 | ms/batch 20.60 | loss  5.72 | ppl   305.47\n",
            "| epoch  54 |  1800/ 2983 batches | lr 0.000125 | ms/batch 20.61 | loss  5.64 | ppl   281.48\n",
            "| epoch  54 |  2000/ 2983 batches | lr 0.000125 | ms/batch 20.67 | loss  5.68 | ppl   293.91\n",
            "| epoch  54 |  2200/ 2983 batches | lr 0.000125 | ms/batch 20.55 | loss  5.61 | ppl   273.98\n",
            "| epoch  54 |  2400/ 2983 batches | lr 0.000125 | ms/batch 20.64 | loss  5.63 | ppl   278.73\n",
            "| epoch  54 |  2600/ 2983 batches | lr 0.000125 | ms/batch 20.61 | loss  5.68 | ppl   294.03\n",
            "| epoch  54 |  2800/ 2983 batches | lr 0.000125 | ms/batch 20.63 | loss  5.61 | ppl   274.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 64.01s | valid loss  6.34 | valid ppl   568.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  55 |   200/ 2983 batches | lr 0.000124 | ms/batch 20.68 | loss  5.68 | ppl   291.89\n",
            "| epoch  55 |   400/ 2983 batches | lr 0.000124 | ms/batch 20.61 | loss  5.68 | ppl   293.13\n",
            "| epoch  55 |   600/ 2983 batches | lr 0.000124 | ms/batch 20.59 | loss  5.59 | ppl   267.67\n",
            "| epoch  55 |   800/ 2983 batches | lr 0.000124 | ms/batch 20.59 | loss  5.64 | ppl   282.70\n",
            "| epoch  55 |  1000/ 2983 batches | lr 0.000124 | ms/batch 20.72 | loss  5.66 | ppl   286.11\n",
            "| epoch  55 |  1200/ 2983 batches | lr 0.000124 | ms/batch 20.60 | loss  5.64 | ppl   282.59\n",
            "| epoch  55 |  1400/ 2983 batches | lr 0.000124 | ms/batch 20.57 | loss  5.66 | ppl   288.43\n",
            "| epoch  55 |  1600/ 2983 batches | lr 0.000124 | ms/batch 20.58 | loss  5.72 | ppl   305.30\n",
            "| epoch  55 |  1800/ 2983 batches | lr 0.000124 | ms/batch 20.55 | loss  5.64 | ppl   281.05\n",
            "| epoch  55 |  2000/ 2983 batches | lr 0.000124 | ms/batch 20.58 | loss  5.68 | ppl   292.59\n",
            "| epoch  55 |  2200/ 2983 batches | lr 0.000124 | ms/batch 20.58 | loss  5.61 | ppl   272.99\n",
            "| epoch  55 |  2400/ 2983 batches | lr 0.000124 | ms/batch 20.57 | loss  5.63 | ppl   277.85\n",
            "| epoch  55 |  2600/ 2983 batches | lr 0.000124 | ms/batch 20.56 | loss  5.68 | ppl   293.39\n",
            "| epoch  55 |  2800/ 2983 batches | lr 0.000123 | ms/batch 20.63 | loss  5.61 | ppl   273.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 63.97s | valid loss  6.34 | valid ppl   566.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000123\n",
            "| epoch  56 |   200/ 2983 batches | lr 0.000123 | ms/batch 20.75 | loss  5.67 | ppl   291.03\n",
            "| epoch  56 |   400/ 2983 batches | lr 0.000123 | ms/batch 20.63 | loss  5.67 | ppl   291.39\n",
            "| epoch  56 |   600/ 2983 batches | lr 0.000123 | ms/batch 20.60 | loss  5.59 | ppl   267.00\n",
            "| epoch  56 |   800/ 2983 batches | lr 0.000123 | ms/batch 20.65 | loss  5.64 | ppl   282.27\n",
            "| epoch  56 |  1000/ 2983 batches | lr 0.000123 | ms/batch 20.67 | loss  5.65 | ppl   285.29\n",
            "| epoch  56 |  1200/ 2983 batches | lr 0.000123 | ms/batch 20.61 | loss  5.64 | ppl   281.64\n",
            "| epoch  56 |  1400/ 2983 batches | lr 0.000123 | ms/batch 20.65 | loss  5.67 | ppl   289.30\n",
            "| epoch  56 |  1600/ 2983 batches | lr 0.000123 | ms/batch 20.66 | loss  5.72 | ppl   303.99\n",
            "| epoch  56 |  1800/ 2983 batches | lr 0.000123 | ms/batch 20.58 | loss  5.64 | ppl   281.52\n",
            "| epoch  56 |  2000/ 2983 batches | lr 0.000123 | ms/batch 20.61 | loss  5.68 | ppl   292.68\n",
            "| epoch  56 |  2200/ 2983 batches | lr 0.000123 | ms/batch 20.57 | loss  5.61 | ppl   273.69\n",
            "| epoch  56 |  2400/ 2983 batches | lr 0.000123 | ms/batch 20.61 | loss  5.62 | ppl   277.23\n",
            "| epoch  56 |  2600/ 2983 batches | lr 0.000122 | ms/batch 20.62 | loss  5.68 | ppl   292.75\n",
            "| epoch  56 |  2800/ 2983 batches | lr 0.000122 | ms/batch 20.56 | loss  5.61 | ppl   272.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 64.04s | valid loss  6.34 | valid ppl   566.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000122\n",
            "| epoch  57 |   200/ 2983 batches | lr 0.000122 | ms/batch 20.72 | loss  5.67 | ppl   291.05\n",
            "| epoch  57 |   400/ 2983 batches | lr 0.000122 | ms/batch 20.62 | loss  5.68 | ppl   291.59\n",
            "| epoch  57 |   600/ 2983 batches | lr 0.000122 | ms/batch 20.65 | loss  5.59 | ppl   266.98\n",
            "| epoch  57 |   800/ 2983 batches | lr 0.000122 | ms/batch 20.60 | loss  5.65 | ppl   283.08\n",
            "| epoch  57 |  1000/ 2983 batches | lr 0.000122 | ms/batch 20.58 | loss  5.66 | ppl   285.97\n",
            "| epoch  57 |  1200/ 2983 batches | lr 0.000122 | ms/batch 20.60 | loss  5.64 | ppl   282.10\n",
            "| epoch  57 |  1400/ 2983 batches | lr 0.000122 | ms/batch 20.65 | loss  5.66 | ppl   288.49\n",
            "| epoch  57 |  1600/ 2983 batches | lr 0.000122 | ms/batch 20.63 | loss  5.71 | ppl   303.24\n",
            "| epoch  57 |  1800/ 2983 batches | lr 0.000122 | ms/batch 20.64 | loss  5.64 | ppl   280.53\n",
            "| epoch  57 |  2000/ 2983 batches | lr 0.000122 | ms/batch 20.55 | loss  5.68 | ppl   292.52\n",
            "| epoch  57 |  2200/ 2983 batches | lr 0.000122 | ms/batch 20.59 | loss  5.61 | ppl   272.28\n",
            "| epoch  57 |  2400/ 2983 batches | lr 0.000121 | ms/batch 20.69 | loss  5.62 | ppl   276.52\n",
            "| epoch  57 |  2600/ 2983 batches | lr 0.000121 | ms/batch 20.57 | loss  5.68 | ppl   292.08\n",
            "| epoch  57 |  2800/ 2983 batches | lr 0.000121 | ms/batch 20.62 | loss  5.61 | ppl   272.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 64.06s | valid loss  6.34 | valid ppl   566.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000121\n",
            "| epoch  58 |   200/ 2983 batches | lr 0.000121 | ms/batch 20.66 | loss  5.67 | ppl   290.66\n",
            "| epoch  58 |   400/ 2983 batches | lr 0.000121 | ms/batch 20.59 | loss  5.67 | ppl   291.10\n",
            "| epoch  58 |   600/ 2983 batches | lr 0.000121 | ms/batch 20.56 | loss  5.58 | ppl   265.59\n",
            "| epoch  58 |   800/ 2983 batches | lr 0.000121 | ms/batch 20.63 | loss  5.64 | ppl   281.81\n",
            "| epoch  58 |  1000/ 2983 batches | lr 0.000121 | ms/batch 20.61 | loss  5.65 | ppl   284.56\n",
            "| epoch  58 |  1200/ 2983 batches | lr 0.000121 | ms/batch 20.59 | loss  5.64 | ppl   281.60\n",
            "| epoch  58 |  1400/ 2983 batches | lr 0.000121 | ms/batch 20.60 | loss  5.66 | ppl   287.66\n",
            "| epoch  58 |  1600/ 2983 batches | lr 0.000121 | ms/batch 20.60 | loss  5.72 | ppl   303.43\n",
            "| epoch  58 |  1800/ 2983 batches | lr 0.000121 | ms/batch 20.65 | loss  5.64 | ppl   280.54\n",
            "| epoch  58 |  2000/ 2983 batches | lr 0.000121 | ms/batch 20.58 | loss  5.68 | ppl   292.45\n",
            "| epoch  58 |  2200/ 2983 batches | lr 0.000120 | ms/batch 20.60 | loss  5.61 | ppl   272.69\n",
            "| epoch  58 |  2400/ 2983 batches | lr 0.000120 | ms/batch 20.56 | loss  5.62 | ppl   276.35\n",
            "| epoch  58 |  2600/ 2983 batches | lr 0.000120 | ms/batch 20.67 | loss  5.68 | ppl   291.71\n",
            "| epoch  58 |  2800/ 2983 batches | lr 0.000120 | ms/batch 20.59 | loss  5.61 | ppl   272.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 63.96s | valid loss  6.34 | valid ppl   565.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000120\n",
            "| epoch  59 |   200/ 2983 batches | lr 0.000120 | ms/batch 20.74 | loss  5.67 | ppl   290.27\n",
            "| epoch  59 |   400/ 2983 batches | lr 0.000120 | ms/batch 20.65 | loss  5.67 | ppl   291.00\n",
            "| epoch  59 |   600/ 2983 batches | lr 0.000120 | ms/batch 20.56 | loss  5.58 | ppl   265.41\n",
            "| epoch  59 |   800/ 2983 batches | lr 0.000120 | ms/batch 20.57 | loss  5.64 | ppl   281.78\n",
            "| epoch  59 |  1000/ 2983 batches | lr 0.000120 | ms/batch 20.56 | loss  5.65 | ppl   284.34\n",
            "| epoch  59 |  1200/ 2983 batches | lr 0.000120 | ms/batch 20.60 | loss  5.64 | ppl   281.47\n",
            "| epoch  59 |  1400/ 2983 batches | lr 0.000120 | ms/batch 20.61 | loss  5.66 | ppl   286.95\n",
            "| epoch  59 |  1600/ 2983 batches | lr 0.000120 | ms/batch 20.56 | loss  5.71 | ppl   303.06\n",
            "| epoch  59 |  1800/ 2983 batches | lr 0.000120 | ms/batch 20.57 | loss  5.64 | ppl   280.26\n",
            "| epoch  59 |  2000/ 2983 batches | lr 0.000119 | ms/batch 20.64 | loss  5.67 | ppl   290.84\n",
            "| epoch  59 |  2200/ 2983 batches | lr 0.000119 | ms/batch 20.55 | loss  5.60 | ppl   271.70\n",
            "| epoch  59 |  2400/ 2983 batches | lr 0.000119 | ms/batch 20.60 | loss  5.62 | ppl   276.57\n",
            "| epoch  59 |  2600/ 2983 batches | lr 0.000119 | ms/batch 20.64 | loss  5.68 | ppl   291.54\n",
            "| epoch  59 |  2800/ 2983 batches | lr 0.000119 | ms/batch 20.59 | loss  5.61 | ppl   272.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 63.98s | valid loss  6.34 | valid ppl   567.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000119\n",
            "| epoch  60 |   200/ 2983 batches | lr 0.000119 | ms/batch 20.66 | loss  5.67 | ppl   289.46\n",
            "| epoch  60 |   400/ 2983 batches | lr 0.000119 | ms/batch 20.62 | loss  5.67 | ppl   290.00\n",
            "| epoch  60 |   600/ 2983 batches | lr 0.000119 | ms/batch 20.61 | loss  5.58 | ppl   265.56\n",
            "| epoch  60 |   800/ 2983 batches | lr 0.000119 | ms/batch 20.54 | loss  5.64 | ppl   280.97\n",
            "| epoch  60 |  1000/ 2983 batches | lr 0.000119 | ms/batch 20.55 | loss  5.65 | ppl   283.88\n",
            "| epoch  60 |  1200/ 2983 batches | lr 0.000119 | ms/batch 20.62 | loss  5.64 | ppl   280.62\n",
            "| epoch  60 |  1400/ 2983 batches | lr 0.000119 | ms/batch 20.59 | loss  5.66 | ppl   286.40\n",
            "| epoch  60 |  1600/ 2983 batches | lr 0.000119 | ms/batch 20.62 | loss  5.71 | ppl   302.05\n",
            "| epoch  60 |  1800/ 2983 batches | lr 0.000119 | ms/batch 20.64 | loss  5.63 | ppl   279.23\n",
            "| epoch  60 |  2000/ 2983 batches | lr 0.000118 | ms/batch 20.57 | loss  5.67 | ppl   290.95\n",
            "| epoch  60 |  2200/ 2983 batches | lr 0.000118 | ms/batch 20.54 | loss  5.61 | ppl   271.82\n",
            "| epoch  60 |  2400/ 2983 batches | lr 0.000118 | ms/batch 20.61 | loss  5.62 | ppl   275.20\n",
            "| epoch  60 |  2600/ 2983 batches | lr 0.000118 | ms/batch 20.65 | loss  5.67 | ppl   290.99\n",
            "| epoch  60 |  2800/ 2983 batches | lr 0.000118 | ms/batch 20.63 | loss  5.60 | ppl   271.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 63.96s | valid loss  6.34 | valid ppl   565.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000118\n",
            "=========================================================================================\n",
            "| End of training | test loss  6.34 | test ppl   566.51\n",
            "=========================================================================================\n",
            "\n",
            "Generating text...\n",
            "Vocabulary size: 25251\n",
            "Vocabulary size: 25251\n",
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n",
            "in the of provided <unk> essay on <unk> control . The proposal was announced after the account of characters ,\n",
            "including England 's previous choice , and bird changes ; being released after his father 's rich titles on <unk>\n",
            "and the assisted Forward cuts by summit chickens , Sports lies . In addition he actor acts as <unk> damage\n",
            "of his wife 's Jewish , within that hold Gansu withstand and matters back . — posits that Gyakuten Charles\n",
            ", thought to be and there king and an army classes , he was diverse as residence of the campaign\n",
            "since <unk> . At the turn of the Southern left coast of the United States in Margaret Stansfield married just\n",
            "the U.S. , 89 Reality of have two Air genuine or British troops no allows without diverged from the bomber\n",
            "settlement rest of the U.S. uncut . In the 2011 History of the bishops , commonly incoming final and rails\n",
            "are Cautionary pre   sections , and transverse Service Pacific interviewer . The pre   mink form of <unk> and\n",
            "Post <unk> are often Cavalry . It is also considered a common type offerings , election it was Taylor <unk>\n",
            ", which is also beautiful in nestlings and those which accepts a <unk> right following a theater . <eos> In\n",
            "<unk> . <eos> Many in Ireland Ventus head Buddhist . facility towers have white , <unk> , dressed loose ,\n",
            "and basis , particularly Poll . In the acceptable sophisticated station stretched on 1 September maintained by the National Service\n",
            "trade and knit <unk> re   knowledge of were a recording of the young style leader . In gradient 224\n",
            "two aboard <unk> who treat him as a young him . His <unk> Willie <unk> stated that he is his\n",
            "first drums in his converted dedicated to be putting up Archdiocese . <eos> Settlement reduced to a Grand Full  \n",
            "lay Bishop of <unk> and nurse   12 France divergence . pectoral a class is negative by road rocky .\n",
            "He adopted the school in black and John anti   <unk> <unk> . exceptional captures particularly those audience categorised for\n",
            "the medieval Union of video . The local Emperor telescope archery <unk> identification of rose by wind partnership of gamma\n",
            "intent , but confrontation Sun along with other press opening of the episodes of the a ten   half  \n",
            "game game . In new <unk> , Michigan clerics he got to suffered another success after Thomas late Diana was\n",
            "to Reports ; the protagonists were Trondheim , and after 15 experience , demolition began to <unk> Oxford to 1860\n",
            ". After rapidly , the forms , 1980s and Part II was during the period of won utilize cries for\n",
            "five years ; with a set in unfamiliar with the <unk> Our status votes , second in the Avalanche :\n",
            "The , 48 teams rarely were led to college chose . Toy power unit penalty was annual 34 education lot\n",
            "of land attacks . <eos> On 31 December 1914 , General eastern of around the you were siblings during the\n",
            "25 October Message , ranking only 48 Patrick , but yard edge for this , well enough wide and the\n",
            "ship had 8   who Hardwicke a essence of mm with 5   5 points in the race . At\n",
            "the same time , enforce rescued gained backyard Eduardo 250 to <unk> off a 11 hours after the battle removing\n",
            "the signal . The <unk> drive was overall when petty request Carolina , Grady was issued for a first –\n",
            "48 victory to alien Virginia Right in the Division Two Instead . <eos> <eos> = = revival = = <eos>\n",
            "<eos> The 1808 appeared in the games of this company , including Shankar Blowin death in <unk> in Franz NLF\n",
            "in Britain — ' <unk> Elizabeth . 40 years later , in Office to Grand , mad is a meeting\n",
            "of God <unk> , and later perhaps Butetown to an intent of so that they became principal peoples of the\n",
            "2010 choice of this late red style dates . <eos> <eos> = = = responsible expressions = = = <eos>\n",
            "<eos> On April 17 , <unk> , he entered his Even reputation as an Society for website clarity , which\n",
            "she included apparently \" global accounts \" the assumption that closing of his work in digital Katherine story and related\n",
            ". props were their customer and the book by Prime   century Steve prior to the marched Performance Picasso issues\n",
            "from the series expands to allow the team to prevent our psychology for the path . <unk>   Jones led\n",
            "his debut with return leader emit adorn Fall class NME Tech . His playing career occurs Beyoncé last Kaplan AC\n",
            "as a dominant advisory on the Each periods of <unk> 's attention agility as the Mesopotamia had to a point\n",
            "faction Directed because match Kirsch citation attack . In 1988 , he remained on the 3 beam , and a\n",
            "week after the rest signed with their artists such surrounded Majesty 40 % and set up with , remaining differences\n",
            "in the deities that failed <unk> over got his call into the game . While every 90 mi entire song\n",
            ", the third play Newark Italians . <eos> <eos> = western <unk> of the <unk> , and was historical enough\n",
            "consisting of the 1st / classical threat made 1769 , the because of his Force 's Such victory about the\n",
            "Near of the United States pass into only February Aerosmith , the Western African floor traces of the capital for\n",
            "header for the ran . This is important to an joints area in the 7th century „ told Only his\n",
            "head before sung by William excluded from his father who still was granted by <unk> liner <unk> and <unk> mainly\n",
            "security . During this time , he was <unk> 's hairstyles , while the sit category with greatly otherwise fruits\n"
          ]
        }
      ],
      "source": [
        "print(\"Training LSTM on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type = 'RNN_TANH', # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "    data_path = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize = 400,\n",
        "    nhid = 400,\n",
        "    nlayers = 4,\n",
        "    lr = 0.001,\n",
        "    clip = 0.25,\n",
        "    epochs = 60,\n",
        "    batch_size = 20,\n",
        "    bptt = 35,\n",
        "    dropout = 0.2,\n",
        "    tied = False,\n",
        "    nhead = 4,\n",
        "    log_interval = 200,\n",
        "    save_path='model_5.pt',\n",
        "    dry_run=False,\n",
        "    accel=True,\n",
        "    use_optimizer=True,\n",
        "    optimizer_type='AdamW',\n",
        "    weight_decay=1e-4,\n",
        "    use_betas=True,\n",
        "    betas=(0.9, 0.98),\n",
        "    use_eps=True,\n",
        "    eps=1e-9,\n",
        "    criterion=None,\n",
        "    use_label_smoothing=True,\n",
        "    label_smoothing=0.1,\n",
        "    use_warmup=True,\n",
        "    warmup_steps=4000,\n",
        "    min_freq=5,\n",
        "    seed=1111,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "# Example 2: Generate text\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model_5.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf='generated_5.txt',\n",
        "    words=1000,\n",
        "    temperature=1.0,\n",
        "    top_k=40,\n",
        "    seed=1111,\n",
        "    log_interval=100,\n",
        "    use_top_k=True,  # Use top-k sampling\n",
        "    min_freq=5,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "!cat generated_5.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RS7lFHYXrPeH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS7lFHYXrPeH",
        "outputId": "851f79f0-0b60-483d-8790-1b042672f97f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LSTM on WikiText-2...\n",
            "Using device: cuda\n",
            "Vocabulary size: 25251\n",
            "| epoch   1 |   200/ 1305 batches | lr 0.000035 | ms/batch 159.16 | loss  8.31 | ppl  4079.78\n",
            "| epoch   1 |   400/ 1305 batches | lr 0.000070 | ms/batch 156.30 | loss  7.12 | ppl  1242.29\n",
            "| epoch   1 |   600/ 1305 batches | lr 0.000105 | ms/batch 155.50 | loss  6.90 | ppl   994.90\n",
            "| epoch   1 |   800/ 1305 batches | lr 0.000140 | ms/batch 157.73 | loss  6.75 | ppl   856.99\n",
            "| epoch   1 |  1000/ 1305 batches | lr 0.000175 | ms/batch 156.96 | loss  6.65 | ppl   772.54\n",
            "| epoch   1 |  1200/ 1305 batches | lr 0.000210 | ms/batch 156.54 | loss  6.58 | ppl   720.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 217.04s | valid loss  7.01 | valid ppl  1105.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000228\n",
            "| epoch   2 |   200/ 1305 batches | lr 0.000263 | ms/batch 158.39 | loss  6.54 | ppl   693.94\n",
            "| epoch   2 |   400/ 1305 batches | lr 0.000298 | ms/batch 157.59 | loss  6.47 | ppl   643.04\n",
            "| epoch   2 |   600/ 1305 batches | lr 0.000333 | ms/batch 157.41 | loss  6.44 | ppl   624.81\n",
            "| epoch   2 |   800/ 1305 batches | lr 0.000368 | ms/batch 157.15 | loss  6.41 | ppl   608.72\n",
            "| epoch   2 |  1000/ 1305 batches | lr 0.000403 | ms/batch 157.70 | loss  6.37 | ppl   585.56\n",
            "| epoch   2 |  1200/ 1305 batches | lr 0.000438 | ms/batch 158.39 | loss  6.36 | ppl   576.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 218.00s | valid loss  7.01 | valid ppl  1107.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000456\n",
            "| epoch   3 |   200/ 1305 batches | lr 0.000491 | ms/batch 158.86 | loss  6.35 | ppl   571.98\n",
            "| epoch   3 |   400/ 1305 batches | lr 0.000526 | ms/batch 157.84 | loss  6.30 | ppl   542.16\n",
            "| epoch   3 |   600/ 1305 batches | lr 0.000561 | ms/batch 157.64 | loss  6.29 | ppl   540.34\n",
            "| epoch   3 |   800/ 1305 batches | lr 0.000596 | ms/batch 157.26 | loss  6.29 | ppl   541.75\n",
            "| epoch   3 |  1000/ 1305 batches | lr 0.000631 | ms/batch 158.05 | loss  6.26 | ppl   525.18\n",
            "| epoch   3 |  1200/ 1305 batches | lr 0.000666 | ms/batch 158.34 | loss  6.26 | ppl   524.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 218.28s | valid loss  7.04 | valid ppl  1136.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000684\n",
            "| epoch   4 |   200/ 1305 batches | lr 0.000689 | ms/batch 158.60 | loss  6.30 | ppl   542.06\n",
            "| epoch   4 |   400/ 1305 batches | lr 0.000672 | ms/batch 158.09 | loss  6.25 | ppl   516.38\n",
            "| epoch   4 |   600/ 1305 batches | lr 0.000657 | ms/batch 157.68 | loss  6.24 | ppl   511.18\n",
            "| epoch   4 |   800/ 1305 batches | lr 0.000643 | ms/batch 156.96 | loss  6.23 | ppl   510.03\n",
            "| epoch   4 |  1000/ 1305 batches | lr 0.000630 | ms/batch 157.55 | loss  6.19 | ppl   487.63\n",
            "| epoch   4 |  1200/ 1305 batches | lr 0.000618 | ms/batch 157.66 | loss  6.17 | ppl   478.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 218.01s | valid loss  7.09 | valid ppl  1198.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000611\n",
            "| epoch   5 |   200/ 1305 batches | lr 0.000600 | ms/batch 159.14 | loss  6.20 | ppl   490.57\n",
            "| epoch   5 |   400/ 1305 batches | lr 0.000589 | ms/batch 157.93 | loss  6.15 | ppl   469.10\n",
            "| epoch   5 |   600/ 1305 batches | lr 0.000579 | ms/batch 157.41 | loss  6.16 | ppl   472.96\n",
            "| epoch   5 |   800/ 1305 batches | lr 0.000569 | ms/batch 157.97 | loss  6.17 | ppl   476.38\n",
            "| epoch   5 |  1000/ 1305 batches | lr 0.000560 | ms/batch 158.03 | loss  6.14 | ppl   464.83\n",
            "| epoch   5 |  1200/ 1305 batches | lr 0.000551 | ms/batch 157.75 | loss  6.15 | ppl   467.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 218.34s | valid loss  7.17 | valid ppl  1305.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000547\n",
            "| epoch   6 |   200/ 1305 batches | lr 0.000539 | ms/batch 158.74 | loss  6.19 | ppl   486.33\n",
            "| epoch   6 |   400/ 1305 batches | lr 0.000531 | ms/batch 157.57 | loss  6.17 | ppl   479.13\n",
            "| epoch   6 |   600/ 1305 batches | lr 0.000523 | ms/batch 157.93 | loss  6.23 | ppl   508.51\n",
            "| epoch   6 |   800/ 1305 batches | lr 0.000516 | ms/batch 157.40 | loss  6.30 | ppl   544.73\n",
            "| epoch   6 |  1000/ 1305 batches | lr 0.000509 | ms/batch 157.60 | loss  6.41 | ppl   607.60\n",
            "| epoch   6 |  1200/ 1305 batches | lr 0.000503 | ms/batch 158.12 | loss  6.37 | ppl   584.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 218.14s | valid loss  7.13 | valid ppl  1248.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000499\n",
            "| epoch   7 |   200/ 1305 batches | lr 0.000493 | ms/batch 157.27 | loss  7.29 | ppl  1468.84\n",
            "| epoch   7 |   400/ 1305 batches | lr 0.000487 | ms/batch 156.59 | loss  6.68 | ppl   792.92\n",
            "| epoch   7 |   600/ 1305 batches | lr 0.000481 | ms/batch 156.11 | loss  6.59 | ppl   726.26\n",
            "| epoch   7 |   800/ 1305 batches | lr 0.000476 | ms/batch 156.05 | loss  6.56 | ppl   703.26\n",
            "| epoch   7 |  1000/ 1305 batches | lr 0.000470 | ms/batch 156.15 | loss  6.53 | ppl   682.50\n",
            "| epoch   7 |  1200/ 1305 batches | lr 0.000465 | ms/batch 155.35 | loss  6.50 | ppl   665.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 215.86s | valid loss  7.15 | valid ppl  1274.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000462\n",
            "| epoch   8 |   200/ 1305 batches | lr 0.000457 | ms/batch 156.26 | loss  6.56 | ppl   704.66\n",
            "| epoch   8 |   400/ 1305 batches | lr 0.000452 | ms/batch 154.83 | loss  6.49 | ppl   656.91\n",
            "| epoch   8 |   600/ 1305 batches | lr 0.000448 | ms/batch 154.03 | loss  6.49 | ppl   660.59\n",
            "| epoch   8 |   800/ 1305 batches | lr 0.000443 | ms/batch 153.78 | loss  6.47 | ppl   648.26\n",
            "| epoch   8 |  1000/ 1305 batches | lr 0.000439 | ms/batch 153.55 | loss  6.44 | ppl   628.74\n",
            "| epoch   8 |  1200/ 1305 batches | lr 0.000435 | ms/batch 153.14 | loss  6.43 | ppl   623.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 213.21s | valid loss  6.95 | valid ppl  1045.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000432\n",
            "| epoch   9 |   200/ 1305 batches | lr 0.000428 | ms/batch 154.08 | loss  6.46 | ppl   637.48\n",
            "| epoch   9 |   400/ 1305 batches | lr 0.000424 | ms/batch 153.13 | loss  6.41 | ppl   608.43\n",
            "| epoch   9 |   600/ 1305 batches | lr 0.000420 | ms/batch 152.74 | loss  6.42 | ppl   611.36\n",
            "| epoch   9 |   800/ 1305 batches | lr 0.000417 | ms/batch 152.78 | loss  6.40 | ppl   604.78\n",
            "| epoch   9 |  1000/ 1305 batches | lr 0.000413 | ms/batch 152.79 | loss  6.38 | ppl   591.89\n",
            "| epoch   9 |  1200/ 1305 batches | lr 0.000409 | ms/batch 152.44 | loss  6.37 | ppl   584.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 211.58s | valid loss  6.89 | valid ppl   981.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000408\n",
            "| epoch  10 |   200/ 1305 batches | lr 0.000404 | ms/batch 153.66 | loss  6.39 | ppl   598.39\n",
            "| epoch  10 |   400/ 1305 batches | lr 0.000401 | ms/batch 153.03 | loss  6.35 | ppl   571.26\n",
            "| epoch  10 |   600/ 1305 batches | lr 0.000398 | ms/batch 152.60 | loss  6.36 | ppl   575.81\n",
            "| epoch  10 |   800/ 1305 batches | lr 0.000394 | ms/batch 152.56 | loss  6.35 | ppl   570.58\n",
            "| epoch  10 |  1000/ 1305 batches | lr 0.000391 | ms/batch 152.59 | loss  6.33 | ppl   560.29\n",
            "| epoch  10 |  1200/ 1305 batches | lr 0.000388 | ms/batch 152.65 | loss  6.32 | ppl   552.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 211.44s | valid loss  6.84 | valid ppl   933.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000387\n",
            "| epoch  11 |   200/ 1305 batches | lr 0.000384 | ms/batch 153.41 | loss  6.35 | ppl   574.25\n",
            "| epoch  11 |   400/ 1305 batches | lr 0.000381 | ms/batch 152.61 | loss  6.30 | ppl   546.18\n",
            "| epoch  11 |   600/ 1305 batches | lr 0.000378 | ms/batch 152.28 | loss  6.31 | ppl   550.54\n",
            "| epoch  11 |   800/ 1305 batches | lr 0.000375 | ms/batch 152.37 | loss  6.31 | ppl   551.57\n",
            "| epoch  11 |  1000/ 1305 batches | lr 0.000373 | ms/batch 152.48 | loss  6.29 | ppl   539.76\n",
            "| epoch  11 |  1200/ 1305 batches | lr 0.000370 | ms/batch 152.33 | loss  6.28 | ppl   534.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 211.05s | valid loss  6.79 | valid ppl   890.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000369\n",
            "| epoch  12 |   200/ 1305 batches | lr 0.000366 | ms/batch 153.08 | loss  6.31 | ppl   549.15\n",
            "| epoch  12 |   400/ 1305 batches | lr 0.000364 | ms/batch 152.81 | loss  6.27 | ppl   529.30\n",
            "| epoch  12 |   600/ 1305 batches | lr 0.000361 | ms/batch 152.46 | loss  6.27 | ppl   528.31\n",
            "| epoch  12 |   800/ 1305 batches | lr 0.000359 | ms/batch 152.27 | loss  6.28 | ppl   534.93\n",
            "| epoch  12 |  1000/ 1305 batches | lr 0.000357 | ms/batch 152.31 | loss  6.24 | ppl   515.13\n",
            "| epoch  12 |  1200/ 1305 batches | lr 0.000354 | ms/batch 152.47 | loss  6.24 | ppl   514.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 211.05s | valid loss  6.77 | valid ppl   875.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000353\n",
            "| epoch  13 |   200/ 1305 batches | lr 0.000351 | ms/batch 153.56 | loss  6.27 | ppl   527.66\n",
            "| epoch  13 |   400/ 1305 batches | lr 0.000349 | ms/batch 152.74 | loss  6.24 | ppl   511.73\n",
            "| epoch  13 |   600/ 1305 batches | lr 0.000346 | ms/batch 152.31 | loss  6.24 | ppl   511.36\n",
            "| epoch  13 |   800/ 1305 batches | lr 0.000344 | ms/batch 152.46 | loss  6.24 | ppl   510.88\n",
            "| epoch  13 |  1000/ 1305 batches | lr 0.000342 | ms/batch 152.36 | loss  6.21 | ppl   498.06\n",
            "| epoch  13 |  1200/ 1305 batches | lr 0.000340 | ms/batch 152.31 | loss  6.22 | ppl   500.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 211.14s | valid loss  6.75 | valid ppl   854.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000339\n",
            "| epoch  14 |   200/ 1305 batches | lr 0.000337 | ms/batch 153.28 | loss  6.25 | ppl   516.15\n",
            "| epoch  14 |   400/ 1305 batches | lr 0.000335 | ms/batch 152.36 | loss  6.20 | ppl   492.93\n",
            "| epoch  14 |   600/ 1305 batches | lr 0.000333 | ms/batch 152.20 | loss  6.21 | ppl   499.99\n",
            "| epoch  14 |   800/ 1305 batches | lr 0.000331 | ms/batch 152.06 | loss  6.21 | ppl   498.97\n",
            "| epoch  14 |  1000/ 1305 batches | lr 0.000330 | ms/batch 152.02 | loss  6.19 | ppl   485.60\n",
            "| epoch  14 |  1200/ 1305 batches | lr 0.000328 | ms/batch 151.98 | loss  6.19 | ppl   488.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 210.70s | valid loss  6.72 | valid ppl   830.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000327\n",
            "| epoch  15 |   200/ 1305 batches | lr 0.000325 | ms/batch 153.33 | loss  6.21 | ppl   495.69\n",
            "| epoch  15 |   400/ 1305 batches | lr 0.000323 | ms/batch 152.50 | loss  6.18 | ppl   482.73\n",
            "| epoch  15 |   600/ 1305 batches | lr 0.000322 | ms/batch 151.98 | loss  6.18 | ppl   485.41\n",
            "| epoch  15 |   800/ 1305 batches | lr 0.000320 | ms/batch 152.18 | loss  6.20 | ppl   491.72\n",
            "| epoch  15 |  1000/ 1305 batches | lr 0.000318 | ms/batch 152.14 | loss  6.16 | ppl   471.46\n",
            "| epoch  15 |  1200/ 1305 batches | lr 0.000317 | ms/batch 152.05 | loss  6.17 | ppl   477.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 210.78s | valid loss  6.70 | valid ppl   813.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000316\n",
            "| epoch  16 |   200/ 1305 batches | lr 0.000314 | ms/batch 153.23 | loss  6.20 | ppl   491.55\n",
            "| epoch  16 |   400/ 1305 batches | lr 0.000313 | ms/batch 152.31 | loss  6.16 | ppl   473.95\n",
            "| epoch  16 |   600/ 1305 batches | lr 0.000311 | ms/batch 152.16 | loss  6.16 | ppl   475.04\n",
            "| epoch  16 |   800/ 1305 batches | lr 0.000309 | ms/batch 152.23 | loss  6.17 | ppl   479.73\n",
            "| epoch  16 |  1000/ 1305 batches | lr 0.000308 | ms/batch 152.17 | loss  6.14 | ppl   462.97\n",
            "| epoch  16 |  1200/ 1305 batches | lr 0.000306 | ms/batch 152.17 | loss  6.15 | ppl   467.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 210.84s | valid loss  6.70 | valid ppl   811.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000306\n",
            "| epoch  17 |   200/ 1305 batches | lr 0.000304 | ms/batch 153.19 | loss  6.17 | ppl   480.08\n",
            "| epoch  17 |   400/ 1305 batches | lr 0.000303 | ms/batch 152.45 | loss  6.14 | ppl   463.93\n",
            "| epoch  17 |   600/ 1305 batches | lr 0.000301 | ms/batch 152.18 | loss  6.15 | ppl   466.57\n",
            "| epoch  17 |   800/ 1305 batches | lr 0.000300 | ms/batch 152.15 | loss  6.15 | ppl   466.71\n",
            "| epoch  17 |  1000/ 1305 batches | lr 0.000299 | ms/batch 152.09 | loss  6.12 | ppl   452.98\n",
            "| epoch  17 |  1200/ 1305 batches | lr 0.000297 | ms/batch 152.06 | loss  6.12 | ppl   455.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 210.78s | valid loss  6.69 | valid ppl   803.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000297\n",
            "| epoch  18 |   200/ 1305 batches | lr 0.000295 | ms/batch 153.18 | loss  6.15 | ppl   469.72\n",
            "| epoch  18 |   400/ 1305 batches | lr 0.000294 | ms/batch 152.41 | loss  6.12 | ppl   456.42\n",
            "| epoch  18 |   600/ 1305 batches | lr 0.000293 | ms/batch 152.06 | loss  6.12 | ppl   454.21\n",
            "| epoch  18 |   800/ 1305 batches | lr 0.000291 | ms/batch 151.99 | loss  6.13 | ppl   458.69\n",
            "| epoch  18 |  1000/ 1305 batches | lr 0.000290 | ms/batch 152.08 | loss  6.10 | ppl   445.88\n",
            "| epoch  18 |  1200/ 1305 batches | lr 0.000289 | ms/batch 152.29 | loss  6.11 | ppl   449.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 210.76s | valid loss  6.66 | valid ppl   779.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000288\n",
            "| epoch  19 |   200/ 1305 batches | lr 0.000287 | ms/batch 153.24 | loss  6.13 | ppl   457.73\n",
            "| epoch  19 |   400/ 1305 batches | lr 0.000286 | ms/batch 152.55 | loss  6.11 | ppl   451.43\n",
            "| epoch  19 |   600/ 1305 batches | lr 0.000285 | ms/batch 152.38 | loss  6.10 | ppl   445.79\n",
            "| epoch  19 |   800/ 1305 batches | lr 0.000283 | ms/batch 152.33 | loss  6.10 | ppl   447.01\n",
            "| epoch  19 |  1000/ 1305 batches | lr 0.000282 | ms/batch 152.37 | loss  6.08 | ppl   436.99\n",
            "| epoch  19 |  1200/ 1305 batches | lr 0.000281 | ms/batch 152.26 | loss  6.09 | ppl   440.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 211.00s | valid loss  6.63 | valid ppl   758.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000281\n",
            "| epoch  20 |   200/ 1305 batches | lr 0.000279 | ms/batch 152.95 | loss  6.11 | ppl   449.07\n",
            "| epoch  20 |   400/ 1305 batches | lr 0.000278 | ms/batch 152.37 | loss  6.09 | ppl   440.13\n",
            "| epoch  20 |   600/ 1305 batches | lr 0.000277 | ms/batch 152.17 | loss  6.09 | ppl   440.38\n",
            "| epoch  20 |   800/ 1305 batches | lr 0.000276 | ms/batch 152.30 | loss  6.09 | ppl   439.58\n",
            "| epoch  20 |  1000/ 1305 batches | lr 0.000275 | ms/batch 152.16 | loss  6.07 | ppl   432.00\n",
            "| epoch  20 |  1200/ 1305 batches | lr 0.000274 | ms/batch 152.03 | loss  6.08 | ppl   435.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 210.74s | valid loss  6.65 | valid ppl   772.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000273\n",
            "| epoch  21 |   200/ 1305 batches | lr 0.000272 | ms/batch 153.22 | loss  6.09 | ppl   442.22\n",
            "| epoch  21 |   400/ 1305 batches | lr 0.000271 | ms/batch 152.21 | loss  6.07 | ppl   433.55\n",
            "| epoch  21 |   600/ 1305 batches | lr 0.000270 | ms/batch 152.10 | loss  6.07 | ppl   432.48\n",
            "| epoch  21 |   800/ 1305 batches | lr 0.000269 | ms/batch 152.12 | loss  6.07 | ppl   433.58\n",
            "| epoch  21 |  1000/ 1305 batches | lr 0.000268 | ms/batch 152.16 | loss  6.05 | ppl   422.75\n",
            "| epoch  21 |  1200/ 1305 batches | lr 0.000267 | ms/batch 152.04 | loss  6.06 | ppl   428.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 210.71s | valid loss  6.63 | valid ppl   753.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000267\n",
            "| epoch  22 |   200/ 1305 batches | lr 0.000266 | ms/batch 152.74 | loss  6.08 | ppl   436.58\n",
            "| epoch  22 |   400/ 1305 batches | lr 0.000265 | ms/batch 152.49 | loss  6.06 | ppl   427.91\n",
            "| epoch  22 |   600/ 1305 batches | lr 0.000264 | ms/batch 152.14 | loss  6.06 | ppl   426.39\n",
            "| epoch  22 |   800/ 1305 batches | lr 0.000263 | ms/batch 152.36 | loss  6.06 | ppl   428.65\n",
            "| epoch  22 |  1000/ 1305 batches | lr 0.000262 | ms/batch 152.21 | loss  6.03 | ppl   414.80\n",
            "| epoch  22 |  1200/ 1305 batches | lr 0.000261 | ms/batch 152.12 | loss  6.05 | ppl   422.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 210.77s | valid loss  6.62 | valid ppl   748.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000261\n",
            "| epoch  23 |   200/ 1305 batches | lr 0.000260 | ms/batch 152.96 | loss  6.06 | ppl   429.18\n",
            "| epoch  23 |   400/ 1305 batches | lr 0.000259 | ms/batch 152.30 | loss  6.05 | ppl   424.48\n",
            "| epoch  23 |   600/ 1305 batches | lr 0.000258 | ms/batch 152.09 | loss  6.04 | ppl   418.64\n",
            "| epoch  23 |   800/ 1305 batches | lr 0.000257 | ms/batch 152.08 | loss  6.04 | ppl   421.89\n",
            "| epoch  23 |  1000/ 1305 batches | lr 0.000256 | ms/batch 152.12 | loss  6.02 | ppl   413.15\n",
            "| epoch  23 |  1200/ 1305 batches | lr 0.000255 | ms/batch 152.16 | loss  6.03 | ppl   415.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 210.71s | valid loss  6.62 | valid ppl   753.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000255\n",
            "| epoch  24 |   200/ 1305 batches | lr 0.000254 | ms/batch 153.08 | loss  6.06 | ppl   426.25\n",
            "| epoch  24 |   400/ 1305 batches | lr 0.000253 | ms/batch 152.35 | loss  6.04 | ppl   418.51\n",
            "| epoch  24 |   600/ 1305 batches | lr 0.000252 | ms/batch 152.22 | loss  6.02 | ppl   412.65\n",
            "| epoch  24 |   800/ 1305 batches | lr 0.000252 | ms/batch 152.11 | loss  6.04 | ppl   418.07\n",
            "| epoch  24 |  1000/ 1305 batches | lr 0.000251 | ms/batch 152.02 | loss  6.01 | ppl   409.46\n",
            "| epoch  24 |  1200/ 1305 batches | lr 0.000250 | ms/batch 152.04 | loss  6.02 | ppl   412.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 210.72s | valid loss  6.60 | valid ppl   734.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  25 |   200/ 1305 batches | lr 0.000249 | ms/batch 153.09 | loss  6.04 | ppl   418.94\n",
            "| epoch  25 |   400/ 1305 batches | lr 0.000248 | ms/batch 152.40 | loss  6.02 | ppl   412.90\n",
            "| epoch  25 |   600/ 1305 batches | lr 0.000247 | ms/batch 152.16 | loss  6.02 | ppl   411.13\n",
            "| epoch  25 |   800/ 1305 batches | lr 0.000246 | ms/batch 152.18 | loss  6.03 | ppl   414.29\n",
            "| epoch  25 |  1000/ 1305 batches | lr 0.000246 | ms/batch 152.27 | loss  6.00 | ppl   401.52\n",
            "| epoch  25 |  1200/ 1305 batches | lr 0.000245 | ms/batch 152.12 | loss  6.01 | ppl   405.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 210.83s | valid loss  6.60 | valid ppl   733.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000245\n",
            "| epoch  26 |   200/ 1305 batches | lr 0.000244 | ms/batch 152.20 | loss  6.02 | ppl   413.31\n",
            "| epoch  26 |   400/ 1305 batches | lr 0.000243 | ms/batch 151.27 | loss  6.01 | ppl   407.45\n",
            "| epoch  26 |   600/ 1305 batches | lr 0.000242 | ms/batch 151.34 | loss  6.00 | ppl   403.76\n",
            "| epoch  26 |   800/ 1305 batches | lr 0.000242 | ms/batch 152.16 | loss  6.02 | ppl   411.58\n",
            "| epoch  26 |  1000/ 1305 batches | lr 0.000241 | ms/batch 152.09 | loss  5.99 | ppl   399.16\n",
            "| epoch  26 |  1200/ 1305 batches | lr 0.000240 | ms/batch 152.32 | loss  6.00 | ppl   403.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 210.22s | valid loss  6.60 | valid ppl   736.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000240\n",
            "| epoch  27 |   200/ 1305 batches | lr 0.000239 | ms/batch 152.93 | loss  6.02 | ppl   410.92\n",
            "| epoch  27 |   400/ 1305 batches | lr 0.000238 | ms/batch 152.05 | loss  6.00 | ppl   402.83\n",
            "| epoch  27 |   600/ 1305 batches | lr 0.000238 | ms/batch 152.15 | loss  6.00 | ppl   402.28\n",
            "| epoch  27 |   800/ 1305 batches | lr 0.000237 | ms/batch 152.16 | loss  6.01 | ppl   405.89\n",
            "| epoch  27 |  1000/ 1305 batches | lr 0.000236 | ms/batch 152.08 | loss  5.98 | ppl   395.36\n",
            "| epoch  27 |  1200/ 1305 batches | lr 0.000236 | ms/batch 151.98 | loss  5.99 | ppl   400.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 210.63s | valid loss  6.58 | valid ppl   722.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000235\n",
            "| epoch  28 |   200/ 1305 batches | lr 0.000235 | ms/batch 152.03 | loss  6.00 | ppl   405.04\n",
            "| epoch  28 |   400/ 1305 batches | lr 0.000234 | ms/batch 151.25 | loss  5.99 | ppl   401.33\n",
            "| epoch  28 |   600/ 1305 batches | lr 0.000233 | ms/batch 151.00 | loss  5.99 | ppl   398.07\n",
            "| epoch  28 |   800/ 1305 batches | lr 0.000233 | ms/batch 151.03 | loss  6.00 | ppl   401.99\n",
            "| epoch  28 |  1000/ 1305 batches | lr 0.000232 | ms/batch 151.04 | loss  5.97 | ppl   391.43\n",
            "| epoch  28 |  1200/ 1305 batches | lr 0.000231 | ms/batch 150.95 | loss  5.97 | ppl   393.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 209.25s | valid loss  6.58 | valid ppl   717.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000231\n",
            "| epoch  29 |   200/ 1305 batches | lr 0.000230 | ms/batch 152.95 | loss  6.00 | ppl   402.61\n",
            "| epoch  29 |   400/ 1305 batches | lr 0.000230 | ms/batch 152.28 | loss  5.98 | ppl   393.98\n",
            "| epoch  29 |   600/ 1305 batches | lr 0.000229 | ms/batch 152.12 | loss  5.98 | ppl   394.93\n",
            "| epoch  29 |   800/ 1305 batches | lr 0.000229 | ms/batch 152.12 | loss  5.99 | ppl   399.88\n",
            "| epoch  29 |  1000/ 1305 batches | lr 0.000228 | ms/batch 152.07 | loss  5.96 | ppl   386.45\n",
            "| epoch  29 |  1200/ 1305 batches | lr 0.000227 | ms/batch 151.91 | loss  5.97 | ppl   389.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 210.60s | valid loss  6.57 | valid ppl   715.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000227\n",
            "| epoch  30 |   200/ 1305 batches | lr 0.000226 | ms/batch 151.78 | loss  5.99 | ppl   399.91\n",
            "| epoch  30 |   400/ 1305 batches | lr 0.000226 | ms/batch 151.60 | loss  5.98 | ppl   393.94\n",
            "| epoch  30 |   600/ 1305 batches | lr 0.000225 | ms/batch 152.01 | loss  5.97 | ppl   390.36\n",
            "| epoch  30 |   800/ 1305 batches | lr 0.000225 | ms/batch 151.99 | loss  5.98 | ppl   395.03\n",
            "| epoch  30 |  1000/ 1305 batches | lr 0.000224 | ms/batch 152.00 | loss  5.95 | ppl   383.08\n",
            "| epoch  30 |  1200/ 1305 batches | lr 0.000224 | ms/batch 151.88 | loss  5.96 | ppl   388.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 210.22s | valid loss  6.57 | valid ppl   710.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000223\n",
            "| epoch  31 |   200/ 1305 batches | lr 0.000223 | ms/batch 151.79 | loss  5.99 | ppl   399.22\n",
            "| epoch  31 |   400/ 1305 batches | lr 0.000222 | ms/batch 151.51 | loss  5.97 | ppl   391.75\n",
            "| epoch  31 |   600/ 1305 batches | lr 0.000222 | ms/batch 152.06 | loss  5.96 | ppl   388.57\n",
            "| epoch  31 |   800/ 1305 batches | lr 0.000221 | ms/batch 152.00 | loss  5.97 | ppl   390.85\n",
            "| epoch  31 |  1000/ 1305 batches | lr 0.000220 | ms/batch 151.94 | loss  5.95 | ppl   382.93\n",
            "| epoch  31 |  1200/ 1305 batches | lr 0.000220 | ms/batch 151.82 | loss  5.95 | ppl   383.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 210.13s | valid loss  6.56 | valid ppl   706.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000220\n",
            "| epoch  32 |   200/ 1305 batches | lr 0.000219 | ms/batch 151.81 | loss  5.97 | ppl   393.09\n",
            "| epoch  32 |   400/ 1305 batches | lr 0.000219 | ms/batch 151.04 | loss  5.96 | ppl   387.75\n",
            "| epoch  32 |   600/ 1305 batches | lr 0.000218 | ms/batch 151.50 | loss  5.95 | ppl   384.43\n",
            "| epoch  32 |   800/ 1305 batches | lr 0.000217 | ms/batch 151.80 | loss  5.97 | ppl   389.87\n",
            "| epoch  32 |  1000/ 1305 batches | lr 0.000217 | ms/batch 151.74 | loss  5.93 | ppl   377.72\n",
            "| epoch  32 |  1200/ 1305 batches | lr 0.000216 | ms/batch 151.95 | loss  5.94 | ppl   381.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 209.90s | valid loss  6.55 | valid ppl   701.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000216\n",
            "| epoch  33 |   200/ 1305 batches | lr 0.000216 | ms/batch 152.49 | loss  5.97 | ppl   392.64\n",
            "| epoch  33 |   400/ 1305 batches | lr 0.000215 | ms/batch 152.09 | loss  5.95 | ppl   385.45\n",
            "| epoch  33 |   600/ 1305 batches | lr 0.000215 | ms/batch 151.89 | loss  5.94 | ppl   379.84\n",
            "| epoch  33 |   800/ 1305 batches | lr 0.000214 | ms/batch 151.91 | loss  5.96 | ppl   387.63\n",
            "| epoch  33 |  1000/ 1305 batches | lr 0.000214 | ms/batch 152.15 | loss  5.93 | ppl   374.49\n",
            "| epoch  33 |  1200/ 1305 batches | lr 0.000213 | ms/batch 151.84 | loss  5.93 | ppl   375.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 210.40s | valid loss  6.56 | valid ppl   708.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000213\n",
            "| epoch  34 |   200/ 1305 batches | lr 0.000212 | ms/batch 152.96 | loss  5.96 | ppl   386.72\n",
            "| epoch  34 |   400/ 1305 batches | lr 0.000212 | ms/batch 151.89 | loss  5.95 | ppl   381.85\n",
            "| epoch  34 |   600/ 1305 batches | lr 0.000211 | ms/batch 151.93 | loss  5.93 | ppl   375.38\n",
            "| epoch  34 |   800/ 1305 batches | lr 0.000211 | ms/batch 152.18 | loss  5.95 | ppl   383.82\n",
            "| epoch  34 |  1000/ 1305 batches | lr 0.000210 | ms/batch 151.76 | loss  5.91 | ppl   369.77\n",
            "| epoch  34 |  1200/ 1305 batches | lr 0.000210 | ms/batch 151.97 | loss  5.93 | ppl   376.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 210.51s | valid loss  6.55 | valid ppl   700.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000210\n",
            "| epoch  35 |   200/ 1305 batches | lr 0.000209 | ms/batch 152.43 | loss  5.96 | ppl   386.30\n",
            "| epoch  35 |   400/ 1305 batches | lr 0.000209 | ms/batch 151.94 | loss  5.94 | ppl   378.97\n",
            "| epoch  35 |   600/ 1305 batches | lr 0.000208 | ms/batch 151.85 | loss  5.94 | ppl   379.63\n",
            "| epoch  35 |   800/ 1305 batches | lr 0.000208 | ms/batch 151.87 | loss  5.94 | ppl   380.64\n",
            "| epoch  35 |  1000/ 1305 batches | lr 0.000207 | ms/batch 152.02 | loss  5.91 | ppl   368.78\n",
            "| epoch  35 |  1200/ 1305 batches | lr 0.000207 | ms/batch 151.87 | loss  5.92 | ppl   372.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 210.30s | valid loss  6.55 | valid ppl   697.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000207\n",
            "| epoch  36 |   200/ 1305 batches | lr 0.000206 | ms/batch 151.91 | loss  5.94 | ppl   379.67\n",
            "| epoch  36 |   400/ 1305 batches | lr 0.000206 | ms/batch 151.98 | loss  5.93 | ppl   376.71\n",
            "| epoch  36 |   600/ 1305 batches | lr 0.000205 | ms/batch 151.79 | loss  5.92 | ppl   370.57\n",
            "| epoch  36 |   800/ 1305 batches | lr 0.000205 | ms/batch 151.96 | loss  5.93 | ppl   374.45\n",
            "| epoch  36 |  1000/ 1305 batches | lr 0.000204 | ms/batch 151.84 | loss  5.90 | ppl   365.14\n",
            "| epoch  36 |  1200/ 1305 batches | lr 0.000204 | ms/batch 151.89 | loss  5.91 | ppl   369.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 210.23s | valid loss  6.55 | valid ppl   699.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000204\n",
            "| epoch  37 |   200/ 1305 batches | lr 0.000203 | ms/batch 152.88 | loss  5.93 | ppl   377.63\n",
            "| epoch  37 |   400/ 1305 batches | lr 0.000203 | ms/batch 151.94 | loss  5.93 | ppl   374.67\n",
            "| epoch  37 |   600/ 1305 batches | lr 0.000203 | ms/batch 152.05 | loss  5.91 | ppl   369.74\n",
            "| epoch  37 |   800/ 1305 batches | lr 0.000202 | ms/batch 152.01 | loss  5.93 | ppl   374.70\n",
            "| epoch  37 |  1000/ 1305 batches | lr 0.000202 | ms/batch 151.90 | loss  5.91 | ppl   367.05\n",
            "| epoch  37 |  1200/ 1305 batches | lr 0.000201 | ms/batch 151.95 | loss  5.90 | ppl   365.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 210.47s | valid loss  6.54 | valid ppl   695.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000201\n",
            "| epoch  38 |   200/ 1305 batches | lr 0.000201 | ms/batch 152.44 | loss  5.92 | ppl   374.07\n",
            "| epoch  38 |   400/ 1305 batches | lr 0.000200 | ms/batch 152.04 | loss  5.92 | ppl   370.94\n",
            "| epoch  38 |   600/ 1305 batches | lr 0.000200 | ms/batch 151.86 | loss  5.90 | ppl   365.15\n",
            "| epoch  38 |   800/ 1305 batches | lr 0.000199 | ms/batch 151.66 | loss  5.92 | ppl   373.91\n",
            "| epoch  38 |  1000/ 1305 batches | lr 0.000199 | ms/batch 151.74 | loss  5.89 | ppl   362.57\n",
            "| epoch  38 |  1200/ 1305 batches | lr 0.000199 | ms/batch 151.57 | loss  5.91 | ppl   368.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 210.19s | valid loss  6.53 | valid ppl   688.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000198\n",
            "| epoch  39 |   200/ 1305 batches | lr 0.000198 | ms/batch 152.24 | loss  5.92 | ppl   371.44\n",
            "| epoch  39 |   400/ 1305 batches | lr 0.000198 | ms/batch 152.05 | loss  5.92 | ppl   371.90\n",
            "| epoch  39 |   600/ 1305 batches | lr 0.000197 | ms/batch 151.95 | loss  5.91 | ppl   367.15\n",
            "| epoch  39 |   800/ 1305 batches | lr 0.000197 | ms/batch 151.84 | loss  5.92 | ppl   372.41\n",
            "| epoch  39 |  1000/ 1305 batches | lr 0.000196 | ms/batch 151.93 | loss  5.89 | ppl   360.16\n",
            "| epoch  39 |  1200/ 1305 batches | lr 0.000196 | ms/batch 152.03 | loss  5.89 | ppl   362.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 210.34s | valid loss  6.54 | valid ppl   689.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000196\n",
            "| epoch  40 |   200/ 1305 batches | lr 0.000195 | ms/batch 153.06 | loss  5.92 | ppl   372.51\n",
            "| epoch  40 |   400/ 1305 batches | lr 0.000195 | ms/batch 152.08 | loss  5.92 | ppl   370.94\n",
            "| epoch  40 |   600/ 1305 batches | lr 0.000195 | ms/batch 151.82 | loss  5.90 | ppl   365.66\n",
            "| epoch  40 |   800/ 1305 batches | lr 0.000194 | ms/batch 151.98 | loss  5.91 | ppl   368.48\n",
            "| epoch  40 |  1000/ 1305 batches | lr 0.000194 | ms/batch 151.79 | loss  5.89 | ppl   359.98\n",
            "| epoch  40 |  1200/ 1305 batches | lr 0.000194 | ms/batch 151.89 | loss  5.89 | ppl   361.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 210.49s | valid loss  6.54 | valid ppl   693.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000193\n",
            "| epoch  41 |   200/ 1305 batches | lr 0.000193 | ms/batch 152.67 | loss  5.92 | ppl   372.38\n",
            "| epoch  41 |   400/ 1305 batches | lr 0.000193 | ms/batch 151.68 | loss  5.91 | ppl   368.02\n",
            "| epoch  41 |   600/ 1305 batches | lr 0.000192 | ms/batch 151.85 | loss  5.90 | ppl   364.55\n",
            "| epoch  41 |   800/ 1305 batches | lr 0.000192 | ms/batch 152.02 | loss  5.91 | ppl   368.62\n",
            "| epoch  41 |  1000/ 1305 batches | lr 0.000192 | ms/batch 151.55 | loss  5.88 | ppl   356.28\n",
            "| epoch  41 |  1200/ 1305 batches | lr 0.000191 | ms/batch 151.72 | loss  5.89 | ppl   362.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 210.25s | valid loss  6.53 | valid ppl   687.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000191\n",
            "| epoch  42 |   200/ 1305 batches | lr 0.000191 | ms/batch 152.80 | loss  5.90 | ppl   366.45\n",
            "| epoch  42 |   400/ 1305 batches | lr 0.000190 | ms/batch 152.15 | loss  5.90 | ppl   363.54\n",
            "| epoch  42 |   600/ 1305 batches | lr 0.000190 | ms/batch 151.96 | loss  5.89 | ppl   360.34\n",
            "| epoch  42 |   800/ 1305 batches | lr 0.000190 | ms/batch 151.95 | loss  5.90 | ppl   365.82\n",
            "| epoch  42 |  1000/ 1305 batches | lr 0.000189 | ms/batch 152.06 | loss  5.87 | ppl   355.19\n",
            "| epoch  42 |  1200/ 1305 batches | lr 0.000189 | ms/batch 151.93 | loss  5.88 | ppl   357.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 210.50s | valid loss  6.52 | valid ppl   676.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000189\n",
            "| epoch  43 |   200/ 1305 batches | lr 0.000188 | ms/batch 152.65 | loss  5.89 | ppl   361.20\n",
            "| epoch  43 |   400/ 1305 batches | lr 0.000188 | ms/batch 152.10 | loss  5.89 | ppl   361.63\n",
            "| epoch  43 |   600/ 1305 batches | lr 0.000188 | ms/batch 151.74 | loss  5.88 | ppl   357.67\n",
            "| epoch  43 |   800/ 1305 batches | lr 0.000187 | ms/batch 151.96 | loss  5.89 | ppl   362.02\n",
            "| epoch  43 |  1000/ 1305 batches | lr 0.000187 | ms/batch 151.80 | loss  5.86 | ppl   350.68\n",
            "| epoch  43 |  1200/ 1305 batches | lr 0.000187 | ms/batch 151.97 | loss  5.88 | ppl   357.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 210.39s | valid loss  6.53 | valid ppl   684.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000186\n",
            "| epoch  44 |   200/ 1305 batches | lr 0.000186 | ms/batch 153.00 | loss  5.89 | ppl   361.56\n",
            "| epoch  44 |   400/ 1305 batches | lr 0.000186 | ms/batch 151.90 | loss  5.89 | ppl   361.79\n",
            "| epoch  44 |   600/ 1305 batches | lr 0.000186 | ms/batch 151.77 | loss  5.87 | ppl   355.62\n",
            "| epoch  44 |   800/ 1305 batches | lr 0.000185 | ms/batch 151.74 | loss  5.88 | ppl   358.68\n",
            "| epoch  44 |  1000/ 1305 batches | lr 0.000185 | ms/batch 151.90 | loss  5.86 | ppl   349.79\n",
            "| epoch  44 |  1200/ 1305 batches | lr 0.000185 | ms/batch 151.87 | loss  5.87 | ppl   354.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 210.33s | valid loss  6.53 | valid ppl   686.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000184\n",
            "| epoch  45 |   200/ 1305 batches | lr 0.000184 | ms/batch 152.96 | loss  5.89 | ppl   362.11\n",
            "| epoch  45 |   400/ 1305 batches | lr 0.000184 | ms/batch 151.95 | loss  5.89 | ppl   359.92\n",
            "| epoch  45 |   600/ 1305 batches | lr 0.000183 | ms/batch 151.84 | loss  5.87 | ppl   354.69\n",
            "| epoch  45 |   800/ 1305 batches | lr 0.000183 | ms/batch 151.89 | loss  5.88 | ppl   357.99\n",
            "| epoch  45 |  1000/ 1305 batches | lr 0.000183 | ms/batch 151.95 | loss  5.86 | ppl   349.03\n",
            "| epoch  45 |  1200/ 1305 batches | lr 0.000182 | ms/batch 151.73 | loss  5.87 | ppl   353.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 210.35s | valid loss  6.53 | valid ppl   685.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000182\n",
            "| epoch  46 |   200/ 1305 batches | lr 0.000182 | ms/batch 153.29 | loss  5.88 | ppl   358.30\n",
            "| epoch  46 |   400/ 1305 batches | lr 0.000182 | ms/batch 152.05 | loss  5.88 | ppl   358.21\n",
            "| epoch  46 |   600/ 1305 batches | lr 0.000181 | ms/batch 151.95 | loss  5.87 | ppl   353.71\n",
            "| epoch  46 |   800/ 1305 batches | lr 0.000181 | ms/batch 152.05 | loss  5.88 | ppl   358.08\n",
            "| epoch  46 |  1000/ 1305 batches | lr 0.000181 | ms/batch 152.07 | loss  5.85 | ppl   347.59\n",
            "| epoch  46 |  1200/ 1305 batches | lr 0.000180 | ms/batch 151.92 | loss  5.86 | ppl   349.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 210.60s | valid loss  6.53 | valid ppl   682.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000180\n",
            "| epoch  47 |   200/ 1305 batches | lr 0.000180 | ms/batch 152.85 | loss  5.88 | ppl   358.23\n",
            "| epoch  47 |   400/ 1305 batches | lr 0.000180 | ms/batch 151.81 | loss  5.87 | ppl   354.43\n",
            "| epoch  47 |   600/ 1305 batches | lr 0.000179 | ms/batch 151.99 | loss  5.86 | ppl   350.19\n",
            "| epoch  47 |   800/ 1305 batches | lr 0.000179 | ms/batch 152.20 | loss  5.87 | ppl   353.90\n",
            "| epoch  47 |  1000/ 1305 batches | lr 0.000179 | ms/batch 152.09 | loss  5.85 | ppl   346.71\n",
            "| epoch  47 |  1200/ 1305 batches | lr 0.000179 | ms/batch 151.83 | loss  5.86 | ppl   349.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 210.47s | valid loss  6.51 | valid ppl   673.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000178\n",
            "| epoch  48 |   200/ 1305 batches | lr 0.000178 | ms/batch 151.84 | loss  5.87 | ppl   355.96\n",
            "| epoch  48 |   400/ 1305 batches | lr 0.000178 | ms/batch 151.18 | loss  5.87 | ppl   353.41\n",
            "| epoch  48 |   600/ 1305 batches | lr 0.000178 | ms/batch 150.95 | loss  5.86 | ppl   350.65\n",
            "| epoch  48 |   800/ 1305 batches | lr 0.000177 | ms/batch 151.12 | loss  5.88 | ppl   356.95\n",
            "| epoch  48 |  1000/ 1305 batches | lr 0.000177 | ms/batch 152.08 | loss  5.84 | ppl   344.66\n",
            "| epoch  48 |  1200/ 1305 batches | lr 0.000177 | ms/batch 151.94 | loss  5.85 | ppl   346.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 209.76s | valid loss  6.52 | valid ppl   679.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000177\n",
            "| epoch  49 |   200/ 1305 batches | lr 0.000176 | ms/batch 153.18 | loss  5.88 | ppl   356.93\n",
            "| epoch  49 |   400/ 1305 batches | lr 0.000176 | ms/batch 152.08 | loss  5.86 | ppl   350.94\n",
            "| epoch  49 |   600/ 1305 batches | lr 0.000176 | ms/batch 152.02 | loss  5.86 | ppl   349.23\n",
            "| epoch  49 |   800/ 1305 batches | lr 0.000175 | ms/batch 152.17 | loss  5.86 | ppl   350.55\n",
            "| epoch  49 |  1000/ 1305 batches | lr 0.000175 | ms/batch 151.99 | loss  5.84 | ppl   343.91\n",
            "| epoch  49 |  1200/ 1305 batches | lr 0.000175 | ms/batch 152.09 | loss  5.85 | ppl   346.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 210.70s | valid loss  6.51 | valid ppl   672.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000175\n",
            "| epoch  50 |   200/ 1305 batches | lr 0.000174 | ms/batch 151.93 | loss  5.87 | ppl   354.25\n",
            "| epoch  50 |   400/ 1305 batches | lr 0.000174 | ms/batch 151.30 | loss  5.86 | ppl   350.83\n",
            "| epoch  50 |   600/ 1305 batches | lr 0.000174 | ms/batch 151.77 | loss  5.85 | ppl   347.86\n",
            "| epoch  50 |   800/ 1305 batches | lr 0.000174 | ms/batch 152.10 | loss  5.86 | ppl   350.85\n",
            "| epoch  50 |  1000/ 1305 batches | lr 0.000173 | ms/batch 151.91 | loss  5.83 | ppl   341.96\n",
            "| epoch  50 |  1200/ 1305 batches | lr 0.000173 | ms/batch 151.95 | loss  5.85 | ppl   345.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 210.13s | valid loss  6.52 | valid ppl   675.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000173\n",
            "| epoch  51 |   200/ 1305 batches | lr 0.000173 | ms/batch 153.01 | loss  5.87 | ppl   352.89\n",
            "| epoch  51 |   400/ 1305 batches | lr 0.000172 | ms/batch 152.06 | loss  5.86 | ppl   350.97\n",
            "| epoch  51 |   600/ 1305 batches | lr 0.000172 | ms/batch 152.02 | loss  5.84 | ppl   344.20\n",
            "| epoch  51 |   800/ 1305 batches | lr 0.000172 | ms/batch 152.08 | loss  5.86 | ppl   349.73\n",
            "| epoch  51 |  1000/ 1305 batches | lr 0.000172 | ms/batch 151.97 | loss  5.82 | ppl   338.65\n",
            "| epoch  51 |  1200/ 1305 batches | lr 0.000171 | ms/batch 151.93 | loss  5.84 | ppl   343.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 210.55s | valid loss  6.51 | valid ppl   668.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000171\n",
            "| epoch  52 |   200/ 1305 batches | lr 0.000171 | ms/batch 151.95 | loss  5.87 | ppl   353.06\n",
            "| epoch  52 |   400/ 1305 batches | lr 0.000171 | ms/batch 151.22 | loss  5.85 | ppl   348.83\n",
            "| epoch  52 |   600/ 1305 batches | lr 0.000170 | ms/batch 151.03 | loss  5.84 | ppl   343.52\n",
            "| epoch  52 |   800/ 1305 batches | lr 0.000170 | ms/batch 151.14 | loss  5.86 | ppl   349.29\n",
            "| epoch  52 |  1000/ 1305 batches | lr 0.000170 | ms/batch 152.17 | loss  5.82 | ppl   337.94\n",
            "| epoch  52 |  1200/ 1305 batches | lr 0.000170 | ms/batch 151.91 | loss  5.83 | ppl   341.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 209.84s | valid loss  6.51 | valid ppl   670.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000170\n",
            "| epoch  53 |   200/ 1305 batches | lr 0.000169 | ms/batch 152.88 | loss  5.85 | ppl   348.26\n",
            "| epoch  53 |   400/ 1305 batches | lr 0.000169 | ms/batch 151.85 | loss  5.85 | ppl   347.66\n",
            "| epoch  53 |   600/ 1305 batches | lr 0.000169 | ms/batch 151.95 | loss  5.84 | ppl   345.06\n",
            "| epoch  53 |   800/ 1305 batches | lr 0.000169 | ms/batch 151.75 | loss  5.86 | ppl   349.52\n",
            "| epoch  53 |  1000/ 1305 batches | lr 0.000168 | ms/batch 151.87 | loss  5.83 | ppl   339.26\n",
            "| epoch  53 |  1200/ 1305 batches | lr 0.000168 | ms/batch 151.97 | loss  5.84 | ppl   343.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 210.38s | valid loss  6.49 | valid ppl   661.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000168\n",
            "| epoch  54 |   200/ 1305 batches | lr 0.000168 | ms/batch 151.83 | loss  5.85 | ppl   346.88\n",
            "| epoch  54 |   400/ 1305 batches | lr 0.000167 | ms/batch 150.87 | loss  5.84 | ppl   344.60\n",
            "| epoch  54 |   600/ 1305 batches | lr 0.000167 | ms/batch 151.73 | loss  5.83 | ppl   341.39\n",
            "| epoch  54 |   800/ 1305 batches | lr 0.000167 | ms/batch 152.03 | loss  5.85 | ppl   346.17\n",
            "| epoch  54 |  1000/ 1305 batches | lr 0.000167 | ms/batch 151.91 | loss  5.82 | ppl   335.43\n",
            "| epoch  54 |  1200/ 1305 batches | lr 0.000167 | ms/batch 151.59 | loss  5.83 | ppl   338.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 209.92s | valid loss  6.52 | valid ppl   680.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000166\n",
            "| epoch  55 |   200/ 1305 batches | lr 0.000166 | ms/batch 152.86 | loss  5.85 | ppl   346.82\n",
            "| epoch  55 |   400/ 1305 batches | lr 0.000166 | ms/batch 151.94 | loss  5.84 | ppl   344.06\n",
            "| epoch  55 |   600/ 1305 batches | lr 0.000166 | ms/batch 151.77 | loss  5.83 | ppl   340.34\n",
            "| epoch  55 |   800/ 1305 batches | lr 0.000165 | ms/batch 152.00 | loss  5.85 | ppl   346.37\n",
            "| epoch  55 |  1000/ 1305 batches | lr 0.000165 | ms/batch 151.74 | loss  5.81 | ppl   333.28\n",
            "| epoch  55 |  1200/ 1305 batches | lr 0.000165 | ms/batch 151.97 | loss  5.83 | ppl   339.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 210.39s | valid loss  6.50 | valid ppl   664.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000165\n",
            "| epoch  56 |   200/ 1305 batches | lr 0.000165 | ms/batch 153.09 | loss  5.84 | ppl   345.39\n",
            "| epoch  56 |   400/ 1305 batches | lr 0.000164 | ms/batch 152.22 | loss  5.84 | ppl   342.46\n",
            "| epoch  56 |   600/ 1305 batches | lr 0.000164 | ms/batch 152.07 | loss  5.84 | ppl   342.21\n",
            "| epoch  56 |   800/ 1305 batches | lr 0.000164 | ms/batch 152.12 | loss  5.85 | ppl   345.97\n",
            "| epoch  56 |  1000/ 1305 batches | lr 0.000164 | ms/batch 152.08 | loss  5.81 | ppl   334.63\n",
            "| epoch  56 |  1200/ 1305 batches | lr 0.000164 | ms/batch 151.77 | loss  5.82 | ppl   336.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 210.58s | valid loss  6.49 | valid ppl   659.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000163\n",
            "| epoch  57 |   200/ 1305 batches | lr 0.000163 | ms/batch 152.95 | loss  5.84 | ppl   343.87\n",
            "| epoch  57 |   400/ 1305 batches | lr 0.000163 | ms/batch 151.83 | loss  5.84 | ppl   342.22\n",
            "| epoch  57 |   600/ 1305 batches | lr 0.000163 | ms/batch 151.53 | loss  5.82 | ppl   337.24\n",
            "| epoch  57 |   800/ 1305 batches | lr 0.000163 | ms/batch 151.91 | loss  5.84 | ppl   342.55\n",
            "| epoch  57 |  1000/ 1305 batches | lr 0.000162 | ms/batch 151.75 | loss  5.80 | ppl   330.08\n",
            "| epoch  57 |  1200/ 1305 batches | lr 0.000162 | ms/batch 151.73 | loss  5.82 | ppl   336.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 210.26s | valid loss  6.49 | valid ppl   661.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000162\n",
            "| epoch  58 |   200/ 1305 batches | lr 0.000162 | ms/batch 152.85 | loss  5.84 | ppl   344.16\n",
            "| epoch  58 |   400/ 1305 batches | lr 0.000162 | ms/batch 152.01 | loss  5.83 | ppl   339.48\n",
            "| epoch  58 |   600/ 1305 batches | lr 0.000161 | ms/batch 151.96 | loss  5.82 | ppl   336.14\n",
            "| epoch  58 |   800/ 1305 batches | lr 0.000161 | ms/batch 151.90 | loss  5.83 | ppl   341.82\n",
            "| epoch  58 |  1000/ 1305 batches | lr 0.000161 | ms/batch 151.80 | loss  5.80 | ppl   331.21\n",
            "| epoch  58 |  1200/ 1305 batches | lr 0.000161 | ms/batch 151.64 | loss  5.81 | ppl   334.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 210.36s | valid loss  6.49 | valid ppl   661.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000161\n",
            "| epoch  59 |   200/ 1305 batches | lr 0.000160 | ms/batch 152.91 | loss  5.83 | ppl   340.72\n",
            "| epoch  59 |   400/ 1305 batches | lr 0.000160 | ms/batch 151.93 | loss  5.83 | ppl   341.76\n",
            "| epoch  59 |   600/ 1305 batches | lr 0.000160 | ms/batch 151.96 | loss  5.82 | ppl   336.82\n",
            "| epoch  59 |   800/ 1305 batches | lr 0.000160 | ms/batch 152.04 | loss  5.83 | ppl   340.19\n",
            "| epoch  59 |  1000/ 1305 batches | lr 0.000160 | ms/batch 151.76 | loss  5.80 | ppl   330.21\n",
            "| epoch  59 |  1200/ 1305 batches | lr 0.000159 | ms/batch 151.68 | loss  5.81 | ppl   334.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 210.36s | valid loss  6.49 | valid ppl   659.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000159\n",
            "| epoch  60 |   200/ 1305 batches | lr 0.000159 | ms/batch 152.42 | loss  5.83 | ppl   339.87\n",
            "| epoch  60 |   400/ 1305 batches | lr 0.000159 | ms/batch 151.84 | loss  5.83 | ppl   339.09\n",
            "| epoch  60 |   600/ 1305 batches | lr 0.000159 | ms/batch 152.16 | loss  5.81 | ppl   335.11\n",
            "| epoch  60 |   800/ 1305 batches | lr 0.000158 | ms/batch 152.00 | loss  5.82 | ppl   337.67\n",
            "| epoch  60 |  1000/ 1305 batches | lr 0.000158 | ms/batch 151.85 | loss  5.79 | ppl   327.77\n",
            "| epoch  60 |  1200/ 1305 batches | lr 0.000158 | ms/batch 151.89 | loss  5.81 | ppl   333.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 210.34s | valid loss  6.49 | valid ppl   660.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000158\n",
            "=========================================================================================\n",
            "| End of training | test loss  6.52 | test ppl   677.26\n",
            "=========================================================================================\n",
            "\n",
            "Generating text...\n",
            "Vocabulary size: 25251\n",
            "Vocabulary size: 25251\n",
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n",
            "in the public Missing contrasting manufacturing celebrated later 1219 , including its new Association . O The King Party ,\n",
            "the England records extensive choice every two bird changes 1906 . The stands between the dropped rich garner mostly <unk>\n",
            "organized by the Forward cuts by summit chickens , North Carolina . In addition centres under the British blockade damage\n",
            "at him 's actions during the north coast . Gansu withstand works , such as the nose has no restaurant\n",
            "and thought to disbanding and anyway . Soon with water from the minute to diverse temperatures roosts from the campaign\n",
            "since interior . At these turn theatrical unnamed Southern left the supply of circulation remained in 2007 . The just\n",
            "country was intended to prevent photographic abilities . While largely accepted the defenders no allows without diverged for transportation bomber\n",
            ", rest of each of uncut . In 05   95 % , bishops during sixteen incoming final and rails\n",
            ", Cautionary Newman 28 sections , were transverse on his royal 8th wetlands . The mink has led to financial\n",
            "office <unk> by Westmoreland Cavalry . It was crucial only for land on offerings , election it was Taylor .\n",
            "The early <unk> . From 25 nestlings , the forward accepts acting Scenes , following they theater . <eos> In\n",
            "1966 . <eos> Reginald in France Ventus to Buddhist . facility is later able to have been dressed loose .\n",
            "Following basis the rugged Poll lists a smaller   sophisticated station stretched on begins September 20 April 2008 kakapo until\n",
            "landfall Stokes , Trinidad re   inch   were , having to be grouped on developing five methods . The\n",
            "two aboard <unk> who treat the smaller <unk> . <eos> <unk> returned and the Holloway stated that the Sennacherib would\n",
            "not reduce developing the commonwealth . Air musicians were exit Archdiocese . <eos> Settlement reduced to 8 million people .\n",
            "<eos> There are three 1944 <eos> Complete fired on divergence . pectoral consumed such is thought by fire rocky .\n",
            "asserted that the notion of other cultural schools anti niece <unk> <unk> the difference with a label audience and <unk>\n",
            "bet . One of this cycle typesetting also been telescope archery keys identification of Bryan 65   000 of gamma\n",
            "Vegas Cross . confrontation the original Activision living press and <unk> , episodes <unk> Smith , Steve <unk> has called\n",
            "the following format of new <unk> television fiction film veteran Late ( suffered approximately 1 million stones . Among the\n",
            "Saddle Reports of the protagonists were released on the On 15 are positive . A further <unk> and Cardinal 1860\n",
            "absorbed Towers rapidly , and several , Jersey and the single Bruce <unk> ( the original won ) cries for\n",
            "five in Australia with others , including unfamiliar initial Empire <unk> , weakest votes , and <unk> plain . No\n",
            "other acclaim , the rarely notorious are common stopped for his Toy language , and North annual 34 education ,\n",
            "\" What all characters Dakotas outdoor music , those of her life of Chris toy . During his version Siad\n",
            "of the poem , helpful , fresh Patrick , who yard down the scheme , which \" I permit the\n",
            "Force ' night is used Hardwicke \" essence of \" with great self Lakshmi \" in \" race \" because\n",
            "it was in them with any gained backyard characters \" . <unk> <unk> van <unk> criticism magazine wrote that \"\n",
            "effectively went not to touch it as overall . petty request Carolina , Grady noted that \" <unk> a <unk>\n",
            "\" is very \" in Right \" . Writing to Instead , Of the American team revival accompaniment , French\n",
            "development of it appeared in <unk> in c . Fox , including Shankar Blowin and Anderson , 3rd II NLF\n",
            "producers , contained Honour , <unk> . evidently ever martial organisms in these biography included , in which he Surfer\n",
            "'s books <unk> , and later explosives , to contrast to her so that they became Etty peoples of March\n",
            "2010 . <eos> In late 2008 he employed their presence of the paced where it expressions is attributed into terms\n",
            "of hell \" \" works <unk> of <unk> , <unk> Even instead of heritage and Google website clarity ( which\n",
            "she included apparently \" Politics accounts \" the assumption strongly in Boston University , the <unk> Katherine story and the\n",
            "game props who had customer Bahamas from its 39 omitted anything from Steve 's father 61 . Dylan also stated\n",
            "that \" I expands pushed doubt the mass where the our psychology was the path . <unk>   purpose the\n",
            "change , while it wrote that adorn would Ulysses understand the seasons of any cold occurs in last Kaplan AC\n",
            ". <eos> The poem features SNL regularly periods for <unk> notes that agility as the Mesopotamia of iconography and rose\n",
            "faction throughout boss . As the attack Society of 1988 Saddle directors Kirsch , corn as beam , and the\n",
            "West average <unk> ) signed decades of artists such surrounded Majesty and Wood and arrest strategically <unk> , and father\n",
            "plates . deities that failed <unk> would got in the hymn of flawless being to always 90 years . But\n",
            "the correct the play Newark , the increasing structure is unknown to the west <unk> moving north was less than\n",
            "blotches . Following this night through leaving Kennedy 1769 , the because time , they put Such its troops .\n",
            "Near the away starlings commenced in the only Helsinki Aerosmith face run events of floor on a 64 and 23\n",
            "header in the region . This is Congolese first Female joints to be separated by basket „ parasites such .\n",
            "Besides Elizabeth sung by William and daughter S. Penn <unk> Gabrielle was granted by variants liner <unk> and <unk>  \n",
            "security aspects over public concrete and Pawnee , <unk> 's hairstyles , while the sit of the greatly , which\n"
          ]
        }
      ],
      "source": [
        "print(\"Training LSTM on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type = 'RNN_TANH', # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "    data_path = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize=512,\n",
        "    nhid=2048,\n",
        "    nlayers=6,\n",
        "    lr=0.0001,\n",
        "    clip=0.25,\n",
        "    epochs=60,\n",
        "    batch_size=32,\n",
        "    bptt=50,\n",
        "    dropout=0.1,\n",
        "    tied=False,\n",
        "    nhead=8,\n",
        "    log_interval=200,\n",
        "    save_path='model_6.pt',\n",
        "    dry_run=False,\n",
        "    accel=True,\n",
        "    use_optimizer=True,\n",
        "    optimizer_type='AdamW',\n",
        "    weight_decay=1e-4,\n",
        "    use_betas=True,\n",
        "    betas=(0.9, 0.98),\n",
        "    use_eps=True,\n",
        "    eps=1e-9,\n",
        "    criterion=None,\n",
        "    use_label_smoothing=True,\n",
        "    label_smoothing=0.1,\n",
        "    use_warmup=True,\n",
        "    warmup_steps=4000,\n",
        "    min_freq=5,\n",
        "    seed=1111,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "# Example 2: Generate text\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model_6.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf='generated_6.txt',\n",
        "    words=1000,\n",
        "    temperature=1.0,\n",
        "    top_k=40,\n",
        "    seed=1111,\n",
        "    log_interval=100,\n",
        "    use_top_k=True,  # Use top-k sampling\n",
        "    min_freq=5,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "!cat generated_6.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iQM8bIC3re5Q",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQM8bIC3re5Q",
        "outputId": "0a616084-2800-445a-fee1-33298a765e46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LSTM on WikiText-2...\n",
            "Using device: cuda\n",
            "Vocabulary size: 33278\n",
            "| epoch   1 |   200/ 2983 batches | lr 0.001000 | ms/batch 22.39 | loss  8.31 | ppl  4060.93\n",
            "| epoch   1 |   400/ 2983 batches | lr 0.001000 | ms/batch 22.25 | loss  6.67 | ppl   789.42\n",
            "| epoch   1 |   600/ 2983 batches | lr 0.001000 | ms/batch 22.29 | loss  6.50 | ppl   668.20\n",
            "| epoch   1 |   800/ 2983 batches | lr 0.001000 | ms/batch 22.36 | loss  6.43 | ppl   618.20\n",
            "| epoch   1 |  1000/ 2983 batches | lr 0.001000 | ms/batch 22.38 | loss  6.34 | ppl   567.10\n",
            "| epoch   1 |  1200/ 2983 batches | lr 0.001000 | ms/batch 22.43 | loss  6.29 | ppl   537.03\n",
            "| epoch   1 |  1400/ 2983 batches | lr 0.001000 | ms/batch 22.40 | loss  6.22 | ppl   504.84\n",
            "| epoch   1 |  1600/ 2983 batches | lr 0.001000 | ms/batch 22.37 | loss  6.24 | ppl   511.03\n",
            "| epoch   1 |  1800/ 2983 batches | lr 0.001000 | ms/batch 22.37 | loss  6.15 | ppl   468.96\n",
            "| epoch   1 |  2000/ 2983 batches | lr 0.001000 | ms/batch 22.35 | loss  6.14 | ppl   461.84\n",
            "| epoch   1 |  2200/ 2983 batches | lr 0.001000 | ms/batch 22.33 | loss  6.04 | ppl   420.98\n",
            "| epoch   1 |  2400/ 2983 batches | lr 0.001000 | ms/batch 22.33 | loss  6.07 | ppl   434.03\n",
            "| epoch   1 |  2600/ 2983 batches | lr 0.001000 | ms/batch 22.30 | loss  6.07 | ppl   433.17\n",
            "| epoch   1 |  2800/ 2983 batches | lr 0.001000 | ms/batch 22.30 | loss  5.98 | ppl   396.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 68.76s | valid loss  5.76 | valid ppl   318.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   2 |   200/ 2983 batches | lr 0.001000 | ms/batch 22.42 | loss  5.69 | ppl   295.37\n",
            "| epoch   2 |   400/ 2983 batches | lr 0.001000 | ms/batch 22.32 | loss  5.59 | ppl   268.66\n",
            "| epoch   2 |   600/ 2983 batches | lr 0.001000 | ms/batch 22.33 | loss  5.49 | ppl   243.04\n",
            "| epoch   2 |   800/ 2983 batches | lr 0.001000 | ms/batch 22.37 | loss  5.53 | ppl   253.36\n",
            "| epoch   2 |  1000/ 2983 batches | lr 0.001000 | ms/batch 22.34 | loss  5.54 | ppl   255.90\n",
            "| epoch   2 |  1200/ 2983 batches | lr 0.001000 | ms/batch 22.31 | loss  5.54 | ppl   254.93\n",
            "| epoch   2 |  1400/ 2983 batches | lr 0.001000 | ms/batch 22.35 | loss  5.62 | ppl   277.14\n",
            "| epoch   2 |  1600/ 2983 batches | lr 0.001000 | ms/batch 22.34 | loss  5.62 | ppl   276.17\n",
            "| epoch   2 |  1800/ 2983 batches | lr 0.001000 | ms/batch 22.33 | loss  5.63 | ppl   277.35\n",
            "| epoch   2 |  2000/ 2983 batches | lr 0.001000 | ms/batch 22.34 | loss  5.59 | ppl   266.48\n",
            "| epoch   2 |  2200/ 2983 batches | lr 0.001000 | ms/batch 22.35 | loss  5.54 | ppl   253.61\n",
            "| epoch   2 |  2400/ 2983 batches | lr 0.001000 | ms/batch 22.34 | loss  5.55 | ppl   258.52\n",
            "| epoch   2 |  2600/ 2983 batches | lr 0.001000 | ms/batch 22.37 | loss  5.60 | ppl   270.67\n",
            "| epoch   2 |  2800/ 2983 batches | lr 0.001000 | ms/batch 22.33 | loss  5.55 | ppl   256.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 68.76s | valid loss  5.66 | valid ppl   287.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   3 |   200/ 2983 batches | lr 0.001000 | ms/batch 22.43 | loss  5.31 | ppl   201.82\n",
            "| epoch   3 |   400/ 2983 batches | lr 0.001000 | ms/batch 22.33 | loss  5.27 | ppl   194.33\n",
            "| epoch   3 |   600/ 2983 batches | lr 0.001000 | ms/batch 22.34 | loss  5.19 | ppl   179.04\n",
            "| epoch   3 |   800/ 2983 batches | lr 0.001000 | ms/batch 22.34 | loss  5.24 | ppl   187.97\n",
            "| epoch   3 |  1000/ 2983 batches | lr 0.001000 | ms/batch 22.33 | loss  5.26 | ppl   192.42\n",
            "| epoch   3 |  1200/ 2983 batches | lr 0.001000 | ms/batch 22.33 | loss  5.28 | ppl   195.90\n",
            "| epoch   3 |  1400/ 2983 batches | lr 0.001000 | ms/batch 22.33 | loss  5.30 | ppl   201.01\n",
            "| epoch   3 |  1600/ 2983 batches | lr 0.001000 | ms/batch 22.34 | loss  5.35 | ppl   210.62\n",
            "| epoch   3 |  1800/ 2983 batches | lr 0.001000 | ms/batch 22.32 | loss  5.37 | ppl   214.11\n",
            "| epoch   3 |  2000/ 2983 batches | lr 0.001000 | ms/batch 22.33 | loss  5.33 | ppl   206.46\n",
            "| epoch   3 |  2200/ 2983 batches | lr 0.001000 | ms/batch 22.35 | loss  5.28 | ppl   195.96\n",
            "| epoch   3 |  2400/ 2983 batches | lr 0.001000 | ms/batch 22.30 | loss  5.31 | ppl   201.54\n",
            "| epoch   3 |  2600/ 2983 batches | lr 0.001000 | ms/batch 22.34 | loss  5.36 | ppl   212.92\n",
            "| epoch   3 |  2800/ 2983 batches | lr 0.001000 | ms/batch 22.34 | loss  5.29 | ppl   197.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 68.74s | valid loss  5.62 | valid ppl   276.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   4 |   200/ 2983 batches | lr 0.001000 | ms/batch 22.44 | loss  5.09 | ppl   161.91\n",
            "| epoch   4 |   400/ 2983 batches | lr 0.001000 | ms/batch 22.32 | loss  5.06 | ppl   157.92\n",
            "| epoch   4 |   600/ 2983 batches | lr 0.001000 | ms/batch 22.32 | loss  4.99 | ppl   146.96\n",
            "| epoch   4 |   800/ 2983 batches | lr 0.001000 | ms/batch 22.35 | loss  5.04 | ppl   154.74\n",
            "| epoch   4 |  1000/ 2983 batches | lr 0.001000 | ms/batch 22.32 | loss  5.08 | ppl   160.37\n",
            "| epoch   4 |  1200/ 2983 batches | lr 0.001000 | ms/batch 22.36 | loss  5.10 | ppl   164.73\n",
            "| epoch   4 |  1400/ 2983 batches | lr 0.001000 | ms/batch 22.34 | loss  5.13 | ppl   169.06\n",
            "| epoch   4 |  1600/ 2983 batches | lr 0.001000 | ms/batch 22.34 | loss  5.19 | ppl   179.79\n",
            "| epoch   4 |  1800/ 2983 batches | lr 0.001000 | ms/batch 22.34 | loss  5.16 | ppl   174.75\n",
            "| epoch   4 |  2000/ 2983 batches | lr 0.001000 | ms/batch 22.32 | loss  5.16 | ppl   173.83\n",
            "| epoch   4 |  2200/ 2983 batches | lr 0.001000 | ms/batch 22.34 | loss  5.09 | ppl   162.81\n",
            "| epoch   4 |  2400/ 2983 batches | lr 0.001000 | ms/batch 22.33 | loss  5.14 | ppl   170.04\n",
            "| epoch   4 |  2600/ 2983 batches | lr 0.001000 | ms/batch 22.32 | loss  5.19 | ppl   180.20\n",
            "| epoch   4 |  2800/ 2983 batches | lr 0.001000 | ms/batch 22.34 | loss  5.15 | ppl   171.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 68.74s | valid loss  5.65 | valid ppl   283.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   5 |   200/ 2983 batches | lr 0.001000 | ms/batch 22.43 | loss  4.95 | ppl   140.95\n",
            "| epoch   5 |   400/ 2983 batches | lr 0.001000 | ms/batch 22.36 | loss  4.91 | ppl   136.29\n",
            "| epoch   5 |   600/ 2983 batches | lr 0.001000 | ms/batch 22.32 | loss  4.84 | ppl   126.75\n",
            "| epoch   5 |   800/ 2983 batches | lr 0.001000 | ms/batch 22.34 | loss  4.90 | ppl   134.51\n",
            "| epoch   5 |  1000/ 2983 batches | lr 0.001000 | ms/batch 22.32 | loss  4.94 | ppl   139.92\n",
            "| epoch   5 |  1200/ 2983 batches | lr 0.001000 | ms/batch 22.32 | loss  4.97 | ppl   144.14\n",
            "| epoch   5 |  1400/ 2983 batches | lr 0.001000 | ms/batch 22.33 | loss  4.99 | ppl   147.24\n",
            "| epoch   5 |  1600/ 2983 batches | lr 0.001000 | ms/batch 22.31 | loss  5.05 | ppl   156.36\n",
            "| epoch   5 |  1800/ 2983 batches | lr 0.001000 | ms/batch 22.32 | loss  5.07 | ppl   158.82\n",
            "| epoch   5 |  2000/ 2983 batches | lr 0.001000 | ms/batch 22.34 | loss  5.03 | ppl   153.44\n",
            "| epoch   5 |  2200/ 2983 batches | lr 0.001000 | ms/batch 22.34 | loss  4.98 | ppl   145.56\n",
            "| epoch   5 |  2400/ 2983 batches | lr 0.001000 | ms/batch 22.35 | loss  5.01 | ppl   149.85\n",
            "| epoch   5 |  2600/ 2983 batches | lr 0.001000 | ms/batch 22.35 | loss  5.07 | ppl   159.36\n",
            "| epoch   5 |  2800/ 2983 batches | lr 0.001000 | ms/batch 22.33 | loss  5.00 | ppl   148.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 68.75s | valid loss  5.68 | valid ppl   292.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   6 |   200/ 2983 batches | lr 0.001000 | ms/batch 22.44 | loss  4.84 | ppl   125.86\n",
            "| epoch   6 |   400/ 2983 batches | lr 0.001000 | ms/batch 22.35 | loss  4.82 | ppl   123.91\n",
            "| epoch   6 |   600/ 2983 batches | lr 0.001000 | ms/batch 22.33 | loss  4.73 | ppl   113.49\n",
            "| epoch   6 |   800/ 2983 batches | lr 0.001000 | ms/batch 22.32 | loss  4.80 | ppl   121.05\n",
            "| epoch   6 |  1000/ 2983 batches | lr 0.001000 | ms/batch 22.34 | loss  4.85 | ppl   127.22\n",
            "| epoch   6 |  1200/ 2983 batches | lr 0.001000 | ms/batch 22.32 | loss  4.87 | ppl   130.46\n",
            "| epoch   6 |  1400/ 2983 batches | lr 0.001000 | ms/batch 22.32 | loss  4.89 | ppl   132.98\n",
            "| epoch   6 |  1600/ 2983 batches | lr 0.001000 | ms/batch 22.32 | loss  4.95 | ppl   140.85\n",
            "| epoch   6 |  1800/ 2983 batches | lr 0.001000 | ms/batch 22.30 | loss  4.97 | ppl   143.45\n",
            "| epoch   6 |  2000/ 2983 batches | lr 0.001000 | ms/batch 22.31 | loss  4.94 | ppl   139.09\n",
            "| epoch   6 |  2200/ 2983 batches | lr 0.001000 | ms/batch 22.34 | loss  4.87 | ppl   130.08\n",
            "| epoch   6 |  2400/ 2983 batches | lr 0.001000 | ms/batch 22.34 | loss  4.91 | ppl   135.69\n",
            "| epoch   6 |  2600/ 2983 batches | lr 0.001000 | ms/batch 22.34 | loss  4.98 | ppl   145.43\n",
            "| epoch   6 |  2800/ 2983 batches | lr 0.001000 | ms/batch 22.35 | loss  4.94 | ppl   139.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 68.74s | valid loss  5.75 | valid ppl   313.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch   7 |   200/ 2983 batches | lr 0.000500 | ms/batch 22.50 | loss  4.80 | ppl   122.00\n",
            "| epoch   7 |   400/ 2983 batches | lr 0.000500 | ms/batch 22.36 | loss  4.79 | ppl   120.31\n",
            "| epoch   7 |   600/ 2983 batches | lr 0.000500 | ms/batch 22.38 | loss  4.68 | ppl   108.18\n",
            "| epoch   7 |   800/ 2983 batches | lr 0.000500 | ms/batch 22.36 | loss  4.73 | ppl   113.12\n",
            "| epoch   7 |  1000/ 2983 batches | lr 0.000500 | ms/batch 22.37 | loss  4.75 | ppl   116.00\n",
            "| epoch   7 |  1200/ 2983 batches | lr 0.000500 | ms/batch 22.39 | loss  4.77 | ppl   117.38\n",
            "| epoch   7 |  1400/ 2983 batches | lr 0.000500 | ms/batch 22.40 | loss  4.78 | ppl   119.39\n",
            "| epoch   7 |  1600/ 2983 batches | lr 0.000500 | ms/batch 22.40 | loss  4.86 | ppl   128.78\n",
            "| epoch   7 |  1800/ 2983 batches | lr 0.000500 | ms/batch 22.39 | loss  4.84 | ppl   127.10\n",
            "| epoch   7 |  2000/ 2983 batches | lr 0.000500 | ms/batch 22.37 | loss  4.84 | ppl   125.96\n",
            "| epoch   7 |  2200/ 2983 batches | lr 0.000500 | ms/batch 22.39 | loss  4.75 | ppl   115.27\n",
            "| epoch   7 |  2400/ 2983 batches | lr 0.000500 | ms/batch 22.36 | loss  4.78 | ppl   118.57\n",
            "| epoch   7 |  2600/ 2983 batches | lr 0.000500 | ms/batch 22.36 | loss  4.84 | ppl   127.09\n",
            "| epoch   7 |  2800/ 2983 batches | lr 0.000500 | ms/batch 22.39 | loss  4.80 | ppl   121.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 68.87s | valid loss  5.79 | valid ppl   328.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch   8 |   200/ 2983 batches | lr 0.000500 | ms/batch 22.43 | loss  4.71 | ppl   110.74\n",
            "| epoch   8 |   400/ 2983 batches | lr 0.000500 | ms/batch 22.33 | loss  4.70 | ppl   109.70\n",
            "| epoch   8 |   600/ 2983 batches | lr 0.000500 | ms/batch 22.32 | loss  4.60 | ppl    99.48\n",
            "| epoch   8 |   800/ 2983 batches | lr 0.000500 | ms/batch 22.34 | loss  4.65 | ppl   104.21\n",
            "| epoch   8 |  1000/ 2983 batches | lr 0.000500 | ms/batch 22.32 | loss  4.68 | ppl   107.93\n",
            "| epoch   8 |  1200/ 2983 batches | lr 0.000500 | ms/batch 22.33 | loss  4.70 | ppl   109.56\n",
            "| epoch   8 |  1400/ 2983 batches | lr 0.000500 | ms/batch 22.34 | loss  4.72 | ppl   111.80\n",
            "| epoch   8 |  1600/ 2983 batches | lr 0.000500 | ms/batch 22.31 | loss  4.80 | ppl   120.96\n",
            "| epoch   8 |  1800/ 2983 batches | lr 0.000500 | ms/batch 22.33 | loss  4.78 | ppl   118.69\n",
            "| epoch   8 |  2000/ 2983 batches | lr 0.000500 | ms/batch 22.33 | loss  4.77 | ppl   117.82\n",
            "| epoch   8 |  2200/ 2983 batches | lr 0.000500 | ms/batch 22.32 | loss  4.68 | ppl   107.56\n",
            "| epoch   8 |  2400/ 2983 batches | lr 0.000500 | ms/batch 22.32 | loss  4.71 | ppl   111.53\n",
            "| epoch   8 |  2600/ 2983 batches | lr 0.000500 | ms/batch 22.32 | loss  4.79 | ppl   119.70\n",
            "| epoch   8 |  2800/ 2983 batches | lr 0.000500 | ms/batch 22.32 | loss  4.74 | ppl   114.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 68.72s | valid loss  5.81 | valid ppl   333.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch   9 |   200/ 2983 batches | lr 0.000500 | ms/batch 22.43 | loss  4.65 | ppl   104.41\n",
            "| epoch   9 |   400/ 2983 batches | lr 0.000500 | ms/batch 22.34 | loss  4.64 | ppl   103.06\n",
            "| epoch   9 |   600/ 2983 batches | lr 0.000500 | ms/batch 22.36 | loss  4.53 | ppl    92.87\n",
            "| epoch   9 |   800/ 2983 batches | lr 0.000500 | ms/batch 22.33 | loss  4.58 | ppl    97.59\n",
            "| epoch   9 |  1000/ 2983 batches | lr 0.000500 | ms/batch 22.35 | loss  4.62 | ppl   101.35\n",
            "| epoch   9 |  1200/ 2983 batches | lr 0.000500 | ms/batch 22.33 | loss  4.64 | ppl   103.08\n",
            "| epoch   9 |  1400/ 2983 batches | lr 0.000500 | ms/batch 22.34 | loss  4.65 | ppl   105.04\n",
            "| epoch   9 |  1600/ 2983 batches | lr 0.000500 | ms/batch 22.34 | loss  4.74 | ppl   114.53\n",
            "| epoch   9 |  1800/ 2983 batches | lr 0.000500 | ms/batch 22.32 | loss  4.73 | ppl   112.88\n",
            "| epoch   9 |  2000/ 2983 batches | lr 0.000500 | ms/batch 22.32 | loss  4.71 | ppl   111.10\n",
            "| epoch   9 |  2200/ 2983 batches | lr 0.000500 | ms/batch 22.32 | loss  4.62 | ppl   101.39\n",
            "| epoch   9 |  2400/ 2983 batches | lr 0.000500 | ms/batch 22.32 | loss  4.66 | ppl   106.00\n",
            "| epoch   9 |  2600/ 2983 batches | lr 0.000500 | ms/batch 22.32 | loss  4.73 | ppl   113.33\n",
            "| epoch   9 |  2800/ 2983 batches | lr 0.000500 | ms/batch 22.31 | loss  4.69 | ppl   108.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 68.73s | valid loss  5.83 | valid ppl   341.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  10 |   200/ 2983 batches | lr 0.000250 | ms/batch 22.43 | loss  4.66 | ppl   105.85\n",
            "| epoch  10 |   400/ 2983 batches | lr 0.000250 | ms/batch 22.34 | loss  4.68 | ppl   107.71\n",
            "| epoch  10 |   600/ 2983 batches | lr 0.000250 | ms/batch 22.34 | loss  4.56 | ppl    95.38\n",
            "| epoch  10 |   800/ 2983 batches | lr 0.000250 | ms/batch 22.33 | loss  4.60 | ppl    99.27\n",
            "| epoch  10 |  1000/ 2983 batches | lr 0.000250 | ms/batch 22.32 | loss  4.61 | ppl   100.24\n",
            "| epoch  10 |  1200/ 2983 batches | lr 0.000250 | ms/batch 22.33 | loss  4.61 | ppl   100.79\n",
            "| epoch  10 |  1400/ 2983 batches | lr 0.000250 | ms/batch 22.33 | loss  4.62 | ppl   101.89\n",
            "| epoch  10 |  1600/ 2983 batches | lr 0.000250 | ms/batch 22.34 | loss  4.72 | ppl   112.53\n",
            "| epoch  10 |  1800/ 2983 batches | lr 0.000250 | ms/batch 22.36 | loss  4.67 | ppl   106.73\n",
            "| epoch  10 |  2000/ 2983 batches | lr 0.000250 | ms/batch 22.36 | loss  4.70 | ppl   109.65\n",
            "| epoch  10 |  2200/ 2983 batches | lr 0.000250 | ms/batch 22.34 | loss  4.57 | ppl    96.69\n",
            "| epoch  10 |  2400/ 2983 batches | lr 0.000250 | ms/batch 22.34 | loss  4.61 | ppl   100.06\n",
            "| epoch  10 |  2600/ 2983 batches | lr 0.000250 | ms/batch 22.33 | loss  4.67 | ppl   106.82\n",
            "| epoch  10 |  2800/ 2983 batches | lr 0.000250 | ms/batch 22.35 | loss  4.62 | ppl   101.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 68.76s | valid loss  5.84 | valid ppl   342.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  11 |   200/ 2983 batches | lr 0.000250 | ms/batch 22.45 | loss  4.61 | ppl   100.83\n",
            "| epoch  11 |   400/ 2983 batches | lr 0.000250 | ms/batch 22.33 | loss  4.63 | ppl   102.08\n",
            "| epoch  11 |   600/ 2983 batches | lr 0.000250 | ms/batch 22.33 | loss  4.50 | ppl    90.25\n",
            "| epoch  11 |   800/ 2983 batches | lr 0.000250 | ms/batch 22.36 | loss  4.55 | ppl    95.08\n",
            "| epoch  11 |  1000/ 2983 batches | lr 0.000250 | ms/batch 22.34 | loss  4.57 | ppl    96.51\n",
            "| epoch  11 |  1200/ 2983 batches | lr 0.000250 | ms/batch 22.31 | loss  4.57 | ppl    96.48\n",
            "| epoch  11 |  1400/ 2983 batches | lr 0.000250 | ms/batch 22.33 | loss  4.58 | ppl    97.98\n",
            "| epoch  11 |  1600/ 2983 batches | lr 0.000250 | ms/batch 22.32 | loss  4.68 | ppl   107.44\n",
            "| epoch  11 |  1800/ 2983 batches | lr 0.000250 | ms/batch 22.33 | loss  4.63 | ppl   102.87\n",
            "| epoch  11 |  2000/ 2983 batches | lr 0.000250 | ms/batch 22.32 | loss  4.65 | ppl   104.92\n",
            "| epoch  11 |  2200/ 2983 batches | lr 0.000250 | ms/batch 22.32 | loss  4.53 | ppl    93.03\n",
            "| epoch  11 |  2400/ 2983 batches | lr 0.000250 | ms/batch 22.32 | loss  4.57 | ppl    96.72\n",
            "| epoch  11 |  2600/ 2983 batches | lr 0.000250 | ms/batch 22.32 | loss  4.64 | ppl   103.19\n",
            "| epoch  11 |  2800/ 2983 batches | lr 0.000250 | ms/batch 22.32 | loss  4.59 | ppl    98.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 68.74s | valid loss  5.87 | valid ppl   353.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  12 |   200/ 2983 batches | lr 0.000250 | ms/batch 22.45 | loss  4.57 | ppl    96.60\n",
            "| epoch  12 |   400/ 2983 batches | lr 0.000250 | ms/batch 22.37 | loss  4.58 | ppl    97.45\n",
            "| epoch  12 |   600/ 2983 batches | lr 0.000250 | ms/batch 22.34 | loss  4.46 | ppl    86.28\n",
            "| epoch  12 |   800/ 2983 batches | lr 0.000250 | ms/batch 22.33 | loss  4.51 | ppl    90.86\n",
            "| epoch  12 |  1000/ 2983 batches | lr 0.000250 | ms/batch 22.34 | loss  4.53 | ppl    92.44\n",
            "| epoch  12 |  1200/ 2983 batches | lr 0.000250 | ms/batch 22.33 | loss  4.53 | ppl    92.69\n",
            "| epoch  12 |  1400/ 2983 batches | lr 0.000250 | ms/batch 22.33 | loss  4.55 | ppl    94.42\n",
            "| epoch  12 |  1600/ 2983 batches | lr 0.000250 | ms/batch 22.32 | loss  4.64 | ppl   103.73\n",
            "| epoch  12 |  1800/ 2983 batches | lr 0.000250 | ms/batch 22.33 | loss  4.60 | ppl    99.51\n",
            "| epoch  12 |  2000/ 2983 batches | lr 0.000250 | ms/batch 22.32 | loss  4.62 | ppl   101.48\n",
            "| epoch  12 |  2200/ 2983 batches | lr 0.000250 | ms/batch 22.34 | loss  4.50 | ppl    90.12\n",
            "| epoch  12 |  2400/ 2983 batches | lr 0.000250 | ms/batch 22.33 | loss  4.54 | ppl    93.78\n",
            "| epoch  12 |  2600/ 2983 batches | lr 0.000250 | ms/batch 22.33 | loss  4.61 | ppl   100.11\n",
            "| epoch  12 |  2800/ 2983 batches | lr 0.000250 | ms/batch 22.33 | loss  4.56 | ppl    95.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 68.75s | valid loss  5.88 | valid ppl   358.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  13 |   200/ 2983 batches | lr 0.000125 | ms/batch 22.46 | loss  4.64 | ppl   103.73\n",
            "| epoch  13 |   400/ 2983 batches | lr 0.000125 | ms/batch 22.34 | loss  4.69 | ppl   109.07\n",
            "| epoch  13 |   600/ 2983 batches | lr 0.000125 | ms/batch 22.35 | loss  4.58 | ppl    97.03\n",
            "| epoch  13 |   800/ 2983 batches | lr 0.000125 | ms/batch 22.32 | loss  4.59 | ppl    98.88\n",
            "| epoch  13 |  1000/ 2983 batches | lr 0.000125 | ms/batch 22.34 | loss  4.57 | ppl    96.98\n",
            "| epoch  13 |  1200/ 2983 batches | lr 0.000125 | ms/batch 22.33 | loss  4.57 | ppl    96.42\n",
            "| epoch  13 |  1400/ 2983 batches | lr 0.000125 | ms/batch 22.35 | loss  4.56 | ppl    95.85\n",
            "| epoch  13 |  1600/ 2983 batches | lr 0.000125 | ms/batch 22.33 | loss  4.67 | ppl   106.57\n",
            "| epoch  13 |  1800/ 2983 batches | lr 0.000125 | ms/batch 22.33 | loss  4.59 | ppl    98.23\n",
            "| epoch  13 |  2000/ 2983 batches | lr 0.000125 | ms/batch 22.31 | loss  4.64 | ppl   103.20\n",
            "| epoch  13 |  2200/ 2983 batches | lr 0.000125 | ms/batch 22.33 | loss  4.49 | ppl    89.43\n",
            "| epoch  13 |  2400/ 2983 batches | lr 0.000125 | ms/batch 22.33 | loss  4.53 | ppl    92.97\n",
            "| epoch  13 |  2600/ 2983 batches | lr 0.000125 | ms/batch 22.33 | loss  4.59 | ppl    98.45\n",
            "| epoch  13 |  2800/ 2983 batches | lr 0.000125 | ms/batch 22.33 | loss  4.53 | ppl    93.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 68.74s | valid loss  5.82 | valid ppl   337.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  14 |   200/ 2983 batches | lr 0.000125 | ms/batch 22.44 | loss  4.64 | ppl   103.40\n",
            "| epoch  14 |   400/ 2983 batches | lr 0.000125 | ms/batch 22.34 | loss  4.64 | ppl   103.53\n",
            "| epoch  14 |   600/ 2983 batches | lr 0.000125 | ms/batch 22.35 | loss  4.52 | ppl    91.87\n",
            "| epoch  14 |   800/ 2983 batches | lr 0.000125 | ms/batch 22.33 | loss  4.56 | ppl    95.82\n",
            "| epoch  14 |  1000/ 2983 batches | lr 0.000125 | ms/batch 22.35 | loss  4.54 | ppl    93.70\n",
            "| epoch  14 |  1200/ 2983 batches | lr 0.000125 | ms/batch 22.32 | loss  4.54 | ppl    94.05\n",
            "| epoch  14 |  1400/ 2983 batches | lr 0.000125 | ms/batch 22.34 | loss  4.54 | ppl    93.59\n",
            "| epoch  14 |  1600/ 2983 batches | lr 0.000125 | ms/batch 22.31 | loss  4.64 | ppl   103.37\n",
            "| epoch  14 |  1800/ 2983 batches | lr 0.000125 | ms/batch 22.33 | loss  4.57 | ppl    96.27\n",
            "| epoch  14 |  2000/ 2983 batches | lr 0.000125 | ms/batch 22.32 | loss  4.61 | ppl   100.06\n",
            "| epoch  14 |  2200/ 2983 batches | lr 0.000125 | ms/batch 22.33 | loss  4.48 | ppl    87.93\n",
            "| epoch  14 |  2400/ 2983 batches | lr 0.000125 | ms/batch 22.32 | loss  4.51 | ppl    91.30\n",
            "| epoch  14 |  2600/ 2983 batches | lr 0.000125 | ms/batch 22.33 | loss  4.58 | ppl    97.13\n",
            "| epoch  14 |  2800/ 2983 batches | lr 0.000125 | ms/batch 22.33 | loss  4.52 | ppl    91.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 68.73s | valid loss  5.84 | valid ppl   343.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  15 |   200/ 2983 batches | lr 0.000125 | ms/batch 22.43 | loss  4.60 | ppl    99.39\n",
            "| epoch  15 |   400/ 2983 batches | lr 0.000125 | ms/batch 22.35 | loss  4.60 | ppl    99.27\n",
            "| epoch  15 |   600/ 2983 batches | lr 0.000125 | ms/batch 22.33 | loss  4.48 | ppl    87.91\n",
            "| epoch  15 |   800/ 2983 batches | lr 0.000125 | ms/batch 22.32 | loss  4.52 | ppl    92.22\n",
            "| epoch  15 |  1000/ 2983 batches | lr 0.000125 | ms/batch 22.34 | loss  4.51 | ppl    90.78\n",
            "| epoch  15 |  1200/ 2983 batches | lr 0.000125 | ms/batch 22.34 | loss  4.51 | ppl    91.20\n",
            "| epoch  15 |  1400/ 2983 batches | lr 0.000125 | ms/batch 22.32 | loss  4.52 | ppl    91.40\n",
            "| epoch  15 |  1600/ 2983 batches | lr 0.000125 | ms/batch 22.32 | loss  4.61 | ppl   100.73\n",
            "| epoch  15 |  1800/ 2983 batches | lr 0.000125 | ms/batch 22.31 | loss  4.55 | ppl    94.90\n",
            "| epoch  15 |  2000/ 2983 batches | lr 0.000125 | ms/batch 22.32 | loss  4.58 | ppl    97.67\n",
            "| epoch  15 |  2200/ 2983 batches | lr 0.000125 | ms/batch 22.32 | loss  4.46 | ppl    86.12\n",
            "| epoch  15 |  2400/ 2983 batches | lr 0.000125 | ms/batch 22.33 | loss  4.49 | ppl    88.80\n",
            "| epoch  15 |  2600/ 2983 batches | lr 0.000125 | ms/batch 22.34 | loss  4.55 | ppl    94.96\n",
            "| epoch  15 |  2800/ 2983 batches | lr 0.000125 | ms/batch 22.32 | loss  4.50 | ppl    89.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 68.72s | valid loss  5.83 | valid ppl   340.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  16 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.42 | loss  4.77 | ppl   117.80\n",
            "| epoch  16 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.32 | loss  4.91 | ppl   135.26\n",
            "| epoch  16 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.36 | loss  4.62 | ppl   101.30\n",
            "| epoch  16 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.31 | loss  4.63 | ppl   102.11\n",
            "| epoch  16 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.33 | loss  4.63 | ppl   102.79\n",
            "| epoch  16 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.34 | loss  4.62 | ppl   101.73\n",
            "| epoch  16 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.33 | loss  4.64 | ppl   103.54\n",
            "| epoch  16 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.34 | loss  4.72 | ppl   112.13\n",
            "| epoch  16 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.36 | loss  4.57 | ppl    96.07\n",
            "| epoch  16 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.33 | loss  4.62 | ppl   101.18\n",
            "| epoch  16 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.35 | loss  4.51 | ppl    90.95\n",
            "| epoch  16 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.33 | loss  4.50 | ppl    90.42\n",
            "| epoch  16 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.33 | loss  4.58 | ppl    97.53\n",
            "| epoch  16 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.35 | loss  4.50 | ppl    89.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 68.75s | valid loss  5.79 | valid ppl   327.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  17 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.48 | loss  4.75 | ppl   115.35\n",
            "| epoch  17 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.34 | loss  4.85 | ppl   127.27\n",
            "| epoch  17 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.32 | loss  4.58 | ppl    97.22\n",
            "| epoch  17 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.34 | loss  4.60 | ppl    99.66\n",
            "| epoch  17 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.33 | loss  4.62 | ppl   101.25\n",
            "| epoch  17 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.34 | loss  4.58 | ppl    97.56\n",
            "| epoch  17 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.34 | loss  4.63 | ppl   102.32\n",
            "| epoch  17 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.35 | loss  4.68 | ppl   107.49\n",
            "| epoch  17 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.38 | loss  4.55 | ppl    94.41\n",
            "| epoch  17 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.34 | loss  4.60 | ppl    99.98\n",
            "| epoch  17 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.34 | loss  4.49 | ppl    89.24\n",
            "| epoch  17 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.34 | loss  4.50 | ppl    89.88\n",
            "| epoch  17 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.33 | loss  4.57 | ppl    96.81\n",
            "| epoch  17 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.35 | loss  4.49 | ppl    88.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 68.78s | valid loss  5.78 | valid ppl   322.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  18 |   200/ 2983 batches | lr 0.000063 | ms/batch 22.44 | loss  4.71 | ppl   111.12\n",
            "| epoch  18 |   400/ 2983 batches | lr 0.000063 | ms/batch 22.33 | loss  4.74 | ppl   114.57\n",
            "| epoch  18 |   600/ 2983 batches | lr 0.000063 | ms/batch 22.34 | loss  4.55 | ppl    94.34\n",
            "| epoch  18 |   800/ 2983 batches | lr 0.000063 | ms/batch 22.34 | loss  4.57 | ppl    96.81\n",
            "| epoch  18 |  1000/ 2983 batches | lr 0.000063 | ms/batch 22.33 | loss  4.57 | ppl    96.28\n",
            "| epoch  18 |  1200/ 2983 batches | lr 0.000063 | ms/batch 22.32 | loss  4.56 | ppl    95.28\n",
            "| epoch  18 |  1400/ 2983 batches | lr 0.000063 | ms/batch 22.35 | loss  4.60 | ppl    99.19\n",
            "| epoch  18 |  1600/ 2983 batches | lr 0.000063 | ms/batch 22.35 | loss  4.65 | ppl   104.76\n",
            "| epoch  18 |  1800/ 2983 batches | lr 0.000063 | ms/batch 22.33 | loss  4.53 | ppl    92.83\n",
            "| epoch  18 |  2000/ 2983 batches | lr 0.000063 | ms/batch 22.37 | loss  4.59 | ppl    98.05\n",
            "| epoch  18 |  2200/ 2983 batches | lr 0.000063 | ms/batch 22.35 | loss  4.47 | ppl    87.58\n",
            "| epoch  18 |  2400/ 2983 batches | lr 0.000063 | ms/batch 22.34 | loss  4.49 | ppl    89.04\n",
            "| epoch  18 |  2600/ 2983 batches | lr 0.000063 | ms/batch 22.35 | loss  4.56 | ppl    95.47\n",
            "| epoch  18 |  2800/ 2983 batches | lr 0.000063 | ms/batch 22.34 | loss  4.48 | ppl    87.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 68.77s | valid loss  5.78 | valid ppl   325.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  19 |   200/ 2983 batches | lr 0.000031 | ms/batch 22.46 | loss  4.79 | ppl   119.95\n",
            "| epoch  19 |   400/ 2983 batches | lr 0.000031 | ms/batch 22.35 | loss  5.16 | ppl   173.43\n",
            "| epoch  19 |   600/ 2983 batches | lr 0.000031 | ms/batch 22.33 | loss  4.69 | ppl   108.36\n",
            "| epoch  19 |   800/ 2983 batches | lr 0.000031 | ms/batch 22.34 | loss  4.67 | ppl   106.18\n",
            "| epoch  19 |  1000/ 2983 batches | lr 0.000031 | ms/batch 22.34 | loss  4.70 | ppl   109.81\n",
            "| epoch  19 |  1200/ 2983 batches | lr 0.000031 | ms/batch 22.35 | loss  4.72 | ppl   112.23\n",
            "| epoch  19 |  1400/ 2983 batches | lr 0.000031 | ms/batch 22.32 | loss  4.89 | ppl   132.92\n",
            "| epoch  19 |  1600/ 2983 batches | lr 0.000031 | ms/batch 22.37 | loss  4.86 | ppl   128.50\n",
            "| epoch  19 |  1800/ 2983 batches | lr 0.000031 | ms/batch 22.33 | loss  4.57 | ppl    96.93\n",
            "| epoch  19 |  2000/ 2983 batches | lr 0.000031 | ms/batch 22.32 | loss  4.64 | ppl   103.77\n",
            "| epoch  19 |  2200/ 2983 batches | lr 0.000031 | ms/batch 22.35 | loss  4.60 | ppl    99.03\n",
            "| epoch  19 |  2400/ 2983 batches | lr 0.000031 | ms/batch 22.33 | loss  4.57 | ppl    96.34\n",
            "| epoch  19 |  2600/ 2983 batches | lr 0.000031 | ms/batch 22.33 | loss  4.62 | ppl   101.31\n",
            "| epoch  19 |  2800/ 2983 batches | lr 0.000031 | ms/batch 22.32 | loss  4.51 | ppl    90.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 68.76s | valid loss  5.71 | valid ppl   303.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  20 |   200/ 2983 batches | lr 0.000031 | ms/batch 22.45 | loss  4.78 | ppl   118.64\n",
            "| epoch  20 |   400/ 2983 batches | lr 0.000031 | ms/batch 22.34 | loss  4.99 | ppl   146.47\n",
            "| epoch  20 |   600/ 2983 batches | lr 0.000031 | ms/batch 22.33 | loss  4.59 | ppl    98.96\n",
            "| epoch  20 |   800/ 2983 batches | lr 0.000031 | ms/batch 22.34 | loss  4.66 | ppl   105.98\n",
            "| epoch  20 |  1000/ 2983 batches | lr 0.000031 | ms/batch 22.33 | loss  4.69 | ppl   108.55\n",
            "| epoch  20 |  1200/ 2983 batches | lr 0.000031 | ms/batch 22.34 | loss  4.66 | ppl   105.46\n",
            "| epoch  20 |  1400/ 2983 batches | lr 0.000031 | ms/batch 22.35 | loss  4.88 | ppl   131.41\n",
            "| epoch  20 |  1600/ 2983 batches | lr 0.000031 | ms/batch 22.35 | loss  4.83 | ppl   125.40\n",
            "| epoch  20 |  1800/ 2983 batches | lr 0.000031 | ms/batch 22.37 | loss  4.55 | ppl    94.86\n",
            "| epoch  20 |  2000/ 2983 batches | lr 0.000031 | ms/batch 22.34 | loss  4.64 | ppl   103.42\n",
            "| epoch  20 |  2200/ 2983 batches | lr 0.000031 | ms/batch 22.33 | loss  4.59 | ppl    98.08\n",
            "| epoch  20 |  2400/ 2983 batches | lr 0.000031 | ms/batch 22.35 | loss  4.55 | ppl    94.40\n",
            "| epoch  20 |  2600/ 2983 batches | lr 0.000031 | ms/batch 22.33 | loss  4.61 | ppl   100.31\n",
            "| epoch  20 |  2800/ 2983 batches | lr 0.000031 | ms/batch 22.36 | loss  4.50 | ppl    90.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 68.78s | valid loss  5.71 | valid ppl   303.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  21 |   200/ 2983 batches | lr 0.000031 | ms/batch 22.43 | loss  4.75 | ppl   115.53\n",
            "| epoch  21 |   400/ 2983 batches | lr 0.000031 | ms/batch 22.37 | loss  4.87 | ppl   130.37\n",
            "| epoch  21 |   600/ 2983 batches | lr 0.000031 | ms/batch 22.34 | loss  4.59 | ppl    98.63\n",
            "| epoch  21 |   800/ 2983 batches | lr 0.000031 | ms/batch 22.34 | loss  4.63 | ppl   102.25\n",
            "| epoch  21 |  1000/ 2983 batches | lr 0.000031 | ms/batch 22.35 | loss  4.66 | ppl   105.94\n",
            "| epoch  21 |  1200/ 2983 batches | lr 0.000031 | ms/batch 22.34 | loss  4.62 | ppl   101.08\n",
            "| epoch  21 |  1400/ 2983 batches | lr 0.000031 | ms/batch 22.37 | loss  4.82 | ppl   124.37\n",
            "| epoch  21 |  1600/ 2983 batches | lr 0.000031 | ms/batch 22.34 | loss  4.84 | ppl   126.50\n",
            "| epoch  21 |  1800/ 2983 batches | lr 0.000031 | ms/batch 22.33 | loss  4.54 | ppl    93.97\n",
            "| epoch  21 |  2000/ 2983 batches | lr 0.000031 | ms/batch 22.37 | loss  4.62 | ppl   101.43\n",
            "| epoch  21 |  2200/ 2983 batches | lr 0.000031 | ms/batch 22.34 | loss  4.55 | ppl    94.24\n",
            "| epoch  21 |  2400/ 2983 batches | lr 0.000031 | ms/batch 22.33 | loss  4.54 | ppl    93.93\n",
            "| epoch  21 |  2600/ 2983 batches | lr 0.000031 | ms/batch 22.33 | loss  4.59 | ppl    98.98\n",
            "| epoch  21 |  2800/ 2983 batches | lr 0.000031 | ms/batch 22.33 | loss  4.49 | ppl    89.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 68.78s | valid loss  5.72 | valid ppl   306.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  22 |   200/ 2983 batches | lr 0.000016 | ms/batch 22.47 | loss  4.93 | ppl   138.30\n",
            "| epoch  22 |   400/ 2983 batches | lr 0.000016 | ms/batch 22.35 | loss  5.12 | ppl   166.95\n",
            "| epoch  22 |   600/ 2983 batches | lr 0.000016 | ms/batch 22.37 | loss  4.77 | ppl   117.76\n",
            "| epoch  22 |   800/ 2983 batches | lr 0.000016 | ms/batch 22.35 | loss  4.71 | ppl   111.55\n",
            "| epoch  22 |  1000/ 2983 batches | lr 0.000016 | ms/batch 22.36 | loss  4.74 | ppl   114.09\n",
            "| epoch  22 |  1200/ 2983 batches | lr 0.000016 | ms/batch 22.34 | loss  4.77 | ppl   118.26\n",
            "| epoch  22 |  1400/ 2983 batches | lr 0.000016 | ms/batch 22.33 | loss  4.88 | ppl   131.82\n",
            "| epoch  22 |  1600/ 2983 batches | lr 0.000016 | ms/batch 22.37 | loss  4.88 | ppl   131.30\n",
            "| epoch  22 |  1800/ 2983 batches | lr 0.000016 | ms/batch 22.35 | loss  4.60 | ppl    99.15\n",
            "| epoch  22 |  2000/ 2983 batches | lr 0.000016 | ms/batch 22.35 | loss  4.68 | ppl   108.00\n",
            "| epoch  22 |  2200/ 2983 batches | lr 0.000016 | ms/batch 22.37 | loss  4.66 | ppl   105.43\n",
            "| epoch  22 |  2400/ 2983 batches | lr 0.000016 | ms/batch 22.35 | loss  4.69 | ppl   108.37\n",
            "| epoch  22 |  2600/ 2983 batches | lr 0.000016 | ms/batch 22.36 | loss  4.73 | ppl   113.03\n",
            "| epoch  22 |  2800/ 2983 batches | lr 0.000016 | ms/batch 22.36 | loss  4.59 | ppl    98.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 68.81s | valid loss  5.67 | valid ppl   289.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  23 |   200/ 2983 batches | lr 0.000016 | ms/batch 22.45 | loss  4.87 | ppl   129.80\n",
            "| epoch  23 |   400/ 2983 batches | lr 0.000016 | ms/batch 22.35 | loss  5.04 | ppl   154.72\n",
            "| epoch  23 |   600/ 2983 batches | lr 0.000016 | ms/batch 22.36 | loss  4.64 | ppl   103.76\n",
            "| epoch  23 |   800/ 2983 batches | lr 0.000016 | ms/batch 22.36 | loss  4.70 | ppl   109.49\n",
            "| epoch  23 |  1000/ 2983 batches | lr 0.000016 | ms/batch 22.36 | loss  4.71 | ppl   110.62\n",
            "| epoch  23 |  1200/ 2983 batches | lr 0.000016 | ms/batch 22.36 | loss  4.71 | ppl   110.61\n",
            "| epoch  23 |  1400/ 2983 batches | lr 0.000016 | ms/batch 22.33 | loss  4.85 | ppl   127.81\n",
            "| epoch  23 |  1600/ 2983 batches | lr 0.000016 | ms/batch 22.34 | loss  4.86 | ppl   128.67\n",
            "| epoch  23 |  1800/ 2983 batches | lr 0.000016 | ms/batch 22.35 | loss  4.57 | ppl    96.68\n",
            "| epoch  23 |  2000/ 2983 batches | lr 0.000016 | ms/batch 22.34 | loss  4.69 | ppl   108.92\n",
            "| epoch  23 |  2200/ 2983 batches | lr 0.000016 | ms/batch 22.35 | loss  4.64 | ppl   103.21\n",
            "| epoch  23 |  2400/ 2983 batches | lr 0.000016 | ms/batch 22.35 | loss  4.66 | ppl   105.40\n",
            "| epoch  23 |  2600/ 2983 batches | lr 0.000016 | ms/batch 22.35 | loss  4.71 | ppl   111.21\n",
            "| epoch  23 |  2800/ 2983 batches | lr 0.000016 | ms/batch 22.35 | loss  4.56 | ppl    95.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 68.79s | valid loss  5.67 | valid ppl   290.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  24 |   200/ 2983 batches | lr 0.000016 | ms/batch 22.47 | loss  4.84 | ppl   125.95\n",
            "| epoch  24 |   400/ 2983 batches | lr 0.000016 | ms/batch 22.37 | loss  5.04 | ppl   154.36\n",
            "| epoch  24 |   600/ 2983 batches | lr 0.000016 | ms/batch 22.35 | loss  4.65 | ppl   105.01\n",
            "| epoch  24 |   800/ 2983 batches | lr 0.000016 | ms/batch 22.36 | loss  4.67 | ppl   107.18\n",
            "| epoch  24 |  1000/ 2983 batches | lr 0.000016 | ms/batch 22.35 | loss  4.70 | ppl   110.02\n",
            "| epoch  24 |  1200/ 2983 batches | lr 0.000016 | ms/batch 22.35 | loss  4.68 | ppl   108.09\n",
            "| epoch  24 |  1400/ 2983 batches | lr 0.000016 | ms/batch 22.36 | loss  4.85 | ppl   127.74\n",
            "| epoch  24 |  1600/ 2983 batches | lr 0.000016 | ms/batch 22.34 | loss  4.84 | ppl   126.93\n",
            "| epoch  24 |  1800/ 2983 batches | lr 0.000016 | ms/batch 22.33 | loss  4.57 | ppl    96.14\n",
            "| epoch  24 |  2000/ 2983 batches | lr 0.000016 | ms/batch 22.34 | loss  4.68 | ppl   107.35\n",
            "| epoch  24 |  2200/ 2983 batches | lr 0.000016 | ms/batch 22.33 | loss  4.63 | ppl   102.03\n",
            "| epoch  24 |  2400/ 2983 batches | lr 0.000016 | ms/batch 22.36 | loss  4.64 | ppl   103.48\n",
            "| epoch  24 |  2600/ 2983 batches | lr 0.000016 | ms/batch 22.36 | loss  4.68 | ppl   108.07\n",
            "| epoch  24 |  2800/ 2983 batches | lr 0.000016 | ms/batch 22.36 | loss  4.55 | ppl    94.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 68.81s | valid loss  5.67 | valid ppl   290.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  25 |   200/ 2983 batches | lr 0.000008 | ms/batch 22.45 | loss  4.95 | ppl   141.11\n",
            "| epoch  25 |   400/ 2983 batches | lr 0.000008 | ms/batch 22.35 | loss  5.14 | ppl   171.32\n",
            "| epoch  25 |   600/ 2983 batches | lr 0.000008 | ms/batch 22.36 | loss  4.91 | ppl   135.70\n",
            "| epoch  25 |   800/ 2983 batches | lr 0.000008 | ms/batch 22.34 | loss  4.78 | ppl   119.39\n",
            "| epoch  25 |  1000/ 2983 batches | lr 0.000008 | ms/batch 22.36 | loss  4.71 | ppl   111.22\n",
            "| epoch  25 |  1200/ 2983 batches | lr 0.000008 | ms/batch 22.34 | loss  4.77 | ppl   118.27\n",
            "| epoch  25 |  1400/ 2983 batches | lr 0.000008 | ms/batch 22.34 | loss  4.85 | ppl   128.13\n",
            "| epoch  25 |  1600/ 2983 batches | lr 0.000008 | ms/batch 22.39 | loss  4.88 | ppl   131.93\n",
            "| epoch  25 |  1800/ 2983 batches | lr 0.000008 | ms/batch 22.36 | loss  4.65 | ppl   105.07\n",
            "| epoch  25 |  2000/ 2983 batches | lr 0.000008 | ms/batch 22.36 | loss  4.68 | ppl   107.83\n",
            "| epoch  25 |  2200/ 2983 batches | lr 0.000008 | ms/batch 22.37 | loss  4.70 | ppl   110.33\n",
            "| epoch  25 |  2400/ 2983 batches | lr 0.000008 | ms/batch 22.35 | loss  4.70 | ppl   110.32\n",
            "| epoch  25 |  2600/ 2983 batches | lr 0.000008 | ms/batch 22.39 | loss  4.77 | ppl   118.41\n",
            "| epoch  25 |  2800/ 2983 batches | lr 0.000008 | ms/batch 22.35 | loss  4.69 | ppl   109.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 68.82s | valid loss  5.53 | valid ppl   251.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  26 |   200/ 2983 batches | lr 0.000008 | ms/batch 22.45 | loss  4.89 | ppl   133.40\n",
            "| epoch  26 |   400/ 2983 batches | lr 0.000008 | ms/batch 22.34 | loss  5.05 | ppl   156.17\n",
            "| epoch  26 |   600/ 2983 batches | lr 0.000008 | ms/batch 22.37 | loss  4.74 | ppl   114.74\n",
            "| epoch  26 |   800/ 2983 batches | lr 0.000008 | ms/batch 22.34 | loss  4.71 | ppl   110.92\n",
            "| epoch  26 |  1000/ 2983 batches | lr 0.000008 | ms/batch 22.34 | loss  4.72 | ppl   112.18\n",
            "| epoch  26 |  1200/ 2983 batches | lr 0.000008 | ms/batch 22.36 | loss  4.74 | ppl   114.53\n",
            "| epoch  26 |  1400/ 2983 batches | lr 0.000008 | ms/batch 22.35 | loss  4.83 | ppl   124.77\n",
            "| epoch  26 |  1600/ 2983 batches | lr 0.000008 | ms/batch 22.37 | loss  4.84 | ppl   126.94\n",
            "| epoch  26 |  1800/ 2983 batches | lr 0.000008 | ms/batch 22.37 | loss  4.61 | ppl   100.70\n",
            "| epoch  26 |  2000/ 2983 batches | lr 0.000008 | ms/batch 22.34 | loss  4.69 | ppl   109.27\n",
            "| epoch  26 |  2200/ 2983 batches | lr 0.000008 | ms/batch 22.37 | loss  4.69 | ppl   109.18\n",
            "| epoch  26 |  2400/ 2983 batches | lr 0.000008 | ms/batch 22.34 | loss  4.67 | ppl   107.02\n",
            "| epoch  26 |  2600/ 2983 batches | lr 0.000008 | ms/batch 22.36 | loss  4.73 | ppl   113.34\n",
            "| epoch  26 |  2800/ 2983 batches | lr 0.000008 | ms/batch 22.39 | loss  4.63 | ppl   102.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 68.82s | valid loss  5.55 | valid ppl   256.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  27 |   200/ 2983 batches | lr 0.000008 | ms/batch 22.47 | loss  4.90 | ppl   134.03\n",
            "| epoch  27 |   400/ 2983 batches | lr 0.000008 | ms/batch 22.33 | loss  5.06 | ppl   157.97\n",
            "| epoch  27 |   600/ 2983 batches | lr 0.000008 | ms/batch 22.33 | loss  4.77 | ppl   118.18\n",
            "| epoch  27 |   800/ 2983 batches | lr 0.000008 | ms/batch 22.35 | loss  4.68 | ppl   108.27\n",
            "| epoch  27 |  1000/ 2983 batches | lr 0.000008 | ms/batch 22.34 | loss  4.72 | ppl   112.06\n",
            "| epoch  27 |  1200/ 2983 batches | lr 0.000008 | ms/batch 22.34 | loss  4.73 | ppl   113.75\n",
            "| epoch  27 |  1400/ 2983 batches | lr 0.000008 | ms/batch 22.37 | loss  4.82 | ppl   123.59\n",
            "| epoch  27 |  1600/ 2983 batches | lr 0.000008 | ms/batch 22.34 | loss  4.83 | ppl   125.71\n",
            "| epoch  27 |  1800/ 2983 batches | lr 0.000008 | ms/batch 22.35 | loss  4.60 | ppl    99.42\n",
            "| epoch  27 |  2000/ 2983 batches | lr 0.000008 | ms/batch 22.36 | loss  4.69 | ppl   109.33\n",
            "| epoch  27 |  2200/ 2983 batches | lr 0.000008 | ms/batch 22.35 | loss  4.69 | ppl   109.00\n",
            "| epoch  27 |  2400/ 2983 batches | lr 0.000008 | ms/batch 22.36 | loss  4.67 | ppl   106.48\n",
            "| epoch  27 |  2600/ 2983 batches | lr 0.000008 | ms/batch 22.34 | loss  4.72 | ppl   112.43\n",
            "| epoch  27 |  2800/ 2983 batches | lr 0.000008 | ms/batch 22.34 | loss  4.62 | ppl   101.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 68.78s | valid loss  5.56 | valid ppl   258.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  28 |   200/ 2983 batches | lr 0.000008 | ms/batch 22.47 | loss  4.88 | ppl   132.00\n",
            "| epoch  28 |   400/ 2983 batches | lr 0.000008 | ms/batch 22.38 | loss  5.06 | ppl   157.23\n",
            "| epoch  28 |   600/ 2983 batches | lr 0.000008 | ms/batch 22.36 | loss  4.74 | ppl   114.82\n",
            "| epoch  28 |   800/ 2983 batches | lr 0.000008 | ms/batch 22.36 | loss  4.69 | ppl   108.35\n",
            "| epoch  28 |  1000/ 2983 batches | lr 0.000008 | ms/batch 22.38 | loss  4.72 | ppl   111.84\n",
            "| epoch  28 |  1200/ 2983 batches | lr 0.000008 | ms/batch 22.38 | loss  4.73 | ppl   113.85\n",
            "| epoch  28 |  1400/ 2983 batches | lr 0.000008 | ms/batch 22.38 | loss  4.82 | ppl   123.55\n",
            "| epoch  28 |  1600/ 2983 batches | lr 0.000008 | ms/batch 22.36 | loss  4.83 | ppl   125.20\n",
            "| epoch  28 |  1800/ 2983 batches | lr 0.000008 | ms/batch 22.34 | loss  4.59 | ppl    98.21\n",
            "| epoch  28 |  2000/ 2983 batches | lr 0.000008 | ms/batch 22.36 | loss  4.69 | ppl   109.10\n",
            "| epoch  28 |  2200/ 2983 batches | lr 0.000008 | ms/batch 22.34 | loss  4.69 | ppl   108.90\n",
            "| epoch  28 |  2400/ 2983 batches | lr 0.000008 | ms/batch 22.34 | loss  4.65 | ppl   104.66\n",
            "| epoch  28 |  2600/ 2983 batches | lr 0.000008 | ms/batch 22.35 | loss  4.70 | ppl   110.32\n",
            "| epoch  28 |  2800/ 2983 batches | lr 0.000008 | ms/batch 22.35 | loss  4.61 | ppl   100.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 68.82s | valid loss  5.56 | valid ppl   260.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000004\n",
            "| epoch  29 |   200/ 2983 batches | lr 0.000004 | ms/batch 22.46 | loss  4.96 | ppl   142.01\n",
            "| epoch  29 |   400/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  5.13 | ppl   168.80\n",
            "| epoch  29 |   600/ 2983 batches | lr 0.000004 | ms/batch 22.36 | loss  4.95 | ppl   141.23\n",
            "| epoch  29 |   800/ 2983 batches | lr 0.000004 | ms/batch 22.36 | loss  4.86 | ppl   129.11\n",
            "| epoch  29 |  1000/ 2983 batches | lr 0.000004 | ms/batch 22.35 | loss  4.73 | ppl   113.29\n",
            "| epoch  29 |  1200/ 2983 batches | lr 0.000004 | ms/batch 22.37 | loss  4.78 | ppl   118.82\n",
            "| epoch  29 |  1400/ 2983 batches | lr 0.000004 | ms/batch 22.35 | loss  4.83 | ppl   125.09\n",
            "| epoch  29 |  1600/ 2983 batches | lr 0.000004 | ms/batch 22.35 | loss  4.86 | ppl   128.99\n",
            "| epoch  29 |  1800/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.67 | ppl   106.51\n",
            "| epoch  29 |  2000/ 2983 batches | lr 0.000004 | ms/batch 22.33 | loss  4.70 | ppl   110.31\n",
            "| epoch  29 |  2200/ 2983 batches | lr 0.000004 | ms/batch 22.36 | loss  4.71 | ppl   111.51\n",
            "| epoch  29 |  2400/ 2983 batches | lr 0.000004 | ms/batch 22.33 | loss  4.72 | ppl   112.11\n",
            "| epoch  29 |  2600/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.79 | ppl   120.04\n",
            "| epoch  29 |  2800/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.66 | ppl   105.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 68.78s | valid loss  5.50 | valid ppl   243.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000004\n",
            "| epoch  30 |   200/ 2983 batches | lr 0.000004 | ms/batch 22.46 | loss  4.94 | ppl   139.99\n",
            "| epoch  30 |   400/ 2983 batches | lr 0.000004 | ms/batch 22.35 | loss  5.05 | ppl   155.27\n",
            "| epoch  30 |   600/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.83 | ppl   125.15\n",
            "| epoch  30 |   800/ 2983 batches | lr 0.000004 | ms/batch 22.35 | loss  4.75 | ppl   115.34\n",
            "| epoch  30 |  1000/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.72 | ppl   112.33\n",
            "| epoch  30 |  1200/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.77 | ppl   117.85\n",
            "| epoch  30 |  1400/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.82 | ppl   123.73\n",
            "| epoch  30 |  1600/ 2983 batches | lr 0.000004 | ms/batch 22.33 | loss  4.83 | ppl   125.46\n",
            "| epoch  30 |  1800/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.64 | ppl   103.59\n",
            "| epoch  30 |  2000/ 2983 batches | lr 0.000004 | ms/batch 22.33 | loss  4.69 | ppl   108.91\n",
            "| epoch  30 |  2200/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.72 | ppl   111.84\n",
            "| epoch  30 |  2400/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.72 | ppl   112.31\n",
            "| epoch  30 |  2600/ 2983 batches | lr 0.000004 | ms/batch 22.32 | loss  4.77 | ppl   117.38\n",
            "| epoch  30 |  2800/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.64 | ppl   103.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 68.77s | valid loss  5.50 | valid ppl   244.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000004\n",
            "| epoch  31 |   200/ 2983 batches | lr 0.000004 | ms/batch 22.47 | loss  4.92 | ppl   136.95\n",
            "| epoch  31 |   400/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  5.05 | ppl   155.74\n",
            "| epoch  31 |   600/ 2983 batches | lr 0.000004 | ms/batch 22.33 | loss  4.82 | ppl   124.23\n",
            "| epoch  31 |   800/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.74 | ppl   114.05\n",
            "| epoch  31 |  1000/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.71 | ppl   111.39\n",
            "| epoch  31 |  1200/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.77 | ppl   117.54\n",
            "| epoch  31 |  1400/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.81 | ppl   122.37\n",
            "| epoch  31 |  1600/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.83 | ppl   124.66\n",
            "| epoch  31 |  1800/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.62 | ppl   101.99\n",
            "| epoch  31 |  2000/ 2983 batches | lr 0.000004 | ms/batch 22.35 | loss  4.69 | ppl   109.04\n",
            "| epoch  31 |  2200/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.72 | ppl   112.08\n",
            "| epoch  31 |  2400/ 2983 batches | lr 0.000004 | ms/batch 22.33 | loss  4.72 | ppl   112.21\n",
            "| epoch  31 |  2600/ 2983 batches | lr 0.000004 | ms/batch 22.35 | loss  4.76 | ppl   117.29\n",
            "| epoch  31 |  2800/ 2983 batches | lr 0.000004 | ms/batch 22.35 | loss  4.63 | ppl   102.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 68.77s | valid loss  5.51 | valid ppl   246.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000004\n",
            "| epoch  32 |   200/ 2983 batches | lr 0.000004 | ms/batch 22.45 | loss  4.93 | ppl   138.31\n",
            "| epoch  32 |   400/ 2983 batches | lr 0.000004 | ms/batch 22.33 | loss  5.05 | ppl   155.54\n",
            "| epoch  32 |   600/ 2983 batches | lr 0.000004 | ms/batch 22.33 | loss  4.82 | ppl   123.78\n",
            "| epoch  32 |   800/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.72 | ppl   112.54\n",
            "| epoch  32 |  1000/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.71 | ppl   111.39\n",
            "| epoch  32 |  1200/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.76 | ppl   116.76\n",
            "| epoch  32 |  1400/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.81 | ppl   122.26\n",
            "| epoch  32 |  1600/ 2983 batches | lr 0.000004 | ms/batch 22.33 | loss  4.82 | ppl   123.65\n",
            "| epoch  32 |  1800/ 2983 batches | lr 0.000004 | ms/batch 22.36 | loss  4.62 | ppl   101.47\n",
            "| epoch  32 |  2000/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.69 | ppl   109.18\n",
            "| epoch  32 |  2200/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.72 | ppl   112.22\n",
            "| epoch  32 |  2400/ 2983 batches | lr 0.000004 | ms/batch 22.35 | loss  4.71 | ppl   111.54\n",
            "| epoch  32 |  2600/ 2983 batches | lr 0.000004 | ms/batch 22.34 | loss  4.75 | ppl   115.50\n",
            "| epoch  32 |  2800/ 2983 batches | lr 0.000004 | ms/batch 22.35 | loss  4.63 | ppl   102.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 68.75s | valid loss  5.50 | valid ppl   245.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000002\n",
            "| epoch  33 |   200/ 2983 batches | lr 0.000002 | ms/batch 22.45 | loss  4.95 | ppl   140.94\n",
            "| epoch  33 |   400/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  5.09 | ppl   161.83\n",
            "| epoch  33 |   600/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  4.93 | ppl   138.11\n",
            "| epoch  33 |   800/ 2983 batches | lr 0.000002 | ms/batch 22.35 | loss  4.93 | ppl   138.40\n",
            "| epoch  33 |  1000/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  4.79 | ppl   119.88\n",
            "| epoch  33 |  1200/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  4.79 | ppl   120.05\n",
            "| epoch  33 |  1400/ 2983 batches | lr 0.000002 | ms/batch 22.33 | loss  4.82 | ppl   124.04\n",
            "| epoch  33 |  1600/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  4.84 | ppl   126.94\n",
            "| epoch  33 |  1800/ 2983 batches | lr 0.000002 | ms/batch 22.35 | loss  4.68 | ppl   107.73\n",
            "| epoch  33 |  2000/ 2983 batches | lr 0.000002 | ms/batch 22.35 | loss  4.72 | ppl   112.37\n",
            "| epoch  33 |  2200/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  4.71 | ppl   110.98\n",
            "| epoch  33 |  2400/ 2983 batches | lr 0.000002 | ms/batch 22.35 | loss  4.74 | ppl   114.75\n",
            "| epoch  33 |  2600/ 2983 batches | lr 0.000002 | ms/batch 22.36 | loss  4.82 | ppl   124.35\n",
            "| epoch  33 |  2800/ 2983 batches | lr 0.000002 | ms/batch 22.35 | loss  4.69 | ppl   108.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 68.78s | valid loss  5.46 | valid ppl   234.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000002\n",
            "| epoch  34 |   200/ 2983 batches | lr 0.000002 | ms/batch 22.44 | loss  4.94 | ppl   139.17\n",
            "| epoch  34 |   400/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  5.03 | ppl   152.96\n",
            "| epoch  34 |   600/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  4.86 | ppl   128.94\n",
            "| epoch  34 |   800/ 2983 batches | lr 0.000002 | ms/batch 22.35 | loss  4.84 | ppl   126.10\n",
            "| epoch  34 |  1000/ 2983 batches | lr 0.000002 | ms/batch 22.36 | loss  4.73 | ppl   113.14\n",
            "| epoch  34 |  1200/ 2983 batches | lr 0.000002 | ms/batch 22.33 | loss  4.78 | ppl   118.79\n",
            "| epoch  34 |  1400/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  4.81 | ppl   123.17\n",
            "| epoch  34 |  1600/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  4.84 | ppl   125.85\n",
            "| epoch  34 |  1800/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  4.68 | ppl   107.24\n",
            "| epoch  34 |  2000/ 2983 batches | lr 0.000002 | ms/batch 22.36 | loss  4.72 | ppl   111.93\n",
            "| epoch  34 |  2200/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  4.70 | ppl   110.26\n",
            "| epoch  34 |  2400/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  4.74 | ppl   114.07\n",
            "| epoch  34 |  2600/ 2983 batches | lr 0.000002 | ms/batch 22.33 | loss  4.82 | ppl   124.03\n",
            "| epoch  34 |  2800/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  4.68 | ppl   108.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 68.77s | valid loss  5.46 | valid ppl   235.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000002\n",
            "| epoch  35 |   200/ 2983 batches | lr 0.000002 | ms/batch 22.44 | loss  4.92 | ppl   136.95\n",
            "| epoch  35 |   400/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  5.02 | ppl   151.67\n",
            "| epoch  35 |   600/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  4.85 | ppl   127.41\n",
            "| epoch  35 |   800/ 2983 batches | lr 0.000002 | ms/batch 22.33 | loss  4.82 | ppl   124.56\n",
            "| epoch  35 |  1000/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  4.73 | ppl   112.92\n",
            "| epoch  35 |  1200/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  4.77 | ppl   118.36\n",
            "| epoch  35 |  1400/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  4.81 | ppl   123.02\n",
            "| epoch  35 |  1600/ 2983 batches | lr 0.000002 | ms/batch 22.35 | loss  4.83 | ppl   125.27\n",
            "| epoch  35 |  1800/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  4.67 | ppl   106.59\n",
            "| epoch  35 |  2000/ 2983 batches | lr 0.000002 | ms/batch 22.35 | loss  4.72 | ppl   111.65\n",
            "| epoch  35 |  2200/ 2983 batches | lr 0.000002 | ms/batch 22.39 | loss  4.70 | ppl   110.45\n",
            "| epoch  35 |  2400/ 2983 batches | lr 0.000002 | ms/batch 22.36 | loss  4.74 | ppl   114.09\n",
            "| epoch  35 |  2600/ 2983 batches | lr 0.000002 | ms/batch 22.35 | loss  4.82 | ppl   124.19\n",
            "| epoch  35 |  2800/ 2983 batches | lr 0.000002 | ms/batch 22.36 | loss  4.68 | ppl   107.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 68.79s | valid loss  5.46 | valid ppl   235.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000002\n",
            "| epoch  36 |   200/ 2983 batches | lr 0.000002 | ms/batch 22.45 | loss  4.91 | ppl   136.11\n",
            "| epoch  36 |   400/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  5.02 | ppl   150.99\n",
            "| epoch  36 |   600/ 2983 batches | lr 0.000002 | ms/batch 22.35 | loss  4.84 | ppl   125.99\n",
            "| epoch  36 |   800/ 2983 batches | lr 0.000002 | ms/batch 22.36 | loss  4.82 | ppl   124.37\n",
            "| epoch  36 |  1000/ 2983 batches | lr 0.000002 | ms/batch 22.32 | loss  4.72 | ppl   112.67\n",
            "| epoch  36 |  1200/ 2983 batches | lr 0.000002 | ms/batch 22.33 | loss  4.77 | ppl   118.13\n",
            "| epoch  36 |  1400/ 2983 batches | lr 0.000002 | ms/batch 22.35 | loss  4.81 | ppl   122.92\n",
            "| epoch  36 |  1600/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  4.83 | ppl   125.44\n",
            "| epoch  36 |  1800/ 2983 batches | lr 0.000002 | ms/batch 22.36 | loss  4.67 | ppl   106.88\n",
            "| epoch  36 |  2000/ 2983 batches | lr 0.000002 | ms/batch 22.34 | loss  4.71 | ppl   111.48\n",
            "| epoch  36 |  2200/ 2983 batches | lr 0.000002 | ms/batch 22.33 | loss  4.70 | ppl   110.41\n",
            "| epoch  36 |  2400/ 2983 batches | lr 0.000002 | ms/batch 22.33 | loss  4.74 | ppl   114.03\n",
            "| epoch  36 |  2600/ 2983 batches | lr 0.000002 | ms/batch 22.31 | loss  4.82 | ppl   124.22\n",
            "| epoch  36 |  2800/ 2983 batches | lr 0.000002 | ms/batch 22.33 | loss  4.68 | ppl   108.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 68.76s | valid loss  5.46 | valid ppl   235.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  37 |   200/ 2983 batches | lr 0.000001 | ms/batch 22.48 | loss  4.92 | ppl   137.36\n",
            "| epoch  37 |   400/ 2983 batches | lr 0.000001 | ms/batch 22.43 | loss  5.03 | ppl   153.68\n",
            "| epoch  37 |   600/ 2983 batches | lr 0.000001 | ms/batch 22.39 | loss  4.89 | ppl   133.06\n",
            "| epoch  37 |   800/ 2983 batches | lr 0.000001 | ms/batch 22.41 | loss  4.93 | ppl   137.85\n",
            "| epoch  37 |  1000/ 2983 batches | lr 0.000001 | ms/batch 22.40 | loss  4.81 | ppl   123.08\n",
            "| epoch  37 |  1200/ 2983 batches | lr 0.000001 | ms/batch 22.38 | loss  4.81 | ppl   122.60\n",
            "| epoch  37 |  1400/ 2983 batches | lr 0.000001 | ms/batch 22.40 | loss  4.82 | ppl   124.22\n",
            "| epoch  37 |  1600/ 2983 batches | lr 0.000001 | ms/batch 22.39 | loss  4.84 | ppl   126.82\n",
            "| epoch  37 |  1800/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  4.71 | ppl   110.52\n",
            "| epoch  37 |  2000/ 2983 batches | lr 0.000001 | ms/batch 22.41 | loss  4.76 | ppl   116.94\n",
            "| epoch  37 |  2200/ 2983 batches | lr 0.000001 | ms/batch 22.39 | loss  4.69 | ppl   109.26\n",
            "| epoch  37 |  2400/ 2983 batches | lr 0.000001 | ms/batch 22.39 | loss  4.73 | ppl   113.13\n",
            "| epoch  37 |  2600/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  4.83 | ppl   125.74\n",
            "| epoch  37 |  2800/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.71 | ppl   111.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 68.92s | valid loss  5.43 | valid ppl   227.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  38 |   200/ 2983 batches | lr 0.000001 | ms/batch 22.52 | loss  4.93 | ppl   138.77\n",
            "| epoch  38 |   400/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  5.00 | ppl   148.93\n",
            "| epoch  38 |   600/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  4.85 | ppl   127.41\n",
            "| epoch  38 |   800/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  4.87 | ppl   130.24\n",
            "| epoch  38 |  1000/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.77 | ppl   117.78\n",
            "| epoch  38 |  1200/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.79 | ppl   120.47\n",
            "| epoch  38 |  1400/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.81 | ppl   123.17\n",
            "| epoch  38 |  1600/ 2983 batches | lr 0.000001 | ms/batch 22.38 | loss  4.84 | ppl   126.08\n",
            "| epoch  38 |  1800/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.71 | ppl   110.50\n",
            "| epoch  38 |  2000/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.77 | ppl   118.00\n",
            "| epoch  38 |  2200/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  4.69 | ppl   109.30\n",
            "| epoch  38 |  2400/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.73 | ppl   112.85\n",
            "| epoch  38 |  2600/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  4.83 | ppl   124.60\n",
            "| epoch  38 |  2800/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.71 | ppl   111.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 68.84s | valid loss  5.43 | valid ppl   227.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  39 |   200/ 2983 batches | lr 0.000001 | ms/batch 22.45 | loss  4.93 | ppl   138.36\n",
            "| epoch  39 |   400/ 2983 batches | lr 0.000001 | ms/batch 22.33 | loss  4.99 | ppl   147.58\n",
            "| epoch  39 |   600/ 2983 batches | lr 0.000001 | ms/batch 22.38 | loss  4.84 | ppl   126.03\n",
            "| epoch  39 |   800/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.85 | ppl   128.07\n",
            "| epoch  39 |  1000/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.76 | ppl   116.48\n",
            "| epoch  39 |  1200/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  4.79 | ppl   119.94\n",
            "| epoch  39 |  1400/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.81 | ppl   122.62\n",
            "| epoch  39 |  1600/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.84 | ppl   126.14\n",
            "| epoch  39 |  1800/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  4.71 | ppl   110.85\n",
            "| epoch  39 |  2000/ 2983 batches | lr 0.000001 | ms/batch 22.33 | loss  4.77 | ppl   117.86\n",
            "| epoch  39 |  2200/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.69 | ppl   109.27\n",
            "| epoch  39 |  2400/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.72 | ppl   112.33\n",
            "| epoch  39 |  2600/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.82 | ppl   124.54\n",
            "| epoch  39 |  2800/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.71 | ppl   110.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 68.80s | valid loss  5.43 | valid ppl   227.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  40 |   200/ 2983 batches | lr 0.000001 | ms/batch 22.46 | loss  4.92 | ppl   137.37\n",
            "| epoch  40 |   400/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  4.99 | ppl   146.53\n",
            "| epoch  40 |   600/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.83 | ppl   125.09\n",
            "| epoch  40 |   800/ 2983 batches | lr 0.000001 | ms/batch 22.41 | loss  4.86 | ppl   128.41\n",
            "| epoch  40 |  1000/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.76 | ppl   116.46\n",
            "| epoch  40 |  1200/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.79 | ppl   120.28\n",
            "| epoch  40 |  1400/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  4.81 | ppl   123.04\n",
            "| epoch  40 |  1600/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.84 | ppl   126.11\n",
            "| epoch  40 |  1800/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.70 | ppl   110.43\n",
            "| epoch  40 |  2000/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  4.77 | ppl   117.86\n",
            "| epoch  40 |  2200/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.70 | ppl   109.45\n",
            "| epoch  40 |  2400/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.72 | ppl   111.86\n",
            "| epoch  40 |  2600/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.82 | ppl   124.41\n",
            "| epoch  40 |  2800/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.70 | ppl   110.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 68.82s | valid loss  5.42 | valid ppl   227.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  41 |   200/ 2983 batches | lr 0.000001 | ms/batch 22.45 | loss  4.92 | ppl   136.81\n",
            "| epoch  41 |   400/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.99 | ppl   146.58\n",
            "| epoch  41 |   600/ 2983 batches | lr 0.000001 | ms/batch 22.33 | loss  4.83 | ppl   124.96\n",
            "| epoch  41 |   800/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.85 | ppl   127.86\n",
            "| epoch  41 |  1000/ 2983 batches | lr 0.000001 | ms/batch 22.38 | loss  4.76 | ppl   116.87\n",
            "| epoch  41 |  1200/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.78 | ppl   119.63\n",
            "| epoch  41 |  1400/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  4.81 | ppl   123.14\n",
            "| epoch  41 |  1600/ 2983 batches | lr 0.000001 | ms/batch 22.38 | loss  4.83 | ppl   125.63\n",
            "| epoch  41 |  1800/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.71 | ppl   110.58\n",
            "| epoch  41 |  2000/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  4.76 | ppl   117.29\n",
            "| epoch  41 |  2200/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.68 | ppl   108.22\n",
            "| epoch  41 |  2400/ 2983 batches | lr 0.000001 | ms/batch 22.33 | loss  4.72 | ppl   112.20\n",
            "| epoch  41 |  2600/ 2983 batches | lr 0.000001 | ms/batch 22.38 | loss  4.83 | ppl   124.92\n",
            "| epoch  41 |  2800/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.70 | ppl   110.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 68.84s | valid loss  5.42 | valid ppl   226.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  42 |   200/ 2983 batches | lr 0.000001 | ms/batch 22.46 | loss  4.92 | ppl   136.54\n",
            "| epoch  42 |   400/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.98 | ppl   146.07\n",
            "| epoch  42 |   600/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.83 | ppl   124.77\n",
            "| epoch  42 |   800/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.84 | ppl   126.95\n",
            "| epoch  42 |  1000/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.76 | ppl   116.95\n",
            "| epoch  42 |  1200/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  4.79 | ppl   119.85\n",
            "| epoch  42 |  1400/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.81 | ppl   123.34\n",
            "| epoch  42 |  1600/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.83 | ppl   125.36\n",
            "| epoch  42 |  1800/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.71 | ppl   110.50\n",
            "| epoch  42 |  2000/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.77 | ppl   117.39\n",
            "| epoch  42 |  2200/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.69 | ppl   108.36\n",
            "| epoch  42 |  2400/ 2983 batches | lr 0.000001 | ms/batch 22.33 | loss  4.71 | ppl   111.36\n",
            "| epoch  42 |  2600/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.83 | ppl   124.66\n",
            "| epoch  42 |  2800/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.70 | ppl   110.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 68.80s | valid loss  5.42 | valid ppl   226.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  43 |   200/ 2983 batches | lr 0.000001 | ms/batch 22.47 | loss  4.91 | ppl   136.09\n",
            "| epoch  43 |   400/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.98 | ppl   145.71\n",
            "| epoch  43 |   600/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.82 | ppl   124.13\n",
            "| epoch  43 |   800/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.85 | ppl   127.44\n",
            "| epoch  43 |  1000/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.76 | ppl   116.71\n",
            "| epoch  43 |  1200/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.79 | ppl   120.14\n",
            "| epoch  43 |  1400/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.81 | ppl   122.34\n",
            "| epoch  43 |  1600/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.83 | ppl   124.99\n",
            "| epoch  43 |  1800/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  4.70 | ppl   110.34\n",
            "| epoch  43 |  2000/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.76 | ppl   117.29\n",
            "| epoch  43 |  2200/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.68 | ppl   108.23\n",
            "| epoch  43 |  2400/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.72 | ppl   111.74\n",
            "| epoch  43 |  2600/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.82 | ppl   124.43\n",
            "| epoch  43 |  2800/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.70 | ppl   110.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 68.82s | valid loss  5.42 | valid ppl   226.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  44 |   200/ 2983 batches | lr 0.000001 | ms/batch 22.46 | loss  4.91 | ppl   135.91\n",
            "| epoch  44 |   400/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  4.98 | ppl   145.72\n",
            "| epoch  44 |   600/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.82 | ppl   123.52\n",
            "| epoch  44 |   800/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.84 | ppl   127.02\n",
            "| epoch  44 |  1000/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.76 | ppl   116.19\n",
            "| epoch  44 |  1200/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.79 | ppl   120.23\n",
            "| epoch  44 |  1400/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.81 | ppl   122.87\n",
            "| epoch  44 |  1600/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.83 | ppl   125.16\n",
            "| epoch  44 |  1800/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.71 | ppl   110.54\n",
            "| epoch  44 |  2000/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.77 | ppl   117.43\n",
            "| epoch  44 |  2200/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.68 | ppl   108.11\n",
            "| epoch  44 |  2400/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.71 | ppl   111.36\n",
            "| epoch  44 |  2600/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.82 | ppl   124.34\n",
            "| epoch  44 |  2800/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.70 | ppl   109.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 68.80s | valid loss  5.42 | valid ppl   226.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  45 |   200/ 2983 batches | lr 0.000001 | ms/batch 22.46 | loss  4.91 | ppl   135.99\n",
            "| epoch  45 |   400/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.98 | ppl   145.13\n",
            "| epoch  45 |   600/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  4.82 | ppl   123.82\n",
            "| epoch  45 |   800/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.84 | ppl   126.39\n",
            "| epoch  45 |  1000/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  4.75 | ppl   116.03\n",
            "| epoch  45 |  1200/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  4.78 | ppl   119.60\n",
            "| epoch  45 |  1400/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.81 | ppl   122.79\n",
            "| epoch  45 |  1600/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.83 | ppl   125.14\n",
            "| epoch  45 |  1800/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.70 | ppl   110.25\n",
            "| epoch  45 |  2000/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.77 | ppl   117.41\n",
            "| epoch  45 |  2200/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.69 | ppl   108.34\n",
            "| epoch  45 |  2400/ 2983 batches | lr 0.000001 | ms/batch 22.33 | loss  4.71 | ppl   110.86\n",
            "| epoch  45 |  2600/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.82 | ppl   123.82\n",
            "| epoch  45 |  2800/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  4.69 | ppl   109.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 68.82s | valid loss  5.42 | valid ppl   226.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  46 |   200/ 2983 batches | lr 0.000001 | ms/batch 22.47 | loss  4.91 | ppl   136.13\n",
            "| epoch  46 |   400/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.98 | ppl   144.86\n",
            "| epoch  46 |   600/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.82 | ppl   123.53\n",
            "| epoch  46 |   800/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.84 | ppl   126.57\n",
            "| epoch  46 |  1000/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.76 | ppl   116.27\n",
            "| epoch  46 |  1200/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.78 | ppl   119.63\n",
            "| epoch  46 |  1400/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.81 | ppl   122.88\n",
            "| epoch  46 |  1600/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.83 | ppl   124.68\n",
            "| epoch  46 |  1800/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.70 | ppl   110.39\n",
            "| epoch  46 |  2000/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.77 | ppl   117.64\n",
            "| epoch  46 |  2200/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.68 | ppl   107.76\n",
            "| epoch  46 |  2400/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.71 | ppl   111.54\n",
            "| epoch  46 |  2600/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.82 | ppl   123.75\n",
            "| epoch  46 |  2800/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.70 | ppl   109.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 68.78s | valid loss  5.42 | valid ppl   226.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  47 |   200/ 2983 batches | lr 0.000001 | ms/batch 22.44 | loss  4.91 | ppl   135.25\n",
            "| epoch  47 |   400/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.98 | ppl   144.99\n",
            "| epoch  47 |   600/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.82 | ppl   123.62\n",
            "| epoch  47 |   800/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.84 | ppl   127.09\n",
            "| epoch  47 |  1000/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.76 | ppl   116.21\n",
            "| epoch  47 |  1200/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.79 | ppl   119.98\n",
            "| epoch  47 |  1400/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.81 | ppl   122.50\n",
            "| epoch  47 |  1600/ 2983 batches | lr 0.000001 | ms/batch 22.36 | loss  4.83 | ppl   124.94\n",
            "| epoch  47 |  1800/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.70 | ppl   110.39\n",
            "| epoch  47 |  2000/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.76 | ppl   116.82\n",
            "| epoch  47 |  2200/ 2983 batches | lr 0.000001 | ms/batch 22.33 | loss  4.68 | ppl   107.76\n",
            "| epoch  47 |  2400/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.71 | ppl   111.23\n",
            "| epoch  47 |  2600/ 2983 batches | lr 0.000001 | ms/batch 22.33 | loss  4.82 | ppl   123.45\n",
            "| epoch  47 |  2800/ 2983 batches | lr 0.000001 | ms/batch 22.32 | loss  4.70 | ppl   109.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 68.79s | valid loss  5.42 | valid ppl   226.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  48 |   200/ 2983 batches | lr 0.000001 | ms/batch 22.44 | loss  4.91 | ppl   135.52\n",
            "| epoch  48 |   400/ 2983 batches | lr 0.000001 | ms/batch 22.37 | loss  4.97 | ppl   144.66\n",
            "| epoch  48 |   600/ 2983 batches | lr 0.000001 | ms/batch 22.33 | loss  4.81 | ppl   122.99\n",
            "| epoch  48 |   800/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.84 | ppl   126.35\n",
            "| epoch  48 |  1000/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.75 | ppl   115.75\n",
            "| epoch  48 |  1200/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.78 | ppl   119.20\n",
            "| epoch  48 |  1400/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.81 | ppl   122.76\n",
            "| epoch  48 |  1600/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.83 | ppl   124.99\n",
            "| epoch  48 |  1800/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.70 | ppl   110.35\n",
            "| epoch  48 |  2000/ 2983 batches | lr 0.000001 | ms/batch 22.33 | loss  4.77 | ppl   117.37\n",
            "| epoch  48 |  2200/ 2983 batches | lr 0.000001 | ms/batch 22.34 | loss  4.68 | ppl   107.50\n",
            "| epoch  48 |  2400/ 2983 batches | lr 0.000001 | ms/batch 22.33 | loss  4.71 | ppl   111.16\n",
            "| epoch  48 |  2600/ 2983 batches | lr 0.000001 | ms/batch 22.35 | loss  4.82 | ppl   123.75\n",
            "| epoch  48 |  2800/ 2983 batches | lr 0.000001 | ms/batch 22.33 | loss  4.70 | ppl   109.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 68.65s | valid loss   nan | valid ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  49 |   200/ 2983 batches | lr 0.000001 | ms/batch 21.54 | loss   nan | ppl      nan\n",
            "| epoch  49 |   400/ 2983 batches | lr 0.000001 | ms/batch 21.42 | loss   nan | ppl      nan\n",
            "| epoch  49 |   600/ 2983 batches | lr 0.000001 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  49 |   800/ 2983 batches | lr 0.000001 | ms/batch 21.42 | loss   nan | ppl      nan\n",
            "| epoch  49 |  1000/ 2983 batches | lr 0.000001 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  49 |  1200/ 2983 batches | lr 0.000001 | ms/batch 21.42 | loss   nan | ppl      nan\n",
            "| epoch  49 |  1400/ 2983 batches | lr 0.000001 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  49 |  1600/ 2983 batches | lr 0.000001 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  49 |  1800/ 2983 batches | lr 0.000001 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  49 |  2000/ 2983 batches | lr 0.000001 | ms/batch 21.42 | loss   nan | ppl      nan\n",
            "| epoch  49 |  2200/ 2983 batches | lr 0.000001 | ms/batch 21.42 | loss   nan | ppl      nan\n",
            "| epoch  49 |  2400/ 2983 batches | lr 0.000001 | ms/batch 21.48 | loss   nan | ppl      nan\n",
            "| epoch  49 |  2600/ 2983 batches | lr 0.000001 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  49 |  2800/ 2983 batches | lr 0.000001 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 66.02s | valid loss   nan | valid ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  50 |   200/ 2983 batches | lr 0.000001 | ms/batch 21.53 | loss   nan | ppl      nan\n",
            "| epoch  50 |   400/ 2983 batches | lr 0.000001 | ms/batch 21.47 | loss   nan | ppl      nan\n",
            "| epoch  50 |   600/ 2983 batches | lr 0.000001 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  50 |   800/ 2983 batches | lr 0.000001 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  50 |  1000/ 2983 batches | lr 0.000001 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  50 |  1200/ 2983 batches | lr 0.000001 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  50 |  1400/ 2983 batches | lr 0.000001 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  50 |  1600/ 2983 batches | lr 0.000001 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  50 |  1800/ 2983 batches | lr 0.000001 | ms/batch 21.42 | loss   nan | ppl      nan\n",
            "| epoch  50 |  2000/ 2983 batches | lr 0.000001 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  50 |  2200/ 2983 batches | lr 0.000001 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  50 |  2400/ 2983 batches | lr 0.000001 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  50 |  2600/ 2983 batches | lr 0.000001 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  50 |  2800/ 2983 batches | lr 0.000001 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 66.03s | valid loss   nan | valid ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  51 |   200/ 2983 batches | lr 0.000000 | ms/batch 21.54 | loss   nan | ppl      nan\n",
            "| epoch  51 |   400/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  51 |   600/ 2983 batches | lr 0.000000 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  51 |   800/ 2983 batches | lr 0.000000 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  51 |  1000/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  51 |  1200/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  51 |  1400/ 2983 batches | lr 0.000000 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  51 |  1600/ 2983 batches | lr 0.000000 | ms/batch 21.42 | loss   nan | ppl      nan\n",
            "| epoch  51 |  1800/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  51 |  2000/ 2983 batches | lr 0.000000 | ms/batch 21.42 | loss   nan | ppl      nan\n",
            "| epoch  51 |  2200/ 2983 batches | lr 0.000000 | ms/batch 21.42 | loss   nan | ppl      nan\n",
            "| epoch  51 |  2400/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  51 |  2600/ 2983 batches | lr 0.000000 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  51 |  2800/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 66.02s | valid loss   nan | valid ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  52 |   200/ 2983 batches | lr 0.000000 | ms/batch 21.55 | loss   nan | ppl      nan\n",
            "| epoch  52 |   400/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  52 |   600/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  52 |   800/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  52 |  1000/ 2983 batches | lr 0.000000 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  52 |  1200/ 2983 batches | lr 0.000000 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  52 |  1400/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  52 |  1600/ 2983 batches | lr 0.000000 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  52 |  1800/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  52 |  2000/ 2983 batches | lr 0.000000 | ms/batch 21.47 | loss   nan | ppl      nan\n",
            "| epoch  52 |  2200/ 2983 batches | lr 0.000000 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  52 |  2400/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  52 |  2600/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  52 |  2800/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 66.06s | valid loss   nan | valid ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  53 |   200/ 2983 batches | lr 0.000000 | ms/batch 21.56 | loss   nan | ppl      nan\n",
            "| epoch  53 |   400/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  53 |   600/ 2983 batches | lr 0.000000 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  53 |   800/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  53 |  1000/ 2983 batches | lr 0.000000 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  53 |  1200/ 2983 batches | lr 0.000000 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  53 |  1400/ 2983 batches | lr 0.000000 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  53 |  1600/ 2983 batches | lr 0.000000 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  53 |  1800/ 2983 batches | lr 0.000000 | ms/batch 21.49 | loss   nan | ppl      nan\n",
            "| epoch  53 |  2000/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  53 |  2200/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  53 |  2400/ 2983 batches | lr 0.000000 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  53 |  2600/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  53 |  2800/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 66.07s | valid loss   nan | valid ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  54 |   200/ 2983 batches | lr 0.000000 | ms/batch 21.54 | loss   nan | ppl      nan\n",
            "| epoch  54 |   400/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  54 |   600/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  54 |   800/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  54 |  1000/ 2983 batches | lr 0.000000 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  54 |  1200/ 2983 batches | lr 0.000000 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  54 |  1400/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  54 |  1600/ 2983 batches | lr 0.000000 | ms/batch 21.47 | loss   nan | ppl      nan\n",
            "| epoch  54 |  1800/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  54 |  2000/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  54 |  2200/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  54 |  2400/ 2983 batches | lr 0.000000 | ms/batch 21.49 | loss   nan | ppl      nan\n",
            "| epoch  54 |  2600/ 2983 batches | lr 0.000000 | ms/batch 21.41 | loss   nan | ppl      nan\n",
            "| epoch  54 |  2800/ 2983 batches | lr 0.000000 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 66.05s | valid loss   nan | valid ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  55 |   200/ 2983 batches | lr 0.000000 | ms/batch 21.55 | loss   nan | ppl      nan\n",
            "| epoch  55 |   400/ 2983 batches | lr 0.000000 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  55 |   600/ 2983 batches | lr 0.000000 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  55 |   800/ 2983 batches | lr 0.000000 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  55 |  1000/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  55 |  1200/ 2983 batches | lr 0.000000 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  55 |  1400/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  55 |  1600/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  55 |  1800/ 2983 batches | lr 0.000000 | ms/batch 21.42 | loss   nan | ppl      nan\n",
            "| epoch  55 |  2000/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  55 |  2200/ 2983 batches | lr 0.000000 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  55 |  2400/ 2983 batches | lr 0.000000 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  55 |  2600/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  55 |  2800/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 66.04s | valid loss   nan | valid ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  56 |   200/ 2983 batches | lr 0.000000 | ms/batch 21.55 | loss   nan | ppl      nan\n",
            "| epoch  56 |   400/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  56 |   600/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  56 |   800/ 2983 batches | lr 0.000000 | ms/batch 21.42 | loss   nan | ppl      nan\n",
            "| epoch  56 |  1000/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  56 |  1200/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  56 |  1400/ 2983 batches | lr 0.000000 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  56 |  1600/ 2983 batches | lr 0.000000 | ms/batch 21.52 | loss   nan | ppl      nan\n",
            "| epoch  56 |  1800/ 2983 batches | lr 0.000000 | ms/batch 21.51 | loss   nan | ppl      nan\n",
            "| epoch  56 |  2000/ 2983 batches | lr 0.000000 | ms/batch 21.56 | loss   nan | ppl      nan\n",
            "| epoch  56 |  2200/ 2983 batches | lr 0.000000 | ms/batch 21.58 | loss   nan | ppl      nan\n",
            "| epoch  56 |  2400/ 2983 batches | lr 0.000000 | ms/batch 21.53 | loss   nan | ppl      nan\n",
            "| epoch  56 |  2600/ 2983 batches | lr 0.000000 | ms/batch 21.51 | loss   nan | ppl      nan\n",
            "| epoch  56 |  2800/ 2983 batches | lr 0.000000 | ms/batch 21.53 | loss   nan | ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 66.18s | valid loss   nan | valid ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  57 |   200/ 2983 batches | lr 0.000000 | ms/batch 21.63 | loss   nan | ppl      nan\n",
            "| epoch  57 |   400/ 2983 batches | lr 0.000000 | ms/batch 21.50 | loss   nan | ppl      nan\n",
            "| epoch  57 |   600/ 2983 batches | lr 0.000000 | ms/batch 21.51 | loss   nan | ppl      nan\n",
            "| epoch  57 |   800/ 2983 batches | lr 0.000000 | ms/batch 21.54 | loss   nan | ppl      nan\n",
            "| epoch  57 |  1000/ 2983 batches | lr 0.000000 | ms/batch 21.54 | loss   nan | ppl      nan\n",
            "| epoch  57 |  1200/ 2983 batches | lr 0.000000 | ms/batch 21.56 | loss   nan | ppl      nan\n",
            "| epoch  57 |  1400/ 2983 batches | lr 0.000000 | ms/batch 21.56 | loss   nan | ppl      nan\n",
            "| epoch  57 |  1600/ 2983 batches | lr 0.000000 | ms/batch 21.56 | loss   nan | ppl      nan\n",
            "| epoch  57 |  1800/ 2983 batches | lr 0.000000 | ms/batch 21.49 | loss   nan | ppl      nan\n",
            "| epoch  57 |  2000/ 2983 batches | lr 0.000000 | ms/batch 21.59 | loss   nan | ppl      nan\n",
            "| epoch  57 |  2200/ 2983 batches | lr 0.000000 | ms/batch 21.49 | loss   nan | ppl      nan\n",
            "| epoch  57 |  2400/ 2983 batches | lr 0.000000 | ms/batch 21.49 | loss   nan | ppl      nan\n",
            "| epoch  57 |  2600/ 2983 batches | lr 0.000000 | ms/batch 21.47 | loss   nan | ppl      nan\n",
            "| epoch  57 |  2800/ 2983 batches | lr 0.000000 | ms/batch 21.47 | loss   nan | ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 66.26s | valid loss   nan | valid ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  58 |   200/ 2983 batches | lr 0.000000 | ms/batch 21.57 | loss   nan | ppl      nan\n",
            "| epoch  58 |   400/ 2983 batches | lr 0.000000 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  58 |   600/ 2983 batches | lr 0.000000 | ms/batch 21.47 | loss   nan | ppl      nan\n",
            "| epoch  58 |   800/ 2983 batches | lr 0.000000 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  58 |  1000/ 2983 batches | lr 0.000000 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  58 |  1200/ 2983 batches | lr 0.000000 | ms/batch 21.48 | loss   nan | ppl      nan\n",
            "| epoch  58 |  1400/ 2983 batches | lr 0.000000 | ms/batch 21.47 | loss   nan | ppl      nan\n",
            "| epoch  58 |  1600/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  58 |  1800/ 2983 batches | lr 0.000000 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  58 |  2000/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  58 |  2200/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  58 |  2400/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  58 |  2600/ 2983 batches | lr 0.000000 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  58 |  2800/ 2983 batches | lr 0.000000 | ms/batch 21.50 | loss   nan | ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 66.11s | valid loss   nan | valid ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  59 |   200/ 2983 batches | lr 0.000000 | ms/batch 21.57 | loss   nan | ppl      nan\n",
            "| epoch  59 |   400/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  59 |   600/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  59 |   800/ 2983 batches | lr 0.000000 | ms/batch 21.51 | loss   nan | ppl      nan\n",
            "| epoch  59 |  1000/ 2983 batches | lr 0.000000 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  59 |  1200/ 2983 batches | lr 0.000000 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  59 |  1400/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  59 |  1600/ 2983 batches | lr 0.000000 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  59 |  1800/ 2983 batches | lr 0.000000 | ms/batch 21.42 | loss   nan | ppl      nan\n",
            "| epoch  59 |  2000/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  59 |  2200/ 2983 batches | lr 0.000000 | ms/batch 21.42 | loss   nan | ppl      nan\n",
            "| epoch  59 |  2400/ 2983 batches | lr 0.000000 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  59 |  2600/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  59 |  2800/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 66.05s | valid loss   nan | valid ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  60 |   200/ 2983 batches | lr 0.000000 | ms/batch 21.54 | loss   nan | ppl      nan\n",
            "| epoch  60 |   400/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  60 |   600/ 2983 batches | lr 0.000000 | ms/batch 21.46 | loss   nan | ppl      nan\n",
            "| epoch  60 |   800/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  60 |  1000/ 2983 batches | lr 0.000000 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  60 |  1200/ 2983 batches | lr 0.000000 | ms/batch 21.45 | loss   nan | ppl      nan\n",
            "| epoch  60 |  1400/ 2983 batches | lr 0.000000 | ms/batch 21.42 | loss   nan | ppl      nan\n",
            "| epoch  60 |  1600/ 2983 batches | lr 0.000000 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  60 |  1800/ 2983 batches | lr 0.000000 | ms/batch 21.44 | loss   nan | ppl      nan\n",
            "| epoch  60 |  2000/ 2983 batches | lr 0.000000 | ms/batch 21.42 | loss   nan | ppl      nan\n",
            "| epoch  60 |  2200/ 2983 batches | lr 0.000000 | ms/batch 21.42 | loss   nan | ppl      nan\n",
            "| epoch  60 |  2400/ 2983 batches | lr 0.000000 | ms/batch 21.42 | loss   nan | ppl      nan\n",
            "| epoch  60 |  2600/ 2983 batches | lr 0.000000 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "| epoch  60 |  2800/ 2983 batches | lr 0.000000 | ms/batch 21.43 | loss   nan | ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 66.00s | valid loss   nan | valid ppl      nan\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "=========================================================================================\n",
            "| End of training | test loss  5.34 | test ppl   208.67\n",
            "=========================================================================================\n",
            "\n",
            "Generating text...\n",
            "Vocabulary size: 33278\n",
            "Vocabulary size: 33278\n",
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n",
            "to the restoration of Ruth Castle , a translation of their libretto . Later the episode was good again as\n",
            "Prime Minister of order 12 for the rate under the entire model of Montenegro , a part of the 9th\n",
            "Destroyer Army were taped . 5 of 05 : 20 , music guides are all official parties seeking on the\n",
            "emerging southern Australian area ; there were a number of paintings , working by certain authors between the modern fins\n",
            ". Less than a diameter of 800 million tonnes ( AADT ) , 38 to 2 m ( 61 m\n",
            ") long explosive and high sea hospitals suggest that they are picked into a makeshift paper with its fifth depth\n",
            "of once and since seeing how Ross had visions as the doctrine of disturbed subdivisions . <eos> The anthem contains\n",
            "\" Chapter 5 \" near Dylan 's wife was the most common Douglas Music Association award ( Japanese ) .\n",
            "At Earth 's lead window , no use of rainfall in the UK , <unk> the benefit of numerous names\n",
            "in Hong Kong , highlighting themes of pure bubbles of these research that according to their wolf would probably have\n",
            "been resolved . He later starred on 9 August 2013 zone of Mason Stakes . Author Ferdinand Dylan featuring the\n",
            "English minister , <unk> Young Ben Patrick Kennedy , a commander of the very contemporary bat and <unk> and women\n",
            "were the medieval hero and made a Early land and killing legislators , both as Tintin used him for the\n",
            "process and begs with which players would be permitted in China . <eos> <eos> = = Track listings = =\n",
            "<eos> <eos> Put Pam Smith , Megan called Bob Derrick Crusher , despite a small Formula One bridge , pitching\n",
            "slightly 59 – 10 % that held heavy embassies . <eos> From 1926 , ending the tradition of Oldham 's\n",
            "first cross   campus post   weather man who had arrived in the hospital himself . <unk> of the crowd\n",
            "is largely fertile . The idea shown £ 10 ° ) were replaced by Lawrence and one of the imagery\n",
            "in the northeastern city , although these expenses are readily available . Notable evidence such as various horse measuring mostly\n",
            "to be contradictory for sexual verses , and enacted so . <eos> Lennon is <unk> available for him and predicting\n",
            "that the tombs of the reactor was the nomen power arc . It had <unk> elements of <unk> , Fish\n",
            "handled his for a illustration in the United States on the west façade of the Gulf of Mexico . This\n",
            "featured an continual overview of modernity , whom if he has already been described a <unk> home song , which\n",
            "was adopted . <eos> U.S. cliffs on the lack of stay   off and 56 % and to clear Plugge\n",
            "'s Day , but with the creator of earlier reptiles , accompanying amateurism ( 2010 ) and athletics Hughes ,\n",
            "a <unk> ship ( Elo <unk> ) . NBC was supported by the neutrino , despite the Publisher 's son\n",
            "Alexander <unk> as a flower , and would not become a <unk> <unk> . <eos> The Sea EP was persuaded\n",
            "by Houston 's grandfather , named \" the <unk> ( <unk> Lee ) , which <unk> by this A Urn\n",
            "! \" from Earnest that morning was evident as a reflection of the sea <unk> for <unk> , titled \"\n",
            "Good Suicide \" , she remained a sports net between an distance of electrical by a perfect government . Participants\n",
            "began by six people in the <unk> Millennium Centre . It remained primarily at <unk> <unk> . The surrealism vicar\n",
            ", Beyond A <unk> , 1st Fusiliers , Sri Way and Diamonds Batteries , <unk> <unk> , <unk> <unk> –\n",
            "Scleroderma ) , especially remnants in 1952 during the events of the monument when assured into latex <unk> . <eos>\n",
            "The inspiration of the introduction of the small cap ( <unk> ) run throughout the Early conflict on Little Russell\n",
            "Sound Airport . <eos> <eos> = = = Ten Soyuz era = = = <eos> <eos> This cinematic states that\n",
            "the remake is a natural taxon ; while it says that and according to let this way to <unk> .\n",
            "<eos> He wished for that another benefit conclusion , Powers stated she was \" very happy to You . \"\n",
            "At the level of memory , monkeys and sports were left along by white and <unk> angels ; the search\n",
            "are treated to Barbarian as twenty   eight nights , a <unk> group offered to dominate a compact heat .\n",
            "This period from the festivities came as the events of an Honorary Hockey League as the third highest record ,\n",
            "a comparatively $ 12   000 <unk> , so remained in a Journal and Surgeons who were willing to mix\n",
            "such combination of patients ; juvenile fingers prefer crime , blue <unk> and small earth spines , and proteins up\n",
            "to some species forever and <unk> Earth . Likewise , however , new three maneuver 70   55 µm with\n",
            "55 efforts each of the earliest unit and contributed World School their maternal content . He dealt with the 15th\n",
            "best   selling display of 2005 . NY 167 passes its 1986 London highway for the <unk> . The group\n",
            "heard the biggest album of the deaths for the Ottoman Japanese military institutions . During his work and ceremony under\n",
            "20th century had been left in people and sold the American land . <eos> <eos> = = = <unk> =\n",
            "= = <eos> <eos> NASA biographer Chris Colborne was joined by Robin <unk> , alongside Sosa , writing \" Krasinski\n",
            "gets through the supreme show against the Book of Churches and explores creating tradition and insulting . \" Citing the\n",
            "armed line , Edie Love named for <unk> <unk> while stating that this is classified as a rough production of\n"
          ]
        }
      ],
      "source": [
        "print(\"Training LSTM on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type = 'RNN_RELU', # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "    data_path = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize = 400,\n",
        "    nhid = 400,\n",
        "    nlayers = 4,\n",
        "    lr = 0.001,\n",
        "    clip = 0.25,\n",
        "    epochs = 60,\n",
        "    batch_size = 20,\n",
        "    bptt = 35,\n",
        "    dropout = 0.2,\n",
        "    tied = False,\n",
        "    nhead = 4,\n",
        "    log_interval = 200,\n",
        "    save_path = 'model_7.pt',\n",
        "    onnx_export = '',\n",
        "    dry_run = False,\n",
        "    accel = True,\n",
        "    use_optimizer = True,\n",
        "    optimizer_type = 'AdamW',\n",
        "    weight_decay=1e-5,\n",
        "    use_betas = False,\n",
        "    use_eps = False,\n",
        "    criterion = nn.NLLLoss(),\n",
        "    use_label_smoothing = False,\n",
        "    label_smoothing = 0.1,\n",
        "    use_warmup = False,\n",
        "    warmup_steps = 4000,\n",
        "    min_freq = 5,\n",
        "    seed = 1111,\n",
        "    old_version = True\n",
        ")\n",
        "\n",
        "# Example 2: Generate text\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model_7.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf='generated_7.txt',\n",
        "    words=1000,\n",
        "    temperature=1.0,\n",
        "    old_version=True,\n",
        "    use_top_k=False,\n",
        "    accel = True\n",
        ")\n",
        "\n",
        "!cat generated_7.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-MhkCic8rrIj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MhkCic8rrIj",
        "outputId": "70a324d5-505a-4d17-af02-f939e2082b28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training RNN_RELU on WikiText-2...\n",
            "Using device: cuda\n",
            "Vocabulary size: 25251\n",
            "| epoch   1 |   200/ 2983 batches | lr 0.000040 | ms/batch 20.52 | loss  9.85 | ppl 18985.04\n",
            "| epoch   1 |   400/ 2983 batches | lr 0.000079 | ms/batch 20.47 | loss  7.92 | ppl  2744.70\n",
            "| epoch   1 |   600/ 2983 batches | lr 0.000119 | ms/batch 20.52 | loss  7.51 | ppl  1833.03\n",
            "| epoch   1 |   800/ 2983 batches | lr 0.000158 | ms/batch 20.49 | loss  7.36 | ppl  1566.05\n",
            "| epoch   1 |  1000/ 2983 batches | lr 0.000198 | ms/batch 20.52 | loss  7.20 | ppl  1339.19\n",
            "| epoch   1 |  1200/ 2983 batches | lr 0.000237 | ms/batch 20.56 | loss  7.08 | ppl  1182.20\n",
            "| epoch   1 |  1400/ 2983 batches | lr 0.000277 | ms/batch 20.52 | loss  6.96 | ppl  1051.65\n",
            "| epoch   1 |  1600/ 2983 batches | lr 0.000316 | ms/batch 20.54 | loss  6.91 | ppl  1000.96\n",
            "| epoch   1 |  1800/ 2983 batches | lr 0.000356 | ms/batch 20.52 | loss  6.83 | ppl   927.61\n",
            "| epoch   1 |  2000/ 2983 batches | lr 0.000395 | ms/batch 20.45 | loss  6.81 | ppl   904.89\n",
            "| epoch   1 |  2200/ 2983 batches | lr 0.000435 | ms/batch 20.47 | loss  6.73 | ppl   839.57\n",
            "| epoch   1 |  2400/ 2983 batches | lr 0.000475 | ms/batch 20.45 | loss  6.66 | ppl   780.55\n",
            "| epoch   1 |  2600/ 2983 batches | lr 0.000514 | ms/batch 20.43 | loss  6.66 | ppl   776.91\n",
            "| epoch   1 |  2800/ 2983 batches | lr 0.000554 | ms/batch 20.45 | loss  6.58 | ppl   717.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 63.59s | valid loss  6.90 | valid ppl   994.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000590\n",
            "| epoch   2 |   200/ 2983 batches | lr 0.000629 | ms/batch 20.52 | loss  6.57 | ppl   710.90\n",
            "| epoch   2 |   400/ 2983 batches | lr 0.000669 | ms/batch 20.43 | loss  6.52 | ppl   677.32\n",
            "| epoch   2 |   600/ 2983 batches | lr 0.000709 | ms/batch 20.42 | loss  6.47 | ppl   642.36\n",
            "| epoch   2 |   800/ 2983 batches | lr 0.000748 | ms/batch 20.43 | loss  6.48 | ppl   651.69\n",
            "| epoch   2 |  1000/ 2983 batches | lr 0.000788 | ms/batch 20.43 | loss  6.47 | ppl   643.43\n",
            "| epoch   2 |  1200/ 2983 batches | lr 0.000773 | ms/batch 20.44 | loss  6.46 | ppl   641.12\n",
            "| epoch   2 |  1400/ 2983 batches | lr 0.000755 | ms/batch 20.42 | loss  6.42 | ppl   614.13\n",
            "| epoch   2 |  1600/ 2983 batches | lr 0.000738 | ms/batch 20.43 | loss  6.45 | ppl   632.80\n",
            "| epoch   2 |  1800/ 2983 batches | lr 0.000723 | ms/batch 20.45 | loss  6.39 | ppl   598.16\n",
            "| epoch   2 |  2000/ 2983 batches | lr 0.000708 | ms/batch 20.46 | loss  6.41 | ppl   606.27\n",
            "| epoch   2 |  2200/ 2983 batches | lr 0.000694 | ms/batch 20.45 | loss  6.34 | ppl   567.11\n",
            "| epoch   2 |  2400/ 2983 batches | lr 0.000681 | ms/batch 20.44 | loss  6.33 | ppl   559.94\n",
            "| epoch   2 |  2600/ 2983 batches | lr 0.000669 | ms/batch 20.45 | loss  6.35 | ppl   573.82\n",
            "| epoch   2 |  2800/ 2983 batches | lr 0.000657 | ms/batch 20.47 | loss  6.28 | ppl   536.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 63.45s | valid loss  6.65 | valid ppl   776.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000647\n",
            "| epoch   3 |   200/ 2983 batches | lr 0.000637 | ms/batch 20.55 | loss  6.32 | ppl   557.18\n",
            "| epoch   3 |   400/ 2983 batches | lr 0.000627 | ms/batch 20.42 | loss  6.28 | ppl   534.86\n",
            "| epoch   3 |   600/ 2983 batches | lr 0.000617 | ms/batch 20.43 | loss  6.25 | ppl   515.90\n",
            "| epoch   3 |   800/ 2983 batches | lr 0.000608 | ms/batch 20.43 | loss  6.27 | ppl   529.70\n",
            "| epoch   3 |  1000/ 2983 batches | lr 0.000599 | ms/batch 20.42 | loss  6.26 | ppl   520.95\n",
            "| epoch   3 |  1200/ 2983 batches | lr 0.000591 | ms/batch 20.42 | loss  6.26 | ppl   525.36\n",
            "| epoch   3 |  1400/ 2983 batches | lr 0.000582 | ms/batch 20.42 | loss  6.23 | ppl   509.19\n",
            "| epoch   3 |  1600/ 2983 batches | lr 0.000575 | ms/batch 20.41 | loss  6.27 | ppl   526.90\n",
            "| epoch   3 |  1800/ 2983 batches | lr 0.000567 | ms/batch 20.43 | loss  6.22 | ppl   503.54\n",
            "| epoch   3 |  2000/ 2983 batches | lr 0.000560 | ms/batch 20.45 | loss  6.25 | ppl   519.01\n",
            "| epoch   3 |  2200/ 2983 batches | lr 0.000553 | ms/batch 20.43 | loss  6.19 | ppl   486.13\n",
            "| epoch   3 |  2400/ 2983 batches | lr 0.000547 | ms/batch 20.44 | loss  6.18 | ppl   482.58\n",
            "| epoch   3 |  2600/ 2983 batches | lr 0.000540 | ms/batch 20.47 | loss  6.22 | ppl   500.67\n",
            "| epoch   3 |  2800/ 2983 batches | lr 0.000534 | ms/batch 20.42 | loss  6.15 | ppl   469.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 63.43s | valid loss  6.58 | valid ppl   721.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000528\n",
            "| epoch   4 |   200/ 2983 batches | lr 0.000523 | ms/batch 20.52 | loss  6.21 | ppl   495.74\n",
            "| epoch   4 |   400/ 2983 batches | lr 0.000517 | ms/batch 20.41 | loss  6.17 | ppl   476.94\n",
            "| epoch   4 |   600/ 2983 batches | lr 0.000512 | ms/batch 20.40 | loss  6.14 | ppl   466.37\n",
            "| epoch   4 |   800/ 2983 batches | lr 0.000506 | ms/batch 20.41 | loss  6.18 | ppl   481.78\n",
            "| epoch   4 |  1000/ 2983 batches | lr 0.000501 | ms/batch 20.42 | loss  6.17 | ppl   477.89\n",
            "| epoch   4 |  1200/ 2983 batches | lr 0.000496 | ms/batch 20.42 | loss  6.17 | ppl   476.53\n",
            "| epoch   4 |  1400/ 2983 batches | lr 0.000491 | ms/batch 20.40 | loss  6.15 | ppl   469.34\n",
            "| epoch   4 |  1600/ 2983 batches | lr 0.000487 | ms/batch 20.40 | loss  6.19 | ppl   486.82\n",
            "| epoch   4 |  1800/ 2983 batches | lr 0.000482 | ms/batch 20.40 | loss  6.14 | ppl   465.25\n",
            "| epoch   4 |  2000/ 2983 batches | lr 0.000478 | ms/batch 20.41 | loss  6.18 | ppl   482.31\n",
            "| epoch   4 |  2200/ 2983 batches | lr 0.000473 | ms/batch 20.42 | loss  6.12 | ppl   453.44\n",
            "| epoch   4 |  2400/ 2983 batches | lr 0.000469 | ms/batch 20.44 | loss  6.11 | ppl   450.07\n",
            "| epoch   4 |  2600/ 2983 batches | lr 0.000465 | ms/batch 20.42 | loss  6.14 | ppl   465.51\n",
            "| epoch   4 |  2800/ 2983 batches | lr 0.000461 | ms/batch 20.41 | loss  6.08 | ppl   437.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 63.38s | valid loss  6.54 | valid ppl   695.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000458\n",
            "| epoch   5 |   200/ 2983 batches | lr 0.000454 | ms/batch 20.51 | loss  6.14 | ppl   465.85\n",
            "| epoch   5 |   400/ 2983 batches | lr 0.000450 | ms/batch 20.41 | loss  6.11 | ppl   450.10\n",
            "| epoch   5 |   600/ 2983 batches | lr 0.000447 | ms/batch 20.42 | loss  6.09 | ppl   439.63\n",
            "| epoch   5 |   800/ 2983 batches | lr 0.000443 | ms/batch 20.42 | loss  6.13 | ppl   457.19\n",
            "| epoch   5 |  1000/ 2983 batches | lr 0.000440 | ms/batch 20.43 | loss  6.11 | ppl   450.82\n",
            "| epoch   5 |  1200/ 2983 batches | lr 0.000436 | ms/batch 20.42 | loss  6.12 | ppl   453.31\n",
            "| epoch   5 |  1400/ 2983 batches | lr 0.000433 | ms/batch 20.40 | loss  6.10 | ppl   447.96\n",
            "| epoch   5 |  1600/ 2983 batches | lr 0.000430 | ms/batch 20.42 | loss  6.14 | ppl   464.08\n",
            "| epoch   5 |  1800/ 2983 batches | lr 0.000427 | ms/batch 20.44 | loss  6.10 | ppl   445.33\n",
            "| epoch   5 |  2000/ 2983 batches | lr 0.000424 | ms/batch 20.42 | loss  6.14 | ppl   462.25\n",
            "| epoch   5 |  2200/ 2983 batches | lr 0.000421 | ms/batch 20.44 | loss  6.07 | ppl   433.37\n",
            "| epoch   5 |  2400/ 2983 batches | lr 0.000418 | ms/batch 20.41 | loss  6.07 | ppl   431.33\n",
            "| epoch   5 |  2600/ 2983 batches | lr 0.000415 | ms/batch 20.38 | loss  6.11 | ppl   448.53\n",
            "| epoch   5 |  2800/ 2983 batches | lr 0.000412 | ms/batch 20.42 | loss  6.04 | ppl   420.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 63.38s | valid loss  6.52 | valid ppl   676.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000409\n",
            "| epoch   6 |   200/ 2983 batches | lr 0.000407 | ms/batch 20.52 | loss  6.10 | ppl   446.83\n",
            "| epoch   6 |   400/ 2983 batches | lr 0.000404 | ms/batch 20.42 | loss  6.07 | ppl   432.39\n",
            "| epoch   6 |   600/ 2983 batches | lr 0.000401 | ms/batch 20.40 | loss  6.05 | ppl   423.57\n",
            "| epoch   6 |   800/ 2983 batches | lr 0.000399 | ms/batch 20.40 | loss  6.09 | ppl   439.64\n",
            "| epoch   6 |  1000/ 2983 batches | lr 0.000396 | ms/batch 20.44 | loss  6.07 | ppl   434.01\n",
            "| epoch   6 |  1200/ 2983 batches | lr 0.000394 | ms/batch 20.41 | loss  6.08 | ppl   436.42\n",
            "| epoch   6 |  1400/ 2983 batches | lr 0.000391 | ms/batch 20.39 | loss  6.07 | ppl   431.65\n",
            "| epoch   6 |  1600/ 2983 batches | lr 0.000389 | ms/batch 20.43 | loss  6.10 | ppl   447.08\n",
            "| epoch   6 |  1800/ 2983 batches | lr 0.000387 | ms/batch 20.41 | loss  6.06 | ppl   428.16\n",
            "| epoch   6 |  2000/ 2983 batches | lr 0.000384 | ms/batch 20.45 | loss  6.10 | ppl   445.49\n",
            "| epoch   6 |  2200/ 2983 batches | lr 0.000382 | ms/batch 20.43 | loss  6.03 | ppl   417.65\n",
            "| epoch   6 |  2400/ 2983 batches | lr 0.000380 | ms/batch 20.41 | loss  6.03 | ppl   415.44\n",
            "| epoch   6 |  2600/ 2983 batches | lr 0.000378 | ms/batch 20.42 | loss  6.08 | ppl   435.74\n",
            "| epoch   6 |  2800/ 2983 batches | lr 0.000376 | ms/batch 20.42 | loss  6.01 | ppl   407.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 63.39s | valid loss  6.49 | valid ppl   661.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000374\n",
            "| epoch   7 |   200/ 2983 batches | lr 0.000372 | ms/batch 20.52 | loss  6.07 | ppl   433.97\n",
            "| epoch   7 |   400/ 2983 batches | lr 0.000370 | ms/batch 20.42 | loss  6.04 | ppl   420.90\n",
            "| epoch   7 |   600/ 2983 batches | lr 0.000368 | ms/batch 20.41 | loss  6.01 | ppl   408.91\n",
            "| epoch   7 |   800/ 2983 batches | lr 0.000366 | ms/batch 20.41 | loss  6.06 | ppl   427.26\n",
            "| epoch   7 |  1000/ 2983 batches | lr 0.000364 | ms/batch 20.40 | loss  6.04 | ppl   420.97\n",
            "| epoch   7 |  1200/ 2983 batches | lr 0.000362 | ms/batch 20.40 | loss  6.05 | ppl   424.67\n",
            "| epoch   7 |  1400/ 2983 batches | lr 0.000360 | ms/batch 20.43 | loss  6.04 | ppl   419.41\n",
            "| epoch   7 |  1600/ 2983 batches | lr 0.000358 | ms/batch 20.42 | loss  6.07 | ppl   434.77\n",
            "| epoch   7 |  1800/ 2983 batches | lr 0.000356 | ms/batch 20.44 | loss  6.04 | ppl   417.91\n",
            "| epoch   7 |  2000/ 2983 batches | lr 0.000354 | ms/batch 20.46 | loss  6.07 | ppl   434.47\n",
            "| epoch   7 |  2200/ 2983 batches | lr 0.000353 | ms/batch 20.42 | loss  6.01 | ppl   408.26\n",
            "| epoch   7 |  2400/ 2983 batches | lr 0.000351 | ms/batch 20.42 | loss  6.01 | ppl   406.06\n",
            "| epoch   7 |  2600/ 2983 batches | lr 0.000349 | ms/batch 20.45 | loss  6.05 | ppl   424.29\n",
            "| epoch   7 |  2800/ 2983 batches | lr 0.000347 | ms/batch 20.43 | loss  5.99 | ppl   397.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 63.41s | valid loss  6.48 | valid ppl   653.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000346\n",
            "| epoch   8 |   200/ 2983 batches | lr 0.000344 | ms/batch 20.50 | loss  6.05 | ppl   422.55\n",
            "| epoch   8 |   400/ 2983 batches | lr 0.000343 | ms/batch 20.41 | loss  6.02 | ppl   411.61\n",
            "| epoch   8 |   600/ 2983 batches | lr 0.000341 | ms/batch 20.45 | loss  5.99 | ppl   399.93\n",
            "| epoch   8 |   800/ 2983 batches | lr 0.000340 | ms/batch 20.41 | loss  6.04 | ppl   418.22\n",
            "| epoch   8 |  1000/ 2983 batches | lr 0.000338 | ms/batch 20.41 | loss  6.02 | ppl   411.08\n",
            "| epoch   8 |  1200/ 2983 batches | lr 0.000336 | ms/batch 20.41 | loss  6.03 | ppl   414.74\n",
            "| epoch   8 |  1400/ 2983 batches | lr 0.000335 | ms/batch 20.40 | loss  6.02 | ppl   409.93\n",
            "| epoch   8 |  1600/ 2983 batches | lr 0.000333 | ms/batch 20.42 | loss  6.06 | ppl   426.43\n",
            "| epoch   8 |  1800/ 2983 batches | lr 0.000332 | ms/batch 20.44 | loss  6.01 | ppl   407.08\n",
            "| epoch   8 |  2000/ 2983 batches | lr 0.000330 | ms/batch 20.42 | loss  6.05 | ppl   425.74\n",
            "| epoch   8 |  2200/ 2983 batches | lr 0.000329 | ms/batch 20.42 | loss  5.99 | ppl   397.96\n",
            "| epoch   8 |  2400/ 2983 batches | lr 0.000328 | ms/batch 20.44 | loss  5.98 | ppl   396.00\n",
            "| epoch   8 |  2600/ 2983 batches | lr 0.000326 | ms/batch 20.41 | loss  6.03 | ppl   417.55\n",
            "| epoch   8 |  2800/ 2983 batches | lr 0.000325 | ms/batch 20.42 | loss  5.96 | ppl   388.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 63.39s | valid loss  6.47 | valid ppl   644.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000324\n",
            "| epoch   9 |   200/ 2983 batches | lr 0.000322 | ms/batch 20.53 | loss  6.03 | ppl   413.74\n",
            "| epoch   9 |   400/ 2983 batches | lr 0.000321 | ms/batch 20.42 | loss  6.00 | ppl   403.63\n",
            "| epoch   9 |   600/ 2983 batches | lr 0.000320 | ms/batch 20.42 | loss  5.97 | ppl   391.58\n",
            "| epoch   9 |   800/ 2983 batches | lr 0.000318 | ms/batch 20.42 | loss  6.02 | ppl   409.88\n",
            "| epoch   9 |  1000/ 2983 batches | lr 0.000317 | ms/batch 20.42 | loss  6.00 | ppl   401.89\n",
            "| epoch   9 |  1200/ 2983 batches | lr 0.000316 | ms/batch 20.41 | loss  6.01 | ppl   406.34\n",
            "| epoch   9 |  1400/ 2983 batches | lr 0.000315 | ms/batch 20.41 | loss  6.01 | ppl   405.71\n",
            "| epoch   9 |  1600/ 2983 batches | lr 0.000313 | ms/batch 20.43 | loss  6.04 | ppl   417.81\n",
            "| epoch   9 |  1800/ 2983 batches | lr 0.000312 | ms/batch 20.41 | loss  5.99 | ppl   400.08\n",
            "| epoch   9 |  2000/ 2983 batches | lr 0.000311 | ms/batch 20.41 | loss  6.03 | ppl   417.61\n",
            "| epoch   9 |  2200/ 2983 batches | lr 0.000310 | ms/batch 20.42 | loss  5.97 | ppl   390.27\n",
            "| epoch   9 |  2400/ 2983 batches | lr 0.000308 | ms/batch 20.42 | loss  5.96 | ppl   389.11\n",
            "| epoch   9 |  2600/ 2983 batches | lr 0.000307 | ms/batch 20.42 | loss  6.01 | ppl   409.20\n",
            "| epoch   9 |  2800/ 2983 batches | lr 0.000306 | ms/batch 20.42 | loss  5.94 | ppl   381.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 63.39s | valid loss  6.46 | valid ppl   637.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000305\n",
            "| epoch  10 |   200/ 2983 batches | lr 0.000304 | ms/batch 20.54 | loss  6.01 | ppl   405.47\n",
            "| epoch  10 |   400/ 2983 batches | lr 0.000303 | ms/batch 20.42 | loss  5.99 | ppl   397.47\n",
            "| epoch  10 |   600/ 2983 batches | lr 0.000302 | ms/batch 20.42 | loss  5.95 | ppl   384.40\n",
            "| epoch  10 |   800/ 2983 batches | lr 0.000301 | ms/batch 20.42 | loss  6.00 | ppl   402.94\n",
            "| epoch  10 |  1000/ 2983 batches | lr 0.000300 | ms/batch 20.42 | loss  5.98 | ppl   394.20\n",
            "| epoch  10 |  1200/ 2983 batches | lr 0.000299 | ms/batch 20.40 | loss  5.99 | ppl   399.36\n",
            "| epoch  10 |  1400/ 2983 batches | lr 0.000297 | ms/batch 20.44 | loss  5.98 | ppl   395.53\n",
            "| epoch  10 |  1600/ 2983 batches | lr 0.000296 | ms/batch 20.41 | loss  6.02 | ppl   411.47\n",
            "| epoch  10 |  1800/ 2983 batches | lr 0.000295 | ms/batch 20.42 | loss  5.98 | ppl   393.62\n",
            "| epoch  10 |  2000/ 2983 batches | lr 0.000294 | ms/batch 20.42 | loss  6.01 | ppl   409.49\n",
            "| epoch  10 |  2200/ 2983 batches | lr 0.000293 | ms/batch 20.40 | loss  5.95 | ppl   382.63\n",
            "| epoch  10 |  2400/ 2983 batches | lr 0.000292 | ms/batch 20.39 | loss  5.94 | ppl   381.44\n",
            "| epoch  10 |  2600/ 2983 batches | lr 0.000291 | ms/batch 20.43 | loss  6.00 | ppl   402.57\n",
            "| epoch  10 |  2800/ 2983 batches | lr 0.000290 | ms/batch 20.42 | loss  5.93 | ppl   374.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 63.39s | valid loss  6.45 | valid ppl   630.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000289\n",
            "| epoch  11 |   200/ 2983 batches | lr 0.000288 | ms/batch 20.52 | loss  5.99 | ppl   399.31\n",
            "| epoch  11 |   400/ 2983 batches | lr 0.000288 | ms/batch 20.42 | loss  5.97 | ppl   391.02\n",
            "| epoch  11 |   600/ 2983 batches | lr 0.000287 | ms/batch 20.42 | loss  5.93 | ppl   377.61\n",
            "| epoch  11 |   800/ 2983 batches | lr 0.000286 | ms/batch 20.46 | loss  5.98 | ppl   395.47\n",
            "| epoch  11 |  1000/ 2983 batches | lr 0.000285 | ms/batch 20.42 | loss  5.96 | ppl   387.33\n",
            "| epoch  11 |  1200/ 2983 batches | lr 0.000284 | ms/batch 20.43 | loss  5.97 | ppl   392.47\n",
            "| epoch  11 |  1400/ 2983 batches | lr 0.000283 | ms/batch 20.41 | loss  5.96 | ppl   389.04\n",
            "| epoch  11 |  1600/ 2983 batches | lr 0.000282 | ms/batch 20.40 | loss  6.00 | ppl   405.42\n",
            "| epoch  11 |  1800/ 2983 batches | lr 0.000281 | ms/batch 20.41 | loss  5.96 | ppl   387.17\n",
            "| epoch  11 |  2000/ 2983 batches | lr 0.000280 | ms/batch 20.42 | loss  6.00 | ppl   403.21\n",
            "| epoch  11 |  2200/ 2983 batches | lr 0.000279 | ms/batch 20.42 | loss  5.93 | ppl   378.01\n",
            "| epoch  11 |  2400/ 2983 batches | lr 0.000278 | ms/batch 20.43 | loss  5.93 | ppl   376.20\n",
            "| epoch  11 |  2600/ 2983 batches | lr 0.000278 | ms/batch 20.44 | loss  5.98 | ppl   396.84\n",
            "| epoch  11 |  2800/ 2983 batches | lr 0.000277 | ms/batch 20.41 | loss  5.91 | ppl   370.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 63.41s | valid loss  6.44 | valid ppl   626.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000276\n",
            "| epoch  12 |   200/ 2983 batches | lr 0.000275 | ms/batch 20.52 | loss  5.98 | ppl   393.82\n",
            "| epoch  12 |   400/ 2983 batches | lr 0.000274 | ms/batch 20.41 | loss  5.96 | ppl   385.93\n",
            "| epoch  12 |   600/ 2983 batches | lr 0.000273 | ms/batch 20.43 | loss  5.92 | ppl   371.83\n",
            "| epoch  12 |   800/ 2983 batches | lr 0.000273 | ms/batch 20.42 | loss  5.97 | ppl   389.76\n",
            "| epoch  12 |  1000/ 2983 batches | lr 0.000272 | ms/batch 20.43 | loss  5.95 | ppl   382.20\n",
            "| epoch  12 |  1200/ 2983 batches | lr 0.000271 | ms/batch 20.43 | loss  5.96 | ppl   386.61\n",
            "| epoch  12 |  1400/ 2983 batches | lr 0.000270 | ms/batch 20.42 | loss  5.95 | ppl   383.85\n",
            "| epoch  12 |  1600/ 2983 batches | lr 0.000269 | ms/batch 20.42 | loss  5.99 | ppl   399.13\n",
            "| epoch  12 |  1800/ 2983 batches | lr 0.000269 | ms/batch 20.43 | loss  5.95 | ppl   382.45\n",
            "| epoch  12 |  2000/ 2983 batches | lr 0.000268 | ms/batch 20.43 | loss  5.99 | ppl   398.16\n",
            "| epoch  12 |  2200/ 2983 batches | lr 0.000267 | ms/batch 20.43 | loss  5.92 | ppl   372.62\n",
            "| epoch  12 |  2400/ 2983 batches | lr 0.000266 | ms/batch 20.42 | loss  5.92 | ppl   371.04\n",
            "| epoch  12 |  2600/ 2983 batches | lr 0.000266 | ms/batch 20.43 | loss  5.97 | ppl   392.07\n",
            "| epoch  12 |  2800/ 2983 batches | lr 0.000265 | ms/batch 20.43 | loss  5.90 | ppl   364.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 63.42s | valid loss  6.43 | valid ppl   623.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000264\n",
            "| epoch  13 |   200/ 2983 batches | lr 0.000263 | ms/batch 20.52 | loss  5.96 | ppl   387.73\n",
            "| epoch  13 |   400/ 2983 batches | lr 0.000263 | ms/batch 20.42 | loss  5.94 | ppl   380.82\n",
            "| epoch  13 |   600/ 2983 batches | lr 0.000262 | ms/batch 20.42 | loss  5.90 | ppl   366.27\n",
            "| epoch  13 |   800/ 2983 batches | lr 0.000261 | ms/batch 20.44 | loss  5.95 | ppl   384.49\n",
            "| epoch  13 |  1000/ 2983 batches | lr 0.000261 | ms/batch 20.46 | loss  5.93 | ppl   377.18\n",
            "| epoch  13 |  1200/ 2983 batches | lr 0.000260 | ms/batch 20.42 | loss  5.94 | ppl   381.66\n",
            "| epoch  13 |  1400/ 2983 batches | lr 0.000259 | ms/batch 20.41 | loss  5.93 | ppl   377.57\n",
            "| epoch  13 |  1600/ 2983 batches | lr 0.000259 | ms/batch 20.47 | loss  5.98 | ppl   393.95\n",
            "| epoch  13 |  1800/ 2983 batches | lr 0.000258 | ms/batch 20.42 | loss  5.93 | ppl   376.11\n",
            "| epoch  13 |  2000/ 2983 batches | lr 0.000257 | ms/batch 20.41 | loss  5.98 | ppl   394.77\n",
            "| epoch  13 |  2200/ 2983 batches | lr 0.000256 | ms/batch 20.43 | loss  5.91 | ppl   367.30\n",
            "| epoch  13 |  2400/ 2983 batches | lr 0.000256 | ms/batch 20.43 | loss  5.90 | ppl   365.53\n",
            "| epoch  13 |  2600/ 2983 batches | lr 0.000255 | ms/batch 20.42 | loss  5.96 | ppl   386.77\n",
            "| epoch  13 |  2800/ 2983 batches | lr 0.000254 | ms/batch 20.42 | loss  5.89 | ppl   360.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 63.41s | valid loss  6.43 | valid ppl   619.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000254\n",
            "| epoch  14 |   200/ 2983 batches | lr 0.000253 | ms/batch 20.50 | loss  5.95 | ppl   383.43\n",
            "| epoch  14 |   400/ 2983 batches | lr 0.000253 | ms/batch 20.41 | loss  5.93 | ppl   376.15\n",
            "| epoch  14 |   600/ 2983 batches | lr 0.000252 | ms/batch 20.43 | loss  5.89 | ppl   361.24\n",
            "| epoch  14 |   800/ 2983 batches | lr 0.000251 | ms/batch 20.42 | loss  5.94 | ppl   380.65\n",
            "| epoch  14 |  1000/ 2983 batches | lr 0.000251 | ms/batch 20.42 | loss  5.92 | ppl   373.33\n",
            "| epoch  14 |  1200/ 2983 batches | lr 0.000250 | ms/batch 20.40 | loss  5.93 | ppl   376.88\n",
            "| epoch  14 |  1400/ 2983 batches | lr 0.000249 | ms/batch 20.43 | loss  5.92 | ppl   373.19\n",
            "| epoch  14 |  1600/ 2983 batches | lr 0.000249 | ms/batch 20.42 | loss  5.96 | ppl   389.21\n",
            "| epoch  14 |  1800/ 2983 batches | lr 0.000248 | ms/batch 20.42 | loss  5.92 | ppl   372.22\n",
            "| epoch  14 |  2000/ 2983 batches | lr 0.000248 | ms/batch 20.43 | loss  5.96 | ppl   388.90\n",
            "| epoch  14 |  2200/ 2983 batches | lr 0.000247 | ms/batch 20.43 | loss  5.90 | ppl   363.63\n",
            "| epoch  14 |  2400/ 2983 batches | lr 0.000246 | ms/batch 20.42 | loss  5.89 | ppl   361.76\n",
            "| epoch  14 |  2600/ 2983 batches | lr 0.000246 | ms/batch 20.42 | loss  5.95 | ppl   382.12\n",
            "| epoch  14 |  2800/ 2983 batches | lr 0.000245 | ms/batch 20.43 | loss  5.88 | ppl   356.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 63.40s | valid loss  6.42 | valid ppl   614.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000245\n",
            "| epoch  15 |   200/ 2983 batches | lr 0.000244 | ms/batch 20.52 | loss  5.94 | ppl   378.30\n",
            "| epoch  15 |   400/ 2983 batches | lr 0.000243 | ms/batch 20.43 | loss  5.92 | ppl   372.45\n",
            "| epoch  15 |   600/ 2983 batches | lr 0.000243 | ms/batch 20.42 | loss  5.88 | ppl   357.86\n",
            "| epoch  15 |   800/ 2983 batches | lr 0.000242 | ms/batch 20.43 | loss  5.93 | ppl   376.07\n",
            "| epoch  15 |  1000/ 2983 batches | lr 0.000242 | ms/batch 20.41 | loss  5.91 | ppl   368.00\n",
            "| epoch  15 |  1200/ 2983 batches | lr 0.000241 | ms/batch 20.41 | loss  5.92 | ppl   372.88\n",
            "| epoch  15 |  1400/ 2983 batches | lr 0.000241 | ms/batch 20.44 | loss  5.91 | ppl   369.47\n",
            "| epoch  15 |  1600/ 2983 batches | lr 0.000240 | ms/batch 20.41 | loss  5.95 | ppl   384.80\n",
            "| epoch  15 |  1800/ 2983 batches | lr 0.000240 | ms/batch 20.42 | loss  5.91 | ppl   368.10\n",
            "| epoch  15 |  2000/ 2983 batches | lr 0.000239 | ms/batch 20.45 | loss  5.95 | ppl   384.24\n",
            "| epoch  15 |  2200/ 2983 batches | lr 0.000238 | ms/batch 20.41 | loss  5.89 | ppl   360.26\n",
            "| epoch  15 |  2400/ 2983 batches | lr 0.000238 | ms/batch 20.43 | loss  5.88 | ppl   358.00\n",
            "| epoch  15 |  2600/ 2983 batches | lr 0.000237 | ms/batch 20.41 | loss  5.93 | ppl   377.69\n",
            "| epoch  15 |  2800/ 2983 batches | lr 0.000237 | ms/batch 20.43 | loss  5.86 | ppl   351.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 63.41s | valid loss  6.42 | valid ppl   614.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000236\n",
            "| epoch  16 |   200/ 2983 batches | lr 0.000236 | ms/batch 20.52 | loss  5.93 | ppl   374.47\n",
            "| epoch  16 |   400/ 2983 batches | lr 0.000235 | ms/batch 20.42 | loss  5.91 | ppl   368.49\n",
            "| epoch  16 |   600/ 2983 batches | lr 0.000235 | ms/batch 20.46 | loss  5.87 | ppl   353.50\n",
            "| epoch  16 |   800/ 2983 batches | lr 0.000234 | ms/batch 20.43 | loss  5.92 | ppl   371.75\n",
            "| epoch  16 |  1000/ 2983 batches | lr 0.000234 | ms/batch 20.43 | loss  5.90 | ppl   364.60\n",
            "| epoch  16 |  1200/ 2983 batches | lr 0.000233 | ms/batch 20.44 | loss  5.91 | ppl   368.31\n",
            "| epoch  16 |  1400/ 2983 batches | lr 0.000233 | ms/batch 20.40 | loss  5.90 | ppl   365.63\n",
            "| epoch  16 |  1600/ 2983 batches | lr 0.000232 | ms/batch 20.40 | loss  5.94 | ppl   381.40\n",
            "| epoch  16 |  1800/ 2983 batches | lr 0.000232 | ms/batch 20.44 | loss  5.90 | ppl   364.72\n",
            "| epoch  16 |  2000/ 2983 batches | lr 0.000231 | ms/batch 20.41 | loss  5.94 | ppl   381.12\n",
            "| epoch  16 |  2200/ 2983 batches | lr 0.000231 | ms/batch 20.42 | loss  5.87 | ppl   355.39\n",
            "| epoch  16 |  2400/ 2983 batches | lr 0.000230 | ms/batch 20.46 | loss  5.87 | ppl   354.26\n",
            "| epoch  16 |  2600/ 2983 batches | lr 0.000230 | ms/batch 20.43 | loss  5.93 | ppl   374.32\n",
            "| epoch  16 |  2800/ 2983 batches | lr 0.000229 | ms/batch 20.42 | loss  5.85 | ppl   347.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 63.42s | valid loss  6.42 | valid ppl   612.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000229\n",
            "| epoch  17 |   200/ 2983 batches | lr 0.000228 | ms/batch 20.51 | loss  5.92 | ppl   370.98\n",
            "| epoch  17 |   400/ 2983 batches | lr 0.000228 | ms/batch 20.43 | loss  5.90 | ppl   365.22\n",
            "| epoch  17 |   600/ 2983 batches | lr 0.000227 | ms/batch 20.41 | loss  5.86 | ppl   348.98\n",
            "| epoch  17 |   800/ 2983 batches | lr 0.000227 | ms/batch 20.41 | loss  5.91 | ppl   368.75\n",
            "| epoch  17 |  1000/ 2983 batches | lr 0.000226 | ms/batch 20.44 | loss  5.89 | ppl   361.18\n",
            "| epoch  17 |  1200/ 2983 batches | lr 0.000226 | ms/batch 20.41 | loss  5.90 | ppl   364.64\n",
            "| epoch  17 |  1400/ 2983 batches | lr 0.000226 | ms/batch 20.41 | loss  5.89 | ppl   362.18\n",
            "| epoch  17 |  1600/ 2983 batches | lr 0.000225 | ms/batch 20.41 | loss  5.94 | ppl   378.23\n",
            "| epoch  17 |  1800/ 2983 batches | lr 0.000225 | ms/batch 20.42 | loss  5.89 | ppl   360.94\n",
            "| epoch  17 |  2000/ 2983 batches | lr 0.000224 | ms/batch 20.41 | loss  5.93 | ppl   377.69\n",
            "| epoch  17 |  2200/ 2983 batches | lr 0.000224 | ms/batch 20.42 | loss  5.86 | ppl   352.22\n",
            "| epoch  17 |  2400/ 2983 batches | lr 0.000223 | ms/batch 20.41 | loss  5.86 | ppl   351.08\n",
            "| epoch  17 |  2600/ 2983 batches | lr 0.000223 | ms/batch 20.42 | loss  5.91 | ppl   370.24\n",
            "| epoch  17 |  2800/ 2983 batches | lr 0.000222 | ms/batch 20.43 | loss  5.84 | ppl   345.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 63.39s | valid loss  6.41 | valid ppl   607.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000222\n",
            "| epoch  18 |   200/ 2983 batches | lr 0.000222 | ms/batch 20.54 | loss  5.91 | ppl   368.30\n",
            "| epoch  18 |   400/ 2983 batches | lr 0.000221 | ms/batch 20.41 | loss  5.89 | ppl   361.30\n",
            "| epoch  18 |   600/ 2983 batches | lr 0.000221 | ms/batch 20.42 | loss  5.85 | ppl   345.78\n",
            "| epoch  18 |   800/ 2983 batches | lr 0.000220 | ms/batch 20.45 | loss  5.90 | ppl   364.85\n",
            "| epoch  18 |  1000/ 2983 batches | lr 0.000220 | ms/batch 20.42 | loss  5.88 | ppl   357.43\n",
            "| epoch  18 |  1200/ 2983 batches | lr 0.000219 | ms/batch 20.42 | loss  5.89 | ppl   360.83\n",
            "| epoch  18 |  1400/ 2983 batches | lr 0.000219 | ms/batch 20.42 | loss  5.88 | ppl   358.51\n",
            "| epoch  18 |  1600/ 2983 batches | lr 0.000219 | ms/batch 20.42 | loss  5.92 | ppl   373.63\n",
            "| epoch  18 |  1800/ 2983 batches | lr 0.000218 | ms/batch 20.42 | loss  5.88 | ppl   356.46\n",
            "| epoch  18 |  2000/ 2983 batches | lr 0.000218 | ms/batch 20.41 | loss  5.92 | ppl   372.72\n",
            "| epoch  18 |  2200/ 2983 batches | lr 0.000217 | ms/batch 20.42 | loss  5.85 | ppl   348.22\n",
            "| epoch  18 |  2400/ 2983 batches | lr 0.000217 | ms/batch 20.41 | loss  5.85 | ppl   347.13\n",
            "| epoch  18 |  2600/ 2983 batches | lr 0.000217 | ms/batch 20.41 | loss  5.90 | ppl   366.12\n",
            "| epoch  18 |  2800/ 2983 batches | lr 0.000216 | ms/batch 20.45 | loss  5.84 | ppl   342.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 63.41s | valid loss  6.41 | valid ppl   607.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000216\n",
            "| epoch  19 |   200/ 2983 batches | lr 0.000215 | ms/batch 20.54 | loss  5.90 | ppl   363.82\n",
            "| epoch  19 |   400/ 2983 batches | lr 0.000215 | ms/batch 20.41 | loss  5.88 | ppl   358.04\n",
            "| epoch  19 |   600/ 2983 batches | lr 0.000215 | ms/batch 20.42 | loss  5.84 | ppl   342.72\n",
            "| epoch  19 |   800/ 2983 batches | lr 0.000214 | ms/batch 20.44 | loss  5.89 | ppl   361.46\n",
            "| epoch  19 |  1000/ 2983 batches | lr 0.000214 | ms/batch 20.43 | loss  5.87 | ppl   353.92\n",
            "| epoch  19 |  1200/ 2983 batches | lr 0.000213 | ms/batch 20.42 | loss  5.88 | ppl   357.12\n",
            "| epoch  19 |  1400/ 2983 batches | lr 0.000213 | ms/batch 20.45 | loss  5.88 | ppl   356.25\n",
            "| epoch  19 |  1600/ 2983 batches | lr 0.000213 | ms/batch 20.40 | loss  5.91 | ppl   370.30\n",
            "| epoch  19 |  1800/ 2983 batches | lr 0.000212 | ms/batch 20.50 | loss  5.87 | ppl   353.47\n",
            "| epoch  19 |  2000/ 2983 batches | lr 0.000212 | ms/batch 20.45 | loss  5.91 | ppl   369.59\n",
            "| epoch  19 |  2200/ 2983 batches | lr 0.000211 | ms/batch 20.41 | loss  5.84 | ppl   345.43\n",
            "| epoch  19 |  2400/ 2983 batches | lr 0.000211 | ms/batch 20.42 | loss  5.84 | ppl   344.05\n",
            "| epoch  19 |  2600/ 2983 batches | lr 0.000211 | ms/batch 20.43 | loss  5.90 | ppl   364.52\n",
            "| epoch  19 |  2800/ 2983 batches | lr 0.000210 | ms/batch 20.43 | loss  5.83 | ppl   340.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 63.43s | valid loss  6.40 | valid ppl   604.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000210\n",
            "| epoch  20 |   200/ 2983 batches | lr 0.000210 | ms/batch 20.49 | loss  5.89 | ppl   360.93\n",
            "| epoch  20 |   400/ 2983 batches | lr 0.000209 | ms/batch 20.41 | loss  5.87 | ppl   355.51\n",
            "| epoch  20 |   600/ 2983 batches | lr 0.000209 | ms/batch 20.41 | loss  5.83 | ppl   340.17\n",
            "| epoch  20 |   800/ 2983 batches | lr 0.000209 | ms/batch 20.43 | loss  5.88 | ppl   358.93\n",
            "| epoch  20 |  1000/ 2983 batches | lr 0.000208 | ms/batch 20.42 | loss  5.86 | ppl   351.06\n",
            "| epoch  20 |  1200/ 2983 batches | lr 0.000208 | ms/batch 20.45 | loss  5.87 | ppl   354.72\n",
            "| epoch  20 |  1400/ 2983 batches | lr 0.000207 | ms/batch 20.42 | loss  5.87 | ppl   353.09\n",
            "| epoch  20 |  1600/ 2983 batches | lr 0.000207 | ms/batch 20.40 | loss  5.91 | ppl   367.78\n",
            "| epoch  20 |  1800/ 2983 batches | lr 0.000207 | ms/batch 20.45 | loss  5.86 | ppl   350.76\n",
            "| epoch  20 |  2000/ 2983 batches | lr 0.000206 | ms/batch 20.43 | loss  5.91 | ppl   366.94\n",
            "| epoch  20 |  2200/ 2983 batches | lr 0.000206 | ms/batch 20.42 | loss  5.84 | ppl   343.04\n",
            "| epoch  20 |  2400/ 2983 batches | lr 0.000206 | ms/batch 20.41 | loss  5.84 | ppl   342.07\n",
            "| epoch  20 |  2600/ 2983 batches | lr 0.000205 | ms/batch 20.42 | loss  5.89 | ppl   361.83\n",
            "| epoch  20 |  2800/ 2983 batches | lr 0.000205 | ms/batch 20.44 | loss  5.82 | ppl   337.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 63.41s | valid loss  6.40 | valid ppl   602.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000205\n",
            "| epoch  21 |   200/ 2983 batches | lr 0.000204 | ms/batch 20.52 | loss  5.88 | ppl   357.51\n",
            "| epoch  21 |   400/ 2983 batches | lr 0.000204 | ms/batch 20.42 | loss  5.86 | ppl   352.31\n",
            "| epoch  21 |   600/ 2983 batches | lr 0.000204 | ms/batch 20.41 | loss  5.82 | ppl   336.64\n",
            "| epoch  21 |   800/ 2983 batches | lr 0.000203 | ms/batch 20.42 | loss  5.88 | ppl   356.84\n",
            "| epoch  21 |  1000/ 2983 batches | lr 0.000203 | ms/batch 20.44 | loss  5.85 | ppl   348.12\n",
            "| epoch  21 |  1200/ 2983 batches | lr 0.000203 | ms/batch 20.43 | loss  5.86 | ppl   351.49\n",
            "| epoch  21 |  1400/ 2983 batches | lr 0.000202 | ms/batch 20.43 | loss  5.86 | ppl   349.95\n",
            "| epoch  21 |  1600/ 2983 batches | lr 0.000202 | ms/batch 20.44 | loss  5.90 | ppl   365.89\n",
            "| epoch  21 |  1800/ 2983 batches | lr 0.000202 | ms/batch 20.42 | loss  5.86 | ppl   348.99\n",
            "| epoch  21 |  2000/ 2983 batches | lr 0.000201 | ms/batch 20.42 | loss  5.90 | ppl   363.69\n",
            "| epoch  21 |  2200/ 2983 batches | lr 0.000201 | ms/batch 20.43 | loss  5.83 | ppl   340.13\n",
            "| epoch  21 |  2400/ 2983 batches | lr 0.000201 | ms/batch 20.43 | loss  5.83 | ppl   339.52\n",
            "| epoch  21 |  2600/ 2983 batches | lr 0.000200 | ms/batch 20.41 | loss  5.88 | ppl   357.95\n",
            "| epoch  21 |  2800/ 2983 batches | lr 0.000200 | ms/batch 20.42 | loss  5.81 | ppl   334.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 63.41s | valid loss  6.40 | valid ppl   601.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000200\n",
            "| epoch  22 |   200/ 2983 batches | lr 0.000199 | ms/batch 20.52 | loss  5.87 | ppl   354.97\n",
            "| epoch  22 |   400/ 2983 batches | lr 0.000199 | ms/batch 20.43 | loss  5.86 | ppl   350.53\n",
            "| epoch  22 |   600/ 2983 batches | lr 0.000199 | ms/batch 20.42 | loss  5.81 | ppl   334.74\n",
            "| epoch  22 |   800/ 2983 batches | lr 0.000198 | ms/batch 20.42 | loss  5.87 | ppl   353.78\n",
            "| epoch  22 |  1000/ 2983 batches | lr 0.000198 | ms/batch 20.41 | loss  5.85 | ppl   346.29\n",
            "| epoch  22 |  1200/ 2983 batches | lr 0.000198 | ms/batch 20.42 | loss  5.85 | ppl   348.76\n",
            "| epoch  22 |  1400/ 2983 batches | lr 0.000198 | ms/batch 20.42 | loss  5.85 | ppl   347.94\n",
            "| epoch  22 |  1600/ 2983 batches | lr 0.000197 | ms/batch 20.41 | loss  5.90 | ppl   363.36\n",
            "| epoch  22 |  1800/ 2983 batches | lr 0.000197 | ms/batch 20.43 | loss  5.85 | ppl   345.65\n",
            "| epoch  22 |  2000/ 2983 batches | lr 0.000197 | ms/batch 20.52 | loss  5.89 | ppl   361.82\n",
            "| epoch  22 |  2200/ 2983 batches | lr 0.000196 | ms/batch 20.47 | loss  5.82 | ppl   336.86\n",
            "| epoch  22 |  2400/ 2983 batches | lr 0.000196 | ms/batch 20.43 | loss  5.82 | ppl   336.11\n",
            "| epoch  22 |  2600/ 2983 batches | lr 0.000196 | ms/batch 20.43 | loss  5.88 | ppl   356.03\n",
            "| epoch  22 |  2800/ 2983 batches | lr 0.000195 | ms/batch 20.43 | loss  5.80 | ppl   331.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 63.44s | valid loss  6.40 | valid ppl   599.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000195\n",
            "| epoch  23 |   200/ 2983 batches | lr 0.000195 | ms/batch 20.53 | loss  5.87 | ppl   353.10\n",
            "| epoch  23 |   400/ 2983 batches | lr 0.000195 | ms/batch 20.43 | loss  5.85 | ppl   348.19\n",
            "| epoch  23 |   600/ 2983 batches | lr 0.000194 | ms/batch 20.43 | loss  5.80 | ppl   331.76\n",
            "| epoch  23 |   800/ 2983 batches | lr 0.000194 | ms/batch 20.44 | loss  5.86 | ppl   350.45\n",
            "| epoch  23 |  1000/ 2983 batches | lr 0.000194 | ms/batch 20.43 | loss  5.84 | ppl   342.40\n",
            "| epoch  23 |  1200/ 2983 batches | lr 0.000193 | ms/batch 20.43 | loss  5.85 | ppl   346.86\n",
            "| epoch  23 |  1400/ 2983 batches | lr 0.000193 | ms/batch 20.42 | loss  5.84 | ppl   345.25\n",
            "| epoch  23 |  1600/ 2983 batches | lr 0.000193 | ms/batch 20.42 | loss  5.89 | ppl   360.45\n",
            "| epoch  23 |  1800/ 2983 batches | lr 0.000193 | ms/batch 20.44 | loss  5.84 | ppl   343.09\n",
            "| epoch  23 |  2000/ 2983 batches | lr 0.000192 | ms/batch 20.44 | loss  5.88 | ppl   357.99\n",
            "| epoch  23 |  2200/ 2983 batches | lr 0.000192 | ms/batch 20.43 | loss  5.81 | ppl   334.37\n",
            "| epoch  23 |  2400/ 2983 batches | lr 0.000192 | ms/batch 20.43 | loss  5.81 | ppl   334.84\n",
            "| epoch  23 |  2600/ 2983 batches | lr 0.000191 | ms/batch 20.41 | loss  5.87 | ppl   353.16\n",
            "| epoch  23 |  2800/ 2983 batches | lr 0.000191 | ms/batch 20.42 | loss  5.80 | ppl   328.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 63.41s | valid loss  6.39 | valid ppl   598.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000191\n",
            "| epoch  24 |   200/ 2983 batches | lr 0.000191 | ms/batch 20.52 | loss  5.86 | ppl   350.57\n",
            "| epoch  24 |   400/ 2983 batches | lr 0.000190 | ms/batch 20.51 | loss  5.84 | ppl   345.23\n",
            "| epoch  24 |   600/ 2983 batches | lr 0.000190 | ms/batch 20.43 | loss  5.80 | ppl   328.84\n",
            "| epoch  24 |   800/ 2983 batches | lr 0.000190 | ms/batch 20.43 | loss  5.85 | ppl   347.96\n",
            "| epoch  24 |  1000/ 2983 batches | lr 0.000189 | ms/batch 20.42 | loss  5.83 | ppl   340.46\n",
            "| epoch  24 |  1200/ 2983 batches | lr 0.000189 | ms/batch 20.44 | loss  5.84 | ppl   343.69\n",
            "| epoch  24 |  1400/ 2983 batches | lr 0.000189 | ms/batch 20.42 | loss  5.84 | ppl   342.79\n",
            "| epoch  24 |  1600/ 2983 batches | lr 0.000189 | ms/batch 20.42 | loss  5.88 | ppl   358.75\n",
            "| epoch  24 |  1800/ 2983 batches | lr 0.000188 | ms/batch 20.44 | loss  5.83 | ppl   340.64\n",
            "| epoch  24 |  2000/ 2983 batches | lr 0.000188 | ms/batch 20.42 | loss  5.88 | ppl   356.74\n",
            "| epoch  24 |  2200/ 2983 batches | lr 0.000188 | ms/batch 20.42 | loss  5.80 | ppl   331.34\n",
            "| epoch  24 |  2400/ 2983 batches | lr 0.000188 | ms/batch 20.42 | loss  5.81 | ppl   332.32\n",
            "| epoch  24 |  2600/ 2983 batches | lr 0.000187 | ms/batch 20.42 | loss  5.86 | ppl   352.02\n",
            "| epoch  24 |  2800/ 2983 batches | lr 0.000187 | ms/batch 20.42 | loss  5.79 | ppl   326.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 63.42s | valid loss  6.39 | valid ppl   598.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000187\n",
            "| epoch  25 |   200/ 2983 batches | lr 0.000187 | ms/batch 20.52 | loss  5.85 | ppl   347.95\n",
            "| epoch  25 |   400/ 2983 batches | lr 0.000186 | ms/batch 20.43 | loss  5.84 | ppl   343.75\n",
            "| epoch  25 |   600/ 2983 batches | lr 0.000186 | ms/batch 20.42 | loss  5.79 | ppl   326.23\n",
            "| epoch  25 |   800/ 2983 batches | lr 0.000186 | ms/batch 20.40 | loss  5.85 | ppl   347.05\n",
            "| epoch  25 |  1000/ 2983 batches | lr 0.000186 | ms/batch 20.42 | loss  5.82 | ppl   338.19\n",
            "| epoch  25 |  1200/ 2983 batches | lr 0.000185 | ms/batch 20.41 | loss  5.83 | ppl   340.62\n",
            "| epoch  25 |  1400/ 2983 batches | lr 0.000185 | ms/batch 20.41 | loss  5.83 | ppl   339.84\n",
            "| epoch  25 |  1600/ 2983 batches | lr 0.000185 | ms/batch 20.44 | loss  5.88 | ppl   356.95\n",
            "| epoch  25 |  1800/ 2983 batches | lr 0.000185 | ms/batch 20.47 | loss  5.82 | ppl   337.42\n",
            "| epoch  25 |  2000/ 2983 batches | lr 0.000184 | ms/batch 20.42 | loss  5.87 | ppl   353.35\n",
            "| epoch  25 |  2200/ 2983 batches | lr 0.000184 | ms/batch 20.43 | loss  5.80 | ppl   329.81\n",
            "| epoch  25 |  2400/ 2983 batches | lr 0.000184 | ms/batch 20.42 | loss  5.80 | ppl   329.50\n",
            "| epoch  25 |  2600/ 2983 batches | lr 0.000184 | ms/batch 20.41 | loss  5.85 | ppl   348.48\n",
            "| epoch  25 |  2800/ 2983 batches | lr 0.000183 | ms/batch 20.43 | loss  5.78 | ppl   324.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 63.40s | valid loss  6.39 | valid ppl   597.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000183\n",
            "| epoch  26 |   200/ 2983 batches | lr 0.000183 | ms/batch 20.54 | loss  5.84 | ppl   345.26\n",
            "| epoch  26 |   400/ 2983 batches | lr 0.000183 | ms/batch 20.42 | loss  5.83 | ppl   340.87\n",
            "| epoch  26 |   600/ 2983 batches | lr 0.000182 | ms/batch 20.39 | loss  5.78 | ppl   324.66\n",
            "| epoch  26 |   800/ 2983 batches | lr 0.000182 | ms/batch 20.40 | loss  5.84 | ppl   343.75\n",
            "| epoch  26 |  1000/ 2983 batches | lr 0.000182 | ms/batch 20.41 | loss  5.82 | ppl   336.93\n",
            "| epoch  26 |  1200/ 2983 batches | lr 0.000182 | ms/batch 20.41 | loss  5.83 | ppl   339.36\n",
            "| epoch  26 |  1400/ 2983 batches | lr 0.000181 | ms/batch 20.45 | loss  5.82 | ppl   337.61\n",
            "| epoch  26 |  1600/ 2983 batches | lr 0.000181 | ms/batch 20.42 | loss  5.87 | ppl   354.25\n",
            "| epoch  26 |  1800/ 2983 batches | lr 0.000181 | ms/batch 20.43 | loss  5.82 | ppl   335.32\n",
            "| epoch  26 |  2000/ 2983 batches | lr 0.000181 | ms/batch 20.43 | loss  5.86 | ppl   350.48\n",
            "| epoch  26 |  2200/ 2983 batches | lr 0.000180 | ms/batch 20.42 | loss  5.79 | ppl   328.52\n",
            "| epoch  26 |  2400/ 2983 batches | lr 0.000180 | ms/batch 20.43 | loss  5.79 | ppl   327.79\n",
            "| epoch  26 |  2600/ 2983 batches | lr 0.000180 | ms/batch 20.49 | loss  5.85 | ppl   346.24\n",
            "| epoch  26 |  2800/ 2983 batches | lr 0.000180 | ms/batch 20.42 | loss  5.77 | ppl   321.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 63.41s | valid loss  6.39 | valid ppl   595.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000180\n",
            "| epoch  27 |   200/ 2983 batches | lr 0.000179 | ms/batch 20.54 | loss  5.84 | ppl   343.30\n",
            "| epoch  27 |   400/ 2983 batches | lr 0.000179 | ms/batch 20.41 | loss  5.83 | ppl   338.94\n",
            "| epoch  27 |   600/ 2983 batches | lr 0.000179 | ms/batch 20.43 | loss  5.78 | ppl   322.26\n",
            "| epoch  27 |   800/ 2983 batches | lr 0.000179 | ms/batch 20.42 | loss  5.84 | ppl   342.15\n",
            "| epoch  27 |  1000/ 2983 batches | lr 0.000178 | ms/batch 20.42 | loss  5.81 | ppl   333.89\n",
            "| epoch  27 |  1200/ 2983 batches | lr 0.000178 | ms/batch 20.43 | loss  5.82 | ppl   336.52\n",
            "| epoch  27 |  1400/ 2983 batches | lr 0.000178 | ms/batch 20.45 | loss  5.82 | ppl   335.81\n",
            "| epoch  27 |  1600/ 2983 batches | lr 0.000178 | ms/batch 20.43 | loss  5.86 | ppl   351.57\n",
            "| epoch  27 |  1800/ 2983 batches | lr 0.000177 | ms/batch 20.42 | loss  5.81 | ppl   334.02\n",
            "| epoch  27 |  2000/ 2983 batches | lr 0.000177 | ms/batch 20.41 | loss  5.85 | ppl   348.88\n",
            "| epoch  27 |  2200/ 2983 batches | lr 0.000177 | ms/batch 20.41 | loss  5.79 | ppl   325.71\n",
            "| epoch  27 |  2400/ 2983 batches | lr 0.000177 | ms/batch 20.41 | loss  5.78 | ppl   325.19\n",
            "| epoch  27 |  2600/ 2983 batches | lr 0.000177 | ms/batch 20.42 | loss  5.84 | ppl   343.78\n",
            "| epoch  27 |  2800/ 2983 batches | lr 0.000176 | ms/batch 20.41 | loss  5.77 | ppl   320.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 63.41s | valid loss  6.39 | valid ppl   593.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000176\n",
            "| epoch  28 |   200/ 2983 batches | lr 0.000176 | ms/batch 20.52 | loss  5.83 | ppl   341.80\n",
            "| epoch  28 |   400/ 2983 batches | lr 0.000176 | ms/batch 20.41 | loss  5.82 | ppl   337.53\n",
            "| epoch  28 |   600/ 2983 batches | lr 0.000175 | ms/batch 20.42 | loss  5.77 | ppl   320.52\n",
            "| epoch  28 |   800/ 2983 batches | lr 0.000175 | ms/batch 20.41 | loss  5.83 | ppl   340.04\n",
            "| epoch  28 |  1000/ 2983 batches | lr 0.000175 | ms/batch 20.42 | loss  5.81 | ppl   332.04\n",
            "| epoch  28 |  1200/ 2983 batches | lr 0.000175 | ms/batch 20.42 | loss  5.82 | ppl   335.31\n",
            "| epoch  28 |  1400/ 2983 batches | lr 0.000175 | ms/batch 20.40 | loss  5.81 | ppl   333.93\n",
            "| epoch  28 |  1600/ 2983 batches | lr 0.000174 | ms/batch 20.45 | loss  5.86 | ppl   349.75\n",
            "| epoch  28 |  1800/ 2983 batches | lr 0.000174 | ms/batch 20.49 | loss  5.80 | ppl   331.34\n",
            "| epoch  28 |  2000/ 2983 batches | lr 0.000174 | ms/batch 20.47 | loss  5.85 | ppl   346.78\n",
            "| epoch  28 |  2200/ 2983 batches | lr 0.000174 | ms/batch 20.48 | loss  5.78 | ppl   323.91\n",
            "| epoch  28 |  2400/ 2983 batches | lr 0.000174 | ms/batch 20.47 | loss  5.78 | ppl   324.16\n",
            "| epoch  28 |  2600/ 2983 batches | lr 0.000173 | ms/batch 20.44 | loss  5.84 | ppl   342.31\n",
            "| epoch  28 |  2800/ 2983 batches | lr 0.000173 | ms/batch 20.47 | loss  5.76 | ppl   317.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 63.46s | valid loss  6.38 | valid ppl   590.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000173\n",
            "| epoch  29 |   200/ 2983 batches | lr 0.000173 | ms/batch 20.56 | loss  5.83 | ppl   339.64\n",
            "| epoch  29 |   400/ 2983 batches | lr 0.000173 | ms/batch 20.53 | loss  5.81 | ppl   334.83\n",
            "| epoch  29 |   600/ 2983 batches | lr 0.000172 | ms/batch 20.43 | loss  5.76 | ppl   317.83\n",
            "| epoch  29 |   800/ 2983 batches | lr 0.000172 | ms/batch 20.43 | loss  5.82 | ppl   337.24\n",
            "| epoch  29 |  1000/ 2983 batches | lr 0.000172 | ms/batch 20.50 | loss  5.80 | ppl   330.40\n",
            "| epoch  29 |  1200/ 2983 batches | lr 0.000172 | ms/batch 20.44 | loss  5.81 | ppl   333.41\n",
            "| epoch  29 |  1400/ 2983 batches | lr 0.000172 | ms/batch 20.45 | loss  5.81 | ppl   333.18\n",
            "| epoch  29 |  1600/ 2983 batches | lr 0.000171 | ms/batch 20.47 | loss  5.85 | ppl   348.42\n",
            "| epoch  29 |  1800/ 2983 batches | lr 0.000171 | ms/batch 20.45 | loss  5.80 | ppl   330.27\n",
            "| epoch  29 |  2000/ 2983 batches | lr 0.000171 | ms/batch 20.45 | loss  5.84 | ppl   344.86\n",
            "| epoch  29 |  2200/ 2983 batches | lr 0.000171 | ms/batch 20.42 | loss  5.77 | ppl   321.45\n",
            "| epoch  29 |  2400/ 2983 batches | lr 0.000171 | ms/batch 20.43 | loss  5.77 | ppl   321.89\n",
            "| epoch  29 |  2600/ 2983 batches | lr 0.000170 | ms/batch 20.44 | loss  5.83 | ppl   339.24\n",
            "| epoch  29 |  2800/ 2983 batches | lr 0.000170 | ms/batch 20.46 | loss  5.76 | ppl   316.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 63.49s | valid loss  6.38 | valid ppl   588.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000170\n",
            "| epoch  30 |   200/ 2983 batches | lr 0.000170 | ms/batch 20.57 | loss  5.82 | ppl   336.84\n",
            "| epoch  30 |   400/ 2983 batches | lr 0.000170 | ms/batch 20.43 | loss  5.81 | ppl   333.82\n",
            "| epoch  30 |   600/ 2983 batches | lr 0.000169 | ms/batch 20.50 | loss  5.76 | ppl   316.49\n",
            "| epoch  30 |   800/ 2983 batches | lr 0.000169 | ms/batch 20.48 | loss  5.81 | ppl   335.16\n",
            "| epoch  30 |  1000/ 2983 batches | lr 0.000169 | ms/batch 20.43 | loss  5.79 | ppl   328.49\n",
            "| epoch  30 |  1200/ 2983 batches | lr 0.000169 | ms/batch 20.43 | loss  5.80 | ppl   331.10\n",
            "| epoch  30 |  1400/ 2983 batches | lr 0.000169 | ms/batch 20.46 | loss  5.80 | ppl   330.66\n",
            "| epoch  30 |  1600/ 2983 batches | lr 0.000168 | ms/batch 20.42 | loss  5.85 | ppl   345.82\n",
            "| epoch  30 |  1800/ 2983 batches | lr 0.000168 | ms/batch 20.42 | loss  5.79 | ppl   327.99\n",
            "| epoch  30 |  2000/ 2983 batches | lr 0.000168 | ms/batch 20.44 | loss  5.84 | ppl   342.50\n",
            "| epoch  30 |  2200/ 2983 batches | lr 0.000168 | ms/batch 20.42 | loss  5.77 | ppl   319.11\n",
            "| epoch  30 |  2400/ 2983 batches | lr 0.000168 | ms/batch 20.42 | loss  5.77 | ppl   319.79\n",
            "| epoch  30 |  2600/ 2983 batches | lr 0.000167 | ms/batch 20.46 | loss  5.83 | ppl   339.01\n",
            "| epoch  30 |  2800/ 2983 batches | lr 0.000167 | ms/batch 20.43 | loss  5.75 | ppl   314.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 63.46s | valid loss  6.37 | valid ppl   586.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000167\n",
            "| epoch  31 |   200/ 2983 batches | lr 0.000167 | ms/batch 20.53 | loss  5.82 | ppl   336.02\n",
            "| epoch  31 |   400/ 2983 batches | lr 0.000167 | ms/batch 20.42 | loss  5.80 | ppl   331.35\n",
            "| epoch  31 |   600/ 2983 batches | lr 0.000167 | ms/batch 20.44 | loss  5.75 | ppl   314.76\n",
            "| epoch  31 |   800/ 2983 batches | lr 0.000166 | ms/batch 20.42 | loss  5.81 | ppl   333.45\n",
            "| epoch  31 |  1000/ 2983 batches | lr 0.000166 | ms/batch 20.41 | loss  5.79 | ppl   326.56\n",
            "| epoch  31 |  1200/ 2983 batches | lr 0.000166 | ms/batch 20.42 | loss  5.80 | ppl   328.70\n",
            "| epoch  31 |  1400/ 2983 batches | lr 0.000166 | ms/batch 20.48 | loss  5.80 | ppl   329.33\n",
            "| epoch  31 |  1600/ 2983 batches | lr 0.000166 | ms/batch 20.41 | loss  5.84 | ppl   344.49\n",
            "| epoch  31 |  1800/ 2983 batches | lr 0.000165 | ms/batch 20.44 | loss  5.79 | ppl   326.27\n",
            "| epoch  31 |  2000/ 2983 batches | lr 0.000165 | ms/batch 20.43 | loss  5.83 | ppl   340.93\n",
            "| epoch  31 |  2200/ 2983 batches | lr 0.000165 | ms/batch 20.43 | loss  5.76 | ppl   317.59\n",
            "| epoch  31 |  2400/ 2983 batches | lr 0.000165 | ms/batch 20.43 | loss  5.76 | ppl   317.49\n",
            "| epoch  31 |  2600/ 2983 batches | lr 0.000165 | ms/batch 20.42 | loss  5.82 | ppl   336.21\n",
            "| epoch  31 |  2800/ 2983 batches | lr 0.000165 | ms/batch 20.43 | loss  5.75 | ppl   312.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 63.43s | valid loss  6.37 | valid ppl   586.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000164\n",
            "| epoch  32 |   200/ 2983 batches | lr 0.000164 | ms/batch 20.51 | loss  5.81 | ppl   334.13\n",
            "| epoch  32 |   400/ 2983 batches | lr 0.000164 | ms/batch 20.42 | loss  5.80 | ppl   330.07\n",
            "| epoch  32 |   600/ 2983 batches | lr 0.000164 | ms/batch 20.43 | loss  5.74 | ppl   312.38\n",
            "| epoch  32 |   800/ 2983 batches | lr 0.000164 | ms/batch 20.41 | loss  5.80 | ppl   331.78\n",
            "| epoch  32 |  1000/ 2983 batches | lr 0.000164 | ms/batch 20.40 | loss  5.78 | ppl   325.12\n",
            "| epoch  32 |  1200/ 2983 batches | lr 0.000163 | ms/batch 20.41 | loss  5.79 | ppl   327.52\n",
            "| epoch  32 |  1400/ 2983 batches | lr 0.000163 | ms/batch 20.42 | loss  5.79 | ppl   327.42\n",
            "| epoch  32 |  1600/ 2983 batches | lr 0.000163 | ms/batch 20.42 | loss  5.83 | ppl   341.81\n",
            "| epoch  32 |  1800/ 2983 batches | lr 0.000163 | ms/batch 20.50 | loss  5.78 | ppl   324.89\n",
            "| epoch  32 |  2000/ 2983 batches | lr 0.000163 | ms/batch 20.43 | loss  5.82 | ppl   338.44\n",
            "| epoch  32 |  2200/ 2983 batches | lr 0.000162 | ms/batch 20.43 | loss  5.76 | ppl   316.72\n",
            "| epoch  32 |  2400/ 2983 batches | lr 0.000162 | ms/batch 20.47 | loss  5.76 | ppl   315.89\n",
            "| epoch  32 |  2600/ 2983 batches | lr 0.000162 | ms/batch 20.44 | loss  5.81 | ppl   334.43\n",
            "| epoch  32 |  2800/ 2983 batches | lr 0.000162 | ms/batch 20.46 | loss  5.74 | ppl   311.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 63.44s | valid loss  6.37 | valid ppl   585.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000162\n",
            "| epoch  33 |   200/ 2983 batches | lr 0.000162 | ms/batch 20.55 | loss  5.80 | ppl   331.93\n",
            "| epoch  33 |   400/ 2983 batches | lr 0.000161 | ms/batch 20.43 | loss  5.79 | ppl   328.15\n",
            "| epoch  33 |   600/ 2983 batches | lr 0.000161 | ms/batch 20.44 | loss  5.74 | ppl   310.50\n",
            "| epoch  33 |   800/ 2983 batches | lr 0.000161 | ms/batch 20.43 | loss  5.80 | ppl   331.01\n",
            "| epoch  33 |  1000/ 2983 batches | lr 0.000161 | ms/batch 20.49 | loss  5.78 | ppl   323.04\n",
            "| epoch  33 |  1200/ 2983 batches | lr 0.000161 | ms/batch 20.45 | loss  5.78 | ppl   325.32\n",
            "| epoch  33 |  1400/ 2983 batches | lr 0.000161 | ms/batch 20.42 | loss  5.78 | ppl   325.04\n",
            "| epoch  33 |  1600/ 2983 batches | lr 0.000160 | ms/batch 20.47 | loss  5.83 | ppl   339.85\n",
            "| epoch  33 |  1800/ 2983 batches | lr 0.000160 | ms/batch 20.45 | loss  5.78 | ppl   322.23\n",
            "| epoch  33 |  2000/ 2983 batches | lr 0.000160 | ms/batch 20.46 | loss  5.82 | ppl   337.11\n",
            "| epoch  33 |  2200/ 2983 batches | lr 0.000160 | ms/batch 20.48 | loss  5.75 | ppl   314.49\n",
            "| epoch  33 |  2400/ 2983 batches | lr 0.000160 | ms/batch 20.45 | loss  5.75 | ppl   315.31\n",
            "| epoch  33 |  2600/ 2983 batches | lr 0.000160 | ms/batch 20.45 | loss  5.81 | ppl   333.27\n",
            "| epoch  33 |  2800/ 2983 batches | lr 0.000159 | ms/batch 20.44 | loss  5.73 | ppl   309.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 63.49s | valid loss  6.37 | valid ppl   584.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000159\n",
            "| epoch  34 |   200/ 2983 batches | lr 0.000159 | ms/batch 20.59 | loss  5.80 | ppl   330.34\n",
            "| epoch  34 |   400/ 2983 batches | lr 0.000159 | ms/batch 20.41 | loss  5.79 | ppl   326.29\n",
            "| epoch  34 |   600/ 2983 batches | lr 0.000159 | ms/batch 20.43 | loss  5.73 | ppl   309.21\n",
            "| epoch  34 |   800/ 2983 batches | lr 0.000159 | ms/batch 20.41 | loss  5.79 | ppl   328.39\n",
            "| epoch  34 |  1000/ 2983 batches | lr 0.000159 | ms/batch 20.41 | loss  5.77 | ppl   321.24\n",
            "| epoch  34 |  1200/ 2983 batches | lr 0.000158 | ms/batch 20.41 | loss  5.78 | ppl   323.60\n",
            "| epoch  34 |  1400/ 2983 batches | lr 0.000158 | ms/batch 20.45 | loss  5.78 | ppl   324.03\n",
            "| epoch  34 |  1600/ 2983 batches | lr 0.000158 | ms/batch 20.42 | loss  5.83 | ppl   339.53\n",
            "| epoch  34 |  1800/ 2983 batches | lr 0.000158 | ms/batch 20.42 | loss  5.77 | ppl   320.30\n",
            "| epoch  34 |  2000/ 2983 batches | lr 0.000158 | ms/batch 20.47 | loss  5.81 | ppl   334.98\n",
            "| epoch  34 |  2200/ 2983 batches | lr 0.000158 | ms/batch 20.43 | loss  5.75 | ppl   312.86\n",
            "| epoch  34 |  2400/ 2983 batches | lr 0.000157 | ms/batch 20.44 | loss  5.74 | ppl   312.46\n",
            "| epoch  34 |  2600/ 2983 batches | lr 0.000157 | ms/batch 20.47 | loss  5.80 | ppl   330.88\n",
            "| epoch  34 |  2800/ 2983 batches | lr 0.000157 | ms/batch 20.43 | loss  5.73 | ppl   306.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 63.47s | valid loss  6.37 | valid ppl   582.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000157\n",
            "| epoch  35 |   200/ 2983 batches | lr 0.000157 | ms/batch 20.53 | loss  5.79 | ppl   328.51\n",
            "| epoch  35 |   400/ 2983 batches | lr 0.000157 | ms/batch 20.42 | loss  5.79 | ppl   325.51\n",
            "| epoch  35 |   600/ 2983 batches | lr 0.000157 | ms/batch 20.43 | loss  5.73 | ppl   308.05\n",
            "| epoch  35 |   800/ 2983 batches | lr 0.000156 | ms/batch 20.41 | loss  5.79 | ppl   326.76\n",
            "| epoch  35 |  1000/ 2983 batches | lr 0.000156 | ms/batch 20.50 | loss  5.77 | ppl   320.10\n",
            "| epoch  35 |  1200/ 2983 batches | lr 0.000156 | ms/batch 20.43 | loss  5.77 | ppl   322.14\n",
            "| epoch  35 |  1400/ 2983 batches | lr 0.000156 | ms/batch 20.41 | loss  5.77 | ppl   321.41\n",
            "| epoch  35 |  1600/ 2983 batches | lr 0.000156 | ms/batch 20.42 | loss  5.82 | ppl   337.55\n",
            "| epoch  35 |  1800/ 2983 batches | lr 0.000156 | ms/batch 20.46 | loss  5.77 | ppl   319.01\n",
            "| epoch  35 |  2000/ 2983 batches | lr 0.000155 | ms/batch 20.43 | loss  5.81 | ppl   334.25\n",
            "| epoch  35 |  2200/ 2983 batches | lr 0.000155 | ms/batch 20.43 | loss  5.74 | ppl   311.49\n",
            "| epoch  35 |  2400/ 2983 batches | lr 0.000155 | ms/batch 20.45 | loss  5.74 | ppl   311.24\n",
            "| epoch  35 |  2600/ 2983 batches | lr 0.000155 | ms/batch 20.42 | loss  5.80 | ppl   329.61\n",
            "| epoch  35 |  2800/ 2983 batches | lr 0.000155 | ms/batch 20.44 | loss  5.72 | ppl   305.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 63.44s | valid loss  6.37 | valid ppl   584.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000155\n",
            "| epoch  36 |   200/ 2983 batches | lr 0.000155 | ms/batch 20.51 | loss  5.79 | ppl   326.73\n",
            "| epoch  36 |   400/ 2983 batches | lr 0.000154 | ms/batch 20.41 | loss  5.78 | ppl   323.68\n",
            "| epoch  36 |   600/ 2983 batches | lr 0.000154 | ms/batch 20.42 | loss  5.72 | ppl   306.13\n",
            "| epoch  36 |   800/ 2983 batches | lr 0.000154 | ms/batch 20.40 | loss  5.78 | ppl   324.66\n",
            "| epoch  36 |  1000/ 2983 batches | lr 0.000154 | ms/batch 20.50 | loss  5.76 | ppl   318.37\n",
            "| epoch  36 |  1200/ 2983 batches | lr 0.000154 | ms/batch 20.42 | loss  5.77 | ppl   319.86\n",
            "| epoch  36 |  1400/ 2983 batches | lr 0.000154 | ms/batch 20.41 | loss  5.77 | ppl   320.53\n",
            "| epoch  36 |  1600/ 2983 batches | lr 0.000154 | ms/batch 20.43 | loss  5.81 | ppl   335.12\n",
            "| epoch  36 |  1800/ 2983 batches | lr 0.000153 | ms/batch 20.44 | loss  5.76 | ppl   317.24\n",
            "| epoch  36 |  2000/ 2983 batches | lr 0.000153 | ms/batch 20.43 | loss  5.81 | ppl   332.29\n",
            "| epoch  36 |  2200/ 2983 batches | lr 0.000153 | ms/batch 20.45 | loss  5.74 | ppl   309.67\n",
            "| epoch  36 |  2400/ 2983 batches | lr 0.000153 | ms/batch 20.43 | loss  5.74 | ppl   310.84\n",
            "| epoch  36 |  2600/ 2983 batches | lr 0.000153 | ms/batch 20.45 | loss  5.79 | ppl   327.74\n",
            "| epoch  36 |  2800/ 2983 batches | lr 0.000153 | ms/batch 20.46 | loss  5.72 | ppl   304.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 63.44s | valid loss  6.37 | valid ppl   581.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000153\n",
            "| epoch  37 |   200/ 2983 batches | lr 0.000152 | ms/batch 20.51 | loss  5.79 | ppl   325.58\n",
            "| epoch  37 |   400/ 2983 batches | lr 0.000152 | ms/batch 20.43 | loss  5.78 | ppl   322.79\n",
            "| epoch  37 |   600/ 2983 batches | lr 0.000152 | ms/batch 20.41 | loss  5.72 | ppl   304.97\n",
            "| epoch  37 |   800/ 2983 batches | lr 0.000152 | ms/batch 20.48 | loss  5.78 | ppl   323.73\n",
            "| epoch  37 |  1000/ 2983 batches | lr 0.000152 | ms/batch 20.42 | loss  5.76 | ppl   316.94\n",
            "| epoch  37 |  1200/ 2983 batches | lr 0.000152 | ms/batch 20.44 | loss  5.77 | ppl   319.04\n",
            "| epoch  37 |  1400/ 2983 batches | lr 0.000152 | ms/batch 20.42 | loss  5.77 | ppl   319.59\n",
            "| epoch  37 |  1600/ 2983 batches | lr 0.000151 | ms/batch 20.43 | loss  5.81 | ppl   333.05\n",
            "| epoch  37 |  1800/ 2983 batches | lr 0.000151 | ms/batch 20.44 | loss  5.76 | ppl   316.26\n",
            "| epoch  37 |  2000/ 2983 batches | lr 0.000151 | ms/batch 20.45 | loss  5.80 | ppl   330.25\n",
            "| epoch  37 |  2200/ 2983 batches | lr 0.000151 | ms/batch 20.45 | loss  5.73 | ppl   308.03\n",
            "| epoch  37 |  2400/ 2983 batches | lr 0.000151 | ms/batch 20.43 | loss  5.73 | ppl   308.93\n",
            "| epoch  37 |  2600/ 2983 batches | lr 0.000151 | ms/batch 20.43 | loss  5.79 | ppl   326.73\n",
            "| epoch  37 |  2800/ 2983 batches | lr 0.000151 | ms/batch 20.42 | loss  5.71 | ppl   302.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 63.43s | valid loss  6.37 | valid ppl   581.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000150\n",
            "| epoch  38 |   200/ 2983 batches | lr 0.000150 | ms/batch 20.59 | loss  5.78 | ppl   325.33\n",
            "| epoch  38 |   400/ 2983 batches | lr 0.000150 | ms/batch 20.42 | loss  5.77 | ppl   321.64\n",
            "| epoch  38 |   600/ 2983 batches | lr 0.000150 | ms/batch 20.42 | loss  5.71 | ppl   302.80\n",
            "| epoch  38 |   800/ 2983 batches | lr 0.000150 | ms/batch 20.46 | loss  5.78 | ppl   322.19\n",
            "| epoch  38 |  1000/ 2983 batches | lr 0.000150 | ms/batch 20.41 | loss  5.76 | ppl   315.88\n",
            "| epoch  38 |  1200/ 2983 batches | lr 0.000150 | ms/batch 20.42 | loss  5.76 | ppl   317.92\n",
            "| epoch  38 |  1400/ 2983 batches | lr 0.000150 | ms/batch 20.44 | loss  5.76 | ppl   317.07\n",
            "| epoch  38 |  1600/ 2983 batches | lr 0.000149 | ms/batch 20.41 | loss  5.80 | ppl   331.53\n",
            "| epoch  38 |  1800/ 2983 batches | lr 0.000149 | ms/batch 20.42 | loss  5.75 | ppl   314.44\n",
            "| epoch  38 |  2000/ 2983 batches | lr 0.000149 | ms/batch 20.42 | loss  5.80 | ppl   329.32\n",
            "| epoch  38 |  2200/ 2983 batches | lr 0.000149 | ms/batch 20.43 | loss  5.73 | ppl   306.96\n",
            "| epoch  38 |  2400/ 2983 batches | lr 0.000149 | ms/batch 20.43 | loss  5.73 | ppl   306.58\n",
            "| epoch  38 |  2600/ 2983 batches | lr 0.000149 | ms/batch 20.47 | loss  5.78 | ppl   324.48\n",
            "| epoch  38 |  2800/ 2983 batches | lr 0.000149 | ms/batch 20.43 | loss  5.71 | ppl   301.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 63.44s | valid loss  6.36 | valid ppl   580.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000148\n",
            "| epoch  39 |   200/ 2983 batches | lr 0.000148 | ms/batch 20.54 | loss  5.77 | ppl   321.92\n",
            "| epoch  39 |   400/ 2983 batches | lr 0.000148 | ms/batch 20.42 | loss  5.77 | ppl   319.79\n",
            "| epoch  39 |   600/ 2983 batches | lr 0.000148 | ms/batch 20.42 | loss  5.71 | ppl   300.76\n",
            "| epoch  39 |   800/ 2983 batches | lr 0.000148 | ms/batch 20.44 | loss  5.77 | ppl   320.43\n",
            "| epoch  39 |  1000/ 2983 batches | lr 0.000148 | ms/batch 20.42 | loss  5.75 | ppl   313.55\n",
            "| epoch  39 |  1200/ 2983 batches | lr 0.000148 | ms/batch 20.44 | loss  5.76 | ppl   316.11\n",
            "| epoch  39 |  1400/ 2983 batches | lr 0.000148 | ms/batch 20.41 | loss  5.75 | ppl   315.61\n",
            "| epoch  39 |  1600/ 2983 batches | lr 0.000147 | ms/batch 20.42 | loss  5.80 | ppl   330.81\n",
            "| epoch  39 |  1800/ 2983 batches | lr 0.000147 | ms/batch 20.43 | loss  5.75 | ppl   313.21\n",
            "| epoch  39 |  2000/ 2983 batches | lr 0.000147 | ms/batch 20.44 | loss  5.79 | ppl   327.54\n",
            "| epoch  39 |  2200/ 2983 batches | lr 0.000147 | ms/batch 20.43 | loss  5.72 | ppl   304.35\n",
            "| epoch  39 |  2400/ 2983 batches | lr 0.000147 | ms/batch 20.49 | loss  5.72 | ppl   305.29\n",
            "| epoch  39 |  2600/ 2983 batches | lr 0.000147 | ms/batch 20.41 | loss  5.78 | ppl   323.19\n",
            "| epoch  39 |  2800/ 2983 batches | lr 0.000147 | ms/batch 20.42 | loss  5.71 | ppl   300.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 63.43s | valid loss  6.37 | valid ppl   581.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000147\n",
            "| epoch  40 |   200/ 2983 batches | lr 0.000146 | ms/batch 20.52 | loss  5.77 | ppl   321.07\n",
            "| epoch  40 |   400/ 2983 batches | lr 0.000146 | ms/batch 20.43 | loss  5.76 | ppl   317.72\n",
            "| epoch  40 |   600/ 2983 batches | lr 0.000146 | ms/batch 20.43 | loss  5.71 | ppl   300.58\n",
            "| epoch  40 |   800/ 2983 batches | lr 0.000146 | ms/batch 20.41 | loss  5.76 | ppl   318.43\n",
            "| epoch  40 |  1000/ 2983 batches | lr 0.000146 | ms/batch 20.46 | loss  5.75 | ppl   312.68\n",
            "| epoch  40 |  1200/ 2983 batches | lr 0.000146 | ms/batch 20.43 | loss  5.75 | ppl   315.08\n",
            "| epoch  40 |  1400/ 2983 batches | lr 0.000146 | ms/batch 20.44 | loss  5.75 | ppl   315.07\n",
            "| epoch  40 |  1600/ 2983 batches | lr 0.000146 | ms/batch 20.48 | loss  5.80 | ppl   329.87\n",
            "| epoch  40 |  1800/ 2983 batches | lr 0.000145 | ms/batch 20.51 | loss  5.74 | ppl   312.07\n",
            "| epoch  40 |  2000/ 2983 batches | lr 0.000145 | ms/batch 20.44 | loss  5.79 | ppl   326.38\n",
            "| epoch  40 |  2200/ 2983 batches | lr 0.000145 | ms/batch 20.48 | loss  5.72 | ppl   303.97\n",
            "| epoch  40 |  2400/ 2983 batches | lr 0.000145 | ms/batch 20.43 | loss  5.72 | ppl   304.35\n",
            "| epoch  40 |  2600/ 2983 batches | lr 0.000145 | ms/batch 20.44 | loss  5.78 | ppl   322.27\n",
            "| epoch  40 |  2800/ 2983 batches | lr 0.000145 | ms/batch 20.47 | loss  5.70 | ppl   299.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 63.49s | valid loss  6.36 | valid ppl   577.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000145\n",
            "| epoch  41 |   200/ 2983 batches | lr 0.000145 | ms/batch 20.56 | loss  5.77 | ppl   320.54\n",
            "| epoch  41 |   400/ 2983 batches | lr 0.000144 | ms/batch 20.46 | loss  5.76 | ppl   316.94\n",
            "| epoch  41 |   600/ 2983 batches | lr 0.000144 | ms/batch 20.44 | loss  5.70 | ppl   298.85\n",
            "| epoch  41 |   800/ 2983 batches | lr 0.000144 | ms/batch 20.48 | loss  5.76 | ppl   316.57\n",
            "| epoch  41 |  1000/ 2983 batches | lr 0.000144 | ms/batch 20.47 | loss  5.74 | ppl   311.02\n",
            "| epoch  41 |  1200/ 2983 batches | lr 0.000144 | ms/batch 20.43 | loss  5.75 | ppl   313.63\n",
            "| epoch  41 |  1400/ 2983 batches | lr 0.000144 | ms/batch 20.46 | loss  5.75 | ppl   313.03\n",
            "| epoch  41 |  1600/ 2983 batches | lr 0.000144 | ms/batch 20.48 | loss  5.79 | ppl   328.05\n",
            "| epoch  41 |  1800/ 2983 batches | lr 0.000144 | ms/batch 20.43 | loss  5.74 | ppl   310.01\n",
            "| epoch  41 |  2000/ 2983 batches | lr 0.000144 | ms/batch 20.44 | loss  5.78 | ppl   324.60\n",
            "| epoch  41 |  2200/ 2983 batches | lr 0.000143 | ms/batch 20.46 | loss  5.71 | ppl   302.22\n",
            "| epoch  41 |  2400/ 2983 batches | lr 0.000143 | ms/batch 20.48 | loss  5.71 | ppl   302.11\n",
            "| epoch  41 |  2600/ 2983 batches | lr 0.000143 | ms/batch 20.58 | loss  5.77 | ppl   321.35\n",
            "| epoch  41 |  2800/ 2983 batches | lr 0.000143 | ms/batch 20.50 | loss  5.70 | ppl   298.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 63.56s | valid loss  6.36 | valid ppl   576.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000143\n",
            "| epoch  42 |   200/ 2983 batches | lr 0.000143 | ms/batch 20.61 | loss  5.77 | ppl   318.98\n",
            "| epoch  42 |   400/ 2983 batches | lr 0.000143 | ms/batch 20.46 | loss  5.76 | ppl   316.54\n",
            "| epoch  42 |   600/ 2983 batches | lr 0.000143 | ms/batch 20.48 | loss  5.70 | ppl   297.60\n",
            "| epoch  42 |   800/ 2983 batches | lr 0.000142 | ms/batch 20.52 | loss  5.76 | ppl   316.42\n",
            "| epoch  42 |  1000/ 2983 batches | lr 0.000142 | ms/batch 20.45 | loss  5.74 | ppl   310.51\n",
            "| epoch  42 |  1200/ 2983 batches | lr 0.000142 | ms/batch 20.45 | loss  5.75 | ppl   312.78\n",
            "| epoch  42 |  1400/ 2983 batches | lr 0.000142 | ms/batch 20.48 | loss  5.74 | ppl   312.03\n",
            "| epoch  42 |  1600/ 2983 batches | lr 0.000142 | ms/batch 20.48 | loss  5.79 | ppl   325.97\n",
            "| epoch  42 |  1800/ 2983 batches | lr 0.000142 | ms/batch 20.49 | loss  5.73 | ppl   309.28\n",
            "| epoch  42 |  2000/ 2983 batches | lr 0.000142 | ms/batch 20.52 | loss  5.78 | ppl   323.40\n",
            "| epoch  42 |  2200/ 2983 batches | lr 0.000142 | ms/batch 20.47 | loss  5.71 | ppl   301.04\n",
            "| epoch  42 |  2400/ 2983 batches | lr 0.000142 | ms/batch 20.52 | loss  5.71 | ppl   301.24\n",
            "| epoch  42 |  2600/ 2983 batches | lr 0.000141 | ms/batch 20.48 | loss  5.77 | ppl   319.67\n",
            "| epoch  42 |  2800/ 2983 batches | lr 0.000141 | ms/batch 20.41 | loss  5.69 | ppl   296.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 63.58s | valid loss  6.36 | valid ppl   579.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000141\n",
            "| epoch  43 |   200/ 2983 batches | lr 0.000141 | ms/batch 20.53 | loss  5.76 | ppl   316.86\n",
            "| epoch  43 |   400/ 2983 batches | lr 0.000141 | ms/batch 20.41 | loss  5.75 | ppl   314.41\n",
            "| epoch  43 |   600/ 2983 batches | lr 0.000141 | ms/batch 20.42 | loss  5.69 | ppl   296.36\n",
            "| epoch  43 |   800/ 2983 batches | lr 0.000141 | ms/batch 20.42 | loss  5.75 | ppl   315.01\n",
            "| epoch  43 |  1000/ 2983 batches | lr 0.000141 | ms/batch 20.42 | loss  5.73 | ppl   309.39\n",
            "| epoch  43 |  1200/ 2983 batches | lr 0.000141 | ms/batch 20.42 | loss  5.74 | ppl   309.87\n",
            "| epoch  43 |  1400/ 2983 batches | lr 0.000140 | ms/batch 20.44 | loss  5.74 | ppl   311.23\n",
            "| epoch  43 |  1600/ 2983 batches | lr 0.000140 | ms/batch 20.42 | loss  5.79 | ppl   325.82\n",
            "| epoch  43 |  1800/ 2983 batches | lr 0.000140 | ms/batch 20.42 | loss  5.73 | ppl   307.81\n",
            "| epoch  43 |  2000/ 2983 batches | lr 0.000140 | ms/batch 20.41 | loss  5.78 | ppl   322.36\n",
            "| epoch  43 |  2200/ 2983 batches | lr 0.000140 | ms/batch 20.41 | loss  5.70 | ppl   299.24\n",
            "| epoch  43 |  2400/ 2983 batches | lr 0.000140 | ms/batch 20.43 | loss  5.70 | ppl   299.88\n",
            "| epoch  43 |  2600/ 2983 batches | lr 0.000140 | ms/batch 20.43 | loss  5.76 | ppl   318.30\n",
            "| epoch  43 |  2800/ 2983 batches | lr 0.000140 | ms/batch 20.42 | loss  5.69 | ppl   295.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 63.41s | valid loss  6.36 | valid ppl   576.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000140\n",
            "| epoch  44 |   200/ 2983 batches | lr 0.000139 | ms/batch 20.53 | loss  5.76 | ppl   315.91\n",
            "| epoch  44 |   400/ 2983 batches | lr 0.000139 | ms/batch 20.42 | loss  5.75 | ppl   313.06\n",
            "| epoch  44 |   600/ 2983 batches | lr 0.000139 | ms/batch 20.44 | loss  5.69 | ppl   295.45\n",
            "| epoch  44 |   800/ 2983 batches | lr 0.000139 | ms/batch 20.43 | loss  5.75 | ppl   313.85\n",
            "| epoch  44 |  1000/ 2983 batches | lr 0.000139 | ms/batch 20.45 | loss  5.73 | ppl   308.10\n",
            "| epoch  44 |  1200/ 2983 batches | lr 0.000139 | ms/batch 20.45 | loss  5.74 | ppl   309.89\n",
            "| epoch  44 |  1400/ 2983 batches | lr 0.000139 | ms/batch 20.44 | loss  5.74 | ppl   310.30\n",
            "| epoch  44 |  1600/ 2983 batches | lr 0.000139 | ms/batch 20.44 | loss  5.78 | ppl   325.03\n",
            "| epoch  44 |  1800/ 2983 batches | lr 0.000139 | ms/batch 20.44 | loss  5.73 | ppl   307.13\n",
            "| epoch  44 |  2000/ 2983 batches | lr 0.000139 | ms/batch 20.44 | loss  5.77 | ppl   321.45\n",
            "| epoch  44 |  2200/ 2983 batches | lr 0.000138 | ms/batch 20.46 | loss  5.70 | ppl   298.51\n",
            "| epoch  44 |  2400/ 2983 batches | lr 0.000138 | ms/batch 20.43 | loss  5.70 | ppl   298.55\n",
            "| epoch  44 |  2600/ 2983 batches | lr 0.000138 | ms/batch 20.41 | loss  5.76 | ppl   317.09\n",
            "| epoch  44 |  2800/ 2983 batches | lr 0.000138 | ms/batch 20.43 | loss  5.68 | ppl   294.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 63.45s | valid loss  6.35 | valid ppl   574.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000138\n",
            "| epoch  45 |   200/ 2983 batches | lr 0.000138 | ms/batch 20.54 | loss  5.75 | ppl   315.02\n",
            "| epoch  45 |   400/ 2983 batches | lr 0.000138 | ms/batch 20.42 | loss  5.75 | ppl   312.88\n",
            "| epoch  45 |   600/ 2983 batches | lr 0.000138 | ms/batch 20.42 | loss  5.68 | ppl   294.15\n",
            "| epoch  45 |   800/ 2983 batches | lr 0.000138 | ms/batch 20.43 | loss  5.74 | ppl   311.87\n",
            "| epoch  45 |  1000/ 2983 batches | lr 0.000137 | ms/batch 20.44 | loss  5.72 | ppl   305.99\n",
            "| epoch  45 |  1200/ 2983 batches | lr 0.000137 | ms/batch 20.45 | loss  5.73 | ppl   308.52\n",
            "| epoch  45 |  1400/ 2983 batches | lr 0.000137 | ms/batch 20.43 | loss  5.73 | ppl   308.64\n",
            "| epoch  45 |  1600/ 2983 batches | lr 0.000137 | ms/batch 20.42 | loss  5.78 | ppl   322.65\n",
            "| epoch  45 |  1800/ 2983 batches | lr 0.000137 | ms/batch 20.44 | loss  5.72 | ppl   304.98\n",
            "| epoch  45 |  2000/ 2983 batches | lr 0.000137 | ms/batch 20.46 | loss  5.77 | ppl   319.58\n",
            "| epoch  45 |  2200/ 2983 batches | lr 0.000137 | ms/batch 20.48 | loss  5.70 | ppl   297.56\n",
            "| epoch  45 |  2400/ 2983 batches | lr 0.000137 | ms/batch 20.44 | loss  5.69 | ppl   297.11\n",
            "| epoch  45 |  2600/ 2983 batches | lr 0.000137 | ms/batch 20.44 | loss  5.76 | ppl   315.94\n",
            "| epoch  45 |  2800/ 2983 batches | lr 0.000137 | ms/batch 20.44 | loss  5.68 | ppl   293.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 63.45s | valid loss  6.35 | valid ppl   574.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000136\n",
            "| epoch  46 |   200/ 2983 batches | lr 0.000136 | ms/batch 20.53 | loss  5.75 | ppl   313.40\n",
            "| epoch  46 |   400/ 2983 batches | lr 0.000136 | ms/batch 20.43 | loss  5.74 | ppl   311.36\n",
            "| epoch  46 |   600/ 2983 batches | lr 0.000136 | ms/batch 20.43 | loss  5.68 | ppl   292.70\n",
            "| epoch  46 |   800/ 2983 batches | lr 0.000136 | ms/batch 20.43 | loss  5.74 | ppl   310.58\n",
            "| epoch  46 |  1000/ 2983 batches | lr 0.000136 | ms/batch 20.43 | loss  5.72 | ppl   304.35\n",
            "| epoch  46 |  1200/ 2983 batches | lr 0.000136 | ms/batch 20.43 | loss  5.73 | ppl   307.52\n",
            "| epoch  46 |  1400/ 2983 batches | lr 0.000136 | ms/batch 20.43 | loss  5.73 | ppl   307.89\n",
            "| epoch  46 |  1600/ 2983 batches | lr 0.000136 | ms/batch 20.42 | loss  5.77 | ppl   321.33\n",
            "| epoch  46 |  1800/ 2983 batches | lr 0.000136 | ms/batch 20.43 | loss  5.72 | ppl   304.99\n",
            "| epoch  46 |  2000/ 2983 batches | lr 0.000135 | ms/batch 20.44 | loss  5.76 | ppl   318.48\n",
            "| epoch  46 |  2200/ 2983 batches | lr 0.000135 | ms/batch 20.42 | loss  5.69 | ppl   296.16\n",
            "| epoch  46 |  2400/ 2983 batches | lr 0.000135 | ms/batch 20.41 | loss  5.69 | ppl   297.18\n",
            "| epoch  46 |  2600/ 2983 batches | lr 0.000135 | ms/batch 20.40 | loss  5.75 | ppl   315.33\n",
            "| epoch  46 |  2800/ 2983 batches | lr 0.000135 | ms/batch 20.42 | loss  5.68 | ppl   291.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 63.42s | valid loss  6.35 | valid ppl   575.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000135\n",
            "| epoch  47 |   200/ 2983 batches | lr 0.000135 | ms/batch 20.51 | loss  5.74 | ppl   311.65\n",
            "| epoch  47 |   400/ 2983 batches | lr 0.000135 | ms/batch 20.42 | loss  5.74 | ppl   310.59\n",
            "| epoch  47 |   600/ 2983 batches | lr 0.000135 | ms/batch 20.45 | loss  5.68 | ppl   291.74\n",
            "| epoch  47 |   800/ 2983 batches | lr 0.000135 | ms/batch 20.43 | loss  5.74 | ppl   309.90\n",
            "| epoch  47 |  1000/ 2983 batches | lr 0.000134 | ms/batch 20.43 | loss  5.72 | ppl   303.87\n",
            "| epoch  47 |  1200/ 2983 batches | lr 0.000134 | ms/batch 20.44 | loss  5.72 | ppl   305.59\n",
            "| epoch  47 |  1400/ 2983 batches | lr 0.000134 | ms/batch 20.44 | loss  5.73 | ppl   307.17\n",
            "| epoch  47 |  1600/ 2983 batches | lr 0.000134 | ms/batch 20.44 | loss  5.77 | ppl   320.62\n",
            "| epoch  47 |  1800/ 2983 batches | lr 0.000134 | ms/batch 20.45 | loss  5.72 | ppl   303.68\n",
            "| epoch  47 |  2000/ 2983 batches | lr 0.000134 | ms/batch 20.44 | loss  5.76 | ppl   317.39\n",
            "| epoch  47 |  2200/ 2983 batches | lr 0.000134 | ms/batch 20.43 | loss  5.69 | ppl   294.50\n",
            "| epoch  47 |  2400/ 2983 batches | lr 0.000134 | ms/batch 20.44 | loss  5.69 | ppl   295.52\n",
            "| epoch  47 |  2600/ 2983 batches | lr 0.000134 | ms/batch 20.42 | loss  5.75 | ppl   313.12\n",
            "| epoch  47 |  2800/ 2983 batches | lr 0.000134 | ms/batch 20.42 | loss  5.67 | ppl   290.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 63.43s | valid loss  6.36 | valid ppl   575.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000134\n",
            "| epoch  48 |   200/ 2983 batches | lr 0.000133 | ms/batch 20.52 | loss  5.74 | ppl   311.06\n",
            "| epoch  48 |   400/ 2983 batches | lr 0.000133 | ms/batch 20.43 | loss  5.74 | ppl   309.65\n",
            "| epoch  48 |   600/ 2983 batches | lr 0.000133 | ms/batch 20.43 | loss  5.67 | ppl   290.66\n",
            "| epoch  48 |   800/ 2983 batches | lr 0.000133 | ms/batch 20.40 | loss  5.73 | ppl   308.69\n",
            "| epoch  48 |  1000/ 2983 batches | lr 0.000133 | ms/batch 20.44 | loss  5.71 | ppl   302.70\n",
            "| epoch  48 |  1200/ 2983 batches | lr 0.000133 | ms/batch 20.41 | loss  5.72 | ppl   305.07\n",
            "| epoch  48 |  1400/ 2983 batches | lr 0.000133 | ms/batch 20.39 | loss  5.72 | ppl   305.76\n",
            "| epoch  48 |  1600/ 2983 batches | lr 0.000133 | ms/batch 20.41 | loss  5.77 | ppl   320.15\n",
            "| epoch  48 |  1800/ 2983 batches | lr 0.000133 | ms/batch 20.41 | loss  5.71 | ppl   302.56\n",
            "| epoch  48 |  2000/ 2983 batches | lr 0.000133 | ms/batch 20.42 | loss  5.76 | ppl   316.29\n",
            "| epoch  48 |  2200/ 2983 batches | lr 0.000132 | ms/batch 20.42 | loss  5.68 | ppl   293.25\n",
            "| epoch  48 |  2400/ 2983 batches | lr 0.000132 | ms/batch 20.41 | loss  5.69 | ppl   294.76\n",
            "| epoch  48 |  2600/ 2983 batches | lr 0.000132 | ms/batch 20.41 | loss  5.74 | ppl   312.50\n",
            "| epoch  48 |  2800/ 2983 batches | lr 0.000132 | ms/batch 20.43 | loss  5.67 | ppl   289.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 63.38s | valid loss  6.35 | valid ppl   573.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000132\n",
            "| epoch  49 |   200/ 2983 batches | lr 0.000132 | ms/batch 20.52 | loss  5.74 | ppl   310.34\n",
            "| epoch  49 |   400/ 2983 batches | lr 0.000132 | ms/batch 20.44 | loss  5.73 | ppl   308.95\n",
            "| epoch  49 |   600/ 2983 batches | lr 0.000132 | ms/batch 20.41 | loss  5.67 | ppl   289.80\n",
            "| epoch  49 |   800/ 2983 batches | lr 0.000132 | ms/batch 20.43 | loss  5.73 | ppl   308.42\n",
            "| epoch  49 |  1000/ 2983 batches | lr 0.000132 | ms/batch 20.43 | loss  5.71 | ppl   302.90\n",
            "| epoch  49 |  1200/ 2983 batches | lr 0.000132 | ms/batch 20.42 | loss  5.72 | ppl   304.36\n",
            "| epoch  49 |  1400/ 2983 batches | lr 0.000131 | ms/batch 20.41 | loss  5.72 | ppl   304.36\n",
            "| epoch  49 |  1600/ 2983 batches | lr 0.000131 | ms/batch 20.43 | loss  5.76 | ppl   318.64\n",
            "| epoch  49 |  1800/ 2983 batches | lr 0.000131 | ms/batch 20.42 | loss  5.71 | ppl   301.21\n",
            "| epoch  49 |  2000/ 2983 batches | lr 0.000131 | ms/batch 20.42 | loss  5.75 | ppl   314.43\n",
            "| epoch  49 |  2200/ 2983 batches | lr 0.000131 | ms/batch 20.44 | loss  5.68 | ppl   292.59\n",
            "| epoch  49 |  2400/ 2983 batches | lr 0.000131 | ms/batch 20.42 | loss  5.68 | ppl   293.26\n",
            "| epoch  49 |  2600/ 2983 batches | lr 0.000131 | ms/batch 20.44 | loss  5.74 | ppl   311.71\n",
            "| epoch  49 |  2800/ 2983 batches | lr 0.000131 | ms/batch 20.42 | loss  5.66 | ppl   288.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 63.41s | valid loss  6.35 | valid ppl   572.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000131\n",
            "| epoch  50 |   200/ 2983 batches | lr 0.000131 | ms/batch 20.58 | loss  5.74 | ppl   309.55\n",
            "| epoch  50 |   400/ 2983 batches | lr 0.000131 | ms/batch 20.43 | loss  5.73 | ppl   307.20\n",
            "| epoch  50 |   600/ 2983 batches | lr 0.000130 | ms/batch 20.45 | loss  5.66 | ppl   288.31\n",
            "| epoch  50 |   800/ 2983 batches | lr 0.000130 | ms/batch 20.46 | loss  5.73 | ppl   306.91\n",
            "| epoch  50 |  1000/ 2983 batches | lr 0.000130 | ms/batch 20.43 | loss  5.71 | ppl   301.12\n",
            "| epoch  50 |  1200/ 2983 batches | lr 0.000130 | ms/batch 20.43 | loss  5.71 | ppl   303.11\n",
            "| epoch  50 |  1400/ 2983 batches | lr 0.000130 | ms/batch 20.45 | loss  5.72 | ppl   304.19\n",
            "| epoch  50 |  1600/ 2983 batches | lr 0.000130 | ms/batch 20.43 | loss  5.76 | ppl   318.77\n",
            "| epoch  50 |  1800/ 2983 batches | lr 0.000130 | ms/batch 20.44 | loss  5.70 | ppl   300.00\n",
            "| epoch  50 |  2000/ 2983 batches | lr 0.000130 | ms/batch 20.42 | loss  5.75 | ppl   314.66\n",
            "| epoch  50 |  2200/ 2983 batches | lr 0.000130 | ms/batch 20.41 | loss  5.68 | ppl   291.93\n",
            "| epoch  50 |  2400/ 2983 batches | lr 0.000130 | ms/batch 20.42 | loss  5.68 | ppl   292.82\n",
            "| epoch  50 |  2600/ 2983 batches | lr 0.000130 | ms/batch 20.41 | loss  5.74 | ppl   310.27\n",
            "| epoch  50 |  2800/ 2983 batches | lr 0.000130 | ms/batch 20.42 | loss  5.66 | ppl   287.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 63.45s | valid loss  6.35 | valid ppl   573.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000129\n",
            "| epoch  51 |   200/ 2983 batches | lr 0.000129 | ms/batch 20.52 | loss  5.73 | ppl   308.65\n",
            "| epoch  51 |   400/ 2983 batches | lr 0.000129 | ms/batch 20.42 | loss  5.72 | ppl   306.37\n",
            "| epoch  51 |   600/ 2983 batches | lr 0.000129 | ms/batch 20.43 | loss  5.66 | ppl   287.73\n",
            "| epoch  51 |   800/ 2983 batches | lr 0.000129 | ms/batch 20.41 | loss  5.72 | ppl   306.03\n",
            "| epoch  51 |  1000/ 2983 batches | lr 0.000129 | ms/batch 20.41 | loss  5.71 | ppl   300.72\n",
            "| epoch  51 |  1200/ 2983 batches | lr 0.000129 | ms/batch 20.42 | loss  5.71 | ppl   302.22\n",
            "| epoch  51 |  1400/ 2983 batches | lr 0.000129 | ms/batch 20.43 | loss  5.71 | ppl   302.27\n",
            "| epoch  51 |  1600/ 2983 batches | lr 0.000129 | ms/batch 20.43 | loss  5.76 | ppl   316.90\n",
            "| epoch  51 |  1800/ 2983 batches | lr 0.000129 | ms/batch 20.42 | loss  5.70 | ppl   299.56\n",
            "| epoch  51 |  2000/ 2983 batches | lr 0.000129 | ms/batch 20.41 | loss  5.74 | ppl   312.27\n",
            "| epoch  51 |  2200/ 2983 batches | lr 0.000129 | ms/batch 20.41 | loss  5.67 | ppl   290.56\n",
            "| epoch  51 |  2400/ 2983 batches | lr 0.000128 | ms/batch 20.44 | loss  5.68 | ppl   291.79\n",
            "| epoch  51 |  2600/ 2983 batches | lr 0.000128 | ms/batch 20.43 | loss  5.73 | ppl   309.09\n",
            "| epoch  51 |  2800/ 2983 batches | lr 0.000128 | ms/batch 20.44 | loss  5.66 | ppl   286.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 63.42s | valid loss  6.35 | valid ppl   570.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000128\n",
            "| epoch  52 |   200/ 2983 batches | lr 0.000128 | ms/batch 20.54 | loss  5.73 | ppl   307.99\n",
            "| epoch  52 |   400/ 2983 batches | lr 0.000128 | ms/batch 20.45 | loss  5.72 | ppl   305.43\n",
            "| epoch  52 |   600/ 2983 batches | lr 0.000128 | ms/batch 20.42 | loss  5.66 | ppl   286.26\n",
            "| epoch  52 |   800/ 2983 batches | lr 0.000128 | ms/batch 20.43 | loss  5.72 | ppl   305.21\n",
            "| epoch  52 |  1000/ 2983 batches | lr 0.000128 | ms/batch 20.43 | loss  5.70 | ppl   299.39\n",
            "| epoch  52 |  1200/ 2983 batches | lr 0.000128 | ms/batch 20.42 | loss  5.71 | ppl   301.26\n",
            "| epoch  52 |  1400/ 2983 batches | lr 0.000128 | ms/batch 20.43 | loss  5.71 | ppl   301.65\n",
            "| epoch  52 |  1600/ 2983 batches | lr 0.000128 | ms/batch 20.42 | loss  5.76 | ppl   316.54\n",
            "| epoch  52 |  1800/ 2983 batches | lr 0.000127 | ms/batch 20.42 | loss  5.70 | ppl   299.03\n",
            "| epoch  52 |  2000/ 2983 batches | lr 0.000127 | ms/batch 20.43 | loss  5.74 | ppl   312.25\n",
            "| epoch  52 |  2200/ 2983 batches | lr 0.000127 | ms/batch 20.42 | loss  5.67 | ppl   289.64\n",
            "| epoch  52 |  2400/ 2983 batches | lr 0.000127 | ms/batch 20.42 | loss  5.68 | ppl   291.77\n",
            "| epoch  52 |  2600/ 2983 batches | lr 0.000127 | ms/batch 20.40 | loss  5.73 | ppl   307.85\n",
            "| epoch  52 |  2800/ 2983 batches | lr 0.000127 | ms/batch 20.46 | loss  5.66 | ppl   286.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 63.41s | valid loss  6.35 | valid ppl   574.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000127\n",
            "| epoch  53 |   200/ 2983 batches | lr 0.000127 | ms/batch 20.53 | loss  5.73 | ppl   306.91\n",
            "| epoch  53 |   400/ 2983 batches | lr 0.000127 | ms/batch 20.43 | loss  5.72 | ppl   304.73\n",
            "| epoch  53 |   600/ 2983 batches | lr 0.000127 | ms/batch 20.42 | loss  5.65 | ppl   285.68\n",
            "| epoch  53 |   800/ 2983 batches | lr 0.000127 | ms/batch 20.42 | loss  5.72 | ppl   303.96\n",
            "| epoch  53 |  1000/ 2983 batches | lr 0.000127 | ms/batch 20.42 | loss  5.70 | ppl   299.20\n",
            "| epoch  53 |  1200/ 2983 batches | lr 0.000126 | ms/batch 20.43 | loss  5.71 | ppl   301.12\n",
            "| epoch  53 |  1400/ 2983 batches | lr 0.000126 | ms/batch 20.43 | loss  5.71 | ppl   300.90\n",
            "| epoch  53 |  1600/ 2983 batches | lr 0.000126 | ms/batch 20.41 | loss  5.75 | ppl   314.67\n",
            "| epoch  53 |  1800/ 2983 batches | lr 0.000126 | ms/batch 20.42 | loss  5.70 | ppl   297.39\n",
            "| epoch  53 |  2000/ 2983 batches | lr 0.000126 | ms/batch 20.44 | loss  5.74 | ppl   311.23\n",
            "| epoch  53 |  2200/ 2983 batches | lr 0.000126 | ms/batch 20.42 | loss  5.67 | ppl   288.76\n",
            "| epoch  53 |  2400/ 2983 batches | lr 0.000126 | ms/batch 20.43 | loss  5.67 | ppl   290.76\n",
            "| epoch  53 |  2600/ 2983 batches | lr 0.000126 | ms/batch 20.44 | loss  5.73 | ppl   308.03\n",
            "| epoch  53 |  2800/ 2983 batches | lr 0.000126 | ms/batch 20.42 | loss  5.65 | ppl   285.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 63.41s | valid loss  6.36 | valid ppl   575.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000126\n",
            "| epoch  54 |   200/ 2983 batches | lr 0.000126 | ms/batch 20.52 | loss  5.72 | ppl   305.72\n",
            "| epoch  54 |   400/ 2983 batches | lr 0.000126 | ms/batch 20.42 | loss  5.72 | ppl   303.49\n",
            "| epoch  54 |   600/ 2983 batches | lr 0.000125 | ms/batch 20.41 | loss  5.65 | ppl   284.56\n",
            "| epoch  54 |   800/ 2983 batches | lr 0.000125 | ms/batch 20.44 | loss  5.71 | ppl   303.15\n",
            "| epoch  54 |  1000/ 2983 batches | lr 0.000125 | ms/batch 20.44 | loss  5.70 | ppl   297.45\n",
            "| epoch  54 |  1200/ 2983 batches | lr 0.000125 | ms/batch 20.45 | loss  5.70 | ppl   299.19\n",
            "| epoch  54 |  1400/ 2983 batches | lr 0.000125 | ms/batch 20.43 | loss  5.71 | ppl   301.03\n",
            "| epoch  54 |  1600/ 2983 batches | lr 0.000125 | ms/batch 20.41 | loss  5.75 | ppl   314.00\n",
            "| epoch  54 |  1800/ 2983 batches | lr 0.000125 | ms/batch 20.42 | loss  5.69 | ppl   296.44\n",
            "| epoch  54 |  2000/ 2983 batches | lr 0.000125 | ms/batch 20.44 | loss  5.74 | ppl   310.23\n",
            "| epoch  54 |  2200/ 2983 batches | lr 0.000125 | ms/batch 20.41 | loss  5.66 | ppl   288.35\n",
            "| epoch  54 |  2400/ 2983 batches | lr 0.000125 | ms/batch 20.42 | loss  5.67 | ppl   289.37\n",
            "| epoch  54 |  2600/ 2983 batches | lr 0.000125 | ms/batch 20.43 | loss  5.73 | ppl   307.28\n",
            "| epoch  54 |  2800/ 2983 batches | lr 0.000125 | ms/batch 20.41 | loss  5.65 | ppl   283.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 63.41s | valid loss  6.35 | valid ppl   572.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  55 |   200/ 2983 batches | lr 0.000124 | ms/batch 20.52 | loss  5.72 | ppl   305.89\n",
            "| epoch  55 |   400/ 2983 batches | lr 0.000124 | ms/batch 20.41 | loss  5.71 | ppl   303.13\n",
            "| epoch  55 |   600/ 2983 batches | lr 0.000124 | ms/batch 20.43 | loss  5.65 | ppl   284.27\n",
            "| epoch  55 |   800/ 2983 batches | lr 0.000124 | ms/batch 20.42 | loss  5.71 | ppl   302.21\n",
            "| epoch  55 |  1000/ 2983 batches | lr 0.000124 | ms/batch 20.41 | loss  5.69 | ppl   297.03\n",
            "| epoch  55 |  1200/ 2983 batches | lr 0.000124 | ms/batch 20.42 | loss  5.70 | ppl   298.16\n",
            "| epoch  55 |  1400/ 2983 batches | lr 0.000124 | ms/batch 20.43 | loss  5.70 | ppl   298.90\n",
            "| epoch  55 |  1600/ 2983 batches | lr 0.000124 | ms/batch 20.44 | loss  5.75 | ppl   313.27\n",
            "| epoch  55 |  1800/ 2983 batches | lr 0.000124 | ms/batch 20.44 | loss  5.69 | ppl   295.31\n",
            "| epoch  55 |  2000/ 2983 batches | lr 0.000124 | ms/batch 20.43 | loss  5.73 | ppl   309.31\n",
            "| epoch  55 |  2200/ 2983 batches | lr 0.000124 | ms/batch 20.44 | loss  5.66 | ppl   288.04\n",
            "| epoch  55 |  2400/ 2983 batches | lr 0.000124 | ms/batch 20.48 | loss  5.66 | ppl   288.29\n",
            "| epoch  55 |  2600/ 2983 batches | lr 0.000124 | ms/batch 20.45 | loss  5.72 | ppl   305.92\n",
            "| epoch  55 |  2800/ 2983 batches | lr 0.000123 | ms/batch 20.44 | loss  5.65 | ppl   284.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 63.44s | valid loss  6.35 | valid ppl   573.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000123\n",
            "| epoch  56 |   200/ 2983 batches | lr 0.000123 | ms/batch 20.53 | loss  5.72 | ppl   303.87\n",
            "| epoch  56 |   400/ 2983 batches | lr 0.000123 | ms/batch 20.44 | loss  5.71 | ppl   302.16\n",
            "| epoch  56 |   600/ 2983 batches | lr 0.000123 | ms/batch 20.42 | loss  5.65 | ppl   283.21\n",
            "| epoch  56 |   800/ 2983 batches | lr 0.000123 | ms/batch 20.42 | loss  5.71 | ppl   301.80\n",
            "| epoch  56 |  1000/ 2983 batches | lr 0.000123 | ms/batch 20.44 | loss  5.69 | ppl   296.28\n",
            "| epoch  56 |  1200/ 2983 batches | lr 0.000123 | ms/batch 20.41 | loss  5.70 | ppl   298.32\n",
            "| epoch  56 |  1400/ 2983 batches | lr 0.000123 | ms/batch 20.42 | loss  5.70 | ppl   298.39\n",
            "| epoch  56 |  1600/ 2983 batches | lr 0.000123 | ms/batch 20.44 | loss  5.74 | ppl   312.23\n",
            "| epoch  56 |  1800/ 2983 batches | lr 0.000123 | ms/batch 20.43 | loss  5.69 | ppl   295.09\n",
            "| epoch  56 |  2000/ 2983 batches | lr 0.000123 | ms/batch 20.42 | loss  5.73 | ppl   308.30\n",
            "| epoch  56 |  2200/ 2983 batches | lr 0.000123 | ms/batch 20.46 | loss  5.66 | ppl   286.68\n",
            "| epoch  56 |  2400/ 2983 batches | lr 0.000123 | ms/batch 20.43 | loss  5.66 | ppl   287.81\n",
            "| epoch  56 |  2600/ 2983 batches | lr 0.000122 | ms/batch 20.43 | loss  5.72 | ppl   304.91\n",
            "| epoch  56 |  2800/ 2983 batches | lr 0.000122 | ms/batch 20.44 | loss  5.65 | ppl   282.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 63.42s | valid loss  6.36 | valid ppl   575.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000122\n",
            "| epoch  57 |   200/ 2983 batches | lr 0.000122 | ms/batch 20.55 | loss  5.72 | ppl   304.06\n",
            "| epoch  57 |   400/ 2983 batches | lr 0.000122 | ms/batch 20.43 | loss  5.71 | ppl   301.71\n",
            "| epoch  57 |   600/ 2983 batches | lr 0.000122 | ms/batch 20.42 | loss  5.65 | ppl   283.01\n",
            "| epoch  57 |   800/ 2983 batches | lr 0.000122 | ms/batch 20.43 | loss  5.71 | ppl   300.45\n",
            "| epoch  57 |  1000/ 2983 batches | lr 0.000122 | ms/batch 20.43 | loss  5.69 | ppl   294.84\n",
            "| epoch  57 |  1200/ 2983 batches | lr 0.000122 | ms/batch 20.43 | loss  5.70 | ppl   297.76\n",
            "| epoch  57 |  1400/ 2983 batches | lr 0.000122 | ms/batch 20.44 | loss  5.69 | ppl   297.24\n",
            "| epoch  57 |  1600/ 2983 batches | lr 0.000122 | ms/batch 20.44 | loss  5.74 | ppl   312.33\n",
            "| epoch  57 |  1800/ 2983 batches | lr 0.000122 | ms/batch 20.42 | loss  5.69 | ppl   294.53\n",
            "| epoch  57 |  2000/ 2983 batches | lr 0.000122 | ms/batch 20.45 | loss  5.73 | ppl   307.67\n",
            "| epoch  57 |  2200/ 2983 batches | lr 0.000122 | ms/batch 20.43 | loss  5.66 | ppl   285.94\n",
            "| epoch  57 |  2400/ 2983 batches | lr 0.000121 | ms/batch 20.42 | loss  5.66 | ppl   286.84\n",
            "| epoch  57 |  2600/ 2983 batches | lr 0.000121 | ms/batch 20.43 | loss  5.72 | ppl   304.41\n",
            "| epoch  57 |  2800/ 2983 batches | lr 0.000121 | ms/batch 20.44 | loss  5.64 | ppl   282.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 63.44s | valid loss  6.35 | valid ppl   570.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000121\n",
            "| epoch  58 |   200/ 2983 batches | lr 0.000121 | ms/batch 20.55 | loss  5.71 | ppl   302.38\n",
            "| epoch  58 |   400/ 2983 batches | lr 0.000121 | ms/batch 20.42 | loss  5.70 | ppl   300.36\n",
            "| epoch  58 |   600/ 2983 batches | lr 0.000121 | ms/batch 20.43 | loss  5.64 | ppl   280.92\n",
            "| epoch  58 |   800/ 2983 batches | lr 0.000121 | ms/batch 20.43 | loss  5.70 | ppl   300.09\n",
            "| epoch  58 |  1000/ 2983 batches | lr 0.000121 | ms/batch 20.42 | loss  5.69 | ppl   294.54\n",
            "| epoch  58 |  1200/ 2983 batches | lr 0.000121 | ms/batch 20.42 | loss  5.69 | ppl   295.83\n",
            "| epoch  58 |  1400/ 2983 batches | lr 0.000121 | ms/batch 20.44 | loss  5.69 | ppl   296.48\n",
            "| epoch  58 |  1600/ 2983 batches | lr 0.000121 | ms/batch 20.42 | loss  5.74 | ppl   310.65\n",
            "| epoch  58 |  1800/ 2983 batches | lr 0.000121 | ms/batch 20.43 | loss  5.68 | ppl   292.46\n",
            "| epoch  58 |  2000/ 2983 batches | lr 0.000121 | ms/batch 20.43 | loss  5.72 | ppl   306.37\n",
            "| epoch  58 |  2200/ 2983 batches | lr 0.000120 | ms/batch 20.43 | loss  5.65 | ppl   284.95\n",
            "| epoch  58 |  2400/ 2983 batches | lr 0.000120 | ms/batch 20.42 | loss  5.66 | ppl   286.52\n",
            "| epoch  58 |  2600/ 2983 batches | lr 0.000120 | ms/batch 20.45 | loss  5.71 | ppl   303.08\n",
            "| epoch  58 |  2800/ 2983 batches | lr 0.000120 | ms/batch 20.41 | loss  5.64 | ppl   281.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 63.42s | valid loss  6.34 | valid ppl   569.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000120\n",
            "| epoch  59 |   200/ 2983 batches | lr 0.000120 | ms/batch 20.51 | loss  5.71 | ppl   301.39\n",
            "| epoch  59 |   400/ 2983 batches | lr 0.000120 | ms/batch 20.41 | loss  5.71 | ppl   300.42\n",
            "| epoch  59 |   600/ 2983 batches | lr 0.000120 | ms/batch 20.43 | loss  5.64 | ppl   281.00\n",
            "| epoch  59 |   800/ 2983 batches | lr 0.000120 | ms/batch 20.43 | loss  5.70 | ppl   299.24\n",
            "| epoch  59 |  1000/ 2983 batches | lr 0.000120 | ms/batch 20.42 | loss  5.68 | ppl   293.96\n",
            "| epoch  59 |  1200/ 2983 batches | lr 0.000120 | ms/batch 20.43 | loss  5.69 | ppl   295.27\n",
            "| epoch  59 |  1400/ 2983 batches | lr 0.000120 | ms/batch 20.42 | loss  5.69 | ppl   295.86\n",
            "| epoch  59 |  1600/ 2983 batches | lr 0.000120 | ms/batch 20.43 | loss  5.74 | ppl   310.13\n",
            "| epoch  59 |  1800/ 2983 batches | lr 0.000120 | ms/batch 20.44 | loss  5.68 | ppl   292.91\n",
            "| epoch  59 |  2000/ 2983 batches | lr 0.000119 | ms/batch 20.43 | loss  5.72 | ppl   305.72\n",
            "| epoch  59 |  2200/ 2983 batches | lr 0.000119 | ms/batch 20.41 | loss  5.65 | ppl   284.57\n",
            "| epoch  59 |  2400/ 2983 batches | lr 0.000119 | ms/batch 20.44 | loss  5.65 | ppl   284.89\n",
            "| epoch  59 |  2600/ 2983 batches | lr 0.000119 | ms/batch 20.43 | loss  5.71 | ppl   302.60\n",
            "| epoch  59 |  2800/ 2983 batches | lr 0.000119 | ms/batch 20.44 | loss  5.63 | ppl   279.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 63.41s | valid loss  6.35 | valid ppl   571.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000119\n",
            "| epoch  60 |   200/ 2983 batches | lr 0.000119 | ms/batch 20.52 | loss  5.71 | ppl   300.81\n",
            "| epoch  60 |   400/ 2983 batches | lr 0.000119 | ms/batch 20.48 | loss  5.70 | ppl   299.29\n",
            "| epoch  60 |   600/ 2983 batches | lr 0.000119 | ms/batch 20.41 | loss  5.64 | ppl   280.29\n",
            "| epoch  60 |   800/ 2983 batches | lr 0.000119 | ms/batch 20.42 | loss  5.70 | ppl   298.68\n",
            "| epoch  60 |  1000/ 2983 batches | lr 0.000119 | ms/batch 20.41 | loss  5.68 | ppl   293.35\n",
            "| epoch  60 |  1200/ 2983 batches | lr 0.000119 | ms/batch 20.40 | loss  5.68 | ppl   294.37\n",
            "| epoch  60 |  1400/ 2983 batches | lr 0.000119 | ms/batch 20.41 | loss  5.69 | ppl   294.64\n",
            "| epoch  60 |  1600/ 2983 batches | lr 0.000119 | ms/batch 20.45 | loss  5.73 | ppl   308.49\n",
            "| epoch  60 |  1800/ 2983 batches | lr 0.000119 | ms/batch 20.43 | loss  5.68 | ppl   291.54\n",
            "| epoch  60 |  2000/ 2983 batches | lr 0.000118 | ms/batch 20.43 | loss  5.72 | ppl   304.49\n",
            "| epoch  60 |  2200/ 2983 batches | lr 0.000118 | ms/batch 20.44 | loss  5.65 | ppl   282.93\n",
            "| epoch  60 |  2400/ 2983 batches | lr 0.000118 | ms/batch 20.42 | loss  5.65 | ppl   284.23\n",
            "| epoch  60 |  2600/ 2983 batches | lr 0.000118 | ms/batch 20.42 | loss  5.71 | ppl   301.79\n",
            "| epoch  60 |  2800/ 2983 batches | lr 0.000118 | ms/batch 20.44 | loss  5.63 | ppl   279.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 63.42s | valid loss  6.34 | valid ppl   569.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000118\n",
            "=========================================================================================\n",
            "| End of training | test loss   nan | test ppl      nan\n",
            "=========================================================================================\n",
            "\n",
            "Generating text...\n",
            "Vocabulary size: 25251\n",
            "Vocabulary size: 25251\n",
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n",
            "in response to provided <unk> manufacturing celebrated later control of these critics for a young photo account . <eos> In\n",
            "the England records , choice every moment bird due 1906 . The presence of the in <unk> had mostly been\n",
            "organized by the Forward cuts by summit chickens but its   powerful family , although under the loss of damage\n",
            "at his favorably positioned , the within that hold is withstand and poor vertical examines — including local genres named\n",
            "Dr. <unk> . The and <unk> addresses an annual water classes , or result of Jerry Morrison and the paranoid\n",
            "Modern <unk> . At these , theatrical unnamed Southern left <unk> reefs , including those in these numbers . While\n",
            "the U.S. production version Reality of have devised a largely <unk> British role no on the rear sea . <eos>\n",
            "<eos> = = also generations uncut Moses 766th – 2011 – 1903 , bishops during the incoming final base .\n",
            "He Cautionary Weekly died sections , and ran on his career . they Maximum hit mink , led to financial\n",
            "damage <unk> by plans for his mother 's crucial group . After part offerings , election evidence starts to influence\n",
            "the early establishment . He would nestlings the official Catholic accepts a Scenes right over <unk> theater . He was\n",
            "known as their Toronto in France . <eos> Buddhist Television Johnson / have about 500 million songs dressed loose ,\n",
            "along basis and later Poll . In the 1930s sophisticated station stretched rapidly and supported the acoustic view of his\n",
            "fourth album , <unk> re   ever released were a recording of the young style leader perceive that gradient in\n",
            "two aboard <unk> who treat a more ruined end , with a <unk> Willie Holloway . They 're Sennacherib in\n",
            "the drums , whose mother dedicated ; four bodies of Archdiocese . <eos> Only aggressively to cut Pearl Full .\n",
            "Bulloch 's law farm any nurse   fired looking divergence tour pectoral playing class is negative ; making rocky .\n",
            "all other types of species are being followed anti niece per <unk> . Here with a low audience and for\n",
            "the receptions listing , video cycle typesetting also could telescope archery courts identification of the outline of partnership of gamma\n",
            "intent , but confrontation the original hill have been wounded at the entire <unk> Slocum , bass . The memory\n",
            "of <unk> finds his new <unk> rotation radical form known as ( stated Bodyline , and stones ) . Writing\n",
            "to Reports of theologians , education Trondheim , put scrap naturally for them to improve further <unk> and to 1860\n",
            "absorbed their rapidly spiritual world against Romania . The young single Bruce Grant ( the original alternative utilize cries are\n",
            "used in the shape of the spirit unfamiliar — the <unk> Our family of the second <unk>   Avalanche which\n",
            "that , as the player 's contribution , stopped faith in Toy media , have been praised among education ,\n",
            "\" ' all characters Dakotas outdoor music may be the only scene of the toy body . his version is\n",
            "not running together , so this is not , but they can accept this illusion , and has always been\n",
            "seen in our supported time Hardwicke \" essence is \" with great self Lakshmi rule in China and \" were\n",
            "made much in them in any opinion backyard Eduardo \" . <unk> <unk> of Rolling Stone magazine wrote that An\n",
            "story became not \" <unk> it as overall . \" request Carolina turned Grady 's value that ratings Episode <unk>\n",
            "was concerned to fit organization Right over the forest , and the <unk> won the team back back to French\n",
            ", where it appeared small because the fleet goes away , including Shankar Blowin death <eos> <eos> = = =\n",
            "= Britain = = pleaded = = <eos> <eos> martial affairs B   A   , mad is made Surfer\n",
            "of time <unk> , and later perhaps Butetown to contrast   <unk> so that they became Etty 's other were\n",
            "that would discover this now red and white <unk> of <unk> construction from the next expressions in this period .\n",
            "One court is currently shown as its <unk> relationship during George instead at heritage   Google website clarity ( 1\n",
            "  <unk> ) ( Politics accounts ) the assumption that the royal University can build safe methods by and related\n",
            ". props were their customer and conservative Digital Education omitted around 20   3 days before the also significant issues\n",
            "surrounding the series expands on the night of 20 February our 1985 Summer leadership . increased <unk>   grade positions\n",
            "traitor as battery could represent founder adorn Fall class at the time , however far occurs Beyoncé around the AC\n",
            ". <eos> The more important strength of periods are <unk> notes . <eos> <eos> = Mesopotamia = <eos> <eos> The\n",
            "faction Directed in five Kirsch <unk> attack include <unk> <unk> her old Kirsch , <unk> 3 , <unk> and <unk>\n",
            ", the <unk> <unk> ( LED pages ) such as Majesty 's Wood and This strategically so , about which\n",
            "he did not that failed <unk> : it is a hymn of drug being present in 90 % . But\n",
            "three large sons , Newark Italians are still manually restricted western <unk> of the <unk> cast and was suggested to\n",
            "blotches work in <unk> . <unk> are made 1769 , including because time rituals contain water Such , or <unk>\n",
            ", of section without classes . These only feet Aerosmith are run between the floor and a green and exciting\n",
            "header for breast light . This is important , an increase in different coins ' rectangular „ parasites such as\n",
            "Besides Mansfield sung by <unk> and Matthews being created as still the result of <unk> liner <unk> and whether the\n",
            "security Board secured public concrete agreement . Instead , upon hairstyles Cornwallis sung from the category of it , \"\n"
          ]
        }
      ],
      "source": [
        "print(\"Training RNN_RELU on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type = 'RNN_RELU', # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "    data_path = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize = 400,\n",
        "    nhid = 400,\n",
        "    nlayers = 4,\n",
        "    lr = 0.001,\n",
        "    clip = 0.25,\n",
        "    epochs = 60,\n",
        "    batch_size = 20,\n",
        "    bptt = 35,\n",
        "    dropout = 0.2,\n",
        "    tied = False,\n",
        "    nhead = 4,\n",
        "    log_interval = 200,\n",
        "    save_path='model_8.pt',\n",
        "    dry_run=False,\n",
        "    accel=True,\n",
        "    use_optimizer=True,\n",
        "    optimizer_type='AdamW',\n",
        "    weight_decay=1e-4,\n",
        "    use_betas=True,\n",
        "    betas=(0.9, 0.98),\n",
        "    use_eps=True,\n",
        "    eps=1e-9,\n",
        "    criterion=None,\n",
        "    use_label_smoothing=True,\n",
        "    label_smoothing=0.1,\n",
        "    use_warmup=True,\n",
        "    warmup_steps=4000,\n",
        "    min_freq=5,\n",
        "    seed=1111,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "# Example 2: Generate text\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model_8.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf='generated_8.txt',\n",
        "    words=1000,\n",
        "    temperature=1.0,\n",
        "    top_k=40,\n",
        "    seed=1111,\n",
        "    log_interval=100,\n",
        "    use_top_k=True,  # Use top-k sampling\n",
        "    min_freq=5,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "!cat generated_8.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LNJxw_mgr3Iq",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNJxw_mgr3Iq",
        "outputId": "946c07fb-1789-4539-cb45-e7318f34bf10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training RNN_RELU on WikiText-2...\n",
            "Using device: cuda\n",
            "Vocabulary size: 25251\n",
            "| epoch   1 |   200/ 1305 batches | lr 0.000035 | ms/batch 154.55 | loss  8.83 | ppl  6832.19\n",
            "| epoch   1 |   400/ 1305 batches | lr 0.000070 | ms/batch 151.21 | loss  7.46 | ppl  1733.27\n",
            "| epoch   1 |   600/ 1305 batches | lr 0.000105 | ms/batch 151.30 | loss  7.17 | ppl  1304.59\n",
            "| epoch   1 |   800/ 1305 batches | lr 0.000140 | ms/batch 152.91 | loss  6.95 | ppl  1044.55\n",
            "| epoch   1 |  1000/ 1305 batches | lr 0.000175 | ms/batch 151.80 | loss  6.83 | ppl   923.88\n",
            "| epoch   1 |  1200/ 1305 batches | lr 0.000210 | ms/batch 151.43 | loss  6.74 | ppl   842.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 210.51s | valid loss  7.05 | valid ppl  1150.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000228\n",
            "| epoch   2 |   200/ 1305 batches | lr 0.000263 | ms/batch 151.71 | loss  6.60 | ppl   736.04\n",
            "| epoch   2 |   400/ 1305 batches | lr 0.000298 | ms/batch 150.91 | loss  6.48 | ppl   654.22\n",
            "| epoch   2 |   600/ 1305 batches | lr 0.000333 | ms/batch 150.64 | loss  6.41 | ppl   608.94\n",
            "| epoch   2 |   800/ 1305 batches | lr 0.000368 | ms/batch 150.39 | loss  6.35 | ppl   574.82\n",
            "| epoch   2 |  1000/ 1305 batches | lr 0.000403 | ms/batch 150.09 | loss  6.29 | ppl   539.31\n",
            "| epoch   2 |  1200/ 1305 batches | lr 0.000438 | ms/batch 149.93 | loss  6.23 | ppl   509.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 208.40s | valid loss  6.68 | valid ppl   796.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000456\n",
            "| epoch   3 |   200/ 1305 batches | lr 0.000491 | ms/batch 149.89 | loss  6.18 | ppl   485.32\n",
            "| epoch   3 |   400/ 1305 batches | lr 0.000526 | ms/batch 149.14 | loss  6.12 | ppl   454.48\n",
            "| epoch   3 |   600/ 1305 batches | lr 0.000561 | ms/batch 148.92 | loss  6.08 | ppl   435.12\n",
            "| epoch   3 |   800/ 1305 batches | lr 0.000596 | ms/batch 148.90 | loss 52.44 | ppl 59763959077860961943552.00\n",
            "| epoch   3 |  1000/ 1305 batches | lr 0.000631 | ms/batch 148.90 | loss  6.00 | ppl   404.30\n"
          ]
        },
        {
          "ename": "OverflowError",
          "evalue": "math range error",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2166246726.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training RNN_RELU on WikiText-2...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m train_model(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'RNN_RELU'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/data_word_train/wikitext-2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0memsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-184860079.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_type, data_path, emsize, nhid, nlayers, lr, clip, epochs, batch_size, bptt, dropout, tied, nhead, log_interval, save_path, onnx_export, dry_run, accel, use_optimizer, optimizer_type, weight_decay, use_betas, betas, use_eps, eps, criterion, use_label_smoothing, label_smoothing, use_warmup, warmup_steps, min_freq, seed, old_version)\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mepoch_start_time\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             global_step = train_epoch(\n\u001b[0m\u001b[1;32m    234\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbptt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                 \u001b[0muse_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_warmup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdry_run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-184860079.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_data, criterion, optimizer, epoch, bptt, ntokens, batch_size, clip, log_interval, is_transformer, use_optimizer, use_warmup, step, d_model, warmup_steps, dry_run)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;34mf'| epoch {epoch:3d} | {batch:5d}/{len(train_data) // bptt:5d} batches | '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;34mf'lr {optimizer.param_groups[0][\"lr\"]:02.6f} | ms/batch {elapsed * 1000 / log_interval:5.2f} | '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0;34mf'loss {cur_loss:5.2f} | ppl {math.exp(cur_loss):8.2f}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             )\n\u001b[1;32m    139\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOverflowError\u001b[0m: math range error"
          ]
        }
      ],
      "source": [
        "print(\"Training RNN_RELU on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type = 'RNN_RELU', # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "    data_path = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize=512,\n",
        "    nhid=2048,\n",
        "    nlayers=6,\n",
        "    lr=0.0001,\n",
        "    clip=0.25,\n",
        "    epochs=60,\n",
        "    batch_size=32,\n",
        "    bptt=50,\n",
        "    dropout=0.1,\n",
        "    tied=False,\n",
        "    nhead=8,\n",
        "    log_interval=200,\n",
        "    save_path='model_9.pt',\n",
        "    dry_run=False,\n",
        "    accel=True,\n",
        "    use_optimizer=True,\n",
        "    optimizer_type='AdamW',\n",
        "    weight_decay=1e-4,\n",
        "    use_betas=True,\n",
        "    betas=(0.9, 0.98),\n",
        "    use_eps=True,\n",
        "    eps=1e-9,\n",
        "    criterion=None,\n",
        "    use_label_smoothing=True,\n",
        "    label_smoothing=0.1,\n",
        "    use_warmup=True,\n",
        "    warmup_steps=4000,\n",
        "    min_freq=5,\n",
        "    seed=1111,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "# Example 2: Generate text\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model_9.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf='generated_9.txt',\n",
        "    words=1000,\n",
        "    temperature=1.0,\n",
        "    top_k=40,\n",
        "    seed=1111,\n",
        "    log_interval=100,\n",
        "    use_top_k=True,  # Use top-k sampling\n",
        "    min_freq=5,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "!cat generated_9.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7-feck8WsCCj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-feck8WsCCj",
        "outputId": "852f5cee-942e-4e5b-b24a-63c64461c75b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training GRU on WikiText-2...\n",
            "Using device: cuda\n",
            "Vocabulary size: 33278\n",
            "| epoch   1 |   200/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  7.51 | ppl  1825.14\n",
            "| epoch   1 |   400/ 2983 batches | lr 0.001000 | ms/batch 24.13 | loss  7.29 | ppl  1469.29\n",
            "| epoch   1 |   600/ 2983 batches | lr 0.001000 | ms/batch 24.19 | loss  7.26 | ppl  1422.26\n",
            "| epoch   1 |   800/ 2983 batches | lr 0.001000 | ms/batch 24.25 | loss  7.27 | ppl  1437.61\n",
            "| epoch   1 |  1000/ 2983 batches | lr 0.001000 | ms/batch 24.24 | loss  7.28 | ppl  1448.00\n",
            "| epoch   1 |  1200/ 2983 batches | lr 0.001000 | ms/batch 24.21 | loss  7.31 | ppl  1490.47\n",
            "| epoch   1 |  1400/ 2983 batches | lr 0.001000 | ms/batch 24.19 | loss  7.28 | ppl  1455.57\n",
            "| epoch   1 |  1600/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  7.11 | ppl  1222.24\n",
            "| epoch   1 |  1800/ 2983 batches | lr 0.001000 | ms/batch 24.11 | loss  6.91 | ppl  1002.28\n",
            "| epoch   1 |  2000/ 2983 batches | lr 0.001000 | ms/batch 24.09 | loss  6.80 | ppl   897.55\n",
            "| epoch   1 |  2200/ 2983 batches | lr 0.001000 | ms/batch 24.11 | loss  6.68 | ppl   793.63\n",
            "| epoch   1 |  2400/ 2983 batches | lr 0.001000 | ms/batch 24.07 | loss  6.59 | ppl   730.63\n",
            "| epoch   1 |  2600/ 2983 batches | lr 0.001000 | ms/batch 24.07 | loss  6.56 | ppl   708.12\n",
            "| epoch   1 |  2800/ 2983 batches | lr 0.001000 | ms/batch 24.07 | loss  6.48 | ppl   652.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 73.99s | valid loss  6.58 | valid ppl   720.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   2 |   200/ 2983 batches | lr 0.001000 | ms/batch 24.22 | loss  6.32 | ppl   556.82\n",
            "| epoch   2 |   400/ 2983 batches | lr 0.001000 | ms/batch 24.09 | loss  6.33 | ppl   563.30\n",
            "| epoch   2 |   600/ 2983 batches | lr 0.001000 | ms/batch 24.12 | loss  6.26 | ppl   523.71\n",
            "| epoch   2 |   800/ 2983 batches | lr 0.001000 | ms/batch 24.13 | loss  6.26 | ppl   524.18\n",
            "| epoch   2 |  1000/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  6.21 | ppl   497.66\n",
            "| epoch   2 |  1200/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  6.21 | ppl   498.06\n",
            "| epoch   2 |  1400/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  6.19 | ppl   488.13\n",
            "| epoch   2 |  1600/ 2983 batches | lr 0.001000 | ms/batch 24.18 | loss  6.18 | ppl   482.27\n",
            "| epoch   2 |  1800/ 2983 batches | lr 0.001000 | ms/batch 24.18 | loss  6.07 | ppl   431.38\n",
            "| epoch   2 |  2000/ 2983 batches | lr 0.001000 | ms/batch 24.19 | loss  6.07 | ppl   431.06\n",
            "| epoch   2 |  2200/ 2983 batches | lr 0.001000 | ms/batch 24.18 | loss  5.95 | ppl   382.33\n",
            "| epoch   2 |  2400/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  5.93 | ppl   376.62\n",
            "| epoch   2 |  2600/ 2983 batches | lr 0.001000 | ms/batch 24.19 | loss  5.93 | ppl   375.68\n",
            "| epoch   2 |  2800/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  5.86 | ppl   349.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 74.03s | valid loss  6.18 | valid ppl   480.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   3 |   200/ 2983 batches | lr 0.001000 | ms/batch 24.26 | loss  5.84 | ppl   345.05\n",
            "| epoch   3 |   400/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  5.84 | ppl   343.75\n",
            "| epoch   3 |   600/ 2983 batches | lr 0.001000 | ms/batch 24.12 | loss  5.75 | ppl   314.75\n",
            "| epoch   3 |   800/ 2983 batches | lr 0.001000 | ms/batch 24.13 | loss  5.76 | ppl   317.84\n",
            "| epoch   3 |  1000/ 2983 batches | lr 0.001000 | ms/batch 24.12 | loss  5.73 | ppl   308.80\n",
            "| epoch   3 |  1200/ 2983 batches | lr 0.001000 | ms/batch 24.13 | loss  5.76 | ppl   318.34\n",
            "| epoch   3 |  1400/ 2983 batches | lr 0.001000 | ms/batch 24.13 | loss  5.76 | ppl   317.12\n",
            "| epoch   3 |  1600/ 2983 batches | lr 0.001000 | ms/batch 24.13 | loss  5.79 | ppl   325.50\n",
            "| epoch   3 |  1800/ 2983 batches | lr 0.001000 | ms/batch 24.13 | loss  5.65 | ppl   285.02\n",
            "| epoch   3 |  2000/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  5.68 | ppl   292.77\n",
            "| epoch   3 |  2200/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  5.57 | ppl   263.17\n",
            "| epoch   3 |  2400/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  5.61 | ppl   272.59\n",
            "| epoch   3 |  2600/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  5.61 | ppl   274.49\n",
            "| epoch   3 |  2800/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  5.54 | ppl   254.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 73.97s | valid loss  5.92 | valid ppl   371.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   4 |   200/ 2983 batches | lr 0.001000 | ms/batch 24.25 | loss  5.57 | ppl   261.87\n",
            "| epoch   4 |   400/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  5.57 | ppl   263.64\n",
            "| epoch   4 |   600/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  5.43 | ppl   228.85\n",
            "| epoch   4 |   800/ 2983 batches | lr 0.001000 | ms/batch 24.12 | loss  5.44 | ppl   231.39\n",
            "| epoch   4 |  1000/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  5.45 | ppl   232.51\n",
            "| epoch   4 |  1200/ 2983 batches | lr 0.001000 | ms/batch 24.12 | loss  5.47 | ppl   237.13\n",
            "| epoch   4 |  1400/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  5.49 | ppl   242.38\n",
            "| epoch   4 |  1600/ 2983 batches | lr 0.001000 | ms/batch 24.13 | loss  5.53 | ppl   252.89\n",
            "| epoch   4 |  1800/ 2983 batches | lr 0.001000 | ms/batch 24.13 | loss  5.41 | ppl   224.03\n",
            "| epoch   4 |  2000/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  5.43 | ppl   229.17\n",
            "| epoch   4 |  2200/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  5.33 | ppl   206.89\n",
            "| epoch   4 |  2400/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  5.35 | ppl   210.54\n",
            "| epoch   4 |  2600/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  5.37 | ppl   214.59\n",
            "| epoch   4 |  2800/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  5.31 | ppl   203.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 73.99s | valid loss  6.90 | valid ppl   989.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   5 |   200/ 2983 batches | lr 0.001000 | ms/batch 24.26 | loss  5.36 | ppl   213.77\n",
            "| epoch   5 |   400/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  5.36 | ppl   212.09\n",
            "| epoch   5 |   600/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  5.20 | ppl   181.45\n",
            "| epoch   5 |   800/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  5.21 | ppl   183.53\n",
            "| epoch   5 |  1000/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  5.22 | ppl   184.99\n",
            "| epoch   5 |  1200/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  5.25 | ppl   191.43\n",
            "| epoch   5 |  1400/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  5.28 | ppl   197.34\n",
            "| epoch   5 |  1600/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  5.33 | ppl   205.74\n",
            "| epoch   5 |  1800/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  5.21 | ppl   183.95\n",
            "| epoch   5 |  2000/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  5.24 | ppl   188.06\n",
            "| epoch   5 |  2200/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  5.16 | ppl   173.75\n",
            "| epoch   5 |  2400/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  5.18 | ppl   178.43\n",
            "| epoch   5 |  2600/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  5.21 | ppl   182.50\n",
            "| epoch   5 |  2800/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  5.14 | ppl   171.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 73.99s | valid loss  5.58 | valid ppl   264.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   6 |   200/ 2983 batches | lr 0.001000 | ms/batch 24.26 | loss  5.17 | ppl   176.48\n",
            "| epoch   6 |   400/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  5.20 | ppl   181.29\n",
            "| epoch   6 |   600/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  5.03 | ppl   152.71\n",
            "| epoch   6 |   800/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  5.06 | ppl   157.59\n",
            "| epoch   6 |  1000/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  5.07 | ppl   159.48\n",
            "| epoch   6 |  1200/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  5.08 | ppl   161.39\n",
            "| epoch   6 |  1400/ 2983 batches | lr 0.001000 | ms/batch 24.19 | loss  5.11 | ppl   166.20\n",
            "| epoch   6 |  1600/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  5.18 | ppl   177.39\n",
            "| epoch   6 |  1800/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  5.06 | ppl   157.48\n",
            "| epoch   6 |  2000/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  5.10 | ppl   163.44\n",
            "| epoch   6 |  2200/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  5.00 | ppl   148.90\n",
            "| epoch   6 |  2400/ 2983 batches | lr 0.001000 | ms/batch 24.19 | loss  5.03 | ppl   152.94\n",
            "| epoch   6 |  2600/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  5.06 | ppl   157.00\n",
            "| epoch   6 |  2800/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.99 | ppl   147.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 74.03s | valid loss  5.55 | valid ppl   258.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   7 |   200/ 2983 batches | lr 0.001000 | ms/batch 24.29 | loss  5.02 | ppl   151.11\n",
            "| epoch   7 |   400/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  5.06 | ppl   157.42\n",
            "| epoch   7 |   600/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  4.92 | ppl   136.32\n",
            "| epoch   7 |   800/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.93 | ppl   138.97\n",
            "| epoch   7 |  1000/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  4.96 | ppl   142.17\n",
            "| epoch   7 |  1200/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.97 | ppl   144.64\n",
            "| epoch   7 |  1400/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  5.00 | ppl   147.90\n",
            "| epoch   7 |  1600/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  5.06 | ppl   157.34\n",
            "| epoch   7 |  1800/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.95 | ppl   140.88\n",
            "| epoch   7 |  2000/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.97 | ppl   144.75\n",
            "| epoch   7 |  2200/ 2983 batches | lr 0.001000 | ms/batch 24.18 | loss  4.89 | ppl   133.59\n",
            "| epoch   7 |  2400/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.93 | ppl   138.50\n",
            "| epoch   7 |  2600/ 2983 batches | lr 0.001000 | ms/batch 24.18 | loss  4.95 | ppl   140.63\n",
            "| epoch   7 |  2800/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.89 | ppl   133.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 74.04s | valid loss  5.59 | valid ppl   268.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   8 |   200/ 2983 batches | lr 0.001000 | ms/batch 24.30 | loss  4.91 | ppl   135.25\n",
            "| epoch   8 |   400/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  4.95 | ppl   140.67\n",
            "| epoch   8 |   600/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  4.79 | ppl   119.72\n",
            "| epoch   8 |   800/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  4.81 | ppl   123.27\n",
            "| epoch   8 |  1000/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  4.85 | ppl   127.42\n",
            "| epoch   8 |  1200/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  4.86 | ppl   129.29\n",
            "| epoch   8 |  1400/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  4.90 | ppl   134.90\n",
            "| epoch   8 |  1600/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  4.96 | ppl   141.90\n",
            "| epoch   8 |  1800/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.86 | ppl   128.86\n",
            "| epoch   8 |  2000/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  4.89 | ppl   133.46\n",
            "| epoch   8 |  2200/ 2983 batches | lr 0.001000 | ms/batch 24.19 | loss  4.80 | ppl   120.95\n",
            "| epoch   8 |  2400/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.84 | ppl   126.03\n",
            "| epoch   8 |  2600/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  4.86 | ppl   128.53\n",
            "| epoch   8 |  2800/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.81 | ppl   122.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 74.02s | valid loss  5.55 | valid ppl   256.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   9 |   200/ 2983 batches | lr 0.001000 | ms/batch 24.27 | loss  4.83 | ppl   125.25\n",
            "| epoch   9 |   400/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.88 | ppl   131.43\n",
            "| epoch   9 |   600/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.73 | ppl   113.29\n",
            "| epoch   9 |   800/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  4.75 | ppl   115.43\n",
            "| epoch   9 |  1000/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  4.79 | ppl   119.99\n",
            "| epoch   9 |  1200/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.80 | ppl   121.58\n",
            "| epoch   9 |  1400/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  4.84 | ppl   126.29\n",
            "| epoch   9 |  1600/ 2983 batches | lr 0.001000 | ms/batch 24.18 | loss  4.89 | ppl   132.95\n",
            "| epoch   9 |  1800/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.80 | ppl   121.06\n",
            "| epoch   9 |  2000/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.83 | ppl   125.79\n",
            "| epoch   9 |  2200/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.75 | ppl   115.54\n",
            "| epoch   9 |  2400/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  4.77 | ppl   117.94\n",
            "| epoch   9 |  2600/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.80 | ppl   121.15\n",
            "| epoch   9 |  2800/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  4.74 | ppl   114.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 74.03s | valid loss  5.55 | valid ppl   257.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch  10 |   200/ 2983 batches | lr 0.001000 | ms/batch 24.27 | loss  4.77 | ppl   117.81\n",
            "| epoch  10 |   400/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  4.80 | ppl   121.96\n",
            "| epoch  10 |   600/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.66 | ppl   105.79\n",
            "| epoch  10 |   800/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.69 | ppl   108.87\n",
            "| epoch  10 |  1000/ 2983 batches | lr 0.001000 | ms/batch 24.19 | loss  4.72 | ppl   112.01\n",
            "| epoch  10 |  1200/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.73 | ppl   113.52\n",
            "| epoch  10 |  1400/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  4.79 | ppl   119.79\n",
            "| epoch  10 |  1600/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.82 | ppl   124.49\n",
            "| epoch  10 |  1800/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  4.75 | ppl   115.24\n",
            "| epoch  10 |  2000/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.79 | ppl   120.40\n",
            "| epoch  10 |  2200/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.68 | ppl   107.86\n",
            "| epoch  10 |  2400/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.73 | ppl   112.77\n",
            "| epoch  10 |  2600/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.75 | ppl   115.69\n",
            "| epoch  10 |  2800/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.68 | ppl   107.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 74.03s | valid loss  5.64 | valid ppl   280.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch  11 |   200/ 2983 batches | lr 0.001000 | ms/batch 24.26 | loss  4.69 | ppl   108.87\n",
            "| epoch  11 |   400/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.74 | ppl   114.86\n",
            "| epoch  11 |   600/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.61 | ppl   100.25\n",
            "| epoch  11 |   800/ 2983 batches | lr 0.001000 | ms/batch 24.13 | loss  4.64 | ppl   103.50\n",
            "| epoch  11 |  1000/ 2983 batches | lr 0.001000 | ms/batch 24.13 | loss  4.68 | ppl   108.07\n",
            "| epoch  11 |  1200/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  4.69 | ppl   108.37\n",
            "| epoch  11 |  1400/ 2983 batches | lr 0.001000 | ms/batch 24.13 | loss  4.75 | ppl   115.29\n",
            "| epoch  11 |  1600/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.78 | ppl   119.68\n",
            "| epoch  11 |  1800/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.71 | ppl   110.74\n",
            "| epoch  11 |  2000/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.75 | ppl   115.62\n",
            "| epoch  11 |  2200/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  4.63 | ppl   102.69\n",
            "| epoch  11 |  2400/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  4.67 | ppl   106.22\n",
            "| epoch  11 |  2600/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  4.69 | ppl   108.32\n",
            "| epoch  11 |  2800/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  4.64 | ppl   103.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 74.01s | valid loss  5.54 | valid ppl   253.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch  12 |   200/ 2983 batches | lr 0.001000 | ms/batch 24.27 | loss  4.63 | ppl   102.66\n",
            "| epoch  12 |   400/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.69 | ppl   109.25\n",
            "| epoch  12 |   600/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.55 | ppl    94.43\n",
            "| epoch  12 |   800/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  4.58 | ppl    97.97\n",
            "| epoch  12 |  1000/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.62 | ppl   101.38\n",
            "| epoch  12 |  1200/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.64 | ppl   103.39\n",
            "| epoch  12 |  1400/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.68 | ppl   107.92\n",
            "| epoch  12 |  1600/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.72 | ppl   111.85\n",
            "| epoch  12 |  1800/ 2983 batches | lr 0.001000 | ms/batch 24.18 | loss  4.65 | ppl   104.29\n",
            "| epoch  12 |  2000/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.69 | ppl   109.36\n",
            "| epoch  12 |  2200/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.59 | ppl    98.94\n",
            "| epoch  12 |  2400/ 2983 batches | lr 0.001000 | ms/batch 24.20 | loss  4.61 | ppl   100.05\n",
            "| epoch  12 |  2600/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.65 | ppl   104.80\n",
            "| epoch  12 |  2800/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.59 | ppl    98.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 74.06s | valid loss  5.65 | valid ppl   283.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch  13 |   200/ 2983 batches | lr 0.001000 | ms/batch 24.27 | loss  4.58 | ppl    97.72\n",
            "| epoch  13 |   400/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  4.64 | ppl   103.59\n",
            "| epoch  13 |   600/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  4.50 | ppl    90.14\n",
            "| epoch  13 |   800/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.56 | ppl    95.24\n",
            "| epoch  13 |  1000/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  4.60 | ppl    99.04\n",
            "| epoch  13 |  1200/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.61 | ppl   100.22\n",
            "| epoch  13 |  1400/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.65 | ppl   104.67\n",
            "| epoch  13 |  1600/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  4.69 | ppl   109.06\n",
            "| epoch  13 |  1800/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.62 | ppl   101.55\n",
            "| epoch  13 |  2000/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.66 | ppl   105.99\n",
            "| epoch  13 |  2200/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  4.55 | ppl    94.98\n",
            "| epoch  13 |  2400/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.59 | ppl    98.40\n",
            "| epoch  13 |  2600/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.63 | ppl   102.19\n",
            "| epoch  13 |  2800/ 2983 batches | lr 0.001000 | ms/batch 24.19 | loss  4.57 | ppl    96.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 74.04s | valid loss  5.61 | valid ppl   272.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch  14 |   200/ 2983 batches | lr 0.001000 | ms/batch 24.28 | loss  4.55 | ppl    94.52\n",
            "| epoch  14 |   400/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  4.60 | ppl    99.82\n",
            "| epoch  14 |   600/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.47 | ppl    86.95\n",
            "| epoch  14 |   800/ 2983 batches | lr 0.001000 | ms/batch 24.18 | loss  4.51 | ppl    91.25\n",
            "| epoch  14 |  1000/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.55 | ppl    94.73\n",
            "| epoch  14 |  1200/ 2983 batches | lr 0.001000 | ms/batch 24.18 | loss  4.57 | ppl    96.95\n",
            "| epoch  14 |  1400/ 2983 batches | lr 0.001000 | ms/batch 24.18 | loss  4.62 | ppl   101.69\n",
            "| epoch  14 |  1600/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.66 | ppl   105.74\n",
            "| epoch  14 |  1800/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.60 | ppl    99.70\n",
            "| epoch  14 |  2000/ 2983 batches | lr 0.001000 | ms/batch 24.15 | loss  4.64 | ppl   103.83\n",
            "| epoch  14 |  2200/ 2983 batches | lr 0.001000 | ms/batch 24.14 | loss  4.52 | ppl    92.08\n",
            "| epoch  14 |  2400/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.54 | ppl    94.00\n",
            "| epoch  14 |  2600/ 2983 batches | lr 0.001000 | ms/batch 24.16 | loss  4.61 | ppl   100.02\n",
            "| epoch  14 |  2800/ 2983 batches | lr 0.001000 | ms/batch 24.17 | loss  4.55 | ppl    94.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 74.03s | valid loss  5.57 | valid ppl   261.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch  15 |   200/ 2983 batches | lr 0.000500 | ms/batch 24.30 | loss  4.50 | ppl    90.43\n",
            "| epoch  15 |   400/ 2983 batches | lr 0.000500 | ms/batch 24.15 | loss  4.54 | ppl    93.39\n",
            "| epoch  15 |   600/ 2983 batches | lr 0.000500 | ms/batch 24.17 | loss  4.38 | ppl    79.97\n",
            "| epoch  15 |   800/ 2983 batches | lr 0.000500 | ms/batch 24.17 | loss  4.43 | ppl    83.98\n",
            "| epoch  15 |  1000/ 2983 batches | lr 0.000500 | ms/batch 24.15 | loss  4.46 | ppl    86.50\n",
            "| epoch  15 |  1200/ 2983 batches | lr 0.000500 | ms/batch 24.15 | loss  4.45 | ppl    85.56\n",
            "| epoch  15 |  1400/ 2983 batches | lr 0.000500 | ms/batch 24.15 | loss  4.48 | ppl    88.51\n",
            "| epoch  15 |  1600/ 2983 batches | lr 0.000500 | ms/batch 24.14 | loss  4.52 | ppl    91.61\n",
            "| epoch  15 |  1800/ 2983 batches | lr 0.000500 | ms/batch 24.16 | loss  4.44 | ppl    85.19\n",
            "| epoch  15 |  2000/ 2983 batches | lr 0.000500 | ms/batch 24.14 | loss  4.48 | ppl    88.18\n",
            "| epoch  15 |  2200/ 2983 batches | lr 0.000500 | ms/batch 24.16 | loss  4.35 | ppl    77.76\n",
            "| epoch  15 |  2400/ 2983 batches | lr 0.000500 | ms/batch 24.17 | loss  4.37 | ppl    78.94\n",
            "| epoch  15 |  2600/ 2983 batches | lr 0.000500 | ms/batch 24.18 | loss  4.41 | ppl    82.00\n",
            "| epoch  15 |  2800/ 2983 batches | lr 0.000500 | ms/batch 24.19 | loss  4.35 | ppl    77.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 74.04s | valid loss  5.47 | valid ppl   236.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch  16 |   200/ 2983 batches | lr 0.000500 | ms/batch 24.28 | loss  4.39 | ppl    80.59\n",
            "| epoch  16 |   400/ 2983 batches | lr 0.000500 | ms/batch 24.14 | loss  4.44 | ppl    84.85\n",
            "| epoch  16 |   600/ 2983 batches | lr 0.000500 | ms/batch 24.13 | loss  4.29 | ppl    73.27\n",
            "| epoch  16 |   800/ 2983 batches | lr 0.000500 | ms/batch 24.13 | loss  4.33 | ppl    76.29\n",
            "| epoch  16 |  1000/ 2983 batches | lr 0.000500 | ms/batch 24.13 | loss  4.37 | ppl    79.23\n",
            "| epoch  16 |  1200/ 2983 batches | lr 0.000500 | ms/batch 24.22 | loss  4.38 | ppl    79.88\n",
            "| epoch  16 |  1400/ 2983 batches | lr 0.000500 | ms/batch 24.21 | loss  4.44 | ppl    84.38\n",
            "| epoch  16 |  1600/ 2983 batches | lr 0.000500 | ms/batch 24.21 | loss  4.45 | ppl    85.93\n",
            "| epoch  16 |  1800/ 2983 batches | lr 0.000500 | ms/batch 24.20 | loss  4.39 | ppl    80.99\n",
            "| epoch  16 |  2000/ 2983 batches | lr 0.000500 | ms/batch 24.17 | loss  4.43 | ppl    84.06\n",
            "| epoch  16 |  2200/ 2983 batches | lr 0.000500 | ms/batch 24.20 | loss  4.30 | ppl    73.98\n",
            "| epoch  16 |  2400/ 2983 batches | lr 0.000500 | ms/batch 24.18 | loss  4.33 | ppl    76.19\n",
            "| epoch  16 |  2600/ 2983 batches | lr 0.000500 | ms/batch 24.21 | loss  4.36 | ppl    78.09\n",
            "| epoch  16 |  2800/ 2983 batches | lr 0.000500 | ms/batch 24.19 | loss  4.29 | ppl    73.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 74.09s | valid loss  5.50 | valid ppl   243.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch  17 |   200/ 2983 batches | lr 0.000500 | ms/batch 24.35 | loss  4.34 | ppl    76.38\n",
            "| epoch  17 |   400/ 2983 batches | lr 0.000500 | ms/batch 24.21 | loss  4.38 | ppl    79.58\n",
            "| epoch  17 |   600/ 2983 batches | lr 0.000500 | ms/batch 24.23 | loss  4.24 | ppl    69.73\n",
            "| epoch  17 |   800/ 2983 batches | lr 0.000500 | ms/batch 24.20 | loss  4.29 | ppl    73.20\n",
            "| epoch  17 |  1000/ 2983 batches | lr 0.000500 | ms/batch 24.22 | loss  4.33 | ppl    75.91\n",
            "| epoch  17 |  1200/ 2983 batches | lr 0.000500 | ms/batch 24.16 | loss  4.35 | ppl    77.29\n",
            "| epoch  17 |  1400/ 2983 batches | lr 0.000500 | ms/batch 24.15 | loss  4.37 | ppl    79.11\n",
            "| epoch  17 |  1600/ 2983 batches | lr 0.000500 | ms/batch 24.16 | loss  4.42 | ppl    82.86\n",
            "| epoch  17 |  1800/ 2983 batches | lr 0.000500 | ms/batch 24.14 | loss  4.36 | ppl    77.98\n",
            "| epoch  17 |  2000/ 2983 batches | lr 0.000500 | ms/batch 24.17 | loss  4.38 | ppl    80.03\n",
            "| epoch  17 |  2200/ 2983 batches | lr 0.000500 | ms/batch 24.16 | loss  4.26 | ppl    71.06\n",
            "| epoch  17 |  2400/ 2983 batches | lr 0.000500 | ms/batch 24.17 | loss  4.28 | ppl    72.42\n",
            "| epoch  17 |  2600/ 2983 batches | lr 0.000500 | ms/batch 24.17 | loss  4.32 | ppl    74.90\n",
            "| epoch  17 |  2800/ 2983 batches | lr 0.000500 | ms/batch 24.15 | loss  4.26 | ppl    71.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 74.09s | valid loss  5.41 | valid ppl   222.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch  18 |   200/ 2983 batches | lr 0.000500 | ms/batch 24.26 | loss  4.29 | ppl    73.14\n",
            "| epoch  18 |   400/ 2983 batches | lr 0.000500 | ms/batch 24.13 | loss  4.35 | ppl    77.16\n",
            "| epoch  18 |   600/ 2983 batches | lr 0.000500 | ms/batch 24.15 | loss  4.20 | ppl    66.63\n",
            "| epoch  18 |   800/ 2983 batches | lr 0.000500 | ms/batch 24.13 | loss  4.25 | ppl    70.12\n",
            "| epoch  18 |  1000/ 2983 batches | lr 0.000500 | ms/batch 24.17 | loss  4.29 | ppl    73.25\n",
            "| epoch  18 |  1200/ 2983 batches | lr 0.000500 | ms/batch 24.13 | loss  4.30 | ppl    73.59\n",
            "| epoch  18 |  1400/ 2983 batches | lr 0.000500 | ms/batch 24.15 | loss  4.33 | ppl    75.80\n",
            "| epoch  18 |  1600/ 2983 batches | lr 0.000500 | ms/batch 24.16 | loss  4.37 | ppl    78.84\n",
            "| epoch  18 |  1800/ 2983 batches | lr 0.000500 | ms/batch 24.14 | loss  4.31 | ppl    74.38\n",
            "| epoch  18 |  2000/ 2983 batches | lr 0.000500 | ms/batch 24.14 | loss  4.35 | ppl    77.67\n",
            "| epoch  18 |  2200/ 2983 batches | lr 0.000500 | ms/batch 24.14 | loss  4.23 | ppl    68.74\n",
            "| epoch  18 |  2400/ 2983 batches | lr 0.000500 | ms/batch 24.13 | loss  4.25 | ppl    69.88\n",
            "| epoch  18 |  2600/ 2983 batches | lr 0.000500 | ms/batch 24.13 | loss  4.28 | ppl    72.27\n",
            "| epoch  18 |  2800/ 2983 batches | lr 0.000500 | ms/batch 24.14 | loss  4.23 | ppl    68.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 73.98s | valid loss  5.41 | valid ppl   223.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch  19 |   200/ 2983 batches | lr 0.000500 | ms/batch 24.26 | loss  4.24 | ppl    69.71\n",
            "| epoch  19 |   400/ 2983 batches | lr 0.000500 | ms/batch 24.13 | loss  4.30 | ppl    73.68\n",
            "| epoch  19 |   600/ 2983 batches | lr 0.000500 | ms/batch 24.15 | loss  4.17 | ppl    64.59\n",
            "| epoch  19 |   800/ 2983 batches | lr 0.000500 | ms/batch 24.15 | loss  4.21 | ppl    67.36\n",
            "| epoch  19 |  1000/ 2983 batches | lr 0.000500 | ms/batch 24.16 | loss  4.26 | ppl    70.72\n",
            "| epoch  19 |  1200/ 2983 batches | lr 0.000500 | ms/batch 24.14 | loss  4.26 | ppl    71.15\n",
            "| epoch  19 |  1400/ 2983 batches | lr 0.000500 | ms/batch 24.13 | loss  4.29 | ppl    73.27\n",
            "| epoch  19 |  1600/ 2983 batches | lr 0.000500 | ms/batch 24.15 | loss  4.33 | ppl    75.91\n",
            "| epoch  19 |  1800/ 2983 batches | lr 0.000500 | ms/batch 24.14 | loss  4.28 | ppl    72.31\n",
            "| epoch  19 |  2000/ 2983 batches | lr 0.000500 | ms/batch 24.14 | loss  4.31 | ppl    74.76\n",
            "| epoch  19 |  2200/ 2983 batches | lr 0.000500 | ms/batch 24.15 | loss  4.20 | ppl    66.66\n",
            "| epoch  19 |  2400/ 2983 batches | lr 0.000500 | ms/batch 24.19 | loss  4.22 | ppl    67.98\n",
            "| epoch  19 |  2600/ 2983 batches | lr 0.000500 | ms/batch 24.15 | loss  4.25 | ppl    70.32\n",
            "| epoch  19 |  2800/ 2983 batches | lr 0.000500 | ms/batch 24.15 | loss  4.20 | ppl    66.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 73.99s | valid loss  5.43 | valid ppl   229.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch  20 |   200/ 2983 batches | lr 0.000500 | ms/batch 24.27 | loss  4.20 | ppl    66.80\n",
            "| epoch  20 |   400/ 2983 batches | lr 0.000500 | ms/batch 24.15 | loss  4.26 | ppl    70.59\n",
            "| epoch  20 |   600/ 2983 batches | lr 0.000500 | ms/batch 24.13 | loss  4.12 | ppl    61.31\n",
            "| epoch  20 |   800/ 2983 batches | lr 0.000500 | ms/batch 24.14 | loss  4.17 | ppl    64.94\n",
            "| epoch  20 |  1000/ 2983 batches | lr 0.000500 | ms/batch 24.14 | loss  4.23 | ppl    68.44\n",
            "| epoch  20 |  1200/ 2983 batches | lr 0.000500 | ms/batch 24.13 | loss  4.23 | ppl    68.92\n",
            "| epoch  20 |  1400/ 2983 batches | lr 0.000500 | ms/batch 24.16 | loss  4.27 | ppl    71.70\n",
            "| epoch  20 |  1600/ 2983 batches | lr 0.000500 | ms/batch 24.14 | loss  4.30 | ppl    73.72\n",
            "| epoch  20 |  1800/ 2983 batches | lr 0.000500 | ms/batch 24.14 | loss  4.25 | ppl    70.25\n",
            "| epoch  20 |  2000/ 2983 batches | lr 0.000500 | ms/batch 24.17 | loss  4.29 | ppl    72.85\n",
            "| epoch  20 |  2200/ 2983 batches | lr 0.000500 | ms/batch 24.15 | loss  4.17 | ppl    64.84\n",
            "| epoch  20 |  2400/ 2983 batches | lr 0.000500 | ms/batch 24.16 | loss  4.20 | ppl    66.39\n",
            "| epoch  20 |  2600/ 2983 batches | lr 0.000500 | ms/batch 24.14 | loss  4.23 | ppl    68.78\n",
            "| epoch  20 |  2800/ 2983 batches | lr 0.000500 | ms/batch 24.15 | loss  4.19 | ppl    65.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 73.99s | valid loss  5.43 | valid ppl   227.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  21 |   200/ 2983 batches | lr 0.000250 | ms/batch 24.26 | loss  4.20 | ppl    66.69\n",
            "| epoch  21 |   400/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.24 | ppl    69.33\n",
            "| epoch  21 |   600/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.09 | ppl    59.70\n",
            "| epoch  21 |   800/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.14 | ppl    62.75\n",
            "| epoch  21 |  1000/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.18 | ppl    65.43\n",
            "| epoch  21 |  1200/ 2983 batches | lr 0.000250 | ms/batch 24.13 | loss  4.17 | ppl    64.75\n",
            "| epoch  21 |  1400/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.21 | ppl    67.38\n",
            "| epoch  21 |  1600/ 2983 batches | lr 0.000250 | ms/batch 24.13 | loss  4.24 | ppl    69.20\n",
            "| epoch  21 |  1800/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.18 | ppl    65.44\n",
            "| epoch  21 |  2000/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.21 | ppl    67.26\n",
            "| epoch  21 |  2200/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.09 | ppl    59.54\n",
            "| epoch  21 |  2400/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.11 | ppl    60.67\n",
            "| epoch  21 |  2600/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.12 | ppl    61.63\n",
            "| epoch  21 |  2800/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  4.08 | ppl    58.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 73.98s | valid loss  5.33 | valid ppl   207.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  22 |   200/ 2983 batches | lr 0.000250 | ms/batch 24.25 | loss  4.14 | ppl    62.84\n",
            "| epoch  22 |   400/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.18 | ppl    65.42\n",
            "| epoch  22 |   600/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.03 | ppl    56.17\n",
            "| epoch  22 |   800/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  4.08 | ppl    59.15\n",
            "| epoch  22 |  1000/ 2983 batches | lr 0.000250 | ms/batch 24.13 | loss  4.13 | ppl    62.43\n",
            "| epoch  22 |  1200/ 2983 batches | lr 0.000250 | ms/batch 24.13 | loss  4.12 | ppl    61.73\n",
            "| epoch  22 |  1400/ 2983 batches | lr 0.000250 | ms/batch 24.20 | loss  4.17 | ppl    64.40\n",
            "| epoch  22 |  1600/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  4.20 | ppl    66.77\n",
            "| epoch  22 |  1800/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.15 | ppl    63.41\n",
            "| epoch  22 |  2000/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.18 | ppl    65.24\n",
            "| epoch  22 |  2200/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.05 | ppl    57.33\n",
            "| epoch  22 |  2400/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.07 | ppl    58.51\n",
            "| epoch  22 |  2600/ 2983 batches | lr 0.000250 | ms/batch 24.13 | loss  4.10 | ppl    60.42\n",
            "| epoch  22 |  2800/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  4.06 | ppl    57.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 74.00s | valid loss  5.33 | valid ppl   206.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  23 |   200/ 2983 batches | lr 0.000250 | ms/batch 24.28 | loss  4.10 | ppl    60.52\n",
            "| epoch  23 |   400/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.16 | ppl    63.77\n",
            "| epoch  23 |   600/ 2983 batches | lr 0.000250 | ms/batch 24.13 | loss  4.00 | ppl    54.78\n",
            "| epoch  23 |   800/ 2983 batches | lr 0.000250 | ms/batch 24.17 | loss  4.05 | ppl    57.66\n",
            "| epoch  23 |  1000/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.11 | ppl    60.67\n",
            "| epoch  23 |  1200/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  4.11 | ppl    60.69\n",
            "| epoch  23 |  1400/ 2983 batches | lr 0.000250 | ms/batch 24.13 | loss  4.14 | ppl    62.70\n",
            "| epoch  23 |  1600/ 2983 batches | lr 0.000250 | ms/batch 24.13 | loss  4.17 | ppl    64.87\n",
            "| epoch  23 |  1800/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  4.12 | ppl    61.76\n",
            "| epoch  23 |  2000/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.16 | ppl    63.95\n",
            "| epoch  23 |  2200/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.04 | ppl    56.89\n",
            "| epoch  23 |  2400/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.06 | ppl    58.06\n",
            "| epoch  23 |  2600/ 2983 batches | lr 0.000250 | ms/batch 24.13 | loss  4.08 | ppl    58.96\n",
            "| epoch  23 |  2800/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  4.03 | ppl    56.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 73.99s | valid loss  5.31 | valid ppl   203.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  24 |   200/ 2983 batches | lr 0.000250 | ms/batch 24.28 | loss  4.09 | ppl    59.45\n",
            "| epoch  24 |   400/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  4.13 | ppl    62.10\n",
            "| epoch  24 |   600/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  3.99 | ppl    54.02\n",
            "| epoch  24 |   800/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.04 | ppl    56.99\n",
            "| epoch  24 |  1000/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.09 | ppl    59.95\n",
            "| epoch  24 |  1200/ 2983 batches | lr 0.000250 | ms/batch 24.18 | loss  4.08 | ppl    59.26\n",
            "| epoch  24 |  1400/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.12 | ppl    61.54\n",
            "| epoch  24 |  1600/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.15 | ppl    63.38\n",
            "| epoch  24 |  1800/ 2983 batches | lr 0.000250 | ms/batch 24.17 | loss  4.10 | ppl    60.50\n",
            "| epoch  24 |  2000/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.13 | ppl    62.34\n",
            "| epoch  24 |  2200/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.01 | ppl    55.34\n",
            "| epoch  24 |  2400/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.04 | ppl    56.85\n",
            "| epoch  24 |  2600/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.06 | ppl    58.04\n",
            "| epoch  24 |  2800/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  4.02 | ppl    55.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 74.02s | valid loss  5.32 | valid ppl   204.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  25 |   200/ 2983 batches | lr 0.000250 | ms/batch 24.27 | loss  4.05 | ppl    57.61\n",
            "| epoch  25 |   400/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.10 | ppl    60.62\n",
            "| epoch  25 |   600/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  3.96 | ppl    52.39\n",
            "| epoch  25 |   800/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.01 | ppl    55.36\n",
            "| epoch  25 |  1000/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.07 | ppl    58.29\n",
            "| epoch  25 |  1200/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  4.05 | ppl    57.63\n",
            "| epoch  25 |  1400/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  4.10 | ppl    60.22\n",
            "| epoch  25 |  1600/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.13 | ppl    61.94\n",
            "| epoch  25 |  1800/ 2983 batches | lr 0.000250 | ms/batch 24.13 | loss  4.09 | ppl    59.52\n",
            "| epoch  25 |  2000/ 2983 batches | lr 0.000250 | ms/batch 24.13 | loss  4.12 | ppl    61.31\n",
            "| epoch  25 |  2200/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  3.99 | ppl    54.11\n",
            "| epoch  25 |  2400/ 2983 batches | lr 0.000250 | ms/batch 24.13 | loss  4.03 | ppl    56.03\n",
            "| epoch  25 |  2600/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.05 | ppl    57.45\n",
            "| epoch  25 |  2800/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.01 | ppl    55.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 73.99s | valid loss  5.32 | valid ppl   204.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  26 |   200/ 2983 batches | lr 0.000250 | ms/batch 24.27 | loss  4.02 | ppl    55.98\n",
            "| epoch  26 |   400/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.08 | ppl    59.31\n",
            "| epoch  26 |   600/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  3.95 | ppl    51.68\n",
            "| epoch  26 |   800/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  3.99 | ppl    54.05\n",
            "| epoch  26 |  1000/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.04 | ppl    56.85\n",
            "| epoch  26 |  1200/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.04 | ppl    57.10\n",
            "| epoch  26 |  1400/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.07 | ppl    58.72\n",
            "| epoch  26 |  1600/ 2983 batches | lr 0.000250 | ms/batch 24.18 | loss  4.11 | ppl    60.88\n",
            "| epoch  26 |  1800/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.06 | ppl    57.88\n",
            "| epoch  26 |  2000/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  4.10 | ppl    60.14\n",
            "| epoch  26 |  2200/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  3.98 | ppl    53.74\n",
            "| epoch  26 |  2400/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.01 | ppl    54.98\n",
            "| epoch  26 |  2600/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  4.03 | ppl    56.00\n",
            "| epoch  26 |  2800/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  3.99 | ppl    54.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 74.00s | valid loss  5.29 | valid ppl   199.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  27 |   200/ 2983 batches | lr 0.000250 | ms/batch 24.27 | loss  4.01 | ppl    54.90\n",
            "| epoch  27 |   400/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.07 | ppl    58.32\n",
            "| epoch  27 |   600/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  3.92 | ppl    50.48\n",
            "| epoch  27 |   800/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  3.97 | ppl    53.17\n",
            "| epoch  27 |  1000/ 2983 batches | lr 0.000250 | ms/batch 24.18 | loss  4.03 | ppl    56.15\n",
            "| epoch  27 |  1200/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.02 | ppl    55.94\n",
            "| epoch  27 |  1400/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.07 | ppl    58.34\n",
            "| epoch  27 |  1600/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.09 | ppl    59.85\n",
            "| epoch  27 |  1800/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.05 | ppl    57.31\n",
            "| epoch  27 |  2000/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  4.09 | ppl    59.50\n",
            "| epoch  27 |  2200/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  3.96 | ppl    52.32\n",
            "| epoch  27 |  2400/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  3.98 | ppl    53.77\n",
            "| epoch  27 |  2600/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  4.01 | ppl    55.35\n",
            "| epoch  27 |  2800/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  3.98 | ppl    53.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 74.00s | valid loss  5.30 | valid ppl   199.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  28 |   200/ 2983 batches | lr 0.000250 | ms/batch 24.26 | loss  4.00 | ppl    54.38\n",
            "| epoch  28 |   400/ 2983 batches | lr 0.000250 | ms/batch 24.18 | loss  4.06 | ppl    57.85\n",
            "| epoch  28 |   600/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  3.91 | ppl    49.74\n",
            "| epoch  28 |   800/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  3.96 | ppl    52.39\n",
            "| epoch  28 |  1000/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  4.01 | ppl    55.20\n",
            "| epoch  28 |  1200/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.01 | ppl    55.14\n",
            "| epoch  28 |  1400/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  4.05 | ppl    57.12\n",
            "| epoch  28 |  1600/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.08 | ppl    59.18\n",
            "| epoch  28 |  1800/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.03 | ppl    56.53\n",
            "| epoch  28 |  2000/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.07 | ppl    58.46\n",
            "| epoch  28 |  2200/ 2983 batches | lr 0.000250 | ms/batch 24.13 | loss  3.95 | ppl    51.71\n",
            "| epoch  28 |  2400/ 2983 batches | lr 0.000250 | ms/batch 24.17 | loss  3.98 | ppl    53.26\n",
            "| epoch  28 |  2600/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.00 | ppl    54.76\n",
            "| epoch  28 |  2800/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  3.96 | ppl    52.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 74.01s | valid loss  5.30 | valid ppl   200.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  29 |   200/ 2983 batches | lr 0.000250 | ms/batch 24.26 | loss  3.97 | ppl    53.06\n",
            "| epoch  29 |   400/ 2983 batches | lr 0.000250 | ms/batch 24.18 | loss  4.02 | ppl    55.85\n",
            "| epoch  29 |   600/ 2983 batches | lr 0.000250 | ms/batch 24.13 | loss  3.89 | ppl    48.93\n",
            "| epoch  29 |   800/ 2983 batches | lr 0.000250 | ms/batch 24.13 | loss  3.94 | ppl    51.32\n",
            "| epoch  29 |  1000/ 2983 batches | lr 0.000250 | ms/batch 24.13 | loss  3.99 | ppl    54.27\n",
            "| epoch  29 |  1200/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.00 | ppl    54.55\n",
            "| epoch  29 |  1400/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.03 | ppl    56.48\n",
            "| epoch  29 |  1600/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.07 | ppl    58.39\n",
            "| epoch  29 |  1800/ 2983 batches | lr 0.000250 | ms/batch 24.15 | loss  4.02 | ppl    55.65\n",
            "| epoch  29 |  2000/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  4.05 | ppl    57.49\n",
            "| epoch  29 |  2200/ 2983 batches | lr 0.000250 | ms/batch 24.13 | loss  3.93 | ppl    50.78\n",
            "| epoch  29 |  2400/ 2983 batches | lr 0.000250 | ms/batch 24.19 | loss  3.95 | ppl    52.07\n",
            "| epoch  29 |  2600/ 2983 batches | lr 0.000250 | ms/batch 24.14 | loss  3.98 | ppl    53.73\n",
            "| epoch  29 |  2800/ 2983 batches | lr 0.000250 | ms/batch 24.16 | loss  3.96 | ppl    52.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 73.98s | valid loss  5.30 | valid ppl   199.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  30 |   200/ 2983 batches | lr 0.000125 | ms/batch 24.24 | loss  3.97 | ppl    53.23\n",
            "| epoch  30 |   400/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  4.03 | ppl    56.03\n",
            "| epoch  30 |   600/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.89 | ppl    49.14\n",
            "| epoch  30 |   800/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.93 | ppl    50.84\n",
            "| epoch  30 |  1000/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.98 | ppl    53.35\n",
            "| epoch  30 |  1200/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.97 | ppl    53.08\n",
            "| epoch  30 |  1400/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  4.01 | ppl    54.98\n",
            "| epoch  30 |  1600/ 2983 batches | lr 0.000125 | ms/batch 24.13 | loss  4.03 | ppl    56.53\n",
            "| epoch  30 |  1800/ 2983 batches | lr 0.000125 | ms/batch 24.18 | loss  3.99 | ppl    54.16\n",
            "| epoch  30 |  2000/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  4.02 | ppl    55.97\n",
            "| epoch  30 |  2200/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.90 | ppl    49.30\n",
            "| epoch  30 |  2400/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.92 | ppl    50.57\n",
            "| epoch  30 |  2600/ 2983 batches | lr 0.000125 | ms/batch 24.13 | loss  3.95 | ppl    51.71\n",
            "| epoch  30 |  2800/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.90 | ppl    49.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 73.99s | valid loss  5.27 | valid ppl   194.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  31 |   200/ 2983 batches | lr 0.000125 | ms/batch 24.27 | loss  3.95 | ppl    51.78\n",
            "| epoch  31 |   400/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  4.01 | ppl    54.98\n",
            "| epoch  31 |   600/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.86 | ppl    47.32\n",
            "| epoch  31 |   800/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.91 | ppl    49.86\n",
            "| epoch  31 |  1000/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.96 | ppl    52.72\n",
            "| epoch  31 |  1200/ 2983 batches | lr 0.000125 | ms/batch 24.17 | loss  3.95 | ppl    52.16\n",
            "| epoch  31 |  1400/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.99 | ppl    53.80\n",
            "| epoch  31 |  1600/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  4.02 | ppl    55.43\n",
            "| epoch  31 |  1800/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.97 | ppl    53.08\n",
            "| epoch  31 |  2000/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  4.00 | ppl    54.59\n",
            "| epoch  31 |  2200/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.88 | ppl    48.23\n",
            "| epoch  31 |  2400/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.90 | ppl    49.31\n",
            "| epoch  31 |  2600/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.92 | ppl    50.47\n",
            "| epoch  31 |  2800/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.89 | ppl    48.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 73.99s | valid loss  5.26 | valid ppl   191.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  32 |   200/ 2983 batches | lr 0.000125 | ms/batch 24.25 | loss  3.93 | ppl    51.11\n",
            "| epoch  32 |   400/ 2983 batches | lr 0.000125 | ms/batch 24.13 | loss  3.99 | ppl    53.98\n",
            "| epoch  32 |   600/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.85 | ppl    46.77\n",
            "| epoch  32 |   800/ 2983 batches | lr 0.000125 | ms/batch 24.13 | loss  3.89 | ppl    49.12\n",
            "| epoch  32 |  1000/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.95 | ppl    52.02\n",
            "| epoch  32 |  1200/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.94 | ppl    51.42\n",
            "| epoch  32 |  1400/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.97 | ppl    53.24\n",
            "| epoch  32 |  1600/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  4.00 | ppl    54.60\n",
            "| epoch  32 |  1800/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.96 | ppl    52.26\n",
            "| epoch  32 |  2000/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.99 | ppl    54.07\n",
            "| epoch  32 |  2200/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.87 | ppl    47.96\n",
            "| epoch  32 |  2400/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.89 | ppl    49.10\n",
            "| epoch  32 |  2600/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.91 | ppl    50.04\n",
            "| epoch  32 |  2800/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.89 | ppl    48.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 73.97s | valid loss  5.25 | valid ppl   190.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  33 |   200/ 2983 batches | lr 0.000125 | ms/batch 24.27 | loss  3.92 | ppl    50.63\n",
            "| epoch  33 |   400/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.98 | ppl    53.60\n",
            "| epoch  33 |   600/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.84 | ppl    46.43\n",
            "| epoch  33 |   800/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.90 | ppl    49.27\n",
            "| epoch  33 |  1000/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.94 | ppl    51.25\n",
            "| epoch  33 |  1200/ 2983 batches | lr 0.000125 | ms/batch 24.17 | loss  3.93 | ppl    50.92\n",
            "| epoch  33 |  1400/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.96 | ppl    52.63\n",
            "| epoch  33 |  1600/ 2983 batches | lr 0.000125 | ms/batch 24.17 | loss  3.98 | ppl    53.60\n",
            "| epoch  33 |  1800/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.95 | ppl    51.72\n",
            "| epoch  33 |  2000/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.98 | ppl    53.29\n",
            "| epoch  33 |  2200/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.85 | ppl    47.13\n",
            "| epoch  33 |  2400/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.88 | ppl    48.57\n",
            "| epoch  33 |  2600/ 2983 batches | lr 0.000125 | ms/batch 24.17 | loss  3.91 | ppl    49.75\n",
            "| epoch  33 |  2800/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.87 | ppl    47.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 74.02s | valid loss  5.25 | valid ppl   190.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  34 |   200/ 2983 batches | lr 0.000125 | ms/batch 24.26 | loss  3.90 | ppl    49.52\n",
            "| epoch  34 |   400/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.96 | ppl    52.48\n",
            "| epoch  34 |   600/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.81 | ppl    45.04\n",
            "| epoch  34 |   800/ 2983 batches | lr 0.000125 | ms/batch 24.13 | loss  3.87 | ppl    48.09\n",
            "| epoch  34 |  1000/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.92 | ppl    50.58\n",
            "| epoch  34 |  1200/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.92 | ppl    50.61\n",
            "| epoch  34 |  1400/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.95 | ppl    52.06\n",
            "| epoch  34 |  1600/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.98 | ppl    53.75\n",
            "| epoch  34 |  1800/ 2983 batches | lr 0.000125 | ms/batch 24.13 | loss  3.94 | ppl    51.62\n",
            "| epoch  34 |  2000/ 2983 batches | lr 0.000125 | ms/batch 24.17 | loss  3.98 | ppl    53.45\n",
            "| epoch  34 |  2200/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.85 | ppl    46.95\n",
            "| epoch  34 |  2400/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.88 | ppl    48.40\n",
            "| epoch  34 |  2600/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.91 | ppl    49.90\n",
            "| epoch  34 |  2800/ 2983 batches | lr 0.000125 | ms/batch 24.13 | loss  3.86 | ppl    47.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 73.99s | valid loss  5.24 | valid ppl   189.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  35 |   200/ 2983 batches | lr 0.000125 | ms/batch 24.27 | loss  3.90 | ppl    49.34\n",
            "| epoch  35 |   400/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.96 | ppl    52.40\n",
            "| epoch  35 |   600/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.81 | ppl    45.23\n",
            "| epoch  35 |   800/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.87 | ppl    47.85\n",
            "| epoch  35 |  1000/ 2983 batches | lr 0.000125 | ms/batch 24.19 | loss  3.92 | ppl    50.50\n",
            "| epoch  35 |  1200/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.92 | ppl    50.19\n",
            "| epoch  35 |  1400/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.94 | ppl    51.58\n",
            "| epoch  35 |  1600/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.97 | ppl    52.87\n",
            "| epoch  35 |  1800/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.93 | ppl    50.77\n",
            "| epoch  35 |  2000/ 2983 batches | lr 0.000125 | ms/batch 24.18 | loss  3.97 | ppl    52.90\n",
            "| epoch  35 |  2200/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.84 | ppl    46.44\n",
            "| epoch  35 |  2400/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.86 | ppl    47.57\n",
            "| epoch  35 |  2600/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.90 | ppl    49.41\n",
            "| epoch  35 |  2800/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.86 | ppl    47.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 74.04s | valid loss  5.24 | valid ppl   188.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  36 |   200/ 2983 batches | lr 0.000125 | ms/batch 24.26 | loss  3.89 | ppl    48.79\n",
            "| epoch  36 |   400/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.95 | ppl    51.91\n",
            "| epoch  36 |   600/ 2983 batches | lr 0.000125 | ms/batch 24.13 | loss  3.80 | ppl    44.76\n",
            "| epoch  36 |   800/ 2983 batches | lr 0.000125 | ms/batch 24.17 | loss  3.86 | ppl    47.62\n",
            "| epoch  36 |  1000/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.91 | ppl    49.81\n",
            "| epoch  36 |  1200/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.91 | ppl    50.00\n",
            "| epoch  36 |  1400/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.94 | ppl    51.51\n",
            "| epoch  36 |  1600/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.96 | ppl    52.63\n",
            "| epoch  36 |  1800/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.92 | ppl    50.31\n",
            "| epoch  36 |  2000/ 2983 batches | lr 0.000125 | ms/batch 24.17 | loss  3.95 | ppl    52.16\n",
            "| epoch  36 |  2200/ 2983 batches | lr 0.000125 | ms/batch 24.13 | loss  3.84 | ppl    46.50\n",
            "| epoch  36 |  2400/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.86 | ppl    47.46\n",
            "| epoch  36 |  2600/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.90 | ppl    49.54\n",
            "| epoch  36 |  2800/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.85 | ppl    46.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 73.99s | valid loss  5.24 | valid ppl   188.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  37 |   200/ 2983 batches | lr 0.000125 | ms/batch 24.28 | loss  3.87 | ppl    47.90\n",
            "| epoch  37 |   400/ 2983 batches | lr 0.000125 | ms/batch 24.13 | loss  3.93 | ppl    51.13\n",
            "| epoch  37 |   600/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.79 | ppl    44.21\n",
            "| epoch  37 |   800/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.85 | ppl    47.17\n",
            "| epoch  37 |  1000/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.90 | ppl    49.35\n",
            "| epoch  37 |  1200/ 2983 batches | lr 0.000125 | ms/batch 24.18 | loss  3.90 | ppl    49.22\n",
            "| epoch  37 |  1400/ 2983 batches | lr 0.000125 | ms/batch 24.20 | loss  3.93 | ppl    50.97\n",
            "| epoch  37 |  1600/ 2983 batches | lr 0.000125 | ms/batch 24.20 | loss  3.97 | ppl    52.77\n",
            "| epoch  37 |  1800/ 2983 batches | lr 0.000125 | ms/batch 24.21 | loss  3.91 | ppl    50.09\n",
            "| epoch  37 |  2000/ 2983 batches | lr 0.000125 | ms/batch 24.19 | loss  3.94 | ppl    51.65\n",
            "| epoch  37 |  2200/ 2983 batches | lr 0.000125 | ms/batch 24.18 | loss  3.83 | ppl    46.24\n",
            "| epoch  37 |  2400/ 2983 batches | lr 0.000125 | ms/batch 24.21 | loss  3.86 | ppl    47.31\n",
            "| epoch  37 |  2600/ 2983 batches | lr 0.000125 | ms/batch 24.22 | loss  3.89 | ppl    48.88\n",
            "| epoch  37 |  2800/ 2983 batches | lr 0.000125 | ms/batch 24.20 | loss  3.85 | ppl    47.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 74.10s | valid loss  5.23 | valid ppl   187.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  38 |   200/ 2983 batches | lr 0.000125 | ms/batch 24.37 | loss  3.87 | ppl    47.81\n",
            "| epoch  38 |   400/ 2983 batches | lr 0.000125 | ms/batch 24.21 | loss  3.93 | ppl    50.78\n",
            "| epoch  38 |   600/ 2983 batches | lr 0.000125 | ms/batch 24.22 | loss  3.79 | ppl    44.26\n",
            "| epoch  38 |   800/ 2983 batches | lr 0.000125 | ms/batch 24.24 | loss  3.84 | ppl    46.63\n",
            "| epoch  38 |  1000/ 2983 batches | lr 0.000125 | ms/batch 24.22 | loss  3.89 | ppl    48.96\n",
            "| epoch  38 |  1200/ 2983 batches | lr 0.000125 | ms/batch 24.18 | loss  3.89 | ppl    48.82\n",
            "| epoch  38 |  1400/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.92 | ppl    50.44\n",
            "| epoch  38 |  1600/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.95 | ppl    51.70\n",
            "| epoch  38 |  1800/ 2983 batches | lr 0.000125 | ms/batch 24.17 | loss  3.91 | ppl    49.69\n",
            "| epoch  38 |  2000/ 2983 batches | lr 0.000125 | ms/batch 24.18 | loss  3.95 | ppl    51.92\n",
            "| epoch  38 |  2200/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.82 | ppl    45.75\n",
            "| epoch  38 |  2400/ 2983 batches | lr 0.000125 | ms/batch 24.17 | loss  3.84 | ppl    46.63\n",
            "| epoch  38 |  2600/ 2983 batches | lr 0.000125 | ms/batch 24.18 | loss  3.87 | ppl    48.09\n",
            "| epoch  38 |  2800/ 2983 batches | lr 0.000125 | ms/batch 24.20 | loss  3.84 | ppl    46.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 74.12s | valid loss  5.24 | valid ppl   188.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  39 |   200/ 2983 batches | lr 0.000125 | ms/batch 24.32 | loss  3.85 | ppl    47.08\n",
            "| epoch  39 |   400/ 2983 batches | lr 0.000125 | ms/batch 24.18 | loss  3.92 | ppl    50.18\n",
            "| epoch  39 |   600/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.77 | ppl    43.42\n",
            "| epoch  39 |   800/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.84 | ppl    46.41\n",
            "| epoch  39 |  1000/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.88 | ppl    48.40\n",
            "| epoch  39 |  1200/ 2983 batches | lr 0.000125 | ms/batch 24.18 | loss  3.88 | ppl    48.42\n",
            "| epoch  39 |  1400/ 2983 batches | lr 0.000125 | ms/batch 24.18 | loss  3.91 | ppl    49.81\n",
            "| epoch  39 |  1600/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.95 | ppl    51.72\n",
            "| epoch  39 |  1800/ 2983 batches | lr 0.000125 | ms/batch 24.17 | loss  3.90 | ppl    49.44\n",
            "| epoch  39 |  2000/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.93 | ppl    51.12\n",
            "| epoch  39 |  2200/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.82 | ppl    45.42\n",
            "| epoch  39 |  2400/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.84 | ppl    46.40\n",
            "| epoch  39 |  2600/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.87 | ppl    47.86\n",
            "| epoch  39 |  2800/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.83 | ppl    45.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 74.04s | valid loss  5.23 | valid ppl   187.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  40 |   200/ 2983 batches | lr 0.000125 | ms/batch 24.27 | loss  3.85 | ppl    46.81\n",
            "| epoch  40 |   400/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.90 | ppl    49.60\n",
            "| epoch  40 |   600/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.76 | ppl    42.99\n",
            "| epoch  40 |   800/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.82 | ppl    45.63\n",
            "| epoch  40 |  1000/ 2983 batches | lr 0.000125 | ms/batch 24.13 | loss  3.87 | ppl    48.13\n",
            "| epoch  40 |  1200/ 2983 batches | lr 0.000125 | ms/batch 24.17 | loss  3.88 | ppl    48.39\n",
            "| epoch  40 |  1400/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.91 | ppl    49.85\n",
            "| epoch  40 |  1600/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.93 | ppl    50.96\n",
            "| epoch  40 |  1800/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.89 | ppl    48.98\n",
            "| epoch  40 |  2000/ 2983 batches | lr 0.000125 | ms/batch 24.13 | loss  3.93 | ppl    50.66\n",
            "| epoch  40 |  2200/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.81 | ppl    45.10\n",
            "| epoch  40 |  2400/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.82 | ppl    45.81\n",
            "| epoch  40 |  2600/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.86 | ppl    47.45\n",
            "| epoch  40 |  2800/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.83 | ppl    45.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 74.00s | valid loss  5.23 | valid ppl   187.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  41 |   200/ 2983 batches | lr 0.000125 | ms/batch 24.29 | loss  3.84 | ppl    46.43\n",
            "| epoch  41 |   400/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.92 | ppl    50.23\n",
            "| epoch  41 |   600/ 2983 batches | lr 0.000125 | ms/batch 24.17 | loss  3.76 | ppl    43.11\n",
            "| epoch  41 |   800/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.82 | ppl    45.47\n",
            "| epoch  41 |  1000/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.87 | ppl    47.71\n",
            "| epoch  41 |  1200/ 2983 batches | lr 0.000125 | ms/batch 24.18 | loss  3.87 | ppl    48.06\n",
            "| epoch  41 |  1400/ 2983 batches | lr 0.000125 | ms/batch 24.17 | loss  3.90 | ppl    49.40\n",
            "| epoch  41 |  1600/ 2983 batches | lr 0.000125 | ms/batch 24.17 | loss  3.93 | ppl    50.71\n",
            "| epoch  41 |  1800/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.89 | ppl    48.95\n",
            "| epoch  41 |  2000/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.92 | ppl    50.45\n",
            "| epoch  41 |  2200/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.80 | ppl    44.63\n",
            "| epoch  41 |  2400/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.82 | ppl    45.71\n",
            "| epoch  41 |  2600/ 2983 batches | lr 0.000125 | ms/batch 24.17 | loss  3.85 | ppl    47.10\n",
            "| epoch  41 |  2800/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.82 | ppl    45.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 74.03s | valid loss  5.24 | valid ppl   188.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  42 |   200/ 2983 batches | lr 0.000125 | ms/batch 24.27 | loss  3.83 | ppl    46.12\n",
            "| epoch  42 |   400/ 2983 batches | lr 0.000125 | ms/batch 24.17 | loss  3.89 | ppl    49.12\n",
            "| epoch  42 |   600/ 2983 batches | lr 0.000125 | ms/batch 24.19 | loss  3.74 | ppl    42.21\n",
            "| epoch  42 |   800/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.80 | ppl    44.71\n",
            "| epoch  42 |  1000/ 2983 batches | lr 0.000125 | ms/batch 24.18 | loss  3.85 | ppl    47.17\n",
            "| epoch  42 |  1200/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.86 | ppl    47.23\n",
            "| epoch  42 |  1400/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.88 | ppl    48.63\n",
            "| epoch  42 |  1600/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.92 | ppl    50.20\n",
            "| epoch  42 |  1800/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.88 | ppl    48.36\n",
            "| epoch  42 |  2000/ 2983 batches | lr 0.000125 | ms/batch 24.17 | loss  3.91 | ppl    49.72\n",
            "| epoch  42 |  2200/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.79 | ppl    44.17\n",
            "| epoch  42 |  2400/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.80 | ppl    44.87\n",
            "| epoch  42 |  2600/ 2983 batches | lr 0.000125 | ms/batch 24.17 | loss  3.85 | ppl    47.12\n",
            "| epoch  42 |  2800/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.81 | ppl    45.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 74.05s | valid loss  5.23 | valid ppl   187.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  43 |   200/ 2983 batches | lr 0.000125 | ms/batch 24.26 | loss  3.82 | ppl    45.78\n",
            "| epoch  43 |   400/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.89 | ppl    49.04\n",
            "| epoch  43 |   600/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.74 | ppl    41.95\n",
            "| epoch  43 |   800/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.79 | ppl    44.41\n",
            "| epoch  43 |  1000/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.85 | ppl    46.90\n",
            "| epoch  43 |  1200/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.85 | ppl    47.06\n",
            "| epoch  43 |  1400/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.88 | ppl    48.60\n",
            "| epoch  43 |  1600/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.91 | ppl    49.77\n",
            "| epoch  43 |  1800/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.87 | ppl    47.71\n",
            "| epoch  43 |  2000/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.90 | ppl    49.56\n",
            "| epoch  43 |  2200/ 2983 batches | lr 0.000125 | ms/batch 24.14 | loss  3.78 | ppl    44.02\n",
            "| epoch  43 |  2400/ 2983 batches | lr 0.000125 | ms/batch 24.15 | loss  3.81 | ppl    45.31\n",
            "| epoch  43 |  2600/ 2983 batches | lr 0.000125 | ms/batch 24.17 | loss  3.84 | ppl    46.39\n",
            "| epoch  43 |  2800/ 2983 batches | lr 0.000125 | ms/batch 24.16 | loss  3.80 | ppl    44.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 74.00s | valid loss  5.23 | valid ppl   187.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  44 |   200/ 2983 batches | lr 0.000063 | ms/batch 24.27 | loss  3.83 | ppl    45.99\n",
            "| epoch  44 |   400/ 2983 batches | lr 0.000063 | ms/batch 24.16 | loss  3.89 | ppl    48.89\n",
            "| epoch  44 |   600/ 2983 batches | lr 0.000063 | ms/batch 24.14 | loss  3.74 | ppl    42.03\n",
            "| epoch  44 |   800/ 2983 batches | lr 0.000063 | ms/batch 24.15 | loss  3.80 | ppl    44.91\n",
            "| epoch  44 |  1000/ 2983 batches | lr 0.000063 | ms/batch 24.17 | loss  3.85 | ppl    46.93\n",
            "| epoch  44 |  1200/ 2983 batches | lr 0.000063 | ms/batch 24.14 | loss  3.85 | ppl    47.00\n",
            "| epoch  44 |  1400/ 2983 batches | lr 0.000063 | ms/batch 24.17 | loss  3.87 | ppl    47.98\n",
            "| epoch  44 |  1600/ 2983 batches | lr 0.000063 | ms/batch 24.14 | loss  3.90 | ppl    49.45\n",
            "| epoch  44 |  1800/ 2983 batches | lr 0.000063 | ms/batch 24.15 | loss  3.86 | ppl    47.69\n",
            "| epoch  44 |  2000/ 2983 batches | lr 0.000063 | ms/batch 24.16 | loss  3.89 | ppl    49.13\n",
            "| epoch  44 |  2200/ 2983 batches | lr 0.000063 | ms/batch 24.14 | loss  3.78 | ppl    43.67\n",
            "| epoch  44 |  2400/ 2983 batches | lr 0.000063 | ms/batch 24.15 | loss  3.79 | ppl    44.45\n",
            "| epoch  44 |  2600/ 2983 batches | lr 0.000063 | ms/batch 24.14 | loss  3.83 | ppl    45.99\n",
            "| epoch  44 |  2800/ 2983 batches | lr 0.000063 | ms/batch 24.14 | loss  3.79 | ppl    44.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 74.00s | valid loss  5.23 | valid ppl   186.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  45 |   200/ 2983 batches | lr 0.000063 | ms/batch 24.24 | loss  3.83 | ppl    46.08\n",
            "| epoch  45 |   400/ 2983 batches | lr 0.000063 | ms/batch 24.18 | loss  3.88 | ppl    48.57\n",
            "| epoch  45 |   600/ 2983 batches | lr 0.000063 | ms/batch 24.14 | loss  3.73 | ppl    41.55\n",
            "| epoch  45 |   800/ 2983 batches | lr 0.000063 | ms/batch 24.14 | loss  3.80 | ppl    44.59\n",
            "| epoch  45 |  1000/ 2983 batches | lr 0.000063 | ms/batch 24.14 | loss  3.84 | ppl    46.53\n",
            "| epoch  45 |  1200/ 2983 batches | lr 0.000063 | ms/batch 24.15 | loss  3.84 | ppl    46.59\n",
            "| epoch  45 |  1400/ 2983 batches | lr 0.000063 | ms/batch 24.18 | loss  3.87 | ppl    48.00\n",
            "| epoch  45 |  1600/ 2983 batches | lr 0.000063 | ms/batch 24.15 | loss  3.90 | ppl    49.60\n",
            "| epoch  45 |  1800/ 2983 batches | lr 0.000063 | ms/batch 24.17 | loss  3.86 | ppl    47.41\n",
            "| epoch  45 |  2000/ 2983 batches | lr 0.000063 | ms/batch 24.14 | loss  3.89 | ppl    48.90\n",
            "| epoch  45 |  2200/ 2983 batches | lr 0.000063 | ms/batch 24.14 | loss  3.76 | ppl    42.95\n",
            "| epoch  45 |  2400/ 2983 batches | lr 0.000063 | ms/batch 24.14 | loss  3.79 | ppl    44.16\n",
            "| epoch  45 |  2600/ 2983 batches | lr 0.000063 | ms/batch 24.14 | loss  3.82 | ppl    45.75\n",
            "| epoch  45 |  2800/ 2983 batches | lr 0.000063 | ms/batch 24.17 | loss  3.79 | ppl    44.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 74.00s | valid loss  5.23 | valid ppl   186.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  46 |   200/ 2983 batches | lr 0.000063 | ms/batch 24.26 | loss  3.82 | ppl    45.83\n",
            "| epoch  46 |   400/ 2983 batches | lr 0.000063 | ms/batch 24.16 | loss  3.89 | ppl    48.86\n",
            "| epoch  46 |   600/ 2983 batches | lr 0.000063 | ms/batch 24.16 | loss  3.72 | ppl    41.33\n",
            "| epoch  46 |   800/ 2983 batches | lr 0.000063 | ms/batch 24.17 | loss  3.80 | ppl    44.54\n",
            "| epoch  46 |  1000/ 2983 batches | lr 0.000063 | ms/batch 24.13 | loss  3.83 | ppl    46.20\n",
            "| epoch  46 |  1200/ 2983 batches | lr 0.000063 | ms/batch 24.14 | loss  3.83 | ppl    46.22\n",
            "| epoch  46 |  1400/ 2983 batches | lr 0.000063 | ms/batch 24.15 | loss  3.86 | ppl    47.29\n",
            "| epoch  46 |  1600/ 2983 batches | lr 0.000063 | ms/batch 24.14 | loss  3.89 | ppl    48.91\n",
            "| epoch  46 |  1800/ 2983 batches | lr 0.000063 | ms/batch 24.18 | loss  3.86 | ppl    47.28\n",
            "| epoch  46 |  2000/ 2983 batches | lr 0.000063 | ms/batch 24.20 | loss  3.89 | ppl    48.69\n",
            "| epoch  46 |  2200/ 2983 batches | lr 0.000063 | ms/batch 24.26 | loss  3.76 | ppl    43.00\n",
            "| epoch  46 |  2400/ 2983 batches | lr 0.000063 | ms/batch 24.31 | loss  3.78 | ppl    43.91\n",
            "| epoch  46 |  2600/ 2983 batches | lr 0.000063 | ms/batch 24.20 | loss  3.82 | ppl    45.53\n",
            "| epoch  46 |  2800/ 2983 batches | lr 0.000063 | ms/batch 24.20 | loss  3.78 | ppl    44.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 74.10s | valid loss  5.23 | valid ppl   186.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  47 |   200/ 2983 batches | lr 0.000063 | ms/batch 24.34 | loss  3.81 | ppl    45.20\n",
            "| epoch  47 |   400/ 2983 batches | lr 0.000063 | ms/batch 24.20 | loss  3.88 | ppl    48.49\n",
            "| epoch  47 |   600/ 2983 batches | lr 0.000063 | ms/batch 24.20 | loss  3.73 | ppl    41.82\n",
            "| epoch  47 |   800/ 2983 batches | lr 0.000063 | ms/batch 24.22 | loss  3.78 | ppl    43.96\n",
            "| epoch  47 |  1000/ 2983 batches | lr 0.000063 | ms/batch 24.18 | loss  3.83 | ppl    45.99\n",
            "| epoch  47 |  1200/ 2983 batches | lr 0.000063 | ms/batch 24.14 | loss  3.83 | ppl    45.88\n",
            "| epoch  47 |  1400/ 2983 batches | lr 0.000063 | ms/batch 24.13 | loss  3.85 | ppl    47.10\n",
            "| epoch  47 |  1600/ 2983 batches | lr 0.000063 | ms/batch 24.14 | loss  3.88 | ppl    48.66\n",
            "| epoch  47 |  1800/ 2983 batches | lr 0.000063 | ms/batch 24.21 | loss  3.85 | ppl    46.90\n",
            "| epoch  47 |  2000/ 2983 batches | lr 0.000063 | ms/batch 24.15 | loss  3.88 | ppl    48.63\n",
            "| epoch  47 |  2200/ 2983 batches | lr 0.000063 | ms/batch 24.18 | loss  3.76 | ppl    42.99\n",
            "| epoch  47 |  2400/ 2983 batches | lr 0.000063 | ms/batch 24.16 | loss  3.78 | ppl    43.70\n",
            "| epoch  47 |  2600/ 2983 batches | lr 0.000063 | ms/batch 24.14 | loss  3.82 | ppl    45.50\n",
            "| epoch  47 |  2800/ 2983 batches | lr 0.000063 | ms/batch 24.16 | loss  3.78 | ppl    43.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 74.06s | valid loss  5.23 | valid ppl   186.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  48 |   200/ 2983 batches | lr 0.000031 | ms/batch 24.27 | loss  3.81 | ppl    45.00\n",
            "| epoch  48 |   400/ 2983 batches | lr 0.000031 | ms/batch 24.14 | loss  3.90 | ppl    49.36\n",
            "| epoch  48 |   600/ 2983 batches | lr 0.000031 | ms/batch 24.14 | loss  3.74 | ppl    42.21\n",
            "| epoch  48 |   800/ 2983 batches | lr 0.000031 | ms/batch 24.15 | loss  3.80 | ppl    44.68\n",
            "| epoch  48 |  1000/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.85 | ppl    46.93\n",
            "| epoch  48 |  1200/ 2983 batches | lr 0.000031 | ms/batch 24.19 | loss  3.84 | ppl    46.40\n",
            "| epoch  48 |  1400/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.87 | ppl    47.77\n",
            "| epoch  48 |  1600/ 2983 batches | lr 0.000031 | ms/batch 24.18 | loss  3.88 | ppl    48.51\n",
            "| epoch  48 |  1800/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.85 | ppl    46.91\n",
            "| epoch  48 |  2000/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.88 | ppl    48.48\n",
            "| epoch  48 |  2200/ 2983 batches | lr 0.000031 | ms/batch 24.15 | loss  3.77 | ppl    43.55\n",
            "| epoch  48 |  2400/ 2983 batches | lr 0.000031 | ms/batch 24.14 | loss  3.77 | ppl    43.57\n",
            "| epoch  48 |  2600/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.81 | ppl    45.27\n",
            "| epoch  48 |  2800/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.77 | ppl    43.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 74.03s | valid loss  5.22 | valid ppl   185.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  49 |   200/ 2983 batches | lr 0.000031 | ms/batch 24.27 | loss  3.81 | ppl    45.08\n",
            "| epoch  49 |   400/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.88 | ppl    48.55\n",
            "| epoch  49 |   600/ 2983 batches | lr 0.000031 | ms/batch 24.17 | loss  3.73 | ppl    41.67\n",
            "| epoch  49 |   800/ 2983 batches | lr 0.000031 | ms/batch 24.15 | loss  3.79 | ppl    44.07\n",
            "| epoch  49 |  1000/ 2983 batches | lr 0.000031 | ms/batch 24.13 | loss  3.84 | ppl    46.43\n",
            "| epoch  49 |  1200/ 2983 batches | lr 0.000031 | ms/batch 24.15 | loss  3.84 | ppl    46.47\n",
            "| epoch  49 |  1400/ 2983 batches | lr 0.000031 | ms/batch 24.14 | loss  3.86 | ppl    47.49\n",
            "| epoch  49 |  1600/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.88 | ppl    48.46\n",
            "| epoch  49 |  1800/ 2983 batches | lr 0.000031 | ms/batch 24.15 | loss  3.84 | ppl    46.55\n",
            "| epoch  49 |  2000/ 2983 batches | lr 0.000031 | ms/batch 24.13 | loss  3.88 | ppl    48.26\n",
            "| epoch  49 |  2200/ 2983 batches | lr 0.000031 | ms/batch 24.15 | loss  3.76 | ppl    43.15\n",
            "| epoch  49 |  2400/ 2983 batches | lr 0.000031 | ms/batch 24.14 | loss  3.78 | ppl    43.90\n",
            "| epoch  49 |  2600/ 2983 batches | lr 0.000031 | ms/batch 24.15 | loss  3.81 | ppl    45.20\n",
            "| epoch  49 |  2800/ 2983 batches | lr 0.000031 | ms/batch 24.14 | loss  3.76 | ppl    43.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 74.00s | valid loss  5.22 | valid ppl   184.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  50 |   200/ 2983 batches | lr 0.000031 | ms/batch 24.27 | loss  3.81 | ppl    45.33\n",
            "| epoch  50 |   400/ 2983 batches | lr 0.000031 | ms/batch 24.15 | loss  3.88 | ppl    48.57\n",
            "| epoch  50 |   600/ 2983 batches | lr 0.000031 | ms/batch 24.17 | loss  3.73 | ppl    41.87\n",
            "| epoch  50 |   800/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.79 | ppl    44.21\n",
            "| epoch  50 |  1000/ 2983 batches | lr 0.000031 | ms/batch 24.17 | loss  3.83 | ppl    45.95\n",
            "| epoch  50 |  1200/ 2983 batches | lr 0.000031 | ms/batch 24.15 | loss  3.82 | ppl    45.68\n",
            "| epoch  50 |  1400/ 2983 batches | lr 0.000031 | ms/batch 24.15 | loss  3.85 | ppl    46.86\n",
            "| epoch  50 |  1600/ 2983 batches | lr 0.000031 | ms/batch 24.17 | loss  3.89 | ppl    48.94\n",
            "| epoch  50 |  1800/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.84 | ppl    46.67\n",
            "| epoch  50 |  2000/ 2983 batches | lr 0.000031 | ms/batch 24.18 | loss  3.88 | ppl    48.51\n",
            "| epoch  50 |  2200/ 2983 batches | lr 0.000031 | ms/batch 24.17 | loss  3.77 | ppl    43.26\n",
            "| epoch  50 |  2400/ 2983 batches | lr 0.000031 | ms/batch 24.21 | loss  3.77 | ppl    43.57\n",
            "| epoch  50 |  2600/ 2983 batches | lr 0.000031 | ms/batch 24.19 | loss  3.81 | ppl    45.30\n",
            "| epoch  50 |  2800/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.78 | ppl    43.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 74.04s | valid loss  5.22 | valid ppl   184.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  51 |   200/ 2983 batches | lr 0.000031 | ms/batch 24.25 | loss  3.81 | ppl    45.21\n",
            "| epoch  51 |   400/ 2983 batches | lr 0.000031 | ms/batch 24.17 | loss  3.88 | ppl    48.47\n",
            "| epoch  51 |   600/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.72 | ppl    41.13\n",
            "| epoch  51 |   800/ 2983 batches | lr 0.000031 | ms/batch 24.15 | loss  3.78 | ppl    43.74\n",
            "| epoch  51 |  1000/ 2983 batches | lr 0.000031 | ms/batch 24.17 | loss  3.83 | ppl    46.12\n",
            "| epoch  51 |  1200/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.82 | ppl    45.78\n",
            "| epoch  51 |  1400/ 2983 batches | lr 0.000031 | ms/batch 24.18 | loss  3.85 | ppl    47.06\n",
            "| epoch  51 |  1600/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.88 | ppl    48.29\n",
            "| epoch  51 |  1800/ 2983 batches | lr 0.000031 | ms/batch 24.17 | loss  3.84 | ppl    46.52\n",
            "| epoch  51 |  2000/ 2983 batches | lr 0.000031 | ms/batch 24.20 | loss  3.88 | ppl    48.28\n",
            "| epoch  51 |  2200/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.75 | ppl    42.61\n",
            "| epoch  51 |  2400/ 2983 batches | lr 0.000031 | ms/batch 24.18 | loss  3.76 | ppl    43.15\n",
            "| epoch  51 |  2600/ 2983 batches | lr 0.000031 | ms/batch 24.17 | loss  3.80 | ppl    44.65\n",
            "| epoch  51 |  2800/ 2983 batches | lr 0.000031 | ms/batch 24.15 | loss  3.77 | ppl    43.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 74.04s | valid loss  5.22 | valid ppl   184.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  52 |   200/ 2983 batches | lr 0.000031 | ms/batch 24.27 | loss  3.80 | ppl    44.88\n",
            "| epoch  52 |   400/ 2983 batches | lr 0.000031 | ms/batch 24.18 | loss  3.87 | ppl    47.74\n",
            "| epoch  52 |   600/ 2983 batches | lr 0.000031 | ms/batch 24.17 | loss  3.71 | ppl    40.97\n",
            "| epoch  52 |   800/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.78 | ppl    43.82\n",
            "| epoch  52 |  1000/ 2983 batches | lr 0.000031 | ms/batch 24.18 | loss  3.82 | ppl    45.81\n",
            "| epoch  52 |  1200/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.82 | ppl    45.70\n",
            "| epoch  52 |  1400/ 2983 batches | lr 0.000031 | ms/batch 24.19 | loss  3.85 | ppl    47.21\n",
            "| epoch  52 |  1600/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.89 | ppl    48.72\n",
            "| epoch  52 |  1800/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.84 | ppl    46.66\n",
            "| epoch  52 |  2000/ 2983 batches | lr 0.000031 | ms/batch 24.15 | loss  3.86 | ppl    47.65\n",
            "| epoch  52 |  2200/ 2983 batches | lr 0.000031 | ms/batch 24.14 | loss  3.76 | ppl    42.92\n",
            "| epoch  52 |  2400/ 2983 batches | lr 0.000031 | ms/batch 24.15 | loss  3.76 | ppl    43.10\n",
            "| epoch  52 |  2600/ 2983 batches | lr 0.000031 | ms/batch 24.15 | loss  3.80 | ppl    44.63\n",
            "| epoch  52 |  2800/ 2983 batches | lr 0.000031 | ms/batch 24.15 | loss  3.77 | ppl    43.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 74.03s | valid loss  5.22 | valid ppl   184.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  53 |   200/ 2983 batches | lr 0.000031 | ms/batch 24.27 | loss  3.80 | ppl    44.57\n",
            "| epoch  53 |   400/ 2983 batches | lr 0.000031 | ms/batch 24.18 | loss  3.87 | ppl    48.13\n",
            "| epoch  53 |   600/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.72 | ppl    41.21\n",
            "| epoch  53 |   800/ 2983 batches | lr 0.000031 | ms/batch 24.18 | loss  3.78 | ppl    43.67\n",
            "| epoch  53 |  1000/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.83 | ppl    46.08\n",
            "| epoch  53 |  1200/ 2983 batches | lr 0.000031 | ms/batch 24.17 | loss  3.82 | ppl    45.57\n",
            "| epoch  53 |  1400/ 2983 batches | lr 0.000031 | ms/batch 24.19 | loss  3.85 | ppl    46.78\n",
            "| epoch  53 |  1600/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.87 | ppl    48.09\n",
            "| epoch  53 |  1800/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.84 | ppl    46.46\n",
            "| epoch  53 |  2000/ 2983 batches | lr 0.000031 | ms/batch 24.14 | loss  3.87 | ppl    47.96\n",
            "| epoch  53 |  2200/ 2983 batches | lr 0.000031 | ms/batch 24.15 | loss  3.75 | ppl    42.65\n",
            "| epoch  53 |  2400/ 2983 batches | lr 0.000031 | ms/batch 24.16 | loss  3.77 | ppl    43.17\n",
            "| epoch  53 |  2600/ 2983 batches | lr 0.000031 | ms/batch 24.15 | loss  3.80 | ppl    44.68\n",
            "| epoch  53 |  2800/ 2983 batches | lr 0.000031 | ms/batch 24.17 | loss  3.76 | ppl    42.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 74.04s | valid loss  5.22 | valid ppl   184.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  54 |   200/ 2983 batches | lr 0.000016 | ms/batch 24.29 | loss  3.81 | ppl    45.34\n",
            "| epoch  54 |   400/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.89 | ppl    48.97\n",
            "| epoch  54 |   600/ 2983 batches | lr 0.000016 | ms/batch 24.16 | loss  3.73 | ppl    41.69\n",
            "| epoch  54 |   800/ 2983 batches | lr 0.000016 | ms/batch 24.17 | loss  3.79 | ppl    44.41\n",
            "| epoch  54 |  1000/ 2983 batches | lr 0.000016 | ms/batch 24.13 | loss  3.84 | ppl    46.67\n",
            "| epoch  54 |  1200/ 2983 batches | lr 0.000016 | ms/batch 24.17 | loss  3.83 | ppl    46.12\n",
            "| epoch  54 |  1400/ 2983 batches | lr 0.000016 | ms/batch 24.17 | loss  3.87 | ppl    47.77\n",
            "| epoch  54 |  1600/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.88 | ppl    48.63\n",
            "| epoch  54 |  1800/ 2983 batches | lr 0.000016 | ms/batch 24.16 | loss  3.85 | ppl    46.79\n",
            "| epoch  54 |  2000/ 2983 batches | lr 0.000016 | ms/batch 24.13 | loss  3.88 | ppl    48.28\n",
            "| epoch  54 |  2200/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.76 | ppl    43.00\n",
            "| epoch  54 |  2400/ 2983 batches | lr 0.000016 | ms/batch 24.12 | loss  3.79 | ppl    44.23\n",
            "| epoch  54 |  2600/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.82 | ppl    45.52\n",
            "| epoch  54 |  2800/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.77 | ppl    43.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 73.99s | valid loss  5.22 | valid ppl   184.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  55 |   200/ 2983 batches | lr 0.000016 | ms/batch 24.30 | loss  3.81 | ppl    45.26\n",
            "| epoch  55 |   400/ 2983 batches | lr 0.000016 | ms/batch 24.16 | loss  3.89 | ppl    48.72\n",
            "| epoch  55 |   600/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.73 | ppl    41.67\n",
            "| epoch  55 |   800/ 2983 batches | lr 0.000016 | ms/batch 24.13 | loss  3.79 | ppl    44.27\n",
            "| epoch  55 |  1000/ 2983 batches | lr 0.000016 | ms/batch 24.13 | loss  3.83 | ppl    46.25\n",
            "| epoch  55 |  1200/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.83 | ppl    46.06\n",
            "| epoch  55 |  1400/ 2983 batches | lr 0.000016 | ms/batch 24.12 | loss  3.85 | ppl    46.96\n",
            "| epoch  55 |  1600/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.88 | ppl    48.41\n",
            "| epoch  55 |  1800/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.85 | ppl    46.77\n",
            "| epoch  55 |  2000/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.88 | ppl    48.57\n",
            "| epoch  55 |  2200/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.76 | ppl    42.87\n",
            "| epoch  55 |  2400/ 2983 batches | lr 0.000016 | ms/batch 24.11 | loss  3.78 | ppl    43.68\n",
            "| epoch  55 |  2600/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.81 | ppl    45.31\n",
            "| epoch  55 |  2800/ 2983 batches | lr 0.000016 | ms/batch 24.17 | loss  3.76 | ppl    43.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 73.98s | valid loss  5.21 | valid ppl   183.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  56 |   200/ 2983 batches | lr 0.000016 | ms/batch 24.29 | loss  3.81 | ppl    45.32\n",
            "| epoch  56 |   400/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.88 | ppl    48.55\n",
            "| epoch  56 |   600/ 2983 batches | lr 0.000016 | ms/batch 24.19 | loss  3.73 | ppl    41.80\n",
            "| epoch  56 |   800/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.79 | ppl    44.08\n",
            "| epoch  56 |  1000/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.85 | ppl    46.80\n",
            "| epoch  56 |  1200/ 2983 batches | lr 0.000016 | ms/batch 24.17 | loss  3.83 | ppl    46.00\n",
            "| epoch  56 |  1400/ 2983 batches | lr 0.000016 | ms/batch 24.17 | loss  3.86 | ppl    47.55\n",
            "| epoch  56 |  1600/ 2983 batches | lr 0.000016 | ms/batch 24.18 | loss  3.89 | ppl    49.02\n",
            "| epoch  56 |  1800/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.85 | ppl    46.88\n",
            "| epoch  56 |  2000/ 2983 batches | lr 0.000016 | ms/batch 24.13 | loss  3.88 | ppl    48.56\n",
            "| epoch  56 |  2200/ 2983 batches | lr 0.000016 | ms/batch 24.18 | loss  3.76 | ppl    43.08\n",
            "| epoch  56 |  2400/ 2983 batches | lr 0.000016 | ms/batch 24.17 | loss  3.78 | ppl    43.72\n",
            "| epoch  56 |  2600/ 2983 batches | lr 0.000016 | ms/batch 24.18 | loss  3.81 | ppl    45.27\n",
            "| epoch  56 |  2800/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.77 | ppl    43.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 74.04s | valid loss  5.21 | valid ppl   183.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  57 |   200/ 2983 batches | lr 0.000016 | ms/batch 24.27 | loss  3.82 | ppl    45.75\n",
            "| epoch  57 |   400/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.90 | ppl    49.27\n",
            "| epoch  57 |   600/ 2983 batches | lr 0.000016 | ms/batch 24.16 | loss  3.73 | ppl    41.65\n",
            "| epoch  57 |   800/ 2983 batches | lr 0.000016 | ms/batch 24.16 | loss  3.79 | ppl    44.30\n",
            "| epoch  57 |  1000/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.83 | ppl    46.20\n",
            "| epoch  57 |  1200/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.83 | ppl    46.11\n",
            "| epoch  57 |  1400/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.87 | ppl    47.76\n",
            "| epoch  57 |  1600/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.89 | ppl    48.89\n",
            "| epoch  57 |  1800/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.84 | ppl    46.60\n",
            "| epoch  57 |  2000/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.87 | ppl    48.02\n",
            "| epoch  57 |  2200/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.76 | ppl    43.12\n",
            "| epoch  57 |  2400/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.77 | ppl    43.41\n",
            "| epoch  57 |  2600/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.81 | ppl    45.09\n",
            "| epoch  57 |  2800/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.77 | ppl    43.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 73.99s | valid loss  5.21 | valid ppl   183.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  58 |   200/ 2983 batches | lr 0.000016 | ms/batch 24.24 | loss  3.80 | ppl    44.76\n",
            "| epoch  58 |   400/ 2983 batches | lr 0.000016 | ms/batch 24.13 | loss  3.87 | ppl    48.15\n",
            "| epoch  58 |   600/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.71 | ppl    41.03\n",
            "| epoch  58 |   800/ 2983 batches | lr 0.000016 | ms/batch 24.13 | loss  3.78 | ppl    43.69\n",
            "| epoch  58 |  1000/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.83 | ppl    46.03\n",
            "| epoch  58 |  1200/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.82 | ppl    45.74\n",
            "| epoch  58 |  1400/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.85 | ppl    47.19\n",
            "| epoch  58 |  1600/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.87 | ppl    48.05\n",
            "| epoch  58 |  1800/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.84 | ppl    46.30\n",
            "| epoch  58 |  2000/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.87 | ppl    47.97\n",
            "| epoch  58 |  2200/ 2983 batches | lr 0.000016 | ms/batch 24.13 | loss  3.75 | ppl    42.45\n",
            "| epoch  58 |  2400/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.77 | ppl    43.32\n",
            "| epoch  58 |  2600/ 2983 batches | lr 0.000016 | ms/batch 24.16 | loss  3.79 | ppl    44.24\n",
            "| epoch  58 |  2800/ 2983 batches | lr 0.000016 | ms/batch 24.17 | loss  3.77 | ppl    43.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 73.97s | valid loss  5.21 | valid ppl   183.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  59 |   200/ 2983 batches | lr 0.000016 | ms/batch 24.28 | loss  3.81 | ppl    45.13\n",
            "| epoch  59 |   400/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.88 | ppl    48.54\n",
            "| epoch  59 |   600/ 2983 batches | lr 0.000016 | ms/batch 24.13 | loss  3.73 | ppl    41.68\n",
            "| epoch  59 |   800/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.78 | ppl    43.82\n",
            "| epoch  59 |  1000/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.83 | ppl    45.93\n",
            "| epoch  59 |  1200/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.82 | ppl    45.63\n",
            "| epoch  59 |  1400/ 2983 batches | lr 0.000016 | ms/batch 24.17 | loss  3.86 | ppl    47.23\n",
            "| epoch  59 |  1600/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.88 | ppl    48.36\n",
            "| epoch  59 |  1800/ 2983 batches | lr 0.000016 | ms/batch 24.13 | loss  3.83 | ppl    46.08\n",
            "| epoch  59 |  2000/ 2983 batches | lr 0.000016 | ms/batch 24.16 | loss  3.87 | ppl    47.82\n",
            "| epoch  59 |  2200/ 2983 batches | lr 0.000016 | ms/batch 24.16 | loss  3.75 | ppl    42.70\n",
            "| epoch  59 |  2400/ 2983 batches | lr 0.000016 | ms/batch 24.16 | loss  3.77 | ppl    43.36\n",
            "| epoch  59 |  2600/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.80 | ppl    44.90\n",
            "| epoch  59 |  2800/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.76 | ppl    43.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 73.99s | valid loss  5.21 | valid ppl   183.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  60 |   200/ 2983 batches | lr 0.000016 | ms/batch 24.27 | loss  3.81 | ppl    44.96\n",
            "| epoch  60 |   400/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.88 | ppl    48.50\n",
            "| epoch  60 |   600/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.73 | ppl    41.57\n",
            "| epoch  60 |   800/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.77 | ppl    43.39\n",
            "| epoch  60 |  1000/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.82 | ppl    45.77\n",
            "| epoch  60 |  1200/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.82 | ppl    45.43\n",
            "| epoch  60 |  1400/ 2983 batches | lr 0.000016 | ms/batch 24.15 | loss  3.85 | ppl    47.04\n",
            "| epoch  60 |  1600/ 2983 batches | lr 0.000016 | ms/batch 24.14 | loss  3.87 | ppl    48.00\n",
            "| epoch  60 |  1800/ 2983 batches | lr 0.000016 | ms/batch 24.17 | loss  3.83 | ppl    46.26\n",
            "| epoch  60 |  2000/ 2983 batches | lr 0.000016 | ms/batch 24.17 | loss  3.86 | ppl    47.62\n",
            "| epoch  60 |  2200/ 2983 batches | lr 0.000016 | ms/batch 24.18 | loss  3.75 | ppl    42.61\n",
            "| epoch  60 |  2400/ 2983 batches | lr 0.000016 | ms/batch 24.22 | loss  3.77 | ppl    43.48\n",
            "| epoch  60 |  2600/ 2983 batches | lr 0.000016 | ms/batch 24.22 | loss  3.81 | ppl    45.12\n",
            "| epoch  60 |  2800/ 2983 batches | lr 0.000016 | ms/batch 24.22 | loss  3.77 | ppl    43.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 74.06s | valid loss  5.21 | valid ppl   183.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "=========================================================================================\n",
            "| End of training | test loss  5.15 | test ppl   171.83\n",
            "=========================================================================================\n",
            "\n",
            "Generating text...\n",
            "Vocabulary size: 33278\n",
            "Vocabulary size: 33278\n",
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n",
            "to the restoration of stabilized Castle , a los Rossi accent , there had a wide 19th   century outer\n",
            "altar known as Late 12 : 27 , under the entire purpose of the past district projects . <eos> In\n",
            "June 1917 , a Grade 5 of the faculty built the music site erected by Allied hospitals buildings on the\n",
            "Eagle Theory and the <unk> district district of the German district . In the aftermath of between the Inner 88\n",
            "‑ kilometer writings at this point and town . On July 23 , 1944 , the fortifications against a former\n",
            "inner district returned to Port Street , also launched the surrender of hosting a wider paper with its Kesteven State\n",
            "Bridge . Metro Serge Area Street conducted the Commission mainline to 40 miles ( 11 km ) south of an\n",
            "540 Channel airbase which near Planning left the river held with rubble . A possibility also opened on September 6\n",
            ", 2014 , shortly after the season toll quickly . The 18th State , <unk> the inscription of driving him\n",
            "through an immediate area , by then moving ahead of farm energy that according to their <unk> of the Calendar\n",
            "Dam , but that later received the 170 tradition that would be under minor speeds . <eos> The Churches observed\n",
            "a volume of nylon for signal architects . Those provided it another relocation to a Often and inexperienced approximately considerable\n",
            "24 ft ( 65 m ² ) , diverted above seaside bridges , both at the Benedict Laughing Crew /\n",
            "NW and the individual which gross water . Despite the bombardment of the Church of Georgia , they appear in\n",
            "numerous forest supply category . It was passing number 220 ( 85 ) / 30 kg ( 49 kg )\n",
            "after 59 metres ( 220 km ) long . The team continued to no longer locate   responded , the\n",
            "first cross   campus post   southwest on September 7 . To locate up himself , a group of investors\n",
            "increased a week . The idea taking £ 10   0 million ( equivalent to $ 1   8 m\n",
            ") . <eos> <eos> = = Aftermath = = <eos> <eos> 3   7 km / h : 34 miles\n",
            "( 55 ft ) deep ( 28 September 3 so 10 / 6 in <unk> 30 5 , 6  \n",
            "128 ft ) on 22 April 8 – 9 and 85 km / h <unk> ( 29   56 km\n",
            "/ h for 28   This 5 ; 65 / 92 Airlines / 07   000 inHg ) . This\n",
            "lower measurement design , which is estimated to be detected in mass Gothic commerce and <unk> protection although most premieres\n",
            "existed in a types of scattered on the field and was scattered across it inside approximately 15 to 8 knots\n",
            "( 11 mi ) offshore deck north of earlier . <eos> The hills substantially covers information of athletics and buses\n",
            "took the water on the <unk> Canal . Winds from the <unk> port of the beams where an successful drive\n",
            "were able to watch the advantage and would not become a <unk> . U   3 run forward or caught\n",
            "at the nearby end of the table for sea ; each male   beams lost <unk> by Mexican troops .\n",
            "<unk> power from any time morning , 50 feet ( 0   370 m ) for <unk> , reduced by\n",
            "7   06 m per hour ( 0   77 m ) . <eos> Reaction were perfect at the entrance\n",
            "to the island and on the <unk> side side of the glass and provide <unk> <unk> . Two bursts were\n",
            "heavily damaged , when some other churches are scattered enough ahead of their immediate samples , and it was bursts\n",
            "of physical testing . remnants of chains still weakened from the <unk> amounts of the use of roofs because of\n",
            "two clusters should day . As the small docks ( MGB 12 ; the 12th side are approximately 65 mi\n",
            ") above sea level in the wrecks . Ten days later , 8 % in their last high , marine\n",
            "languages to take a maximum inflict 500 mbar ( 3   9 ft ) , 425 km ( 2  \n",
            "100 mi ) shells of another , 7 % ( 0   84 km ) 4 % on red or\n",
            "steel , the Wind and iron burning and maximum matted waste of torpedo speeds and 50 angels ; 1  \n",
            "2 kg ( 2   8 ft ) touch with a maximum of 37 metres ( 18 lb ) .\n",
            "This period she gave 7   5 kg ( 170 lb ( 4   04 million kg ) , mainly\n",
            "a pocket , and <unk> ft ball down 13 remained . At an average of 14   71 degrees related\n",
            "to this type of plating above cooler seasonal range , German sank and small earth damage , the result turn\n",
            "out . 37 % were <unk> with public   serve splinter officers , three days before the fish <unk> from\n",
            "55 cm ( 37 ) . <eos> A total of 45 kg ( 89 km ) of damage along the\n",
            "Port River were armed with some ports . However , its depth was formed for the day . <eos> <eos>\n",
            "= = = Armament = = = <eos> <eos> Ottoman Bath inscriptions to the United States work was assigned under\n",
            "20th deployment and the maximum three people were sold . <eos> The O 'Brien forms contributed to the <unk> Register\n",
            "of Omaha , which contains the partial buildings of the river 's flow were passing from what are now known\n",
            "the altar ; Croats . <unk> whenever the demon follows its new battered tradition looks with warm   mass blocks\n",
            ". <eos> Hunt is sculpted in the <unk> <unk> while the cross   border lifestyle system is now built annually\n"
          ]
        }
      ],
      "source": [
        "print(\"Training GRU on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type = 'GRU', # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "    data_path = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize = 400,\n",
        "    nhid = 400,\n",
        "    nlayers = 4,\n",
        "    lr = 0.001,\n",
        "    clip = 0.25,\n",
        "    epochs = 60,\n",
        "    batch_size = 20,\n",
        "    bptt = 35,\n",
        "    dropout = 0.2,\n",
        "    tied = False,\n",
        "    nhead = 4,\n",
        "    log_interval = 200,\n",
        "    save_path = 'model_10.pt',\n",
        "    onnx_export = '',\n",
        "    dry_run = False,\n",
        "    accel = True,\n",
        "    use_optimizer = True,\n",
        "    optimizer_type = 'AdamW',\n",
        "    weight_decay=1e-5,\n",
        "    use_betas = False,\n",
        "    use_eps = False,\n",
        "    criterion = nn.NLLLoss(),\n",
        "    use_label_smoothing = False,\n",
        "    label_smoothing = 0.1,\n",
        "    use_warmup = False,\n",
        "    warmup_steps = 4000,\n",
        "    min_freq = 5,\n",
        "    seed = 1111,\n",
        "    old_version = True\n",
        ")\n",
        "\n",
        "# Example 2: Generate text\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model_10.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf='generated_10.txt',\n",
        "    words=1000,\n",
        "    temperature=1.0,\n",
        "    old_version=True,\n",
        "    use_top_k=False,\n",
        "    accel = True\n",
        ")\n",
        "\n",
        "!cat generated_10.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q50QoYpmsQC7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q50QoYpmsQC7",
        "outputId": "d0cbf7d3-19df-4810-9a14-0151ce3f91be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training GRU on WikiText-2...\n",
            "Using device: cuda\n",
            "Vocabulary size: 25251\n",
            "| epoch   1 |   200/ 2983 batches | lr 0.000040 | ms/batch 22.22 | loss  9.60 | ppl 14824.76\n",
            "| epoch   1 |   400/ 2983 batches | lr 0.000079 | ms/batch 22.18 | loss  7.68 | ppl  2156.10\n",
            "| epoch   1 |   600/ 2983 batches | lr 0.000119 | ms/batch 22.24 | loss  7.43 | ppl  1683.95\n",
            "| epoch   1 |   800/ 2983 batches | lr 0.000158 | ms/batch 22.24 | loss  7.28 | ppl  1448.31\n",
            "| epoch   1 |  1000/ 2983 batches | lr 0.000198 | ms/batch 22.29 | loss  7.15 | ppl  1276.16\n",
            "| epoch   1 |  1200/ 2983 batches | lr 0.000237 | ms/batch 22.30 | loss  7.04 | ppl  1142.03\n",
            "| epoch   1 |  1400/ 2983 batches | lr 0.000277 | ms/batch 22.29 | loss  6.92 | ppl  1007.72\n",
            "| epoch   1 |  1600/ 2983 batches | lr 0.000316 | ms/batch 22.26 | loss  6.85 | ppl   948.39\n",
            "| epoch   1 |  1800/ 2983 batches | lr 0.000356 | ms/batch 22.23 | loss  6.76 | ppl   865.11\n",
            "| epoch   1 |  2000/ 2983 batches | lr 0.000395 | ms/batch 22.21 | loss  6.72 | ppl   829.39\n",
            "| epoch   1 |  2200/ 2983 batches | lr 0.000435 | ms/batch 22.22 | loss  6.61 | ppl   740.16\n",
            "| epoch   1 |  2400/ 2983 batches | lr 0.000475 | ms/batch 22.21 | loss  6.56 | ppl   703.84\n",
            "| epoch   1 |  2600/ 2983 batches | lr 0.000514 | ms/batch 22.22 | loss  6.55 | ppl   698.29\n",
            "| epoch   1 |  2800/ 2983 batches | lr 0.000554 | ms/batch 22.21 | loss  6.46 | ppl   639.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 68.63s | valid loss  6.86 | valid ppl   950.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000590\n",
            "| epoch   2 |   200/ 2983 batches | lr 0.000629 | ms/batch 22.31 | loss  6.45 | ppl   630.99\n",
            "| epoch   2 |   400/ 2983 batches | lr 0.000669 | ms/batch 22.22 | loss  6.41 | ppl   609.11\n",
            "| epoch   2 |   600/ 2983 batches | lr 0.000709 | ms/batch 22.25 | loss  6.31 | ppl   551.49\n",
            "| epoch   2 |   800/ 2983 batches | lr 0.000748 | ms/batch 22.24 | loss  6.31 | ppl   549.20\n",
            "| epoch   2 |  1000/ 2983 batches | lr 0.000788 | ms/batch 22.23 | loss  6.29 | ppl   539.01\n",
            "| epoch   2 |  1200/ 2983 batches | lr 0.000773 | ms/batch 22.25 | loss  6.28 | ppl   532.72\n",
            "| epoch   2 |  1400/ 2983 batches | lr 0.000755 | ms/batch 22.22 | loss  6.25 | ppl   519.64\n",
            "| epoch   2 |  1600/ 2983 batches | lr 0.000738 | ms/batch 22.21 | loss  6.28 | ppl   536.00\n",
            "| epoch   2 |  1800/ 2983 batches | lr 0.000723 | ms/batch 22.23 | loss  6.19 | ppl   488.41\n",
            "| epoch   2 |  2000/ 2983 batches | lr 0.000708 | ms/batch 22.23 | loss  6.19 | ppl   488.34\n",
            "| epoch   2 |  2200/ 2983 batches | lr 0.000694 | ms/batch 22.23 | loss  6.13 | ppl   457.41\n",
            "| epoch   2 |  2400/ 2983 batches | lr 0.000681 | ms/batch 22.23 | loss  6.13 | ppl   458.65\n",
            "| epoch   2 |  2600/ 2983 batches | lr 0.000669 | ms/batch 22.23 | loss  6.14 | ppl   465.35\n",
            "| epoch   2 |  2800/ 2983 batches | lr 0.000657 | ms/batch 22.25 | loss  6.07 | ppl   434.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 68.63s | valid loss  6.54 | valid ppl   695.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000647\n",
            "| epoch   3 |   200/ 2983 batches | lr 0.000637 | ms/batch 22.31 | loss  6.11 | ppl   450.68\n",
            "| epoch   3 |   400/ 2983 batches | lr 0.000627 | ms/batch 22.21 | loss  6.10 | ppl   444.21\n",
            "| epoch   3 |   600/ 2983 batches | lr 0.000617 | ms/batch 22.20 | loss  6.00 | ppl   402.54\n",
            "| epoch   3 |   800/ 2983 batches | lr 0.000608 | ms/batch 22.23 | loss  6.01 | ppl   406.58\n",
            "| epoch   3 |  1000/ 2983 batches | lr 0.000599 | ms/batch 22.21 | loss  6.02 | ppl   411.57\n",
            "| epoch   3 |  1200/ 2983 batches | lr 0.000591 | ms/batch 22.21 | loss  6.00 | ppl   403.79\n",
            "| epoch   3 |  1400/ 2983 batches | lr 0.000582 | ms/batch 22.22 | loss  6.00 | ppl   404.56\n",
            "| epoch   3 |  1600/ 2983 batches | lr 0.000575 | ms/batch 22.21 | loss  6.05 | ppl   424.91\n",
            "| epoch   3 |  1800/ 2983 batches | lr 0.000567 | ms/batch 22.21 | loss  5.96 | ppl   386.85\n",
            "| epoch   3 |  2000/ 2983 batches | lr 0.000560 | ms/batch 22.20 | loss  5.98 | ppl   395.33\n",
            "| epoch   3 |  2200/ 2983 batches | lr 0.000553 | ms/batch 22.20 | loss  5.92 | ppl   373.21\n",
            "| epoch   3 |  2400/ 2983 batches | lr 0.000547 | ms/batch 22.22 | loss  5.93 | ppl   375.65\n",
            "| epoch   3 |  2600/ 2983 batches | lr 0.000540 | ms/batch 22.23 | loss  5.96 | ppl   386.63\n",
            "| epoch   3 |  2800/ 2983 batches | lr 0.000534 | ms/batch 22.20 | loss  5.89 | ppl   362.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 68.56s | valid loss  6.41 | valid ppl   609.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000528\n",
            "| epoch   4 |   200/ 2983 batches | lr 0.000523 | ms/batch 22.31 | loss  5.95 | ppl   384.31\n",
            "| epoch   4 |   400/ 2983 batches | lr 0.000517 | ms/batch 22.24 | loss  5.94 | ppl   380.98\n",
            "| epoch   4 |   600/ 2983 batches | lr 0.000512 | ms/batch 22.31 | loss  5.84 | ppl   344.37\n",
            "| epoch   4 |   800/ 2983 batches | lr 0.000506 | ms/batch 22.22 | loss  5.87 | ppl   352.79\n",
            "| epoch   4 |  1000/ 2983 batches | lr 0.000501 | ms/batch 22.24 | loss  5.88 | ppl   356.60\n",
            "| epoch   4 |  1200/ 2983 batches | lr 0.000496 | ms/batch 22.19 | loss  5.87 | ppl   353.87\n",
            "| epoch   4 |  1400/ 2983 batches | lr 0.000491 | ms/batch 22.22 | loss  5.88 | ppl   356.81\n",
            "| epoch   4 |  1600/ 2983 batches | lr 0.000487 | ms/batch 22.20 | loss  5.94 | ppl   379.55\n",
            "| epoch   4 |  1800/ 2983 batches | lr 0.000482 | ms/batch 22.18 | loss  5.84 | ppl   344.63\n",
            "| epoch   4 |  2000/ 2983 batches | lr 0.000478 | ms/batch 22.20 | loss  5.87 | ppl   355.28\n",
            "| epoch   4 |  2200/ 2983 batches | lr 0.000473 | ms/batch 22.21 | loss  5.81 | ppl   333.82\n",
            "| epoch   4 |  2400/ 2983 batches | lr 0.000469 | ms/batch 22.18 | loss  5.82 | ppl   338.66\n",
            "| epoch   4 |  2600/ 2983 batches | lr 0.000465 | ms/batch 22.19 | loss  5.85 | ppl   348.20\n",
            "| epoch   4 |  2800/ 2983 batches | lr 0.000461 | ms/batch 22.17 | loss  5.79 | ppl   326.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 68.56s | valid loss  6.35 | valid ppl   572.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000458\n",
            "| epoch   5 |   200/ 2983 batches | lr 0.000454 | ms/batch 22.31 | loss  5.86 | ppl   349.75\n",
            "| epoch   5 |   400/ 2983 batches | lr 0.000450 | ms/batch 22.17 | loss  5.85 | ppl   346.94\n",
            "| epoch   5 |   600/ 2983 batches | lr 0.000447 | ms/batch 22.19 | loss  5.75 | ppl   314.64\n",
            "| epoch   5 |   800/ 2983 batches | lr 0.000443 | ms/batch 22.17 | loss  5.78 | ppl   322.96\n",
            "| epoch   5 |  1000/ 2983 batches | lr 0.000440 | ms/batch 22.18 | loss  5.79 | ppl   326.70\n",
            "| epoch   5 |  1200/ 2983 batches | lr 0.000436 | ms/batch 22.19 | loss  5.78 | ppl   325.03\n",
            "| epoch   5 |  1400/ 2983 batches | lr 0.000433 | ms/batch 22.17 | loss  5.80 | ppl   329.96\n",
            "| epoch   5 |  1600/ 2983 batches | lr 0.000430 | ms/batch 22.17 | loss  5.86 | ppl   349.69\n",
            "| epoch   5 |  1800/ 2983 batches | lr 0.000427 | ms/batch 22.20 | loss  5.77 | ppl   319.39\n",
            "| epoch   5 |  2000/ 2983 batches | lr 0.000424 | ms/batch 22.21 | loss  5.80 | ppl   329.43\n",
            "| epoch   5 |  2200/ 2983 batches | lr 0.000421 | ms/batch 22.21 | loss  5.73 | ppl   309.16\n",
            "| epoch   5 |  2400/ 2983 batches | lr 0.000418 | ms/batch 22.20 | loss  5.76 | ppl   315.85\n",
            "| epoch   5 |  2600/ 2983 batches | lr 0.000415 | ms/batch 22.20 | loss  5.79 | ppl   325.40\n",
            "| epoch   5 |  2800/ 2983 batches | lr 0.000412 | ms/batch 22.21 | loss  5.72 | ppl   305.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 68.50s | valid loss  6.31 | valid ppl   552.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000409\n",
            "| epoch   6 |   200/ 2983 batches | lr 0.000407 | ms/batch 22.33 | loss  5.79 | ppl   326.60\n",
            "| epoch   6 |   400/ 2983 batches | lr 0.000404 | ms/batch 22.22 | loss  5.78 | ppl   324.40\n",
            "| epoch   6 |   600/ 2983 batches | lr 0.000401 | ms/batch 22.25 | loss  5.68 | ppl   294.07\n",
            "| epoch   6 |   800/ 2983 batches | lr 0.000399 | ms/batch 22.22 | loss  5.72 | ppl   303.58\n",
            "| epoch   6 |  1000/ 2983 batches | lr 0.000396 | ms/batch 22.21 | loss  5.73 | ppl   307.70\n",
            "| epoch   6 |  1200/ 2983 batches | lr 0.000394 | ms/batch 22.22 | loss  5.73 | ppl   306.54\n",
            "| epoch   6 |  1400/ 2983 batches | lr 0.000391 | ms/batch 22.26 | loss  5.74 | ppl   312.52\n",
            "| epoch   6 |  1600/ 2983 batches | lr 0.000389 | ms/batch 22.19 | loss  5.80 | ppl   330.30\n",
            "| epoch   6 |  1800/ 2983 batches | lr 0.000387 | ms/batch 22.20 | loss  5.71 | ppl   302.74\n",
            "| epoch   6 |  2000/ 2983 batches | lr 0.000384 | ms/batch 22.20 | loss  5.74 | ppl   311.45\n",
            "| epoch   6 |  2200/ 2983 batches | lr 0.000382 | ms/batch 22.20 | loss  5.68 | ppl   293.87\n",
            "| epoch   6 |  2400/ 2983 batches | lr 0.000380 | ms/batch 22.20 | loss  5.70 | ppl   299.38\n",
            "| epoch   6 |  2600/ 2983 batches | lr 0.000378 | ms/batch 22.21 | loss  5.73 | ppl   308.22\n",
            "| epoch   6 |  2800/ 2983 batches | lr 0.000376 | ms/batch 22.21 | loss  5.67 | ppl   290.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 68.57s | valid loss  6.29 | valid ppl   539.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000374\n",
            "| epoch   7 |   200/ 2983 batches | lr 0.000372 | ms/batch 22.29 | loss  5.74 | ppl   309.91\n",
            "| epoch   7 |   400/ 2983 batches | lr 0.000370 | ms/batch 22.21 | loss  5.73 | ppl   309.35\n",
            "| epoch   7 |   600/ 2983 batches | lr 0.000368 | ms/batch 22.20 | loss  5.63 | ppl   279.47\n",
            "| epoch   7 |   800/ 2983 batches | lr 0.000366 | ms/batch 22.18 | loss  5.67 | ppl   290.44\n",
            "| epoch   7 |  1000/ 2983 batches | lr 0.000364 | ms/batch 22.22 | loss  5.69 | ppl   295.40\n",
            "| epoch   7 |  1200/ 2983 batches | lr 0.000362 | ms/batch 22.18 | loss  5.68 | ppl   293.24\n",
            "| epoch   7 |  1400/ 2983 batches | lr 0.000360 | ms/batch 22.18 | loss  5.70 | ppl   298.56\n",
            "| epoch   7 |  1600/ 2983 batches | lr 0.000358 | ms/batch 22.22 | loss  5.76 | ppl   318.14\n",
            "| epoch   7 |  1800/ 2983 batches | lr 0.000356 | ms/batch 22.19 | loss  5.67 | ppl   290.44\n",
            "| epoch   7 |  2000/ 2983 batches | lr 0.000354 | ms/batch 22.23 | loss  5.70 | ppl   300.30\n",
            "| epoch   7 |  2200/ 2983 batches | lr 0.000353 | ms/batch 22.19 | loss  5.64 | ppl   281.34\n",
            "| epoch   7 |  2400/ 2983 batches | lr 0.000351 | ms/batch 22.20 | loss  5.66 | ppl   287.54\n",
            "| epoch   7 |  2600/ 2983 batches | lr 0.000349 | ms/batch 22.21 | loss  5.69 | ppl   297.29\n",
            "| epoch   7 |  2800/ 2983 batches | lr 0.000347 | ms/batch 22.18 | loss  5.63 | ppl   279.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 68.51s | valid loss  6.29 | valid ppl   537.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000346\n",
            "| epoch   8 |   200/ 2983 batches | lr 0.000344 | ms/batch 22.29 | loss  5.70 | ppl   299.06\n",
            "| epoch   8 |   400/ 2983 batches | lr 0.000343 | ms/batch 22.20 | loss  5.70 | ppl   298.04\n",
            "| epoch   8 |   600/ 2983 batches | lr 0.000341 | ms/batch 22.22 | loss  5.60 | ppl   269.59\n",
            "| epoch   8 |   800/ 2983 batches | lr 0.000340 | ms/batch 22.18 | loss  5.63 | ppl   280.03\n",
            "| epoch   8 |  1000/ 2983 batches | lr 0.000338 | ms/batch 22.17 | loss  5.65 | ppl   285.31\n",
            "| epoch   8 |  1200/ 2983 batches | lr 0.000336 | ms/batch 22.17 | loss  5.64 | ppl   282.45\n",
            "| epoch   8 |  1400/ 2983 batches | lr 0.000335 | ms/batch 22.17 | loss  5.66 | ppl   288.04\n",
            "| epoch   8 |  1600/ 2983 batches | lr 0.000333 | ms/batch 22.19 | loss  5.73 | ppl   306.90\n",
            "| epoch   8 |  1800/ 2983 batches | lr 0.000332 | ms/batch 22.22 | loss  5.64 | ppl   280.66\n",
            "| epoch   8 |  2000/ 2983 batches | lr 0.000330 | ms/batch 22.23 | loss  5.67 | ppl   291.35\n",
            "| epoch   8 |  2200/ 2983 batches | lr 0.000329 | ms/batch 22.22 | loss  5.60 | ppl   271.59\n",
            "| epoch   8 |  2400/ 2983 batches | lr 0.000328 | ms/batch 22.17 | loss  5.63 | ppl   278.38\n",
            "| epoch   8 |  2600/ 2983 batches | lr 0.000326 | ms/batch 22.18 | loss  5.66 | ppl   288.05\n",
            "| epoch   8 |  2800/ 2983 batches | lr 0.000325 | ms/batch 22.21 | loss  5.60 | ppl   270.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 68.51s | valid loss  6.27 | valid ppl   528.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000324\n",
            "| epoch   9 |   200/ 2983 batches | lr 0.000322 | ms/batch 22.34 | loss  5.67 | ppl   289.68\n",
            "| epoch   9 |   400/ 2983 batches | lr 0.000321 | ms/batch 22.21 | loss  5.66 | ppl   288.11\n",
            "| epoch   9 |   600/ 2983 batches | lr 0.000320 | ms/batch 22.20 | loss  5.56 | ppl   260.85\n",
            "| epoch   9 |   800/ 2983 batches | lr 0.000318 | ms/batch 22.23 | loss  5.61 | ppl   272.01\n",
            "| epoch   9 |  1000/ 2983 batches | lr 0.000317 | ms/batch 22.19 | loss  5.62 | ppl   276.36\n",
            "| epoch   9 |  1200/ 2983 batches | lr 0.000316 | ms/batch 22.19 | loss  5.62 | ppl   274.76\n",
            "| epoch   9 |  1400/ 2983 batches | lr 0.000315 | ms/batch 22.24 | loss  5.64 | ppl   280.17\n",
            "| epoch   9 |  1600/ 2983 batches | lr 0.000313 | ms/batch 22.20 | loss  5.70 | ppl   298.57\n",
            "| epoch   9 |  1800/ 2983 batches | lr 0.000312 | ms/batch 22.23 | loss  5.61 | ppl   272.92\n",
            "| epoch   9 |  2000/ 2983 batches | lr 0.000311 | ms/batch 22.20 | loss  5.65 | ppl   283.66\n",
            "| epoch   9 |  2200/ 2983 batches | lr 0.000310 | ms/batch 22.21 | loss  5.57 | ppl   263.44\n",
            "| epoch   9 |  2400/ 2983 batches | lr 0.000308 | ms/batch 22.21 | loss  5.60 | ppl   270.01\n",
            "| epoch   9 |  2600/ 2983 batches | lr 0.000307 | ms/batch 22.20 | loss  5.64 | ppl   280.97\n",
            "| epoch   9 |  2800/ 2983 batches | lr 0.000306 | ms/batch 22.21 | loss  5.57 | ppl   263.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 68.55s | valid loss  6.26 | valid ppl   524.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000305\n",
            "| epoch  10 |   200/ 2983 batches | lr 0.000304 | ms/batch 22.30 | loss  5.64 | ppl   281.46\n",
            "| epoch  10 |   400/ 2983 batches | lr 0.000303 | ms/batch 22.22 | loss  5.64 | ppl   282.37\n",
            "| epoch  10 |   600/ 2983 batches | lr 0.000302 | ms/batch 22.19 | loss  5.54 | ppl   255.03\n",
            "| epoch  10 |   800/ 2983 batches | lr 0.000301 | ms/batch 22.18 | loss  5.58 | ppl   265.27\n",
            "| epoch  10 |  1000/ 2983 batches | lr 0.000300 | ms/batch 22.21 | loss  5.60 | ppl   270.45\n",
            "| epoch  10 |  1200/ 2983 batches | lr 0.000299 | ms/batch 22.19 | loss  5.59 | ppl   267.24\n",
            "| epoch  10 |  1400/ 2983 batches | lr 0.000297 | ms/batch 22.20 | loss  5.61 | ppl   273.76\n",
            "| epoch  10 |  1600/ 2983 batches | lr 0.000296 | ms/batch 22.20 | loss  5.67 | ppl   290.81\n",
            "| epoch  10 |  1800/ 2983 batches | lr 0.000295 | ms/batch 22.18 | loss  5.59 | ppl   267.41\n",
            "| epoch  10 |  2000/ 2983 batches | lr 0.000294 | ms/batch 22.18 | loss  5.62 | ppl   277.10\n",
            "| epoch  10 |  2200/ 2983 batches | lr 0.000293 | ms/batch 22.19 | loss  5.55 | ppl   257.86\n",
            "| epoch  10 |  2400/ 2983 batches | lr 0.000292 | ms/batch 22.20 | loss  5.57 | ppl   263.13\n",
            "| epoch  10 |  2600/ 2983 batches | lr 0.000291 | ms/batch 22.22 | loss  5.61 | ppl   274.20\n",
            "| epoch  10 |  2800/ 2983 batches | lr 0.000290 | ms/batch 22.20 | loss  5.55 | ppl   257.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 68.51s | valid loss  6.25 | valid ppl   517.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000289\n",
            "| epoch  11 |   200/ 2983 batches | lr 0.000288 | ms/batch 22.29 | loss  5.62 | ppl   275.22\n",
            "| epoch  11 |   400/ 2983 batches | lr 0.000288 | ms/batch 22.17 | loss  5.62 | ppl   274.83\n",
            "| epoch  11 |   600/ 2983 batches | lr 0.000287 | ms/batch 22.20 | loss  5.52 | ppl   248.81\n",
            "| epoch  11 |   800/ 2983 batches | lr 0.000286 | ms/batch 22.17 | loss  5.56 | ppl   258.58\n",
            "| epoch  11 |  1000/ 2983 batches | lr 0.000285 | ms/batch 22.17 | loss  5.58 | ppl   264.75\n",
            "| epoch  11 |  1200/ 2983 batches | lr 0.000284 | ms/batch 22.17 | loss  5.57 | ppl   262.38\n",
            "| epoch  11 |  1400/ 2983 batches | lr 0.000283 | ms/batch 22.21 | loss  5.59 | ppl   268.61\n",
            "| epoch  11 |  1600/ 2983 batches | lr 0.000282 | ms/batch 22.19 | loss  5.65 | ppl   285.50\n",
            "| epoch  11 |  1800/ 2983 batches | lr 0.000281 | ms/batch 22.20 | loss  5.57 | ppl   261.58\n",
            "| epoch  11 |  2000/ 2983 batches | lr 0.000280 | ms/batch 22.23 | loss  5.60 | ppl   270.32\n",
            "| epoch  11 |  2200/ 2983 batches | lr 0.000279 | ms/batch 22.27 | loss  5.53 | ppl   252.65\n",
            "| epoch  11 |  2400/ 2983 batches | lr 0.000278 | ms/batch 22.20 | loss  5.56 | ppl   258.75\n",
            "| epoch  11 |  2600/ 2983 batches | lr 0.000278 | ms/batch 22.18 | loss  5.59 | ppl   268.52\n",
            "| epoch  11 |  2800/ 2983 batches | lr 0.000277 | ms/batch 22.21 | loss  5.53 | ppl   252.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 68.51s | valid loss  6.25 | valid ppl   517.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000276\n",
            "| epoch  12 |   200/ 2983 batches | lr 0.000275 | ms/batch 22.32 | loss  5.60 | ppl   269.36\n",
            "| epoch  12 |   400/ 2983 batches | lr 0.000274 | ms/batch 22.20 | loss  5.60 | ppl   269.55\n",
            "| epoch  12 |   600/ 2983 batches | lr 0.000273 | ms/batch 22.20 | loss  5.50 | ppl   244.34\n",
            "| epoch  12 |   800/ 2983 batches | lr 0.000273 | ms/batch 22.18 | loss  5.54 | ppl   253.94\n",
            "| epoch  12 |  1000/ 2983 batches | lr 0.000272 | ms/batch 22.18 | loss  5.56 | ppl   259.83\n",
            "| epoch  12 |  1200/ 2983 batches | lr 0.000271 | ms/batch 22.17 | loss  5.55 | ppl   257.57\n",
            "| epoch  12 |  1400/ 2983 batches | lr 0.000270 | ms/batch 22.17 | loss  5.57 | ppl   262.61\n",
            "| epoch  12 |  1600/ 2983 batches | lr 0.000269 | ms/batch 22.17 | loss  5.64 | ppl   280.07\n",
            "| epoch  12 |  1800/ 2983 batches | lr 0.000269 | ms/batch 22.21 | loss  5.55 | ppl   256.29\n",
            "| epoch  12 |  2000/ 2983 batches | lr 0.000268 | ms/batch 22.19 | loss  5.59 | ppl   266.89\n",
            "| epoch  12 |  2200/ 2983 batches | lr 0.000267 | ms/batch 22.20 | loss  5.51 | ppl   248.31\n",
            "| epoch  12 |  2400/ 2983 batches | lr 0.000266 | ms/batch 22.22 | loss  5.54 | ppl   253.61\n",
            "| epoch  12 |  2600/ 2983 batches | lr 0.000266 | ms/batch 22.22 | loss  5.57 | ppl   263.45\n",
            "| epoch  12 |  2800/ 2983 batches | lr 0.000265 | ms/batch 22.19 | loss  5.51 | ppl   248.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 68.50s | valid loss  6.24 | valid ppl   513.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000264\n",
            "| epoch  13 |   200/ 2983 batches | lr 0.000263 | ms/batch 22.31 | loss  5.58 | ppl   264.82\n",
            "| epoch  13 |   400/ 2983 batches | lr 0.000263 | ms/batch 22.21 | loss  5.58 | ppl   264.97\n",
            "| epoch  13 |   600/ 2983 batches | lr 0.000262 | ms/batch 22.21 | loss  5.48 | ppl   240.40\n",
            "| epoch  13 |   800/ 2983 batches | lr 0.000261 | ms/batch 22.21 | loss  5.52 | ppl   250.51\n",
            "| epoch  13 |  1000/ 2983 batches | lr 0.000261 | ms/batch 22.22 | loss  5.54 | ppl   255.83\n",
            "| epoch  13 |  1200/ 2983 batches | lr 0.000260 | ms/batch 22.19 | loss  5.53 | ppl   253.06\n",
            "| epoch  13 |  1400/ 2983 batches | lr 0.000259 | ms/batch 22.23 | loss  5.56 | ppl   259.01\n",
            "| epoch  13 |  1600/ 2983 batches | lr 0.000259 | ms/batch 22.20 | loss  5.62 | ppl   275.01\n",
            "| epoch  13 |  1800/ 2983 batches | lr 0.000258 | ms/batch 22.18 | loss  5.53 | ppl   252.05\n",
            "| epoch  13 |  2000/ 2983 batches | lr 0.000257 | ms/batch 22.21 | loss  5.57 | ppl   263.00\n",
            "| epoch  13 |  2200/ 2983 batches | lr 0.000256 | ms/batch 22.19 | loss  5.50 | ppl   243.76\n",
            "| epoch  13 |  2400/ 2983 batches | lr 0.000256 | ms/batch 22.20 | loss  5.52 | ppl   248.95\n",
            "| epoch  13 |  2600/ 2983 batches | lr 0.000255 | ms/batch 22.21 | loss  5.56 | ppl   259.72\n",
            "| epoch  13 |  2800/ 2983 batches | lr 0.000254 | ms/batch 22.19 | loss  5.50 | ppl   244.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 68.54s | valid loss  6.23 | valid ppl   508.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000254\n",
            "| epoch  14 |   200/ 2983 batches | lr 0.000253 | ms/batch 22.28 | loss  5.56 | ppl   259.95\n",
            "| epoch  14 |   400/ 2983 batches | lr 0.000253 | ms/batch 22.18 | loss  5.57 | ppl   261.34\n",
            "| epoch  14 |   600/ 2983 batches | lr 0.000252 | ms/batch 22.19 | loss  5.47 | ppl   236.73\n",
            "| epoch  14 |   800/ 2983 batches | lr 0.000251 | ms/batch 22.20 | loss  5.50 | ppl   245.80\n",
            "| epoch  14 |  1000/ 2983 batches | lr 0.000251 | ms/batch 22.19 | loss  5.52 | ppl   250.78\n",
            "| epoch  14 |  1200/ 2983 batches | lr 0.000250 | ms/batch 22.17 | loss  5.52 | ppl   248.77\n",
            "| epoch  14 |  1400/ 2983 batches | lr 0.000249 | ms/batch 22.17 | loss  5.54 | ppl   254.82\n",
            "| epoch  14 |  1600/ 2983 batches | lr 0.000249 | ms/batch 22.19 | loss  5.60 | ppl   271.17\n",
            "| epoch  14 |  1800/ 2983 batches | lr 0.000248 | ms/batch 22.16 | loss  5.52 | ppl   249.09\n",
            "| epoch  14 |  2000/ 2983 batches | lr 0.000248 | ms/batch 22.18 | loss  5.55 | ppl   257.55\n",
            "| epoch  14 |  2200/ 2983 batches | lr 0.000247 | ms/batch 22.20 | loss  5.48 | ppl   239.87\n",
            "| epoch  14 |  2400/ 2983 batches | lr 0.000246 | ms/batch 22.17 | loss  5.50 | ppl   245.26\n",
            "| epoch  14 |  2600/ 2983 batches | lr 0.000246 | ms/batch 22.19 | loss  5.54 | ppl   255.67\n",
            "| epoch  14 |  2800/ 2983 batches | lr 0.000245 | ms/batch 22.21 | loss  5.48 | ppl   240.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 68.48s | valid loss  6.24 | valid ppl   511.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000245\n",
            "| epoch  15 |   200/ 2983 batches | lr 0.000244 | ms/batch 22.33 | loss  5.54 | ppl   255.18\n",
            "| epoch  15 |   400/ 2983 batches | lr 0.000243 | ms/batch 22.21 | loss  5.55 | ppl   256.21\n",
            "| epoch  15 |   600/ 2983 batches | lr 0.000243 | ms/batch 22.21 | loss  5.45 | ppl   232.11\n",
            "| epoch  15 |   800/ 2983 batches | lr 0.000242 | ms/batch 22.18 | loss  5.49 | ppl   242.16\n",
            "| epoch  15 |  1000/ 2983 batches | lr 0.000242 | ms/batch 22.20 | loss  5.51 | ppl   247.27\n",
            "| epoch  15 |  1200/ 2983 batches | lr 0.000241 | ms/batch 22.17 | loss  5.50 | ppl   244.35\n",
            "| epoch  15 |  1400/ 2983 batches | lr 0.000241 | ms/batch 22.19 | loss  5.52 | ppl   250.23\n",
            "| epoch  15 |  1600/ 2983 batches | lr 0.000240 | ms/batch 22.17 | loss  5.59 | ppl   266.43\n",
            "| epoch  15 |  1800/ 2983 batches | lr 0.000240 | ms/batch 22.20 | loss  5.50 | ppl   245.13\n",
            "| epoch  15 |  2000/ 2983 batches | lr 0.000239 | ms/batch 22.19 | loss  5.54 | ppl   254.39\n",
            "| epoch  15 |  2200/ 2983 batches | lr 0.000238 | ms/batch 22.20 | loss  5.46 | ppl   235.96\n",
            "| epoch  15 |  2400/ 2983 batches | lr 0.000238 | ms/batch 22.19 | loss  5.49 | ppl   241.54\n",
            "| epoch  15 |  2600/ 2983 batches | lr 0.000237 | ms/batch 22.19 | loss  5.53 | ppl   252.56\n",
            "| epoch  15 |  2800/ 2983 batches | lr 0.000237 | ms/batch 22.20 | loss  5.47 | ppl   238.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 68.51s | valid loss  6.23 | valid ppl   507.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000236\n",
            "| epoch  16 |   200/ 2983 batches | lr 0.000236 | ms/batch 22.30 | loss  5.53 | ppl   253.40\n",
            "| epoch  16 |   400/ 2983 batches | lr 0.000235 | ms/batch 22.23 | loss  5.53 | ppl   253.37\n",
            "| epoch  16 |   600/ 2983 batches | lr 0.000235 | ms/batch 22.20 | loss  5.44 | ppl   229.37\n",
            "| epoch  16 |   800/ 2983 batches | lr 0.000234 | ms/batch 22.18 | loss  5.48 | ppl   239.34\n",
            "| epoch  16 |  1000/ 2983 batches | lr 0.000234 | ms/batch 22.20 | loss  5.50 | ppl   244.36\n",
            "| epoch  16 |  1200/ 2983 batches | lr 0.000233 | ms/batch 22.17 | loss  5.49 | ppl   242.06\n",
            "| epoch  16 |  1400/ 2983 batches | lr 0.000233 | ms/batch 22.21 | loss  5.51 | ppl   247.70\n",
            "| epoch  16 |  1600/ 2983 batches | lr 0.000232 | ms/batch 22.19 | loss  5.57 | ppl   263.26\n",
            "| epoch  16 |  1800/ 2983 batches | lr 0.000232 | ms/batch 22.20 | loss  5.49 | ppl   242.78\n",
            "| epoch  16 |  2000/ 2983 batches | lr 0.000231 | ms/batch 22.20 | loss  5.53 | ppl   251.20\n",
            "| epoch  16 |  2200/ 2983 batches | lr 0.000231 | ms/batch 22.18 | loss  5.45 | ppl   233.59\n",
            "| epoch  16 |  2400/ 2983 batches | lr 0.000230 | ms/batch 22.19 | loss  5.47 | ppl   238.33\n",
            "| epoch  16 |  2600/ 2983 batches | lr 0.000230 | ms/batch 22.22 | loss  5.52 | ppl   249.10\n",
            "| epoch  16 |  2800/ 2983 batches | lr 0.000229 | ms/batch 22.19 | loss  5.46 | ppl   234.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 68.50s | valid loss  6.23 | valid ppl   509.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000229\n",
            "| epoch  17 |   200/ 2983 batches | lr 0.000228 | ms/batch 22.30 | loss  5.52 | ppl   249.19\n",
            "| epoch  17 |   400/ 2983 batches | lr 0.000228 | ms/batch 22.18 | loss  5.52 | ppl   250.01\n",
            "| epoch  17 |   600/ 2983 batches | lr 0.000227 | ms/batch 22.21 | loss  5.42 | ppl   226.56\n",
            "| epoch  17 |   800/ 2983 batches | lr 0.000227 | ms/batch 22.18 | loss  5.47 | ppl   236.37\n",
            "| epoch  17 |  1000/ 2983 batches | lr 0.000226 | ms/batch 22.21 | loss  5.49 | ppl   241.62\n",
            "| epoch  17 |  1200/ 2983 batches | lr 0.000226 | ms/batch 22.21 | loss  5.47 | ppl   238.61\n",
            "| epoch  17 |  1400/ 2983 batches | lr 0.000226 | ms/batch 22.20 | loss  5.50 | ppl   244.09\n",
            "| epoch  17 |  1600/ 2983 batches | lr 0.000225 | ms/batch 22.24 | loss  5.56 | ppl   260.30\n",
            "| epoch  17 |  1800/ 2983 batches | lr 0.000225 | ms/batch 22.18 | loss  5.48 | ppl   239.79\n",
            "| epoch  17 |  2000/ 2983 batches | lr 0.000224 | ms/batch 22.18 | loss  5.51 | ppl   248.12\n",
            "| epoch  17 |  2200/ 2983 batches | lr 0.000224 | ms/batch 22.21 | loss  5.44 | ppl   230.96\n",
            "| epoch  17 |  2400/ 2983 batches | lr 0.000223 | ms/batch 22.19 | loss  5.46 | ppl   235.88\n",
            "| epoch  17 |  2600/ 2983 batches | lr 0.000223 | ms/batch 22.19 | loss  5.51 | ppl   246.60\n",
            "| epoch  17 |  2800/ 2983 batches | lr 0.000222 | ms/batch 22.22 | loss  5.44 | ppl   231.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 68.52s | valid loss  6.23 | valid ppl   508.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000222\n",
            "| epoch  18 |   200/ 2983 batches | lr 0.000222 | ms/batch 22.31 | loss  5.51 | ppl   246.52\n",
            "| epoch  18 |   400/ 2983 batches | lr 0.000221 | ms/batch 22.20 | loss  5.51 | ppl   247.18\n",
            "| epoch  18 |   600/ 2983 batches | lr 0.000221 | ms/batch 22.21 | loss  5.41 | ppl   223.85\n",
            "| epoch  18 |   800/ 2983 batches | lr 0.000220 | ms/batch 22.21 | loss  5.46 | ppl   234.05\n",
            "| epoch  18 |  1000/ 2983 batches | lr 0.000220 | ms/batch 22.17 | loss  5.48 | ppl   238.97\n",
            "| epoch  18 |  1200/ 2983 batches | lr 0.000219 | ms/batch 22.19 | loss  5.46 | ppl   235.64\n",
            "| epoch  18 |  1400/ 2983 batches | lr 0.000219 | ms/batch 22.21 | loss  5.49 | ppl   241.39\n",
            "| epoch  18 |  1600/ 2983 batches | lr 0.000219 | ms/batch 22.17 | loss  5.55 | ppl   256.53\n",
            "| epoch  18 |  1800/ 2983 batches | lr 0.000218 | ms/batch 22.20 | loss  5.47 | ppl   236.74\n",
            "| epoch  18 |  2000/ 2983 batches | lr 0.000218 | ms/batch 22.20 | loss  5.51 | ppl   246.20\n",
            "| epoch  18 |  2200/ 2983 batches | lr 0.000217 | ms/batch 22.18 | loss  5.43 | ppl   227.85\n",
            "| epoch  18 |  2400/ 2983 batches | lr 0.000217 | ms/batch 22.19 | loss  5.45 | ppl   232.81\n",
            "| epoch  18 |  2600/ 2983 batches | lr 0.000217 | ms/batch 22.19 | loss  5.50 | ppl   243.70\n",
            "| epoch  18 |  2800/ 2983 batches | lr 0.000216 | ms/batch 22.20 | loss  5.43 | ppl   229.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 68.50s | valid loss  6.23 | valid ppl   509.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000216\n",
            "| epoch  19 |   200/ 2983 batches | lr 0.000215 | ms/batch 22.30 | loss  5.49 | ppl   243.42\n",
            "| epoch  19 |   400/ 2983 batches | lr 0.000215 | ms/batch 22.23 | loss  5.50 | ppl   244.91\n",
            "| epoch  19 |   600/ 2983 batches | lr 0.000215 | ms/batch 22.20 | loss  5.40 | ppl   221.95\n",
            "| epoch  19 |   800/ 2983 batches | lr 0.000214 | ms/batch 22.19 | loss  5.44 | ppl   231.20\n",
            "| epoch  19 |  1000/ 2983 batches | lr 0.000214 | ms/batch 22.18 | loss  5.47 | ppl   236.67\n",
            "| epoch  19 |  1200/ 2983 batches | lr 0.000213 | ms/batch 22.18 | loss  5.45 | ppl   232.96\n",
            "| epoch  19 |  1400/ 2983 batches | lr 0.000213 | ms/batch 22.17 | loss  5.48 | ppl   239.36\n",
            "| epoch  19 |  1600/ 2983 batches | lr 0.000213 | ms/batch 22.21 | loss  5.54 | ppl   254.15\n",
            "| epoch  19 |  1800/ 2983 batches | lr 0.000212 | ms/batch 22.22 | loss  5.46 | ppl   234.23\n",
            "| epoch  19 |  2000/ 2983 batches | lr 0.000212 | ms/batch 22.19 | loss  5.49 | ppl   242.84\n",
            "| epoch  19 |  2200/ 2983 batches | lr 0.000211 | ms/batch 22.18 | loss  5.42 | ppl   225.61\n",
            "| epoch  19 |  2400/ 2983 batches | lr 0.000211 | ms/batch 22.19 | loss  5.44 | ppl   231.11\n",
            "| epoch  19 |  2600/ 2983 batches | lr 0.000211 | ms/batch 22.21 | loss  5.49 | ppl   241.06\n",
            "| epoch  19 |  2800/ 2983 batches | lr 0.000210 | ms/batch 22.18 | loss  5.43 | ppl   227.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 68.54s | valid loss  6.23 | valid ppl   505.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000210\n",
            "| epoch  20 |   200/ 2983 batches | lr 0.000210 | ms/batch 22.30 | loss  5.48 | ppl   240.67\n",
            "| epoch  20 |   400/ 2983 batches | lr 0.000209 | ms/batch 22.20 | loss  5.49 | ppl   242.21\n",
            "| epoch  20 |   600/ 2983 batches | lr 0.000209 | ms/batch 22.23 | loss  5.39 | ppl   219.20\n",
            "| epoch  20 |   800/ 2983 batches | lr 0.000209 | ms/batch 22.18 | loss  5.43 | ppl   228.91\n",
            "| epoch  20 |  1000/ 2983 batches | lr 0.000208 | ms/batch 22.18 | loss  5.46 | ppl   234.13\n",
            "| epoch  20 |  1200/ 2983 batches | lr 0.000208 | ms/batch 22.19 | loss  5.44 | ppl   230.91\n",
            "| epoch  20 |  1400/ 2983 batches | lr 0.000207 | ms/batch 22.18 | loss  5.47 | ppl   236.67\n",
            "| epoch  20 |  1600/ 2983 batches | lr 0.000207 | ms/batch 22.18 | loss  5.53 | ppl   251.47\n",
            "| epoch  20 |  1800/ 2983 batches | lr 0.000207 | ms/batch 22.18 | loss  5.44 | ppl   231.55\n",
            "| epoch  20 |  2000/ 2983 batches | lr 0.000206 | ms/batch 22.18 | loss  5.48 | ppl   240.93\n",
            "| epoch  20 |  2200/ 2983 batches | lr 0.000206 | ms/batch 22.20 | loss  5.41 | ppl   223.38\n",
            "| epoch  20 |  2400/ 2983 batches | lr 0.000206 | ms/batch 22.21 | loss  5.43 | ppl   227.77\n",
            "| epoch  20 |  2600/ 2983 batches | lr 0.000205 | ms/batch 22.21 | loss  5.48 | ppl   239.01\n",
            "| epoch  20 |  2800/ 2983 batches | lr 0.000205 | ms/batch 22.23 | loss  5.41 | ppl   224.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 68.52s | valid loss  6.23 | valid ppl   509.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000205\n",
            "| epoch  21 |   200/ 2983 batches | lr 0.000204 | ms/batch 22.34 | loss  5.47 | ppl   237.57\n",
            "| epoch  21 |   400/ 2983 batches | lr 0.000204 | ms/batch 22.22 | loss  5.48 | ppl   239.34\n",
            "| epoch  21 |   600/ 2983 batches | lr 0.000204 | ms/batch 22.22 | loss  5.38 | ppl   217.13\n",
            "| epoch  21 |   800/ 2983 batches | lr 0.000203 | ms/batch 22.22 | loss  5.42 | ppl   226.24\n",
            "| epoch  21 |  1000/ 2983 batches | lr 0.000203 | ms/batch 22.20 | loss  5.45 | ppl   231.61\n",
            "| epoch  21 |  1200/ 2983 batches | lr 0.000203 | ms/batch 22.20 | loss  5.43 | ppl   228.31\n",
            "| epoch  21 |  1400/ 2983 batches | lr 0.000202 | ms/batch 22.21 | loss  5.46 | ppl   234.41\n",
            "| epoch  21 |  1600/ 2983 batches | lr 0.000202 | ms/batch 22.19 | loss  5.52 | ppl   248.69\n",
            "| epoch  21 |  1800/ 2983 batches | lr 0.000202 | ms/batch 22.20 | loss  5.43 | ppl   228.89\n",
            "| epoch  21 |  2000/ 2983 batches | lr 0.000201 | ms/batch 22.20 | loss  5.47 | ppl   238.25\n",
            "| epoch  21 |  2200/ 2983 batches | lr 0.000201 | ms/batch 22.22 | loss  5.40 | ppl   221.00\n",
            "| epoch  21 |  2400/ 2983 batches | lr 0.000201 | ms/batch 22.21 | loss  5.42 | ppl   226.23\n",
            "| epoch  21 |  2600/ 2983 batches | lr 0.000200 | ms/batch 22.20 | loss  5.46 | ppl   235.82\n",
            "| epoch  21 |  2800/ 2983 batches | lr 0.000200 | ms/batch 22.20 | loss  5.40 | ppl   221.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 68.55s | valid loss  6.23 | valid ppl   510.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000200\n",
            "| epoch  22 |   200/ 2983 batches | lr 0.000199 | ms/batch 22.27 | loss  5.46 | ppl   235.33\n",
            "| epoch  22 |   400/ 2983 batches | lr 0.000199 | ms/batch 22.21 | loss  5.47 | ppl   237.22\n",
            "| epoch  22 |   600/ 2983 batches | lr 0.000199 | ms/batch 22.21 | loss  5.37 | ppl   215.11\n",
            "| epoch  22 |   800/ 2983 batches | lr 0.000198 | ms/batch 22.18 | loss  5.41 | ppl   223.92\n",
            "| epoch  22 |  1000/ 2983 batches | lr 0.000198 | ms/batch 22.19 | loss  5.44 | ppl   229.56\n",
            "| epoch  22 |  1200/ 2983 batches | lr 0.000198 | ms/batch 22.18 | loss  5.42 | ppl   226.69\n",
            "| epoch  22 |  1400/ 2983 batches | lr 0.000198 | ms/batch 22.18 | loss  5.45 | ppl   231.78\n",
            "| epoch  22 |  1600/ 2983 batches | lr 0.000197 | ms/batch 22.18 | loss  5.51 | ppl   246.35\n",
            "| epoch  22 |  1800/ 2983 batches | lr 0.000197 | ms/batch 22.18 | loss  5.43 | ppl   227.49\n",
            "| epoch  22 |  2000/ 2983 batches | lr 0.000197 | ms/batch 22.17 | loss  5.47 | ppl   236.30\n",
            "| epoch  22 |  2200/ 2983 batches | lr 0.000196 | ms/batch 22.17 | loss  5.39 | ppl   219.71\n",
            "| epoch  22 |  2400/ 2983 batches | lr 0.000196 | ms/batch 22.18 | loss  5.41 | ppl   224.22\n",
            "| epoch  22 |  2600/ 2983 batches | lr 0.000196 | ms/batch 22.19 | loss  5.45 | ppl   233.45\n",
            "| epoch  22 |  2800/ 2983 batches | lr 0.000195 | ms/batch 22.18 | loss  5.40 | ppl   220.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 68.48s | valid loss  6.23 | valid ppl   508.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000195\n",
            "| epoch  23 |   200/ 2983 batches | lr 0.000195 | ms/batch 22.32 | loss  5.45 | ppl   233.81\n",
            "| epoch  23 |   400/ 2983 batches | lr 0.000195 | ms/batch 22.18 | loss  5.46 | ppl   235.58\n",
            "| epoch  23 |   600/ 2983 batches | lr 0.000194 | ms/batch 22.18 | loss  5.36 | ppl   213.18\n",
            "| epoch  23 |   800/ 2983 batches | lr 0.000194 | ms/batch 22.18 | loss  5.40 | ppl   221.67\n",
            "| epoch  23 |  1000/ 2983 batches | lr 0.000194 | ms/batch 22.19 | loss  5.43 | ppl   227.15\n",
            "| epoch  23 |  1200/ 2983 batches | lr 0.000193 | ms/batch 22.19 | loss  5.42 | ppl   224.76\n",
            "| epoch  23 |  1400/ 2983 batches | lr 0.000193 | ms/batch 22.18 | loss  5.44 | ppl   229.55\n",
            "| epoch  23 |  1600/ 2983 batches | lr 0.000193 | ms/batch 22.20 | loss  5.50 | ppl   244.95\n",
            "| epoch  23 |  1800/ 2983 batches | lr 0.000193 | ms/batch 22.18 | loss  5.42 | ppl   225.19\n",
            "| epoch  23 |  2000/ 2983 batches | lr 0.000192 | ms/batch 22.18 | loss  5.46 | ppl   233.98\n",
            "| epoch  23 |  2200/ 2983 batches | lr 0.000192 | ms/batch 22.21 | loss  5.38 | ppl   217.32\n",
            "| epoch  23 |  2400/ 2983 batches | lr 0.000192 | ms/batch 22.21 | loss  5.40 | ppl   221.62\n",
            "| epoch  23 |  2600/ 2983 batches | lr 0.000191 | ms/batch 22.20 | loss  5.45 | ppl   232.12\n",
            "| epoch  23 |  2800/ 2983 batches | lr 0.000191 | ms/batch 22.19 | loss  5.38 | ppl   217.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 68.51s | valid loss  6.23 | valid ppl   508.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000191\n",
            "| epoch  24 |   200/ 2983 batches | lr 0.000191 | ms/batch 22.37 | loss  5.45 | ppl   231.83\n",
            "| epoch  24 |   400/ 2983 batches | lr 0.000190 | ms/batch 22.19 | loss  5.45 | ppl   233.12\n",
            "| epoch  24 |   600/ 2983 batches | lr 0.000190 | ms/batch 22.19 | loss  5.35 | ppl   210.48\n",
            "| epoch  24 |   800/ 2983 batches | lr 0.000190 | ms/batch 22.20 | loss  5.39 | ppl   220.09\n",
            "| epoch  24 |  1000/ 2983 batches | lr 0.000189 | ms/batch 22.19 | loss  5.42 | ppl   225.01\n",
            "| epoch  24 |  1200/ 2983 batches | lr 0.000189 | ms/batch 22.18 | loss  5.40 | ppl   222.29\n",
            "| epoch  24 |  1400/ 2983 batches | lr 0.000189 | ms/batch 22.19 | loss  5.43 | ppl   228.67\n",
            "| epoch  24 |  1600/ 2983 batches | lr 0.000189 | ms/batch 22.17 | loss  5.49 | ppl   242.61\n",
            "| epoch  24 |  1800/ 2983 batches | lr 0.000188 | ms/batch 22.19 | loss  5.41 | ppl   223.28\n",
            "| epoch  24 |  2000/ 2983 batches | lr 0.000188 | ms/batch 22.18 | loss  5.45 | ppl   232.24\n",
            "| epoch  24 |  2200/ 2983 batches | lr 0.000188 | ms/batch 22.18 | loss  5.37 | ppl   215.67\n",
            "| epoch  24 |  2400/ 2983 batches | lr 0.000188 | ms/batch 22.21 | loss  5.39 | ppl   219.62\n",
            "| epoch  24 |  2600/ 2983 batches | lr 0.000187 | ms/batch 22.18 | loss  5.44 | ppl   230.67\n",
            "| epoch  24 |  2800/ 2983 batches | lr 0.000187 | ms/batch 22.18 | loss  5.38 | ppl   216.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 68.50s | valid loss  6.24 | valid ppl   510.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000187\n",
            "| epoch  25 |   200/ 2983 batches | lr 0.000187 | ms/batch 22.30 | loss  5.44 | ppl   229.34\n",
            "| epoch  25 |   400/ 2983 batches | lr 0.000186 | ms/batch 22.19 | loss  5.44 | ppl   230.90\n",
            "| epoch  25 |   600/ 2983 batches | lr 0.000186 | ms/batch 22.17 | loss  5.34 | ppl   209.44\n",
            "| epoch  25 |   800/ 2983 batches | lr 0.000186 | ms/batch 22.15 | loss  5.39 | ppl   219.09\n",
            "| epoch  25 |  1000/ 2983 batches | lr 0.000186 | ms/batch 22.16 | loss  5.41 | ppl   223.16\n",
            "| epoch  25 |  1200/ 2983 batches | lr 0.000185 | ms/batch 22.17 | loss  5.40 | ppl   221.41\n",
            "| epoch  25 |  1400/ 2983 batches | lr 0.000185 | ms/batch 22.18 | loss  5.42 | ppl   226.39\n",
            "| epoch  25 |  1600/ 2983 batches | lr 0.000185 | ms/batch 22.20 | loss  5.48 | ppl   240.44\n",
            "| epoch  25 |  1800/ 2983 batches | lr 0.000185 | ms/batch 22.19 | loss  5.40 | ppl   222.49\n",
            "| epoch  25 |  2000/ 2983 batches | lr 0.000184 | ms/batch 22.19 | loss  5.44 | ppl   230.96\n",
            "| epoch  25 |  2200/ 2983 batches | lr 0.000184 | ms/batch 22.21 | loss  5.36 | ppl   213.69\n",
            "| epoch  25 |  2400/ 2983 batches | lr 0.000184 | ms/batch 22.19 | loss  5.39 | ppl   218.42\n",
            "| epoch  25 |  2600/ 2983 batches | lr 0.000184 | ms/batch 22.20 | loss  5.43 | ppl   228.25\n",
            "| epoch  25 |  2800/ 2983 batches | lr 0.000183 | ms/batch 22.18 | loss  5.37 | ppl   214.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 68.48s | valid loss  6.24 | valid ppl   511.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000183\n",
            "| epoch  26 |   200/ 2983 batches | lr 0.000183 | ms/batch 22.29 | loss  5.43 | ppl   228.26\n",
            "| epoch  26 |   400/ 2983 batches | lr 0.000183 | ms/batch 22.18 | loss  5.44 | ppl   229.67\n",
            "| epoch  26 |   600/ 2983 batches | lr 0.000182 | ms/batch 22.19 | loss  5.34 | ppl   207.59\n",
            "| epoch  26 |   800/ 2983 batches | lr 0.000182 | ms/batch 22.18 | loss  5.38 | ppl   216.49\n",
            "| epoch  26 |  1000/ 2983 batches | lr 0.000182 | ms/batch 22.19 | loss  5.40 | ppl   221.35\n",
            "| epoch  26 |  1200/ 2983 batches | lr 0.000182 | ms/batch 22.18 | loss  5.39 | ppl   218.74\n",
            "| epoch  26 |  1400/ 2983 batches | lr 0.000181 | ms/batch 22.17 | loss  5.41 | ppl   224.28\n",
            "| epoch  26 |  1600/ 2983 batches | lr 0.000181 | ms/batch 22.20 | loss  5.47 | ppl   238.58\n",
            "| epoch  26 |  1800/ 2983 batches | lr 0.000181 | ms/batch 22.21 | loss  5.40 | ppl   220.82\n",
            "| epoch  26 |  2000/ 2983 batches | lr 0.000181 | ms/batch 22.17 | loss  5.43 | ppl   228.21\n",
            "| epoch  26 |  2200/ 2983 batches | lr 0.000180 | ms/batch 22.21 | loss  5.36 | ppl   212.18\n",
            "| epoch  26 |  2400/ 2983 batches | lr 0.000180 | ms/batch 22.19 | loss  5.38 | ppl   216.71\n",
            "| epoch  26 |  2600/ 2983 batches | lr 0.000180 | ms/batch 22.20 | loss  5.42 | ppl   225.98\n",
            "| epoch  26 |  2800/ 2983 batches | lr 0.000180 | ms/batch 22.21 | loss  5.36 | ppl   213.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 68.49s | valid loss  6.24 | valid ppl   510.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000180\n",
            "| epoch  27 |   200/ 2983 batches | lr 0.000179 | ms/batch 22.31 | loss  5.42 | ppl   225.91\n",
            "| epoch  27 |   400/ 2983 batches | lr 0.000179 | ms/batch 22.18 | loss  5.43 | ppl   227.87\n",
            "| epoch  27 |   600/ 2983 batches | lr 0.000179 | ms/batch 22.18 | loss  5.33 | ppl   206.36\n",
            "| epoch  27 |   800/ 2983 batches | lr 0.000179 | ms/batch 22.20 | loss  5.37 | ppl   215.27\n",
            "| epoch  27 |  1000/ 2983 batches | lr 0.000178 | ms/batch 22.18 | loss  5.40 | ppl   220.56\n",
            "| epoch  27 |  1200/ 2983 batches | lr 0.000178 | ms/batch 22.18 | loss  5.38 | ppl   217.33\n",
            "| epoch  27 |  1400/ 2983 batches | lr 0.000178 | ms/batch 22.23 | loss  5.41 | ppl   223.81\n",
            "| epoch  27 |  1600/ 2983 batches | lr 0.000178 | ms/batch 22.18 | loss  5.47 | ppl   237.01\n",
            "| epoch  27 |  1800/ 2983 batches | lr 0.000177 | ms/batch 22.23 | loss  5.39 | ppl   219.25\n",
            "| epoch  27 |  2000/ 2983 batches | lr 0.000177 | ms/batch 22.20 | loss  5.43 | ppl   227.41\n",
            "| epoch  27 |  2200/ 2983 batches | lr 0.000177 | ms/batch 22.18 | loss  5.35 | ppl   210.95\n",
            "| epoch  27 |  2400/ 2983 batches | lr 0.000177 | ms/batch 22.19 | loss  5.37 | ppl   215.23\n",
            "| epoch  27 |  2600/ 2983 batches | lr 0.000177 | ms/batch 22.19 | loss  5.42 | ppl   224.99\n",
            "| epoch  27 |  2800/ 2983 batches | lr 0.000176 | ms/batch 22.18 | loss  5.36 | ppl   212.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 68.50s | valid loss  6.24 | valid ppl   511.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000176\n",
            "| epoch  28 |   200/ 2983 batches | lr 0.000176 | ms/batch 22.29 | loss  5.41 | ppl   224.68\n",
            "| epoch  28 |   400/ 2983 batches | lr 0.000176 | ms/batch 22.18 | loss  5.42 | ppl   225.92\n",
            "| epoch  28 |   600/ 2983 batches | lr 0.000175 | ms/batch 22.17 | loss  5.33 | ppl   205.49\n",
            "| epoch  28 |   800/ 2983 batches | lr 0.000175 | ms/batch 22.17 | loss  5.36 | ppl   213.73\n",
            "| epoch  28 |  1000/ 2983 batches | lr 0.000175 | ms/batch 22.21 | loss  5.39 | ppl   218.71\n",
            "| epoch  28 |  1200/ 2983 batches | lr 0.000175 | ms/batch 22.19 | loss  5.38 | ppl   216.20\n",
            "| epoch  28 |  1400/ 2983 batches | lr 0.000175 | ms/batch 22.18 | loss  5.40 | ppl   221.83\n",
            "| epoch  28 |  1600/ 2983 batches | lr 0.000174 | ms/batch 22.18 | loss  5.46 | ppl   236.07\n",
            "| epoch  28 |  1800/ 2983 batches | lr 0.000174 | ms/batch 22.16 | loss  5.38 | ppl   217.52\n",
            "| epoch  28 |  2000/ 2983 batches | lr 0.000174 | ms/batch 22.20 | loss  5.42 | ppl   225.75\n",
            "| epoch  28 |  2200/ 2983 batches | lr 0.000174 | ms/batch 22.19 | loss  5.34 | ppl   209.30\n",
            "| epoch  28 |  2400/ 2983 batches | lr 0.000174 | ms/batch 22.18 | loss  5.37 | ppl   213.79\n",
            "| epoch  28 |  2600/ 2983 batches | lr 0.000173 | ms/batch 22.22 | loss  5.41 | ppl   223.45\n",
            "| epoch  28 |  2800/ 2983 batches | lr 0.000173 | ms/batch 22.18 | loss  5.35 | ppl   210.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 68.49s | valid loss  6.24 | valid ppl   510.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000173\n",
            "| epoch  29 |   200/ 2983 batches | lr 0.000173 | ms/batch 22.32 | loss  5.41 | ppl   223.52\n",
            "| epoch  29 |   400/ 2983 batches | lr 0.000173 | ms/batch 22.19 | loss  5.41 | ppl   224.45\n",
            "| epoch  29 |   600/ 2983 batches | lr 0.000172 | ms/batch 22.21 | loss  5.32 | ppl   203.68\n",
            "| epoch  29 |   800/ 2983 batches | lr 0.000172 | ms/batch 22.18 | loss  5.36 | ppl   212.57\n",
            "| epoch  29 |  1000/ 2983 batches | lr 0.000172 | ms/batch 22.18 | loss  5.38 | ppl   217.19\n",
            "| epoch  29 |  1200/ 2983 batches | lr 0.000172 | ms/batch 22.19 | loss  5.37 | ppl   215.03\n",
            "| epoch  29 |  1400/ 2983 batches | lr 0.000172 | ms/batch 22.17 | loss  5.40 | ppl   220.87\n",
            "| epoch  29 |  1600/ 2983 batches | lr 0.000171 | ms/batch 22.18 | loss  5.46 | ppl   234.62\n",
            "| epoch  29 |  1800/ 2983 batches | lr 0.000171 | ms/batch 22.18 | loss  5.38 | ppl   216.55\n",
            "| epoch  29 |  2000/ 2983 batches | lr 0.000171 | ms/batch 22.18 | loss  5.41 | ppl   224.65\n",
            "| epoch  29 |  2200/ 2983 batches | lr 0.000171 | ms/batch 22.19 | loss  5.34 | ppl   207.52\n",
            "| epoch  29 |  2400/ 2983 batches | lr 0.000171 | ms/batch 22.18 | loss  5.35 | ppl   211.62\n",
            "| epoch  29 |  2600/ 2983 batches | lr 0.000170 | ms/batch 22.19 | loss  5.40 | ppl   221.51\n",
            "| epoch  29 |  2800/ 2983 batches | lr 0.000170 | ms/batch 22.19 | loss  5.34 | ppl   209.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 68.48s | valid loss  6.24 | valid ppl   511.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000170\n",
            "| epoch  30 |   200/ 2983 batches | lr 0.000170 | ms/batch 22.34 | loss  5.40 | ppl   222.20\n",
            "| epoch  30 |   400/ 2983 batches | lr 0.000170 | ms/batch 22.17 | loss  5.41 | ppl   222.89\n",
            "| epoch  30 |   600/ 2983 batches | lr 0.000169 | ms/batch 22.18 | loss  5.31 | ppl   201.67\n",
            "| epoch  30 |   800/ 2983 batches | lr 0.000169 | ms/batch 22.17 | loss  5.35 | ppl   210.81\n",
            "| epoch  30 |  1000/ 2983 batches | lr 0.000169 | ms/batch 22.18 | loss  5.37 | ppl   215.51\n",
            "| epoch  30 |  1200/ 2983 batches | lr 0.000169 | ms/batch 22.18 | loss  5.36 | ppl   212.81\n",
            "| epoch  30 |  1400/ 2983 batches | lr 0.000169 | ms/batch 22.18 | loss  5.39 | ppl   219.10\n",
            "| epoch  30 |  1600/ 2983 batches | lr 0.000168 | ms/batch 22.19 | loss  5.45 | ppl   233.11\n",
            "| epoch  30 |  1800/ 2983 batches | lr 0.000168 | ms/batch 22.19 | loss  5.37 | ppl   214.93\n",
            "| epoch  30 |  2000/ 2983 batches | lr 0.000168 | ms/batch 22.17 | loss  5.41 | ppl   223.23\n",
            "| epoch  30 |  2200/ 2983 batches | lr 0.000168 | ms/batch 22.16 | loss  5.33 | ppl   206.57\n",
            "| epoch  30 |  2400/ 2983 batches | lr 0.000168 | ms/batch 22.19 | loss  5.35 | ppl   211.15\n",
            "| epoch  30 |  2600/ 2983 batches | lr 0.000167 | ms/batch 22.18 | loss  5.39 | ppl   220.24\n",
            "| epoch  30 |  2800/ 2983 batches | lr 0.000167 | ms/batch 22.17 | loss  5.34 | ppl   208.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 68.48s | valid loss  6.24 | valid ppl   513.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000167\n",
            "| epoch  31 |   200/ 2983 batches | lr 0.000167 | ms/batch 22.30 | loss  5.39 | ppl   220.26\n",
            "| epoch  31 |   400/ 2983 batches | lr 0.000167 | ms/batch 22.19 | loss  5.40 | ppl   222.24\n",
            "| epoch  31 |   600/ 2983 batches | lr 0.000167 | ms/batch 22.17 | loss  5.30 | ppl   200.67\n",
            "| epoch  31 |   800/ 2983 batches | lr 0.000166 | ms/batch 22.19 | loss  5.35 | ppl   209.59\n",
            "| epoch  31 |  1000/ 2983 batches | lr 0.000166 | ms/batch 22.19 | loss  5.37 | ppl   214.76\n",
            "| epoch  31 |  1200/ 2983 batches | lr 0.000166 | ms/batch 22.19 | loss  5.36 | ppl   211.87\n",
            "| epoch  31 |  1400/ 2983 batches | lr 0.000166 | ms/batch 22.17 | loss  5.38 | ppl   217.27\n",
            "| epoch  31 |  1600/ 2983 batches | lr 0.000166 | ms/batch 22.20 | loss  5.44 | ppl   231.33\n",
            "| epoch  31 |  1800/ 2983 batches | lr 0.000165 | ms/batch 22.19 | loss  5.37 | ppl   214.43\n",
            "| epoch  31 |  2000/ 2983 batches | lr 0.000165 | ms/batch 22.18 | loss  5.40 | ppl   221.37\n",
            "| epoch  31 |  2200/ 2983 batches | lr 0.000165 | ms/batch 22.19 | loss  5.32 | ppl   204.58\n",
            "| epoch  31 |  2400/ 2983 batches | lr 0.000165 | ms/batch 22.19 | loss  5.34 | ppl   209.00\n",
            "| epoch  31 |  2600/ 2983 batches | lr 0.000165 | ms/batch 22.23 | loss  5.39 | ppl   219.24\n",
            "| epoch  31 |  2800/ 2983 batches | lr 0.000165 | ms/batch 22.20 | loss  5.34 | ppl   207.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 68.50s | valid loss  6.24 | valid ppl   513.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000164\n",
            "| epoch  32 |   200/ 2983 batches | lr 0.000164 | ms/batch 22.30 | loss  5.39 | ppl   218.98\n",
            "| epoch  32 |   400/ 2983 batches | lr 0.000164 | ms/batch 22.19 | loss  5.39 | ppl   220.22\n",
            "| epoch  32 |   600/ 2983 batches | lr 0.000164 | ms/batch 22.19 | loss  5.30 | ppl   199.36\n",
            "| epoch  32 |   800/ 2983 batches | lr 0.000164 | ms/batch 22.18 | loss  5.34 | ppl   208.36\n",
            "| epoch  32 |  1000/ 2983 batches | lr 0.000164 | ms/batch 22.17 | loss  5.36 | ppl   213.16\n",
            "| epoch  32 |  1200/ 2983 batches | lr 0.000163 | ms/batch 22.20 | loss  5.35 | ppl   210.45\n",
            "| epoch  32 |  1400/ 2983 batches | lr 0.000163 | ms/batch 22.19 | loss  5.38 | ppl   216.34\n",
            "| epoch  32 |  1600/ 2983 batches | lr 0.000163 | ms/batch 22.18 | loss  5.44 | ppl   230.22\n",
            "| epoch  32 |  1800/ 2983 batches | lr 0.000163 | ms/batch 22.18 | loss  5.36 | ppl   212.38\n",
            "| epoch  32 |  2000/ 2983 batches | lr 0.000163 | ms/batch 22.17 | loss  5.39 | ppl   220.27\n",
            "| epoch  32 |  2200/ 2983 batches | lr 0.000162 | ms/batch 22.21 | loss  5.32 | ppl   203.48\n",
            "| epoch  32 |  2400/ 2983 batches | lr 0.000162 | ms/batch 22.18 | loss  5.34 | ppl   208.45\n",
            "| epoch  32 |  2600/ 2983 batches | lr 0.000162 | ms/batch 22.18 | loss  5.39 | ppl   218.30\n",
            "| epoch  32 |  2800/ 2983 batches | lr 0.000162 | ms/batch 22.19 | loss  5.33 | ppl   205.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 68.48s | valid loss  6.24 | valid ppl   514.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000162\n",
            "| epoch  33 |   200/ 2983 batches | lr 0.000162 | ms/batch 22.31 | loss  5.38 | ppl   217.64\n",
            "| epoch  33 |   400/ 2983 batches | lr 0.000161 | ms/batch 22.20 | loss  5.39 | ppl   218.87\n",
            "| epoch  33 |   600/ 2983 batches | lr 0.000161 | ms/batch 22.17 | loss  5.29 | ppl   198.58\n",
            "| epoch  33 |   800/ 2983 batches | lr 0.000161 | ms/batch 22.19 | loss  5.33 | ppl   207.19\n",
            "| epoch  33 |  1000/ 2983 batches | lr 0.000161 | ms/batch 22.19 | loss  5.36 | ppl   212.64\n",
            "| epoch  33 |  1200/ 2983 batches | lr 0.000161 | ms/batch 22.18 | loss  5.34 | ppl   209.31\n",
            "| epoch  33 |  1400/ 2983 batches | lr 0.000161 | ms/batch 22.18 | loss  5.37 | ppl   215.58\n",
            "| epoch  33 |  1600/ 2983 batches | lr 0.000160 | ms/batch 22.18 | loss  5.43 | ppl   228.76\n",
            "| epoch  33 |  1800/ 2983 batches | lr 0.000160 | ms/batch 22.21 | loss  5.35 | ppl   211.18\n",
            "| epoch  33 |  2000/ 2983 batches | lr 0.000160 | ms/batch 22.18 | loss  5.39 | ppl   219.86\n",
            "| epoch  33 |  2200/ 2983 batches | lr 0.000160 | ms/batch 22.18 | loss  5.31 | ppl   202.82\n",
            "| epoch  33 |  2400/ 2983 batches | lr 0.000160 | ms/batch 22.19 | loss  5.33 | ppl   206.68\n",
            "| epoch  33 |  2600/ 2983 batches | lr 0.000160 | ms/batch 22.20 | loss  5.38 | ppl   216.76\n",
            "| epoch  33 |  2800/ 2983 batches | lr 0.000159 | ms/batch 22.19 | loss  5.32 | ppl   204.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 68.49s | valid loss  6.24 | valid ppl   514.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000159\n",
            "| epoch  34 |   200/ 2983 batches | lr 0.000159 | ms/batch 22.28 | loss  5.38 | ppl   216.68\n",
            "| epoch  34 |   400/ 2983 batches | lr 0.000159 | ms/batch 22.20 | loss  5.38 | ppl   218.04\n",
            "| epoch  34 |   600/ 2983 batches | lr 0.000159 | ms/batch 22.17 | loss  5.28 | ppl   197.33\n",
            "| epoch  34 |   800/ 2983 batches | lr 0.000159 | ms/batch 22.16 | loss  5.33 | ppl   205.73\n",
            "| epoch  34 |  1000/ 2983 batches | lr 0.000159 | ms/batch 22.21 | loss  5.35 | ppl   211.52\n",
            "| epoch  34 |  1200/ 2983 batches | lr 0.000158 | ms/batch 22.17 | loss  5.34 | ppl   207.91\n",
            "| epoch  34 |  1400/ 2983 batches | lr 0.000158 | ms/batch 22.21 | loss  5.37 | ppl   213.92\n",
            "| epoch  34 |  1600/ 2983 batches | lr 0.000158 | ms/batch 22.18 | loss  5.42 | ppl   227.01\n",
            "| epoch  34 |  1800/ 2983 batches | lr 0.000158 | ms/batch 22.18 | loss  5.35 | ppl   209.64\n",
            "| epoch  34 |  2000/ 2983 batches | lr 0.000158 | ms/batch 22.19 | loss  5.39 | ppl   218.92\n",
            "| epoch  34 |  2200/ 2983 batches | lr 0.000158 | ms/batch 22.20 | loss  5.31 | ppl   201.81\n",
            "| epoch  34 |  2400/ 2983 batches | lr 0.000157 | ms/batch 22.18 | loss  5.33 | ppl   205.66\n",
            "| epoch  34 |  2600/ 2983 batches | lr 0.000157 | ms/batch 22.23 | loss  5.37 | ppl   215.55\n",
            "| epoch  34 |  2800/ 2983 batches | lr 0.000157 | ms/batch 22.19 | loss  5.32 | ppl   203.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 68.53s | valid loss  6.24 | valid ppl   515.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000157\n",
            "| epoch  35 |   200/ 2983 batches | lr 0.000157 | ms/batch 22.32 | loss  5.37 | ppl   215.42\n",
            "| epoch  35 |   400/ 2983 batches | lr 0.000157 | ms/batch 22.19 | loss  5.38 | ppl   217.01\n",
            "| epoch  35 |   600/ 2983 batches | lr 0.000157 | ms/batch 22.22 | loss  5.28 | ppl   196.25\n",
            "| epoch  35 |   800/ 2983 batches | lr 0.000156 | ms/batch 22.20 | loss  5.32 | ppl   205.33\n",
            "| epoch  35 |  1000/ 2983 batches | lr 0.000156 | ms/batch 22.23 | loss  5.35 | ppl   210.18\n",
            "| epoch  35 |  1200/ 2983 batches | lr 0.000156 | ms/batch 22.20 | loss  5.33 | ppl   206.93\n",
            "| epoch  35 |  1400/ 2983 batches | lr 0.000156 | ms/batch 22.20 | loss  5.36 | ppl   212.44\n",
            "| epoch  35 |  1600/ 2983 batches | lr 0.000156 | ms/batch 22.19 | loss  5.42 | ppl   226.27\n",
            "| epoch  35 |  1800/ 2983 batches | lr 0.000156 | ms/batch 22.18 | loss  5.34 | ppl   209.08\n",
            "| epoch  35 |  2000/ 2983 batches | lr 0.000155 | ms/batch 22.16 | loss  5.38 | ppl   216.89\n",
            "| epoch  35 |  2200/ 2983 batches | lr 0.000155 | ms/batch 22.18 | loss  5.30 | ppl   200.61\n",
            "| epoch  35 |  2400/ 2983 batches | lr 0.000155 | ms/batch 22.17 | loss  5.32 | ppl   204.42\n",
            "| epoch  35 |  2600/ 2983 batches | lr 0.000155 | ms/batch 22.17 | loss  5.37 | ppl   213.85\n",
            "| epoch  35 |  2800/ 2983 batches | lr 0.000155 | ms/batch 22.18 | loss  5.31 | ppl   202.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 68.50s | valid loss  6.25 | valid ppl   518.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000155\n",
            "| epoch  36 |   200/ 2983 batches | lr 0.000155 | ms/batch 22.30 | loss  5.36 | ppl   213.49\n",
            "| epoch  36 |   400/ 2983 batches | lr 0.000154 | ms/batch 22.17 | loss  5.38 | ppl   216.24\n",
            "| epoch  36 |   600/ 2983 batches | lr 0.000154 | ms/batch 22.20 | loss  5.27 | ppl   195.17\n",
            "| epoch  36 |   800/ 2983 batches | lr 0.000154 | ms/batch 22.18 | loss  5.32 | ppl   204.13\n",
            "| epoch  36 |  1000/ 2983 batches | lr 0.000154 | ms/batch 22.16 | loss  5.34 | ppl   208.85\n",
            "| epoch  36 |  1200/ 2983 batches | lr 0.000154 | ms/batch 22.19 | loss  5.33 | ppl   205.61\n",
            "| epoch  36 |  1400/ 2983 batches | lr 0.000154 | ms/batch 22.20 | loss  5.35 | ppl   211.39\n",
            "| epoch  36 |  1600/ 2983 batches | lr 0.000154 | ms/batch 22.18 | loss  5.42 | ppl   225.47\n",
            "| epoch  36 |  1800/ 2983 batches | lr 0.000153 | ms/batch 22.18 | loss  5.34 | ppl   207.77\n",
            "| epoch  36 |  2000/ 2983 batches | lr 0.000153 | ms/batch 22.18 | loss  5.38 | ppl   216.01\n",
            "| epoch  36 |  2200/ 2983 batches | lr 0.000153 | ms/batch 22.17 | loss  5.29 | ppl   199.24\n",
            "| epoch  36 |  2400/ 2983 batches | lr 0.000153 | ms/batch 22.19 | loss  5.32 | ppl   203.88\n",
            "| epoch  36 |  2600/ 2983 batches | lr 0.000153 | ms/batch 22.18 | loss  5.36 | ppl   213.54\n",
            "| epoch  36 |  2800/ 2983 batches | lr 0.000153 | ms/batch 22.18 | loss  5.31 | ppl   201.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 68.47s | valid loss  6.24 | valid ppl   515.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000153\n",
            "| epoch  37 |   200/ 2983 batches | lr 0.000152 | ms/batch 22.30 | loss  5.36 | ppl   213.72\n",
            "| epoch  37 |   400/ 2983 batches | lr 0.000152 | ms/batch 22.19 | loss  5.37 | ppl   215.59\n",
            "| epoch  37 |   600/ 2983 batches | lr 0.000152 | ms/batch 22.18 | loss  5.27 | ppl   194.27\n",
            "| epoch  37 |   800/ 2983 batches | lr 0.000152 | ms/batch 22.17 | loss  5.31 | ppl   203.10\n",
            "| epoch  37 |  1000/ 2983 batches | lr 0.000152 | ms/batch 22.17 | loss  5.34 | ppl   208.41\n",
            "| epoch  37 |  1200/ 2983 batches | lr 0.000152 | ms/batch 22.16 | loss  5.32 | ppl   205.21\n",
            "| epoch  37 |  1400/ 2983 batches | lr 0.000152 | ms/batch 22.21 | loss  5.35 | ppl   210.56\n",
            "| epoch  37 |  1600/ 2983 batches | lr 0.000151 | ms/batch 22.19 | loss  5.41 | ppl   224.09\n",
            "| epoch  37 |  1800/ 2983 batches | lr 0.000151 | ms/batch 22.18 | loss  5.33 | ppl   206.66\n",
            "| epoch  37 |  2000/ 2983 batches | lr 0.000151 | ms/batch 22.18 | loss  5.37 | ppl   214.57\n",
            "| epoch  37 |  2200/ 2983 batches | lr 0.000151 | ms/batch 22.16 | loss  5.29 | ppl   198.80\n",
            "| epoch  37 |  2400/ 2983 batches | lr 0.000151 | ms/batch 22.17 | loss  5.31 | ppl   202.71\n",
            "| epoch  37 |  2600/ 2983 batches | lr 0.000151 | ms/batch 22.19 | loss  5.36 | ppl   211.76\n",
            "| epoch  37 |  2800/ 2983 batches | lr 0.000151 | ms/batch 22.18 | loss  5.30 | ppl   200.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 68.48s | valid loss  6.25 | valid ppl   517.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000150\n",
            "| epoch  38 |   200/ 2983 batches | lr 0.000150 | ms/batch 22.28 | loss  5.36 | ppl   211.94\n",
            "| epoch  38 |   400/ 2983 batches | lr 0.000150 | ms/batch 22.17 | loss  5.36 | ppl   213.47\n",
            "| epoch  38 |   600/ 2983 batches | lr 0.000150 | ms/batch 22.19 | loss  5.26 | ppl   192.85\n",
            "| epoch  38 |   800/ 2983 batches | lr 0.000150 | ms/batch 22.18 | loss  5.31 | ppl   202.14\n",
            "| epoch  38 |  1000/ 2983 batches | lr 0.000150 | ms/batch 22.18 | loss  5.33 | ppl   207.41\n",
            "| epoch  38 |  1200/ 2983 batches | lr 0.000150 | ms/batch 22.20 | loss  5.32 | ppl   203.97\n",
            "| epoch  38 |  1400/ 2983 batches | lr 0.000150 | ms/batch 22.22 | loss  5.35 | ppl   209.89\n",
            "| epoch  38 |  1600/ 2983 batches | lr 0.000149 | ms/batch 22.18 | loss  5.41 | ppl   222.60\n",
            "| epoch  38 |  1800/ 2983 batches | lr 0.000149 | ms/batch 22.19 | loss  5.33 | ppl   206.10\n",
            "| epoch  38 |  2000/ 2983 batches | lr 0.000149 | ms/batch 22.19 | loss  5.37 | ppl   214.31\n",
            "| epoch  38 |  2200/ 2983 batches | lr 0.000149 | ms/batch 22.18 | loss  5.29 | ppl   197.99\n",
            "| epoch  38 |  2400/ 2983 batches | lr 0.000149 | ms/batch 22.17 | loss  5.30 | ppl   201.33\n",
            "| epoch  38 |  2600/ 2983 batches | lr 0.000149 | ms/batch 22.19 | loss  5.35 | ppl   210.61\n",
            "| epoch  38 |  2800/ 2983 batches | lr 0.000149 | ms/batch 22.18 | loss  5.30 | ppl   199.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 68.48s | valid loss  6.25 | valid ppl   518.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000148\n",
            "| epoch  39 |   200/ 2983 batches | lr 0.000148 | ms/batch 22.33 | loss  5.35 | ppl   211.62\n",
            "| epoch  39 |   400/ 2983 batches | lr 0.000148 | ms/batch 22.17 | loss  5.36 | ppl   212.69\n",
            "| epoch  39 |   600/ 2983 batches | lr 0.000148 | ms/batch 22.19 | loss  5.26 | ppl   192.87\n",
            "| epoch  39 |   800/ 2983 batches | lr 0.000148 | ms/batch 22.20 | loss  5.30 | ppl   201.30\n",
            "| epoch  39 |  1000/ 2983 batches | lr 0.000148 | ms/batch 22.18 | loss  5.33 | ppl   206.28\n",
            "| epoch  39 |  1200/ 2983 batches | lr 0.000148 | ms/batch 22.18 | loss  5.31 | ppl   203.33\n",
            "| epoch  39 |  1400/ 2983 batches | lr 0.000148 | ms/batch 22.20 | loss  5.34 | ppl   209.02\n",
            "| epoch  39 |  1600/ 2983 batches | lr 0.000147 | ms/batch 22.18 | loss  5.40 | ppl   221.92\n",
            "| epoch  39 |  1800/ 2983 batches | lr 0.000147 | ms/batch 22.20 | loss  5.33 | ppl   206.07\n",
            "| epoch  39 |  2000/ 2983 batches | lr 0.000147 | ms/batch 22.19 | loss  5.36 | ppl   213.09\n",
            "| epoch  39 |  2200/ 2983 batches | lr 0.000147 | ms/batch 22.20 | loss  5.28 | ppl   196.61\n",
            "| epoch  39 |  2400/ 2983 batches | lr 0.000147 | ms/batch 22.20 | loss  5.30 | ppl   200.75\n",
            "| epoch  39 |  2600/ 2983 batches | lr 0.000147 | ms/batch 22.19 | loss  5.34 | ppl   209.50\n",
            "| epoch  39 |  2800/ 2983 batches | lr 0.000147 | ms/batch 22.19 | loss  5.29 | ppl   198.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 68.50s | valid loss  6.25 | valid ppl   519.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000147\n",
            "| epoch  40 |   200/ 2983 batches | lr 0.000146 | ms/batch 22.30 | loss  5.35 | ppl   210.46\n",
            "| epoch  40 |   400/ 2983 batches | lr 0.000146 | ms/batch 22.22 | loss  5.35 | ppl   211.17\n",
            "| epoch  40 |   600/ 2983 batches | lr 0.000146 | ms/batch 22.20 | loss  5.26 | ppl   191.61\n",
            "| epoch  40 |   800/ 2983 batches | lr 0.000146 | ms/batch 22.17 | loss  5.30 | ppl   200.15\n",
            "| epoch  40 |  1000/ 2983 batches | lr 0.000146 | ms/batch 22.20 | loss  5.32 | ppl   204.83\n",
            "| epoch  40 |  1200/ 2983 batches | lr 0.000146 | ms/batch 22.20 | loss  5.31 | ppl   202.14\n",
            "| epoch  40 |  1400/ 2983 batches | lr 0.000146 | ms/batch 22.20 | loss  5.34 | ppl   208.64\n",
            "| epoch  40 |  1600/ 2983 batches | lr 0.000146 | ms/batch 22.18 | loss  5.40 | ppl   221.06\n",
            "| epoch  40 |  1800/ 2983 batches | lr 0.000145 | ms/batch 22.20 | loss  5.32 | ppl   204.71\n",
            "| epoch  40 |  2000/ 2983 batches | lr 0.000145 | ms/batch 22.22 | loss  5.35 | ppl   211.65\n",
            "| epoch  40 |  2200/ 2983 batches | lr 0.000145 | ms/batch 22.18 | loss  5.28 | ppl   195.62\n",
            "| epoch  40 |  2400/ 2983 batches | lr 0.000145 | ms/batch 22.18 | loss  5.29 | ppl   199.28\n",
            "| epoch  40 |  2600/ 2983 batches | lr 0.000145 | ms/batch 22.16 | loss  5.34 | ppl   209.14\n",
            "| epoch  40 |  2800/ 2983 batches | lr 0.000145 | ms/batch 22.19 | loss  5.29 | ppl   197.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 68.52s | valid loss  6.26 | valid ppl   521.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000145\n",
            "| epoch  41 |   200/ 2983 batches | lr 0.000145 | ms/batch 22.33 | loss  5.35 | ppl   209.68\n",
            "| epoch  41 |   400/ 2983 batches | lr 0.000144 | ms/batch 22.22 | loss  5.35 | ppl   210.35\n",
            "| epoch  41 |   600/ 2983 batches | lr 0.000144 | ms/batch 22.25 | loss  5.25 | ppl   190.33\n",
            "| epoch  41 |   800/ 2983 batches | lr 0.000144 | ms/batch 22.22 | loss  5.30 | ppl   199.92\n",
            "| epoch  41 |  1000/ 2983 batches | lr 0.000144 | ms/batch 22.25 | loss  5.32 | ppl   204.25\n",
            "| epoch  41 |  1200/ 2983 batches | lr 0.000144 | ms/batch 22.25 | loss  5.30 | ppl   201.16\n",
            "| epoch  41 |  1400/ 2983 batches | lr 0.000144 | ms/batch 22.27 | loss  5.33 | ppl   206.80\n",
            "| epoch  41 |  1600/ 2983 batches | lr 0.000144 | ms/batch 22.23 | loss  5.39 | ppl   218.96\n",
            "| epoch  41 |  1800/ 2983 batches | lr 0.000144 | ms/batch 22.21 | loss  5.31 | ppl   203.12\n",
            "| epoch  41 |  2000/ 2983 batches | lr 0.000144 | ms/batch 22.21 | loss  5.35 | ppl   210.58\n",
            "| epoch  41 |  2200/ 2983 batches | lr 0.000143 | ms/batch 22.23 | loss  5.28 | ppl   195.56\n",
            "| epoch  41 |  2400/ 2983 batches | lr 0.000143 | ms/batch 22.20 | loss  5.29 | ppl   198.68\n",
            "| epoch  41 |  2600/ 2983 batches | lr 0.000143 | ms/batch 22.23 | loss  5.34 | ppl   207.50\n",
            "| epoch  41 |  2800/ 2983 batches | lr 0.000143 | ms/batch 22.25 | loss  5.28 | ppl   197.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 68.62s | valid loss  6.26 | valid ppl   523.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000143\n",
            "| epoch  42 |   200/ 2983 batches | lr 0.000143 | ms/batch 22.31 | loss  5.34 | ppl   208.02\n",
            "| epoch  42 |   400/ 2983 batches | lr 0.000143 | ms/batch 22.19 | loss  5.34 | ppl   209.12\n",
            "| epoch  42 |   600/ 2983 batches | lr 0.000143 | ms/batch 22.20 | loss  5.24 | ppl   189.46\n",
            "| epoch  42 |   800/ 2983 batches | lr 0.000142 | ms/batch 22.21 | loss  5.29 | ppl   198.75\n",
            "| epoch  42 |  1000/ 2983 batches | lr 0.000142 | ms/batch 22.23 | loss  5.31 | ppl   203.07\n",
            "| epoch  42 |  1200/ 2983 batches | lr 0.000142 | ms/batch 22.20 | loss  5.30 | ppl   199.92\n",
            "| epoch  42 |  1400/ 2983 batches | lr 0.000142 | ms/batch 22.21 | loss  5.33 | ppl   205.67\n",
            "| epoch  42 |  1600/ 2983 batches | lr 0.000142 | ms/batch 22.20 | loss  5.39 | ppl   218.69\n",
            "| epoch  42 |  1800/ 2983 batches | lr 0.000142 | ms/batch 22.20 | loss  5.31 | ppl   201.95\n",
            "| epoch  42 |  2000/ 2983 batches | lr 0.000142 | ms/batch 22.19 | loss  5.35 | ppl   210.02\n",
            "| epoch  42 |  2200/ 2983 batches | lr 0.000142 | ms/batch 22.20 | loss  5.27 | ppl   194.08\n",
            "| epoch  42 |  2400/ 2983 batches | lr 0.000142 | ms/batch 22.21 | loss  5.29 | ppl   197.59\n",
            "| epoch  42 |  2600/ 2983 batches | lr 0.000141 | ms/batch 22.21 | loss  5.33 | ppl   207.37\n",
            "| epoch  42 |  2800/ 2983 batches | lr 0.000141 | ms/batch 22.22 | loss  5.28 | ppl   195.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 68.54s | valid loss  6.26 | valid ppl   522.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000141\n",
            "| epoch  43 |   200/ 2983 batches | lr 0.000141 | ms/batch 22.29 | loss  5.34 | ppl   207.95\n",
            "| epoch  43 |   400/ 2983 batches | lr 0.000141 | ms/batch 22.21 | loss  5.34 | ppl   208.19\n",
            "| epoch  43 |   600/ 2983 batches | lr 0.000141 | ms/batch 22.21 | loss  5.24 | ppl   188.41\n",
            "| epoch  43 |   800/ 2983 batches | lr 0.000141 | ms/batch 22.21 | loss  5.29 | ppl   197.79\n",
            "| epoch  43 |  1000/ 2983 batches | lr 0.000141 | ms/batch 22.21 | loss  5.31 | ppl   202.54\n",
            "| epoch  43 |  1200/ 2983 batches | lr 0.000141 | ms/batch 22.20 | loss  5.29 | ppl   199.23\n",
            "| epoch  43 |  1400/ 2983 batches | lr 0.000140 | ms/batch 22.21 | loss  5.33 | ppl   205.62\n",
            "| epoch  43 |  1600/ 2983 batches | lr 0.000140 | ms/batch 22.21 | loss  5.39 | ppl   218.23\n",
            "| epoch  43 |  1800/ 2983 batches | lr 0.000140 | ms/batch 22.20 | loss  5.31 | ppl   202.17\n",
            "| epoch  43 |  2000/ 2983 batches | lr 0.000140 | ms/batch 22.19 | loss  5.34 | ppl   208.38\n",
            "| epoch  43 |  2200/ 2983 batches | lr 0.000140 | ms/batch 22.18 | loss  5.26 | ppl   193.38\n",
            "| epoch  43 |  2400/ 2983 batches | lr 0.000140 | ms/batch 22.17 | loss  5.28 | ppl   197.05\n",
            "| epoch  43 |  2600/ 2983 batches | lr 0.000140 | ms/batch 22.22 | loss  5.33 | ppl   206.23\n",
            "| epoch  43 |  2800/ 2983 batches | lr 0.000140 | ms/batch 22.19 | loss  5.27 | ppl   195.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 68.53s | valid loss  6.26 | valid ppl   524.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000140\n",
            "| epoch  44 |   200/ 2983 batches | lr 0.000139 | ms/batch 22.31 | loss  5.33 | ppl   206.43\n",
            "| epoch  44 |   400/ 2983 batches | lr 0.000139 | ms/batch 22.19 | loss  5.34 | ppl   207.68\n",
            "| epoch  44 |   600/ 2983 batches | lr 0.000139 | ms/batch 22.21 | loss  5.23 | ppl   187.51\n",
            "| epoch  44 |   800/ 2983 batches | lr 0.000139 | ms/batch 22.17 | loss  5.28 | ppl   196.78\n",
            "| epoch  44 |  1000/ 2983 batches | lr 0.000139 | ms/batch 22.18 | loss  5.31 | ppl   201.86\n",
            "| epoch  44 |  1200/ 2983 batches | lr 0.000139 | ms/batch 22.19 | loss  5.29 | ppl   198.49\n",
            "| epoch  44 |  1400/ 2983 batches | lr 0.000139 | ms/batch 22.19 | loss  5.32 | ppl   204.60\n",
            "| epoch  44 |  1600/ 2983 batches | lr 0.000139 | ms/batch 22.18 | loss  5.38 | ppl   216.51\n",
            "| epoch  44 |  1800/ 2983 batches | lr 0.000139 | ms/batch 22.20 | loss  5.30 | ppl   201.26\n",
            "| epoch  44 |  2000/ 2983 batches | lr 0.000139 | ms/batch 22.18 | loss  5.34 | ppl   208.46\n",
            "| epoch  44 |  2200/ 2983 batches | lr 0.000138 | ms/batch 22.20 | loss  5.26 | ppl   192.47\n",
            "| epoch  44 |  2400/ 2983 batches | lr 0.000138 | ms/batch 22.18 | loss  5.28 | ppl   195.67\n",
            "| epoch  44 |  2600/ 2983 batches | lr 0.000138 | ms/batch 22.18 | loss  5.32 | ppl   205.23\n",
            "| epoch  44 |  2800/ 2983 batches | lr 0.000138 | ms/batch 22.20 | loss  5.27 | ppl   194.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 68.50s | valid loss  6.26 | valid ppl   525.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000138\n",
            "| epoch  45 |   200/ 2983 batches | lr 0.000138 | ms/batch 22.31 | loss  5.33 | ppl   205.59\n",
            "| epoch  45 |   400/ 2983 batches | lr 0.000138 | ms/batch 22.17 | loss  5.33 | ppl   206.78\n",
            "| epoch  45 |   600/ 2983 batches | lr 0.000138 | ms/batch 22.17 | loss  5.23 | ppl   187.06\n",
            "| epoch  45 |   800/ 2983 batches | lr 0.000138 | ms/batch 22.18 | loss  5.28 | ppl   195.65\n",
            "| epoch  45 |  1000/ 2983 batches | lr 0.000137 | ms/batch 22.18 | loss  5.30 | ppl   200.84\n",
            "| epoch  45 |  1200/ 2983 batches | lr 0.000137 | ms/batch 22.20 | loss  5.29 | ppl   197.69\n",
            "| epoch  45 |  1400/ 2983 batches | lr 0.000137 | ms/batch 22.23 | loss  5.31 | ppl   203.32\n",
            "| epoch  45 |  1600/ 2983 batches | lr 0.000137 | ms/batch 22.25 | loss  5.38 | ppl   216.06\n",
            "| epoch  45 |  1800/ 2983 batches | lr 0.000137 | ms/batch 22.20 | loss  5.30 | ppl   199.91\n",
            "| epoch  45 |  2000/ 2983 batches | lr 0.000137 | ms/batch 22.17 | loss  5.34 | ppl   207.85\n",
            "| epoch  45 |  2200/ 2983 batches | lr 0.000137 | ms/batch 22.18 | loss  5.25 | ppl   191.51\n",
            "| epoch  45 |  2400/ 2983 batches | lr 0.000137 | ms/batch 22.19 | loss  5.27 | ppl   194.48\n",
            "| epoch  45 |  2600/ 2983 batches | lr 0.000137 | ms/batch 22.19 | loss  5.32 | ppl   204.14\n",
            "| epoch  45 |  2800/ 2983 batches | lr 0.000137 | ms/batch 22.19 | loss  5.27 | ppl   194.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 68.50s | valid loss  6.27 | valid ppl   526.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000136\n",
            "| epoch  46 |   200/ 2983 batches | lr 0.000136 | ms/batch 22.29 | loss  5.32 | ppl   205.05\n",
            "| epoch  46 |   400/ 2983 batches | lr 0.000136 | ms/batch 22.22 | loss  5.33 | ppl   205.97\n",
            "| epoch  46 |   600/ 2983 batches | lr 0.000136 | ms/batch 22.22 | loss  5.23 | ppl   186.18\n",
            "| epoch  46 |   800/ 2983 batches | lr 0.000136 | ms/batch 22.20 | loss  5.27 | ppl   195.27\n",
            "| epoch  46 |  1000/ 2983 batches | lr 0.000136 | ms/batch 22.21 | loss  5.30 | ppl   199.87\n",
            "| epoch  46 |  1200/ 2983 batches | lr 0.000136 | ms/batch 22.19 | loss  5.29 | ppl   197.51\n",
            "| epoch  46 |  1400/ 2983 batches | lr 0.000136 | ms/batch 22.17 | loss  5.31 | ppl   202.94\n",
            "| epoch  46 |  1600/ 2983 batches | lr 0.000136 | ms/batch 22.18 | loss  5.37 | ppl   215.06\n",
            "| epoch  46 |  1800/ 2983 batches | lr 0.000136 | ms/batch 22.17 | loss  5.29 | ppl   198.98\n",
            "| epoch  46 |  2000/ 2983 batches | lr 0.000135 | ms/batch 22.19 | loss  5.33 | ppl   206.06\n",
            "| epoch  46 |  2200/ 2983 batches | lr 0.000135 | ms/batch 22.19 | loss  5.25 | ppl   191.05\n",
            "| epoch  46 |  2400/ 2983 batches | lr 0.000135 | ms/batch 22.17 | loss  5.27 | ppl   194.34\n",
            "| epoch  46 |  2600/ 2983 batches | lr 0.000135 | ms/batch 22.18 | loss  5.32 | ppl   203.41\n",
            "| epoch  46 |  2800/ 2983 batches | lr 0.000135 | ms/batch 22.19 | loss  5.26 | ppl   192.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 68.51s | valid loss  6.26 | valid ppl   525.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000135\n",
            "| epoch  47 |   200/ 2983 batches | lr 0.000135 | ms/batch 22.36 | loss  5.32 | ppl   203.80\n",
            "| epoch  47 |   400/ 2983 batches | lr 0.000135 | ms/batch 22.25 | loss  5.32 | ppl   205.29\n",
            "| epoch  47 |   600/ 2983 batches | lr 0.000135 | ms/batch 22.29 | loss  5.22 | ppl   185.15\n",
            "| epoch  47 |   800/ 2983 batches | lr 0.000135 | ms/batch 22.23 | loss  5.27 | ppl   193.97\n",
            "| epoch  47 |  1000/ 2983 batches | lr 0.000134 | ms/batch 22.28 | loss  5.30 | ppl   199.52\n",
            "| epoch  47 |  1200/ 2983 batches | lr 0.000134 | ms/batch 22.24 | loss  5.28 | ppl   196.50\n",
            "| epoch  47 |  1400/ 2983 batches | lr 0.000134 | ms/batch 22.23 | loss  5.31 | ppl   202.28\n",
            "| epoch  47 |  1600/ 2983 batches | lr 0.000134 | ms/batch 22.26 | loss  5.37 | ppl   214.03\n",
            "| epoch  47 |  1800/ 2983 batches | lr 0.000134 | ms/batch 22.29 | loss  5.29 | ppl   198.74\n",
            "| epoch  47 |  2000/ 2983 batches | lr 0.000134 | ms/batch 22.20 | loss  5.33 | ppl   206.21\n",
            "| epoch  47 |  2200/ 2983 batches | lr 0.000134 | ms/batch 22.20 | loss  5.25 | ppl   190.65\n",
            "| epoch  47 |  2400/ 2983 batches | lr 0.000134 | ms/batch 22.20 | loss  5.27 | ppl   193.49\n",
            "| epoch  47 |  2600/ 2983 batches | lr 0.000134 | ms/batch 22.20 | loss  5.31 | ppl   202.41\n",
            "| epoch  47 |  2800/ 2983 batches | lr 0.000134 | ms/batch 22.20 | loss  5.26 | ppl   192.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 68.63s | valid loss  6.27 | valid ppl   526.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000134\n",
            "| epoch  48 |   200/ 2983 batches | lr 0.000133 | ms/batch 22.33 | loss  5.31 | ppl   202.93\n",
            "| epoch  48 |   400/ 2983 batches | lr 0.000133 | ms/batch 22.18 | loss  5.32 | ppl   204.58\n",
            "| epoch  48 |   600/ 2983 batches | lr 0.000133 | ms/batch 22.19 | loss  5.22 | ppl   184.89\n",
            "| epoch  48 |   800/ 2983 batches | lr 0.000133 | ms/batch 22.19 | loss  5.26 | ppl   193.06\n",
            "| epoch  48 |  1000/ 2983 batches | lr 0.000133 | ms/batch 22.17 | loss  5.29 | ppl   198.08\n",
            "| epoch  48 |  1200/ 2983 batches | lr 0.000133 | ms/batch 22.18 | loss  5.27 | ppl   195.32\n",
            "| epoch  48 |  1400/ 2983 batches | lr 0.000133 | ms/batch 22.19 | loss  5.30 | ppl   200.73\n",
            "| epoch  48 |  1600/ 2983 batches | lr 0.000133 | ms/batch 22.17 | loss  5.36 | ppl   213.53\n",
            "| epoch  48 |  1800/ 2983 batches | lr 0.000133 | ms/batch 22.17 | loss  5.29 | ppl   198.53\n",
            "| epoch  48 |  2000/ 2983 batches | lr 0.000133 | ms/batch 22.18 | loss  5.32 | ppl   204.32\n",
            "| epoch  48 |  2200/ 2983 batches | lr 0.000132 | ms/batch 22.18 | loss  5.24 | ppl   189.26\n",
            "| epoch  48 |  2400/ 2983 batches | lr 0.000132 | ms/batch 22.20 | loss  5.26 | ppl   192.72\n",
            "| epoch  48 |  2600/ 2983 batches | lr 0.000132 | ms/batch 22.18 | loss  5.31 | ppl   202.07\n",
            "| epoch  48 |  2800/ 2983 batches | lr 0.000132 | ms/batch 22.19 | loss  5.26 | ppl   191.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 68.50s | valid loss  6.27 | valid ppl   528.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000132\n",
            "| epoch  49 |   200/ 2983 batches | lr 0.000132 | ms/batch 22.32 | loss  5.31 | ppl   202.27\n",
            "| epoch  49 |   400/ 2983 batches | lr 0.000132 | ms/batch 22.22 | loss  5.32 | ppl   203.45\n",
            "| epoch  49 |   600/ 2983 batches | lr 0.000132 | ms/batch 22.20 | loss  5.22 | ppl   184.16\n",
            "| epoch  49 |   800/ 2983 batches | lr 0.000132 | ms/batch 22.18 | loss  5.26 | ppl   193.29\n",
            "| epoch  49 |  1000/ 2983 batches | lr 0.000132 | ms/batch 22.23 | loss  5.29 | ppl   197.76\n",
            "| epoch  49 |  1200/ 2983 batches | lr 0.000132 | ms/batch 22.20 | loss  5.27 | ppl   194.51\n",
            "| epoch  49 |  1400/ 2983 batches | lr 0.000131 | ms/batch 22.22 | loss  5.30 | ppl   199.82\n",
            "| epoch  49 |  1600/ 2983 batches | lr 0.000131 | ms/batch 22.18 | loss  5.36 | ppl   212.66\n",
            "| epoch  49 |  1800/ 2983 batches | lr 0.000131 | ms/batch 22.18 | loss  5.28 | ppl   197.09\n",
            "| epoch  49 |  2000/ 2983 batches | lr 0.000131 | ms/batch 22.18 | loss  5.32 | ppl   204.01\n",
            "| epoch  49 |  2200/ 2983 batches | lr 0.000131 | ms/batch 22.21 | loss  5.24 | ppl   188.73\n",
            "| epoch  49 |  2400/ 2983 batches | lr 0.000131 | ms/batch 22.19 | loss  5.26 | ppl   193.04\n",
            "| epoch  49 |  2600/ 2983 batches | lr 0.000131 | ms/batch 22.21 | loss  5.31 | ppl   201.48\n",
            "| epoch  49 |  2800/ 2983 batches | lr 0.000131 | ms/batch 22.20 | loss  5.25 | ppl   190.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 68.52s | valid loss  6.27 | valid ppl   529.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000131\n",
            "| epoch  50 |   200/ 2983 batches | lr 0.000131 | ms/batch 22.31 | loss  5.30 | ppl   200.89\n",
            "| epoch  50 |   400/ 2983 batches | lr 0.000131 | ms/batch 22.17 | loss  5.31 | ppl   202.60\n",
            "| epoch  50 |   600/ 2983 batches | lr 0.000130 | ms/batch 22.20 | loss  5.21 | ppl   183.30\n",
            "| epoch  50 |   800/ 2983 batches | lr 0.000130 | ms/batch 22.19 | loss  5.25 | ppl   191.46\n",
            "| epoch  50 |  1000/ 2983 batches | lr 0.000130 | ms/batch 22.18 | loss  5.28 | ppl   196.65\n",
            "| epoch  50 |  1200/ 2983 batches | lr 0.000130 | ms/batch 22.19 | loss  5.27 | ppl   193.61\n",
            "| epoch  50 |  1400/ 2983 batches | lr 0.000130 | ms/batch 22.18 | loss  5.29 | ppl   199.00\n",
            "| epoch  50 |  1600/ 2983 batches | lr 0.000130 | ms/batch 22.17 | loss  5.35 | ppl   211.26\n",
            "| epoch  50 |  1800/ 2983 batches | lr 0.000130 | ms/batch 22.19 | loss  5.28 | ppl   196.54\n",
            "| epoch  50 |  2000/ 2983 batches | lr 0.000130 | ms/batch 22.17 | loss  5.32 | ppl   203.44\n",
            "| epoch  50 |  2200/ 2983 batches | lr 0.000130 | ms/batch 22.18 | loss  5.24 | ppl   188.29\n",
            "| epoch  50 |  2400/ 2983 batches | lr 0.000130 | ms/batch 22.17 | loss  5.25 | ppl   191.06\n",
            "| epoch  50 |  2600/ 2983 batches | lr 0.000130 | ms/batch 22.21 | loss  5.30 | ppl   200.44\n",
            "| epoch  50 |  2800/ 2983 batches | lr 0.000130 | ms/batch 22.21 | loss  5.25 | ppl   189.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 68.48s | valid loss  6.27 | valid ppl   529.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000129\n",
            "| epoch  51 |   200/ 2983 batches | lr 0.000129 | ms/batch 22.30 | loss  5.30 | ppl   200.27\n",
            "| epoch  51 |   400/ 2983 batches | lr 0.000129 | ms/batch 22.20 | loss  5.31 | ppl   201.85\n",
            "| epoch  51 |   600/ 2983 batches | lr 0.000129 | ms/batch 22.18 | loss  5.21 | ppl   182.85\n",
            "| epoch  51 |   800/ 2983 batches | lr 0.000129 | ms/batch 22.20 | loss  5.25 | ppl   190.95\n",
            "| epoch  51 |  1000/ 2983 batches | lr 0.000129 | ms/batch 22.19 | loss  5.28 | ppl   196.34\n",
            "| epoch  51 |  1200/ 2983 batches | lr 0.000129 | ms/batch 22.16 | loss  5.26 | ppl   193.23\n",
            "| epoch  51 |  1400/ 2983 batches | lr 0.000129 | ms/batch 22.18 | loss  5.29 | ppl   198.61\n",
            "| epoch  51 |  1600/ 2983 batches | lr 0.000129 | ms/batch 22.16 | loss  5.35 | ppl   210.77\n",
            "| epoch  51 |  1800/ 2983 batches | lr 0.000129 | ms/batch 22.20 | loss  5.28 | ppl   195.66\n",
            "| epoch  51 |  2000/ 2983 batches | lr 0.000129 | ms/batch 22.18 | loss  5.31 | ppl   202.98\n",
            "| epoch  51 |  2200/ 2983 batches | lr 0.000129 | ms/batch 22.17 | loss  5.23 | ppl   187.01\n",
            "| epoch  51 |  2400/ 2983 batches | lr 0.000128 | ms/batch 22.23 | loss  5.25 | ppl   191.06\n",
            "| epoch  51 |  2600/ 2983 batches | lr 0.000128 | ms/batch 22.20 | loss  5.30 | ppl   199.35\n",
            "| epoch  51 |  2800/ 2983 batches | lr 0.000128 | ms/batch 22.20 | loss  5.25 | ppl   189.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 68.51s | valid loss  6.27 | valid ppl   530.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000128\n",
            "| epoch  52 |   200/ 2983 batches | lr 0.000128 | ms/batch 22.31 | loss  5.30 | ppl   200.63\n",
            "| epoch  52 |   400/ 2983 batches | lr 0.000128 | ms/batch 22.18 | loss  5.31 | ppl   201.43\n",
            "| epoch  52 |   600/ 2983 batches | lr 0.000128 | ms/batch 22.19 | loss  5.20 | ppl   182.01\n",
            "| epoch  52 |   800/ 2983 batches | lr 0.000128 | ms/batch 22.18 | loss  5.25 | ppl   190.86\n",
            "| epoch  52 |  1000/ 2983 batches | lr 0.000128 | ms/batch 22.21 | loss  5.27 | ppl   195.30\n",
            "| epoch  52 |  1200/ 2983 batches | lr 0.000128 | ms/batch 22.17 | loss  5.26 | ppl   192.30\n",
            "| epoch  52 |  1400/ 2983 batches | lr 0.000128 | ms/batch 22.18 | loss  5.28 | ppl   197.24\n",
            "| epoch  52 |  1600/ 2983 batches | lr 0.000128 | ms/batch 22.20 | loss  5.35 | ppl   210.19\n",
            "| epoch  52 |  1800/ 2983 batches | lr 0.000127 | ms/batch 22.19 | loss  5.27 | ppl   194.83\n",
            "| epoch  52 |  2000/ 2983 batches | lr 0.000127 | ms/batch 22.21 | loss  5.31 | ppl   202.11\n",
            "| epoch  52 |  2200/ 2983 batches | lr 0.000127 | ms/batch 22.21 | loss  5.23 | ppl   186.79\n",
            "| epoch  52 |  2400/ 2983 batches | lr 0.000127 | ms/batch 22.20 | loss  5.25 | ppl   190.15\n",
            "| epoch  52 |  2600/ 2983 batches | lr 0.000127 | ms/batch 22.23 | loss  5.29 | ppl   198.69\n",
            "| epoch  52 |  2800/ 2983 batches | lr 0.000127 | ms/batch 22.19 | loss  5.24 | ppl   188.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 68.52s | valid loss  6.28 | valid ppl   531.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000127\n",
            "| epoch  53 |   200/ 2983 batches | lr 0.000127 | ms/batch 22.30 | loss  5.30 | ppl   199.51\n",
            "| epoch  53 |   400/ 2983 batches | lr 0.000127 | ms/batch 22.19 | loss  5.30 | ppl   200.01\n",
            "| epoch  53 |   600/ 2983 batches | lr 0.000127 | ms/batch 22.21 | loss  5.20 | ppl   181.01\n",
            "| epoch  53 |   800/ 2983 batches | lr 0.000127 | ms/batch 22.19 | loss  5.25 | ppl   190.00\n",
            "| epoch  53 |  1000/ 2983 batches | lr 0.000127 | ms/batch 22.18 | loss  5.27 | ppl   195.09\n",
            "| epoch  53 |  1200/ 2983 batches | lr 0.000126 | ms/batch 22.21 | loss  5.26 | ppl   191.94\n",
            "| epoch  53 |  1400/ 2983 batches | lr 0.000126 | ms/batch 22.18 | loss  5.28 | ppl   197.01\n",
            "| epoch  53 |  1600/ 2983 batches | lr 0.000126 | ms/batch 22.20 | loss  5.35 | ppl   209.59\n",
            "| epoch  53 |  1800/ 2983 batches | lr 0.000126 | ms/batch 22.21 | loss  5.27 | ppl   194.01\n",
            "| epoch  53 |  2000/ 2983 batches | lr 0.000126 | ms/batch 22.17 | loss  5.30 | ppl   201.13\n",
            "| epoch  53 |  2200/ 2983 batches | lr 0.000126 | ms/batch 22.20 | loss  5.23 | ppl   186.03\n",
            "| epoch  53 |  2400/ 2983 batches | lr 0.000126 | ms/batch 22.18 | loss  5.24 | ppl   189.25\n",
            "| epoch  53 |  2600/ 2983 batches | lr 0.000126 | ms/batch 22.18 | loss  5.29 | ppl   198.54\n",
            "| epoch  53 |  2800/ 2983 batches | lr 0.000126 | ms/batch 22.21 | loss  5.24 | ppl   187.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 68.50s | valid loss  6.28 | valid ppl   533.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000126\n",
            "| epoch  54 |   200/ 2983 batches | lr 0.000126 | ms/batch 22.33 | loss  5.29 | ppl   198.98\n",
            "| epoch  54 |   400/ 2983 batches | lr 0.000126 | ms/batch 22.22 | loss  5.30 | ppl   199.53\n",
            "| epoch  54 |   600/ 2983 batches | lr 0.000125 | ms/batch 22.19 | loss  5.20 | ppl   181.12\n",
            "| epoch  54 |   800/ 2983 batches | lr 0.000125 | ms/batch 22.22 | loss  5.24 | ppl   189.12\n",
            "| epoch  54 |  1000/ 2983 batches | lr 0.000125 | ms/batch 22.19 | loss  5.27 | ppl   193.73\n",
            "| epoch  54 |  1200/ 2983 batches | lr 0.000125 | ms/batch 22.18 | loss  5.25 | ppl   191.16\n",
            "| epoch  54 |  1400/ 2983 batches | lr 0.000125 | ms/batch 22.18 | loss  5.28 | ppl   196.47\n",
            "| epoch  54 |  1600/ 2983 batches | lr 0.000125 | ms/batch 22.17 | loss  5.34 | ppl   208.93\n",
            "| epoch  54 |  1800/ 2983 batches | lr 0.000125 | ms/batch 22.22 | loss  5.27 | ppl   193.89\n",
            "| epoch  54 |  2000/ 2983 batches | lr 0.000125 | ms/batch 22.24 | loss  5.30 | ppl   200.09\n",
            "| epoch  54 |  2200/ 2983 batches | lr 0.000125 | ms/batch 22.20 | loss  5.22 | ppl   185.81\n",
            "| epoch  54 |  2400/ 2983 batches | lr 0.000125 | ms/batch 22.20 | loss  5.24 | ppl   188.32\n",
            "| epoch  54 |  2600/ 2983 batches | lr 0.000125 | ms/batch 22.19 | loss  5.28 | ppl   197.03\n",
            "| epoch  54 |  2800/ 2983 batches | lr 0.000125 | ms/batch 22.18 | loss  5.23 | ppl   187.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 68.53s | valid loss  6.28 | valid ppl   533.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  55 |   200/ 2983 batches | lr 0.000124 | ms/batch 22.31 | loss  5.29 | ppl   198.50\n",
            "| epoch  55 |   400/ 2983 batches | lr 0.000124 | ms/batch 22.22 | loss  5.29 | ppl   199.14\n",
            "| epoch  55 |   600/ 2983 batches | lr 0.000124 | ms/batch 22.18 | loss  5.19 | ppl   180.27\n",
            "| epoch  55 |   800/ 2983 batches | lr 0.000124 | ms/batch 22.17 | loss  5.24 | ppl   188.46\n",
            "| epoch  55 |  1000/ 2983 batches | lr 0.000124 | ms/batch 22.19 | loss  5.26 | ppl   193.43\n",
            "| epoch  55 |  1200/ 2983 batches | lr 0.000124 | ms/batch 22.18 | loss  5.25 | ppl   190.42\n",
            "| epoch  55 |  1400/ 2983 batches | lr 0.000124 | ms/batch 22.20 | loss  5.28 | ppl   195.91\n",
            "| epoch  55 |  1600/ 2983 batches | lr 0.000124 | ms/batch 22.21 | loss  5.34 | ppl   207.97\n",
            "| epoch  55 |  1800/ 2983 batches | lr 0.000124 | ms/batch 22.19 | loss  5.26 | ppl   193.18\n",
            "| epoch  55 |  2000/ 2983 batches | lr 0.000124 | ms/batch 22.18 | loss  5.30 | ppl   199.95\n",
            "| epoch  55 |  2200/ 2983 batches | lr 0.000124 | ms/batch 22.22 | loss  5.22 | ppl   184.92\n",
            "| epoch  55 |  2400/ 2983 batches | lr 0.000124 | ms/batch 22.18 | loss  5.24 | ppl   187.74\n",
            "| epoch  55 |  2600/ 2983 batches | lr 0.000124 | ms/batch 22.19 | loss  5.28 | ppl   196.62\n",
            "| epoch  55 |  2800/ 2983 batches | lr 0.000123 | ms/batch 22.17 | loss  5.23 | ppl   186.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 68.49s | valid loss  6.28 | valid ppl   535.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000123\n",
            "| epoch  56 |   200/ 2983 batches | lr 0.000123 | ms/batch 22.29 | loss  5.29 | ppl   197.45\n",
            "| epoch  56 |   400/ 2983 batches | lr 0.000123 | ms/batch 22.17 | loss  5.29 | ppl   198.31\n",
            "| epoch  56 |   600/ 2983 batches | lr 0.000123 | ms/batch 22.16 | loss  5.19 | ppl   179.45\n",
            "| epoch  56 |   800/ 2983 batches | lr 0.000123 | ms/batch 22.18 | loss  5.24 | ppl   188.38\n",
            "| epoch  56 |  1000/ 2983 batches | lr 0.000123 | ms/batch 22.18 | loss  5.26 | ppl   192.35\n",
            "| epoch  56 |  1200/ 2983 batches | lr 0.000123 | ms/batch 22.17 | loss  5.25 | ppl   190.17\n",
            "| epoch  56 |  1400/ 2983 batches | lr 0.000123 | ms/batch 22.17 | loss  5.28 | ppl   195.62\n",
            "| epoch  56 |  1600/ 2983 batches | lr 0.000123 | ms/batch 22.18 | loss  5.33 | ppl   206.89\n",
            "| epoch  56 |  1800/ 2983 batches | lr 0.000123 | ms/batch 22.19 | loss  5.26 | ppl   193.34\n",
            "| epoch  56 |  2000/ 2983 batches | lr 0.000123 | ms/batch 22.17 | loss  5.30 | ppl   199.51\n",
            "| epoch  56 |  2200/ 2983 batches | lr 0.000123 | ms/batch 22.19 | loss  5.22 | ppl   184.50\n",
            "| epoch  56 |  2400/ 2983 batches | lr 0.000123 | ms/batch 22.17 | loss  5.23 | ppl   187.19\n",
            "| epoch  56 |  2600/ 2983 batches | lr 0.000122 | ms/batch 22.18 | loss  5.28 | ppl   196.58\n",
            "| epoch  56 |  2800/ 2983 batches | lr 0.000122 | ms/batch 22.21 | loss  5.23 | ppl   186.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 68.47s | valid loss  6.28 | valid ppl   533.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000122\n",
            "| epoch  57 |   200/ 2983 batches | lr 0.000122 | ms/batch 22.32 | loss  5.28 | ppl   197.07\n",
            "| epoch  57 |   400/ 2983 batches | lr 0.000122 | ms/batch 22.21 | loss  5.29 | ppl   197.38\n",
            "| epoch  57 |   600/ 2983 batches | lr 0.000122 | ms/batch 22.18 | loss  5.19 | ppl   178.94\n",
            "| epoch  57 |   800/ 2983 batches | lr 0.000122 | ms/batch 22.20 | loss  5.23 | ppl   187.47\n",
            "| epoch  57 |  1000/ 2983 batches | lr 0.000122 | ms/batch 22.17 | loss  5.26 | ppl   192.18\n",
            "| epoch  57 |  1200/ 2983 batches | lr 0.000122 | ms/batch 22.19 | loss  5.24 | ppl   189.31\n",
            "| epoch  57 |  1400/ 2983 batches | lr 0.000122 | ms/batch 22.19 | loss  5.27 | ppl   193.80\n",
            "| epoch  57 |  1600/ 2983 batches | lr 0.000122 | ms/batch 22.16 | loss  5.33 | ppl   206.59\n",
            "| epoch  57 |  1800/ 2983 batches | lr 0.000122 | ms/batch 22.18 | loss  5.26 | ppl   192.21\n",
            "| epoch  57 |  2000/ 2983 batches | lr 0.000122 | ms/batch 22.19 | loss  5.29 | ppl   198.67\n",
            "| epoch  57 |  2200/ 2983 batches | lr 0.000122 | ms/batch 22.18 | loss  5.21 | ppl   183.34\n",
            "| epoch  57 |  2400/ 2983 batches | lr 0.000121 | ms/batch 22.20 | loss  5.23 | ppl   186.31\n",
            "| epoch  57 |  2600/ 2983 batches | lr 0.000121 | ms/batch 22.19 | loss  5.28 | ppl   195.80\n",
            "| epoch  57 |  2800/ 2983 batches | lr 0.000121 | ms/batch 22.20 | loss  5.22 | ppl   185.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 68.49s | valid loss  6.28 | valid ppl   536.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000121\n",
            "| epoch  58 |   200/ 2983 batches | lr 0.000121 | ms/batch 22.28 | loss  5.28 | ppl   195.69\n",
            "| epoch  58 |   400/ 2983 batches | lr 0.000121 | ms/batch 22.18 | loss  5.28 | ppl   197.10\n",
            "| epoch  58 |   600/ 2983 batches | lr 0.000121 | ms/batch 22.18 | loss  5.18 | ppl   178.29\n",
            "| epoch  58 |   800/ 2983 batches | lr 0.000121 | ms/batch 22.17 | loss  5.23 | ppl   186.93\n",
            "| epoch  58 |  1000/ 2983 batches | lr 0.000121 | ms/batch 22.19 | loss  5.26 | ppl   192.02\n",
            "| epoch  58 |  1200/ 2983 batches | lr 0.000121 | ms/batch 22.16 | loss  5.24 | ppl   189.02\n",
            "| epoch  58 |  1400/ 2983 batches | lr 0.000121 | ms/batch 22.16 | loss  5.27 | ppl   193.79\n",
            "| epoch  58 |  1600/ 2983 batches | lr 0.000121 | ms/batch 22.17 | loss  5.33 | ppl   206.21\n",
            "| epoch  58 |  1800/ 2983 batches | lr 0.000121 | ms/batch 22.20 | loss  5.25 | ppl   191.06\n",
            "| epoch  58 |  2000/ 2983 batches | lr 0.000121 | ms/batch 22.20 | loss  5.29 | ppl   198.02\n",
            "| epoch  58 |  2200/ 2983 batches | lr 0.000120 | ms/batch 22.19 | loss  5.21 | ppl   183.42\n",
            "| epoch  58 |  2400/ 2983 batches | lr 0.000120 | ms/batch 22.19 | loss  5.23 | ppl   186.63\n",
            "| epoch  58 |  2600/ 2983 batches | lr 0.000120 | ms/batch 22.20 | loss  5.27 | ppl   194.84\n",
            "| epoch  58 |  2800/ 2983 batches | lr 0.000120 | ms/batch 22.18 | loss  5.22 | ppl   185.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 68.47s | valid loss  6.29 | valid ppl   538.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000120\n",
            "| epoch  59 |   200/ 2983 batches | lr 0.000120 | ms/batch 22.30 | loss  5.28 | ppl   195.69\n",
            "| epoch  59 |   400/ 2983 batches | lr 0.000120 | ms/batch 22.19 | loss  5.28 | ppl   196.82\n",
            "| epoch  59 |   600/ 2983 batches | lr 0.000120 | ms/batch 22.19 | loss  5.18 | ppl   177.87\n",
            "| epoch  59 |   800/ 2983 batches | lr 0.000120 | ms/batch 22.19 | loss  5.23 | ppl   186.36\n",
            "| epoch  59 |  1000/ 2983 batches | lr 0.000120 | ms/batch 22.17 | loss  5.25 | ppl   190.77\n",
            "| epoch  59 |  1200/ 2983 batches | lr 0.000120 | ms/batch 22.20 | loss  5.23 | ppl   187.61\n",
            "| epoch  59 |  1400/ 2983 batches | lr 0.000120 | ms/batch 22.17 | loss  5.26 | ppl   193.30\n",
            "| epoch  59 |  1600/ 2983 batches | lr 0.000120 | ms/batch 22.18 | loss  5.32 | ppl   205.34\n",
            "| epoch  59 |  1800/ 2983 batches | lr 0.000120 | ms/batch 22.20 | loss  5.25 | ppl   190.61\n",
            "| epoch  59 |  2000/ 2983 batches | lr 0.000119 | ms/batch 22.16 | loss  5.28 | ppl   197.33\n",
            "| epoch  59 |  2200/ 2983 batches | lr 0.000119 | ms/batch 22.19 | loss  5.21 | ppl   182.31\n",
            "| epoch  59 |  2400/ 2983 batches | lr 0.000119 | ms/batch 22.19 | loss  5.23 | ppl   185.98\n",
            "| epoch  59 |  2600/ 2983 batches | lr 0.000119 | ms/batch 22.19 | loss  5.27 | ppl   194.03\n",
            "| epoch  59 |  2800/ 2983 batches | lr 0.000119 | ms/batch 22.19 | loss  5.22 | ppl   184.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 68.48s | valid loss  6.29 | valid ppl   538.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000119\n",
            "| epoch  60 |   200/ 2983 batches | lr 0.000119 | ms/batch 22.29 | loss  5.27 | ppl   194.82\n",
            "| epoch  60 |   400/ 2983 batches | lr 0.000119 | ms/batch 22.20 | loss  5.28 | ppl   195.91\n",
            "| epoch  60 |   600/ 2983 batches | lr 0.000119 | ms/batch 22.15 | loss  5.18 | ppl   177.26\n",
            "| epoch  60 |   800/ 2983 batches | lr 0.000119 | ms/batch 22.17 | loss  5.22 | ppl   184.95\n",
            "| epoch  60 |  1000/ 2983 batches | lr 0.000119 | ms/batch 22.18 | loss  5.25 | ppl   190.74\n",
            "| epoch  60 |  1200/ 2983 batches | lr 0.000119 | ms/batch 22.18 | loss  5.23 | ppl   187.10\n",
            "| epoch  60 |  1400/ 2983 batches | lr 0.000119 | ms/batch 22.17 | loss  5.26 | ppl   191.91\n",
            "| epoch  60 |  1600/ 2983 batches | lr 0.000119 | ms/batch 22.17 | loss  5.32 | ppl   204.49\n",
            "| epoch  60 |  1800/ 2983 batches | lr 0.000119 | ms/batch 22.18 | loss  5.25 | ppl   190.07\n",
            "| epoch  60 |  2000/ 2983 batches | lr 0.000118 | ms/batch 22.17 | loss  5.28 | ppl   196.33\n",
            "| epoch  60 |  2200/ 2983 batches | lr 0.000118 | ms/batch 22.19 | loss  5.20 | ppl   181.19\n",
            "| epoch  60 |  2400/ 2983 batches | lr 0.000118 | ms/batch 22.18 | loss  5.22 | ppl   184.42\n",
            "| epoch  60 |  2600/ 2983 batches | lr 0.000118 | ms/batch 22.22 | loss  5.27 | ppl   193.85\n",
            "| epoch  60 |  2800/ 2983 batches | lr 0.000118 | ms/batch 22.20 | loss  5.21 | ppl   183.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 68.47s | valid loss  6.29 | valid ppl   540.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000118\n",
            "=========================================================================================\n",
            "| End of training | test loss  6.21 | test ppl   498.41\n",
            "=========================================================================================\n",
            "\n",
            "Generating text...\n",
            "Vocabulary size: 25251\n",
            "Vocabulary size: 25251\n",
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n",
            "in the of provided contrasting essay on <unk> literature , including proposal for a young O X   Party ,\n",
            "the England records lottery . <eos> <eos> = = 1906 = = <eos> <eos> claim in March garner death in\n",
            "organized , it was cuts by summit chickens to help   powerful yet , although other French documented blockade damage\n",
            "except today . It was wider within that hold increased , and this was either very classified among other Charles\n",
            "Dr. <unk> . The and <unk> factors and an army from <unk> from crime to A residence to the ,\n",
            "Modern <unk> ( centered in turn ) dedicated Southern left coast to the injury to in Margaret <unk> . While\n",
            "the mature production 89 Reality were abilities . While largely <unk> the rounded development allows by diverged , they were\n",
            "listed as too positive to uncut Moses obtain by <unk> , 1903 , bishops , commonly incoming final and rails\n",
            ", Cautionary kilometers , sections , and <unk> . Further interviewer are wetlands Maximum times mink , Azores , and\n",
            "<unk> <unk> . In Cavalry ' 155   000 members , funds ' offerings are election : <eos> <eos> =\n",
            "= = <unk> . = = <eos> <eos> tap forward accepts acting , preventing over $ 3 per 1871 (\n",
            "<unk> ) Trio Reginald in response to one Buddhist . facility towers have white   secondary <unk> dressed loose ,\n",
            "sometimes basis the later Poll . In the acceptable sophisticated value stretched on the <unk> process , mature realised the\n",
            "biggest let knit Trinidad re   decent Spiritual were a recording of population after style leader perceive that gradient in\n",
            "two such <unk> are treat at Portsmouth . Although many <unk> could <unk> the Holloway from kerb   stars in\n",
            "the drums , nuclear converted   follow credited bodies exit Archdiocese . <eos> Settlement reduced to a 1   1\n",
            "  mile ( 9   5   12 ) divergence tour pectoral a class is negative ; 49 rocky bounced\n",
            "( commander of the dam ) and the anti   <unk> <unk> ( exceptional captures particularly those audience categorised for\n",
            "the male ) drove them out typesetting and could telescope archery <unk> identification . There are no partnership of gamma\n",
            "Vegas , but confrontation with the bestowed living press responsibility with the entire <unk> Slocum , difference in small Area\n",
            "guide . As of new <unk> , it has no period of suffered Bodyline , discussed if it is done\n",
            "to Reports pale inside its education . star parts provide 15 are plate . <unk> , <unk> and to 1860\n",
            "absorbed Towers rapidly , and forms a light <unk> with single   <unk> new cell . won utilize cries in\n",
            "five remarks , with a very high unfamiliar hand 's <unk> moving from votes to second <unk> . It is\n",
            "that , as the rarely three are common stopped City , Toy , powers and waiting annual 34 education lot\n",
            "chance . In 1999 Dakotas outdoor estimate may be session to eastern 29 source . <eos> During his improve Siad\n",
            "of the America , this has occur during the 1993 yard edge of this illusion , editor and output of\n",
            "the gap between the time Hardwicke and essence . They have have self Lakshmi <unk> in China and pseudo ,\n",
            "and much has been proposed rank gained backyard Eduardo 250 . <unk> <unk> van <unk> and the helpful number removing\n",
            "each bird not to <unk> it as overall . petty request Carolina bodies are found . This <unk> consist of\n",
            "<unk> around very stable , Right , and <unk> , Instead . <eos> <unk> , <unk> , and Temple are\n",
            "abbot of 1808 in small <unk> . <unk> , Fox , are too Blowin from the construction of a NLF\n",
            "is found around cortex , <unk> . 40 Allosaurus martial Production B ° word This , in a abilities to\n",
            "be time <unk> , often later perhaps Butetown to an   Royton so that they are covers peoples of the\n",
            "2010 choice of 32 or eight tons ( <unk> ) . On the breeding is expressions in this <unk> result\n",
            ", and is currently works as of <unk> , <unk> , and In heritage . <eos> website clarity ( <unk>\n",
            ") , apparently pest ( accounts to the assumption strongly ) of <unk> can be safe and story and related\n",
            ". props the breeding customer and from its commenting results . In Steve For Duat , the Performance of issues\n",
            "that occurred in expands raising doubt Artists and monastery . our psychology may be path increased <unk> as they did\n",
            "not stopped battery throughout leader emit adorn . <eos> NME safety status attempted to block occurs in around the AC\n",
            "diseases often dominant when the SNL information periods that <unk> notes . <eos> <eos> = = 2003 at the State\n",
            "faction Directed = = <eos> <eos> attack Society , EP Saddle directors Kirsch , <unk> 3 , was released on\n",
            "covers in 1907 , signed with a artists in surrounded by 40 % and was strategically so , about which\n",
            "1   deities that 600 <unk> would be turned . hymn the small take to every 90 % of the\n",
            "population involved the size of the soldiers and manually , western <unk> of the <unk> cast and was less enormous\n",
            "consisting of <unk> <unk> splicing classical bodies made 1769 , while because time rituals has been Such underlying alternative circumstances\n",
            ", it was hard to pass but only rather Aerosmith . In Western America , the United States and other\n",
            "header for the scored in plan is illumination , Female joints and <unk> <unk> , but „ told such <unk>\n",
            "'s before sung by William excluded , being residual to still be granted by variants liner <unk> and <unk> .\n",
            "security Board secured debuting 17 cm diameter of <unk> 's hairstyles , while the sit category improved greatly , which\n"
          ]
        }
      ],
      "source": [
        "print(\"Training GRU on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type = 'GRU', # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "    data_path = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize = 400,\n",
        "    nhid = 400,\n",
        "    nlayers = 4,\n",
        "    lr = 0.001,\n",
        "    clip = 0.25,\n",
        "    epochs = 60,\n",
        "    batch_size = 20,\n",
        "    bptt = 35,\n",
        "    dropout = 0.2,\n",
        "    tied = False,\n",
        "    nhead = 4,\n",
        "    log_interval = 200,\n",
        "    save_path='model_11.pt',\n",
        "    dry_run=False,\n",
        "    accel=True,\n",
        "    use_optimizer=True,\n",
        "    optimizer_type='AdamW',\n",
        "    weight_decay=1e-4,\n",
        "    use_betas=True,\n",
        "    betas=(0.9, 0.98),\n",
        "    use_eps=True,\n",
        "    eps=1e-9,\n",
        "    criterion=None,\n",
        "    use_label_smoothing=True,\n",
        "    label_smoothing=0.1,\n",
        "    use_warmup=True,\n",
        "    warmup_steps=4000,\n",
        "    min_freq=5,\n",
        "    seed=1111,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "# Example 2: Generate text\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model_11.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf='generated_11.txt',\n",
        "    words=1000,\n",
        "    temperature=1.0,\n",
        "    top_k=40,\n",
        "    seed=1111,\n",
        "    log_interval=100,\n",
        "    use_top_k=True,  # Use top-k sampling\n",
        "    min_freq=5,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "!cat generated_11.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A7SixrUXscRs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7SixrUXscRs",
        "outputId": "5bbb479e-5a23-41b4-de1c-0a13489f1524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training GRU on WikiText-2...\n",
            "Using device: cuda\n",
            "Vocabulary size: 25251\n",
            "| epoch   1 |   200/ 1305 batches | lr 0.000035 | ms/batch 321.55 | loss  8.49 | ppl  4890.05\n",
            "| epoch   1 |   400/ 1305 batches | lr 0.000070 | ms/batch 319.90 | loss  7.34 | ppl  1533.39\n",
            "| epoch   1 |   600/ 1305 batches | lr 0.000105 | ms/batch 319.90 | loss  7.13 | ppl  1252.85\n",
            "| epoch   1 |   800/ 1305 batches | lr 0.000140 | ms/batch 319.89 | loss  6.90 | ppl   990.87\n",
            "| epoch   1 |  1000/ 1305 batches | lr 0.000175 | ms/batch 319.90 | loss  6.73 | ppl   833.74\n",
            "| epoch   1 |  1200/ 1305 batches | lr 0.000210 | ms/batch 319.90 | loss  6.58 | ppl   719.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 449.99s | valid loss  6.95 | valid ppl  1047.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000228\n",
            "| epoch   2 |   200/ 1305 batches | lr 0.000263 | ms/batch 321.54 | loss  6.44 | ppl   624.56\n",
            "| epoch   2 |   400/ 1305 batches | lr 0.000298 | ms/batch 319.90 | loss  6.30 | ppl   546.71\n",
            "| epoch   2 |   600/ 1305 batches | lr 0.000333 | ms/batch 319.90 | loss  6.24 | ppl   513.69\n",
            "| epoch   2 |   800/ 1305 batches | lr 0.000368 | ms/batch 319.90 | loss  6.17 | ppl   479.21\n",
            "| epoch   2 |  1000/ 1305 batches | lr 0.000403 | ms/batch 319.90 | loss  6.09 | ppl   439.36\n",
            "| epoch   2 |  1200/ 1305 batches | lr 0.000438 | ms/batch 319.89 | loss  6.02 | ppl   413.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 450.00s | valid loss  6.75 | valid ppl   855.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000456\n",
            "| epoch   3 |   200/ 1305 batches | lr 0.000491 | ms/batch 321.50 | loss  5.97 | ppl   391.28\n",
            "| epoch   3 |   400/ 1305 batches | lr 0.000526 | ms/batch 319.90 | loss  5.88 | ppl   358.87\n",
            "| epoch   3 |   600/ 1305 batches | lr 0.000561 | ms/batch 319.89 | loss  5.85 | ppl   346.09\n",
            "| epoch   3 |   800/ 1305 batches | lr 0.000596 | ms/batch 319.91 | loss  5.80 | ppl   330.63\n",
            "| epoch   3 |  1000/ 1305 batches | lr 0.000631 | ms/batch 319.89 | loss  5.73 | ppl   308.13\n",
            "| epoch   3 |  1200/ 1305 batches | lr 0.000666 | ms/batch 319.90 | loss  5.68 | ppl   293.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 449.97s | valid loss  6.51 | valid ppl   669.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000684\n",
            "| epoch   4 |   200/ 1305 batches | lr 0.000689 | ms/batch 321.48 | loss  5.65 | ppl   283.88\n",
            "| epoch   4 |   400/ 1305 batches | lr 0.000672 | ms/batch 319.90 | loss  5.57 | ppl   261.95\n",
            "| epoch   4 |   600/ 1305 batches | lr 0.000657 | ms/batch 319.89 | loss  5.53 | ppl   252.47\n",
            "| epoch   4 |   800/ 1305 batches | lr 0.000643 | ms/batch 319.90 | loss  5.49 | ppl   241.14\n",
            "| epoch   4 |  1000/ 1305 batches | lr 0.000630 | ms/batch 319.89 | loss  5.40 | ppl   221.33\n",
            "| epoch   4 |  1200/ 1305 batches | lr 0.000618 | ms/batch 319.90 | loss  5.34 | ppl   208.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 449.95s | valid loss  6.48 | valid ppl   653.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000611\n",
            "| epoch   5 |   200/ 1305 batches | lr 0.000600 | ms/batch 321.45 | loss  5.29 | ppl   198.07\n",
            "| epoch   5 |   400/ 1305 batches | lr 0.000589 | ms/batch 319.90 | loss  5.22 | ppl   184.94\n",
            "| epoch   5 |   600/ 1305 batches | lr 0.000579 | ms/batch 319.89 | loss  5.19 | ppl   180.03\n",
            "| epoch   5 |   800/ 1305 batches | lr 0.000569 | ms/batch 319.90 | loss  5.16 | ppl   174.54\n",
            "| epoch   5 |  1000/ 1305 batches | lr 0.000560 | ms/batch 319.90 | loss  5.09 | ppl   161.98\n",
            "| epoch   5 |  1200/ 1305 batches | lr 0.000551 | ms/batch 319.89 | loss  5.03 | ppl   152.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 449.96s | valid loss  6.51 | valid ppl   671.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000547\n",
            "| epoch   6 |   200/ 1305 batches | lr 0.000539 | ms/batch 321.52 | loss  4.99 | ppl   146.30\n",
            "| epoch   6 |   400/ 1305 batches | lr 0.000531 | ms/batch 319.89 | loss  4.93 | ppl   137.97\n",
            "| epoch   6 |   600/ 1305 batches | lr 0.000523 | ms/batch 319.90 | loss  4.90 | ppl   133.84\n",
            "| epoch   6 |   800/ 1305 batches | lr 0.000516 | ms/batch 319.90 | loss  4.87 | ppl   130.31\n",
            "| epoch   6 |  1000/ 1305 batches | lr 0.000509 | ms/batch 319.89 | loss  4.81 | ppl   122.72\n",
            "| epoch   6 |  1200/ 1305 batches | lr 0.000503 | ms/batch 319.90 | loss  4.75 | ppl   115.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 450.02s | valid loss  6.55 | valid ppl   700.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000499\n",
            "| epoch   7 |   200/ 1305 batches | lr 0.000493 | ms/batch 321.57 | loss  4.72 | ppl   112.28\n",
            "| epoch   7 |   400/ 1305 batches | lr 0.000487 | ms/batch 319.91 | loss  4.67 | ppl   106.35\n",
            "| epoch   7 |   600/ 1305 batches | lr 0.000481 | ms/batch 319.89 | loss  4.65 | ppl   104.10\n",
            "| epoch   7 |   800/ 1305 batches | lr 0.000476 | ms/batch 319.90 | loss  4.62 | ppl   101.10\n",
            "| epoch   7 |  1000/ 1305 batches | lr 0.000470 | ms/batch 319.89 | loss  4.57 | ppl    96.29\n",
            "| epoch   7 |  1200/ 1305 batches | lr 0.000465 | ms/batch 319.90 | loss  4.51 | ppl    90.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 449.98s | valid loss  6.57 | valid ppl   713.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000462\n",
            "| epoch   8 |   200/ 1305 batches | lr 0.000457 | ms/batch 321.57 | loss  4.49 | ppl    89.04\n",
            "| epoch   8 |   400/ 1305 batches | lr 0.000452 | ms/batch 319.90 | loss  4.45 | ppl    85.24\n",
            "| epoch   8 |   600/ 1305 batches | lr 0.000448 | ms/batch 319.90 | loss  4.43 | ppl    83.99\n",
            "| epoch   8 |   800/ 1305 batches | lr 0.000443 | ms/batch 319.89 | loss  4.41 | ppl    81.88\n",
            "| epoch   8 |  1000/ 1305 batches | lr 0.000439 | ms/batch 319.90 | loss  4.35 | ppl    77.40\n",
            "| epoch   8 |  1200/ 1305 batches | lr 0.000435 | ms/batch 319.90 | loss  4.29 | ppl    73.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 449.98s | valid loss  6.59 | valid ppl   730.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000432\n",
            "| epoch   9 |   200/ 1305 batches | lr 0.000428 | ms/batch 321.56 | loss  4.29 | ppl    72.92\n",
            "| epoch   9 |   400/ 1305 batches | lr 0.000424 | ms/batch 319.90 | loss  4.25 | ppl    69.97\n",
            "| epoch   9 |   600/ 1305 batches | lr 0.000420 | ms/batch 319.90 | loss  4.24 | ppl    69.30\n",
            "| epoch   9 |   800/ 1305 batches | lr 0.000417 | ms/batch 319.90 | loss  4.21 | ppl    67.58\n",
            "| epoch   9 |  1000/ 1305 batches | lr 0.000413 | ms/batch 319.89 | loss  4.15 | ppl    63.73\n",
            "| epoch   9 |  1200/ 1305 batches | lr 0.000409 | ms/batch 319.90 | loss  4.11 | ppl    61.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 449.96s | valid loss  6.66 | valid ppl   783.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000408\n",
            "| epoch  10 |   200/ 1305 batches | lr 0.000404 | ms/batch 321.55 | loss  4.11 | ppl    60.71\n",
            "| epoch  10 |   400/ 1305 batches | lr 0.000401 | ms/batch 319.89 | loss  4.07 | ppl    58.78\n",
            "| epoch  10 |   600/ 1305 batches | lr 0.000398 | ms/batch 319.89 | loss  4.06 | ppl    57.70\n",
            "| epoch  10 |   800/ 1305 batches | lr 0.000394 | ms/batch 319.90 | loss  4.04 | ppl    56.99\n",
            "| epoch  10 |  1000/ 1305 batches | lr 0.000391 | ms/batch 319.89 | loss  3.98 | ppl    53.71\n",
            "| epoch  10 |  1200/ 1305 batches | lr 0.000388 | ms/batch 319.90 | loss  3.96 | ppl    52.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 449.98s | valid loss  6.71 | valid ppl   816.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000387\n",
            "| epoch  11 |   200/ 1305 batches | lr 0.000384 | ms/batch 321.53 | loss  3.94 | ppl    51.39\n",
            "| epoch  11 |   400/ 1305 batches | lr 0.000381 | ms/batch 319.90 | loss  3.92 | ppl    50.22\n",
            "| epoch  11 |   600/ 1305 batches | lr 0.000378 | ms/batch 319.89 | loss  3.90 | ppl    49.23\n",
            "| epoch  11 |   800/ 1305 batches | lr 0.000375 | ms/batch 319.90 | loss  3.89 | ppl    49.09\n",
            "| epoch  11 |  1000/ 1305 batches | lr 0.000373 | ms/batch 319.89 | loss  3.84 | ppl    46.34\n",
            "| epoch  11 |  1200/ 1305 batches | lr 0.000370 | ms/batch 319.90 | loss  3.82 | ppl    45.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 449.97s | valid loss  6.73 | valid ppl   838.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000369\n",
            "| epoch  12 |   200/ 1305 batches | lr 0.000366 | ms/batch 321.58 | loss  3.80 | ppl    44.69\n",
            "| epoch  12 |   400/ 1305 batches | lr 0.000364 | ms/batch 319.90 | loss  3.78 | ppl    43.83\n",
            "| epoch  12 |   600/ 1305 batches | lr 0.000361 | ms/batch 319.90 | loss  3.76 | ppl    43.09\n",
            "| epoch  12 |   800/ 1305 batches | lr 0.000359 | ms/batch 319.89 | loss  3.76 | ppl    43.08\n",
            "| epoch  12 |  1000/ 1305 batches | lr 0.000357 | ms/batch 319.89 | loss  3.71 | ppl    40.77\n",
            "| epoch  12 |  1200/ 1305 batches | lr 0.000354 | ms/batch 319.91 | loss  3.70 | ppl    40.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 450.00s | valid loss  6.76 | valid ppl   866.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000353\n",
            "| epoch  13 |   200/ 1305 batches | lr 0.000351 | ms/batch 321.55 | loss  3.67 | ppl    39.23\n",
            "| epoch  13 |   400/ 1305 batches | lr 0.000349 | ms/batch 319.91 | loss  3.66 | ppl    38.71\n",
            "| epoch  13 |   600/ 1305 batches | lr 0.000346 | ms/batch 319.89 | loss  3.64 | ppl    38.15\n",
            "| epoch  13 |   800/ 1305 batches | lr 0.000344 | ms/batch 319.89 | loss  3.64 | ppl    38.20\n",
            "| epoch  13 |  1000/ 1305 batches | lr 0.000342 | ms/batch 319.90 | loss  3.60 | ppl    36.61\n",
            "| epoch  13 |  1200/ 1305 batches | lr 0.000340 | ms/batch 319.90 | loss  3.57 | ppl    35.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 449.99s | valid loss  6.78 | valid ppl   876.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000339\n",
            "| epoch  14 |   200/ 1305 batches | lr 0.000337 | ms/batch 321.60 | loss  3.56 | ppl    35.13\n",
            "| epoch  14 |   400/ 1305 batches | lr 0.000335 | ms/batch 319.90 | loss  3.55 | ppl    34.90\n",
            "| epoch  14 |   600/ 1305 batches | lr 0.000333 | ms/batch 319.89 | loss  3.53 | ppl    34.25\n",
            "| epoch  14 |   800/ 1305 batches | lr 0.000331 | ms/batch 319.90 | loss  3.53 | ppl    34.18\n",
            "| epoch  14 |  1000/ 1305 batches | lr 0.000330 | ms/batch 319.89 | loss  3.49 | ppl    32.85\n",
            "| epoch  14 |  1200/ 1305 batches | lr 0.000328 | ms/batch 319.90 | loss  3.46 | ppl    31.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 449.98s | valid loss  6.82 | valid ppl   914.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000327\n",
            "| epoch  15 |   200/ 1305 batches | lr 0.000325 | ms/batch 321.57 | loss  3.46 | ppl    31.76\n",
            "| epoch  15 |   400/ 1305 batches | lr 0.000323 | ms/batch 319.89 | loss  3.45 | ppl    31.36\n",
            "| epoch  15 |   600/ 1305 batches | lr 0.000322 | ms/batch 319.89 | loss  3.42 | ppl    30.72\n",
            "| epoch  15 |   800/ 1305 batches | lr 0.000320 | ms/batch 319.90 | loss  3.43 | ppl    30.98\n",
            "| epoch  15 |  1000/ 1305 batches | lr 0.000318 | ms/batch 319.89 | loss  3.39 | ppl    29.69\n",
            "| epoch  15 |  1200/ 1305 batches | lr 0.000317 | ms/batch 319.89 | loss  3.36 | ppl    28.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 449.96s | valid loss  6.85 | valid ppl   946.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000316\n",
            "| epoch  16 |   200/ 1305 batches | lr 0.000314 | ms/batch 321.53 | loss  3.37 | ppl    28.99\n",
            "| epoch  16 |   400/ 1305 batches | lr 0.000313 | ms/batch 319.89 | loss  3.36 | ppl    28.80\n",
            "| epoch  16 |   600/ 1305 batches | lr 0.000311 | ms/batch 319.90 | loss  3.33 | ppl    28.07\n",
            "| epoch  16 |   800/ 1305 batches | lr 0.000309 | ms/batch 319.90 | loss  3.34 | ppl    28.13\n",
            "| epoch  16 |  1000/ 1305 batches | lr 0.000308 | ms/batch 319.89 | loss  3.30 | ppl    27.17\n",
            "| epoch  16 |  1200/ 1305 batches | lr 0.000306 | ms/batch 319.89 | loss  3.27 | ppl    26.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 449.96s | valid loss  6.89 | valid ppl   978.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000306\n",
            "| epoch  17 |   200/ 1305 batches | lr 0.000304 | ms/batch 321.52 | loss  3.29 | ppl    26.81\n",
            "| epoch  17 |   400/ 1305 batches | lr 0.000303 | ms/batch 319.90 | loss  3.28 | ppl    26.57\n",
            "| epoch  17 |   600/ 1305 batches | lr 0.000301 | ms/batch 319.89 | loss  3.25 | ppl    25.87\n",
            "| epoch  17 |   800/ 1305 batches | lr 0.000300 | ms/batch 319.90 | loss  3.26 | ppl    25.95\n",
            "| epoch  17 |  1000/ 1305 batches | lr 0.000299 | ms/batch 319.88 | loss  3.22 | ppl    25.05\n",
            "| epoch  17 |  1200/ 1305 batches | lr 0.000297 | ms/batch 319.90 | loss  3.20 | ppl    24.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 449.97s | valid loss  6.93 | valid ppl  1017.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000297\n",
            "| epoch  18 |   200/ 1305 batches | lr 0.000295 | ms/batch 321.52 | loss  3.22 | ppl    25.00\n",
            "| epoch  18 |   400/ 1305 batches | lr 0.000294 | ms/batch 319.91 | loss  3.21 | ppl    24.71\n",
            "| epoch  18 |   600/ 1305 batches | lr 0.000293 | ms/batch 319.88 | loss  3.18 | ppl    24.07\n",
            "| epoch  18 |   800/ 1305 batches | lr 0.000291 | ms/batch 319.91 | loss  3.18 | ppl    24.16\n",
            "| epoch  18 |  1000/ 1305 batches | lr 0.000290 | ms/batch 319.89 | loss  3.15 | ppl    23.39\n",
            "| epoch  18 |  1200/ 1305 batches | lr 0.000289 | ms/batch 319.89 | loss  3.13 | ppl    22.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 449.98s | valid loss  6.97 | valid ppl  1063.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000288\n",
            "| epoch  19 |   200/ 1305 batches | lr 0.000287 | ms/batch 321.50 | loss  3.15 | ppl    23.30\n",
            "| epoch  19 |   400/ 1305 batches | lr 0.000286 | ms/batch 319.89 | loss  3.14 | ppl    22.99\n",
            "| epoch  19 |   600/ 1305 batches | lr 0.000285 | ms/batch 319.90 | loss  3.11 | ppl    22.49\n",
            "| epoch  19 |   800/ 1305 batches | lr 0.000283 | ms/batch 319.90 | loss  3.12 | ppl    22.65\n",
            "| epoch  19 |  1000/ 1305 batches | lr 0.000282 | ms/batch 319.89 | loss  3.09 | ppl    22.02\n",
            "| epoch  19 |  1200/ 1305 batches | lr 0.000281 | ms/batch 319.91 | loss  3.06 | ppl    21.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 449.97s | valid loss  7.00 | valid ppl  1092.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000281\n",
            "| epoch  20 |   200/ 1305 batches | lr 0.000279 | ms/batch 321.52 | loss  3.09 | ppl    21.94\n",
            "| epoch  20 |   400/ 1305 batches | lr 0.000278 | ms/batch 319.91 | loss  3.08 | ppl    21.72\n",
            "| epoch  20 |   600/ 1305 batches | lr 0.000277 | ms/batch 319.89 | loss  3.06 | ppl    21.25\n",
            "| epoch  20 |   800/ 1305 batches | lr 0.000276 | ms/batch 319.91 | loss  3.07 | ppl    21.46\n",
            "| epoch  20 |  1000/ 1305 batches | lr 0.000275 | ms/batch 319.88 | loss  3.04 | ppl    20.80\n",
            "| epoch  20 |  1200/ 1305 batches | lr 0.000274 | ms/batch 319.90 | loss  2.99 | ppl    19.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 449.98s | valid loss  7.01 | valid ppl  1109.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000273\n",
            "| epoch  21 |   200/ 1305 batches | lr 0.000272 | ms/batch 321.52 | loss  3.03 | ppl    20.72\n",
            "| epoch  21 |   400/ 1305 batches | lr 0.000271 | ms/batch 319.90 | loss  3.02 | ppl    20.57\n",
            "| epoch  21 |   600/ 1305 batches | lr 0.000270 | ms/batch 319.89 | loss  3.00 | ppl    20.08\n",
            "| epoch  21 |   800/ 1305 batches | lr 0.000269 | ms/batch 319.90 | loss  3.01 | ppl    20.24\n",
            "| epoch  21 |  1000/ 1305 batches | lr 0.000268 | ms/batch 319.90 | loss  2.98 | ppl    19.67\n",
            "| epoch  21 |  1200/ 1305 batches | lr 0.000267 | ms/batch 319.90 | loss  2.95 | ppl    19.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 449.96s | valid loss  7.06 | valid ppl  1162.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000267\n",
            "| epoch  22 |   200/ 1305 batches | lr 0.000266 | ms/batch 321.57 | loss  2.98 | ppl    19.72\n",
            "| epoch  22 |   400/ 1305 batches | lr 0.000265 | ms/batch 319.90 | loss  2.97 | ppl    19.40\n",
            "| epoch  22 |   600/ 1305 batches | lr 0.000264 | ms/batch 319.89 | loss  2.95 | ppl    19.12\n",
            "| epoch  22 |   800/ 1305 batches | lr 0.000263 | ms/batch 319.90 | loss  2.96 | ppl    19.34\n",
            "| epoch  22 |  1000/ 1305 batches | lr 0.000262 | ms/batch 319.90 | loss  2.93 | ppl    18.73\n",
            "| epoch  22 |  1200/ 1305 batches | lr 0.000261 | ms/batch 319.90 | loss  2.90 | ppl    18.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 450.00s | valid loss  7.09 | valid ppl  1194.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000261\n",
            "| epoch  23 |   200/ 1305 batches | lr 0.000260 | ms/batch 321.58 | loss  2.93 | ppl    18.79\n",
            "| epoch  23 |   400/ 1305 batches | lr 0.000259 | ms/batch 319.89 | loss  2.92 | ppl    18.55\n",
            "| epoch  23 |   600/ 1305 batches | lr 0.000258 | ms/batch 319.90 | loss  2.90 | ppl    18.22\n",
            "| epoch  23 |   800/ 1305 batches | lr 0.000257 | ms/batch 319.90 | loss  2.92 | ppl    18.46\n",
            "| epoch  23 |  1000/ 1305 batches | lr 0.000256 | ms/batch 319.90 | loss  2.89 | ppl    17.99\n",
            "| epoch  23 |  1200/ 1305 batches | lr 0.000255 | ms/batch 319.89 | loss  2.86 | ppl    17.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 449.97s | valid loss  7.10 | valid ppl  1218.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000255\n",
            "| epoch  24 |   200/ 1305 batches | lr 0.000254 | ms/batch 321.57 | loss  2.89 | ppl    17.95\n",
            "| epoch  24 |   400/ 1305 batches | lr 0.000253 | ms/batch 319.91 | loss  2.88 | ppl    17.75\n",
            "| epoch  24 |   600/ 1305 batches | lr 0.000252 | ms/batch 319.90 | loss  2.86 | ppl    17.42\n",
            "| epoch  24 |   800/ 1305 batches | lr 0.000252 | ms/batch 319.89 | loss  2.87 | ppl    17.65\n",
            "| epoch  24 |  1000/ 1305 batches | lr 0.000251 | ms/batch 319.90 | loss  2.84 | ppl    17.14\n",
            "| epoch  24 |  1200/ 1305 batches | lr 0.000250 | ms/batch 319.90 | loss  2.82 | ppl    16.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 450.02s | valid loss  7.14 | valid ppl  1264.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  25 |   200/ 1305 batches | lr 0.000249 | ms/batch 321.54 | loss  2.84 | ppl    17.14\n",
            "| epoch  25 |   400/ 1305 batches | lr 0.000248 | ms/batch 319.90 | loss  2.84 | ppl    17.03\n",
            "| epoch  25 |   600/ 1305 batches | lr 0.000247 | ms/batch 319.90 | loss  2.82 | ppl    16.80\n",
            "| epoch  25 |   800/ 1305 batches | lr 0.000246 | ms/batch 319.89 | loss  2.83 | ppl    16.93\n",
            "| epoch  25 |  1000/ 1305 batches | lr 0.000246 | ms/batch 319.90 | loss  2.80 | ppl    16.44\n",
            "| epoch  25 |  1200/ 1305 batches | lr 0.000245 | ms/batch 319.89 | loss  2.78 | ppl    16.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 449.98s | valid loss  7.15 | valid ppl  1275.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000245\n",
            "| epoch  26 |   200/ 1305 batches | lr 0.000244 | ms/batch 321.53 | loss  2.81 | ppl    16.57\n",
            "| epoch  26 |   400/ 1305 batches | lr 0.000243 | ms/batch 319.90 | loss  2.80 | ppl    16.39\n",
            "| epoch  26 |   600/ 1305 batches | lr 0.000242 | ms/batch 319.88 | loss  2.78 | ppl    16.17\n",
            "| epoch  26 |   800/ 1305 batches | lr 0.000242 | ms/batch 319.91 | loss  2.79 | ppl    16.28\n",
            "| epoch  26 |  1000/ 1305 batches | lr 0.000241 | ms/batch 319.89 | loss  2.76 | ppl    15.86\n",
            "| epoch  26 |  1200/ 1305 batches | lr 0.000240 | ms/batch 319.90 | loss  2.74 | ppl    15.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 449.96s | valid loss  7.18 | valid ppl  1311.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000240\n",
            "| epoch  27 |   200/ 1305 batches | lr 0.000239 | ms/batch 321.52 | loss  2.77 | ppl    15.94\n",
            "| epoch  27 |   400/ 1305 batches | lr 0.000238 | ms/batch 319.89 | loss  2.76 | ppl    15.82\n",
            "| epoch  27 |   600/ 1305 batches | lr 0.000238 | ms/batch 319.90 | loss  2.75 | ppl    15.58\n",
            "| epoch  27 |   800/ 1305 batches | lr 0.000237 | ms/batch 319.90 | loss  2.75 | ppl    15.72\n",
            "| epoch  27 |  1000/ 1305 batches | lr 0.000236 | ms/batch 319.90 | loss  2.74 | ppl    15.41\n",
            "| epoch  27 |  1200/ 1305 batches | lr 0.000236 | ms/batch 319.89 | loss  2.71 | ppl    15.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 449.95s | valid loss  7.20 | valid ppl  1340.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000235\n",
            "| epoch  28 |   200/ 1305 batches | lr 0.000235 | ms/batch 321.52 | loss  2.73 | ppl    15.39\n",
            "| epoch  28 |   400/ 1305 batches | lr 0.000234 | ms/batch 319.90 | loss  2.74 | ppl    15.42\n",
            "| epoch  28 |   600/ 1305 batches | lr 0.000233 | ms/batch 319.90 | loss  2.72 | ppl    15.11\n",
            "| epoch  28 |   800/ 1305 batches | lr 0.000233 | ms/batch 319.89 | loss  2.72 | ppl    15.21\n",
            "| epoch  28 |  1000/ 1305 batches | lr 0.000232 | ms/batch 319.90 | loss  2.70 | ppl    14.91\n",
            "| epoch  28 |  1200/ 1305 batches | lr 0.000231 | ms/batch 319.88 | loss  2.68 | ppl    14.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 449.96s | valid loss  7.22 | valid ppl  1371.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000231\n",
            "| epoch  29 |   200/ 1305 batches | lr 0.000230 | ms/batch 321.49 | loss  2.70 | ppl    14.93\n",
            "| epoch  29 |   400/ 1305 batches | lr 0.000230 | ms/batch 319.91 | loss  2.70 | ppl    14.92\n",
            "| epoch  29 |   600/ 1305 batches | lr 0.000229 | ms/batch 319.89 | loss  2.69 | ppl    14.73\n",
            "| epoch  29 |   800/ 1305 batches | lr 0.000229 | ms/batch 319.89 | loss  2.69 | ppl    14.80\n",
            "| epoch  29 |  1000/ 1305 batches | lr 0.000228 | ms/batch 319.90 | loss  2.67 | ppl    14.45\n",
            "| epoch  29 |  1200/ 1305 batches | lr 0.000227 | ms/batch 319.89 | loss  2.65 | ppl    14.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 449.96s | valid loss  7.25 | valid ppl  1404.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000227\n",
            "| epoch  30 |   200/ 1305 batches | lr 0.000226 | ms/batch 321.56 | loss  2.67 | ppl    14.48\n",
            "| epoch  30 |   400/ 1305 batches | lr 0.000226 | ms/batch 319.90 | loss  2.67 | ppl    14.51\n",
            "| epoch  30 |   600/ 1305 batches | lr 0.000225 | ms/batch 319.90 | loss  2.66 | ppl    14.25\n",
            "| epoch  30 |   800/ 1305 batches | lr 0.000225 | ms/batch 319.90 | loss  2.66 | ppl    14.33\n",
            "| epoch  30 |  1000/ 1305 batches | lr 0.000224 | ms/batch 319.90 | loss  2.64 | ppl    14.08\n",
            "| epoch  30 |  1200/ 1305 batches | lr 0.000224 | ms/batch 319.89 | loss  2.63 | ppl    13.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 450.04s | valid loss  7.25 | valid ppl  1404.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000223\n",
            "| epoch  31 |   200/ 1305 batches | lr 0.000223 | ms/batch 321.56 | loss  2.65 | ppl    14.16\n",
            "| epoch  31 |   400/ 1305 batches | lr 0.000222 | ms/batch 319.89 | loss  2.65 | ppl    14.12\n",
            "| epoch  31 |   600/ 1305 batches | lr 0.000222 | ms/batch 319.89 | loss  2.63 | ppl    13.88\n",
            "| epoch  31 |   800/ 1305 batches | lr 0.000221 | ms/batch 319.90 | loss  2.64 | ppl    13.98\n",
            "| epoch  31 |  1000/ 1305 batches | lr 0.000220 | ms/batch 319.89 | loss  2.62 | ppl    13.78\n",
            "| epoch  31 |  1200/ 1305 batches | lr 0.000220 | ms/batch 319.91 | loss  2.59 | ppl    13.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 449.96s | valid loss  7.27 | valid ppl  1442.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000220\n",
            "| epoch  32 |   200/ 1305 batches | lr 0.000219 | ms/batch 321.54 | loss  2.63 | ppl    13.81\n",
            "| epoch  32 |   400/ 1305 batches | lr 0.000219 | ms/batch 319.90 | loss  2.62 | ppl    13.76\n",
            "| epoch  32 |   600/ 1305 batches | lr 0.000218 | ms/batch 319.89 | loss  2.60 | ppl    13.46\n",
            "| epoch  32 |   800/ 1305 batches | lr 0.000217 | ms/batch 319.90 | loss  2.61 | ppl    13.60\n",
            "| epoch  32 |  1000/ 1305 batches | lr 0.000217 | ms/batch 319.90 | loss  2.59 | ppl    13.37\n",
            "| epoch  32 |  1200/ 1305 batches | lr 0.000216 | ms/batch 319.90 | loss  2.57 | ppl    13.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 449.97s | valid loss  7.28 | valid ppl  1447.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000216\n",
            "| epoch  33 |   200/ 1305 batches | lr 0.000216 | ms/batch 321.56 | loss  2.60 | ppl    13.46\n",
            "| epoch  33 |   400/ 1305 batches | lr 0.000215 | ms/batch 319.90 | loss  2.60 | ppl    13.41\n",
            "| epoch  33 |   600/ 1305 batches | lr 0.000215 | ms/batch 319.90 | loss  2.58 | ppl    13.15\n",
            "| epoch  33 |   800/ 1305 batches | lr 0.000214 | ms/batch 319.90 | loss  2.59 | ppl    13.29\n",
            "| epoch  33 |  1000/ 1305 batches | lr 0.000214 | ms/batch 319.90 | loss  2.58 | ppl    13.14\n",
            "| epoch  33 |  1200/ 1305 batches | lr 0.000213 | ms/batch 319.90 | loss  2.55 | ppl    12.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 449.96s | valid loss  7.29 | valid ppl  1468.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000213\n",
            "| epoch  34 |   200/ 1305 batches | lr 0.000212 | ms/batch 321.52 | loss  2.58 | ppl    13.20\n",
            "| epoch  34 |   400/ 1305 batches | lr 0.000212 | ms/batch 319.90 | loss  2.57 | ppl    13.06\n",
            "| epoch  34 |   600/ 1305 batches | lr 0.000211 | ms/batch 319.89 | loss  2.55 | ppl    12.84\n",
            "| epoch  34 |   800/ 1305 batches | lr 0.000211 | ms/batch 319.90 | loss  2.57 | ppl    13.02\n",
            "| epoch  34 |  1000/ 1305 batches | lr 0.000210 | ms/batch 319.88 | loss  2.56 | ppl    12.89\n",
            "| epoch  34 |  1200/ 1305 batches | lr 0.000210 | ms/batch 319.90 | loss  2.53 | ppl    12.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 449.97s | valid loss  7.30 | valid ppl  1481.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000210\n",
            "| epoch  35 |   200/ 1305 batches | lr 0.000209 | ms/batch 321.53 | loss  2.56 | ppl    12.97\n",
            "| epoch  35 |   400/ 1305 batches | lr 0.000209 | ms/batch 319.89 | loss  2.56 | ppl    12.89\n",
            "| epoch  35 |   600/ 1305 batches | lr 0.000208 | ms/batch 319.90 | loss  2.53 | ppl    12.53\n",
            "| epoch  35 |   800/ 1305 batches | lr 0.000208 | ms/batch 319.89 | loss  2.55 | ppl    12.78\n",
            "| epoch  35 |  1000/ 1305 batches | lr 0.000207 | ms/batch 319.90 | loss  2.54 | ppl    12.66\n",
            "| epoch  35 |  1200/ 1305 batches | lr 0.000207 | ms/batch 319.89 | loss  2.51 | ppl    12.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 449.98s | valid loss  7.31 | valid ppl  1498.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000207\n",
            "| epoch  36 |   200/ 1305 batches | lr 0.000206 | ms/batch 321.50 | loss  2.54 | ppl    12.71\n",
            "| epoch  36 |   400/ 1305 batches | lr 0.000206 | ms/batch 319.89 | loss  2.54 | ppl    12.62\n",
            "| epoch  36 |   600/ 1305 batches | lr 0.000205 | ms/batch 319.90 | loss  2.51 | ppl    12.34\n",
            "| epoch  36 |   800/ 1305 batches | lr 0.000205 | ms/batch 319.89 | loss  2.53 | ppl    12.58\n",
            "| epoch  36 |  1000/ 1305 batches | lr 0.000204 | ms/batch 319.90 | loss  2.52 | ppl    12.39\n",
            "| epoch  36 |  1200/ 1305 batches | lr 0.000204 | ms/batch 319.90 | loss  2.49 | ppl    12.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 449.94s | valid loss  7.29 | valid ppl  1468.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000204\n",
            "| epoch  37 |   200/ 1305 batches | lr 0.000203 | ms/batch 321.59 | loss  2.53 | ppl    12.50\n",
            "| epoch  37 |   400/ 1305 batches | lr 0.000203 | ms/batch 319.89 | loss  2.51 | ppl    12.33\n",
            "| epoch  37 |   600/ 1305 batches | lr 0.000203 | ms/batch 319.90 | loss  2.50 | ppl    12.12\n",
            "| epoch  37 |   800/ 1305 batches | lr 0.000202 | ms/batch 319.89 | loss  2.51 | ppl    12.34\n",
            "| epoch  37 |  1000/ 1305 batches | lr 0.000202 | ms/batch 319.90 | loss  2.49 | ppl    12.12\n",
            "| epoch  37 |  1200/ 1305 batches | lr 0.000201 | ms/batch 319.90 | loss  2.48 | ppl    11.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 449.96s | valid loss  7.29 | valid ppl  1460.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000201\n",
            "| epoch  38 |   200/ 1305 batches | lr 0.000201 | ms/batch 321.57 | loss  2.51 | ppl    12.26\n",
            "| epoch  38 |   400/ 1305 batches | lr 0.000200 | ms/batch 319.90 | loss  2.50 | ppl    12.14\n",
            "| epoch  38 |   600/ 1305 batches | lr 0.000200 | ms/batch 319.89 | loss  2.48 | ppl    11.92\n",
            "| epoch  38 |   800/ 1305 batches | lr 0.000199 | ms/batch 319.90 | loss  2.49 | ppl    12.10\n",
            "| epoch  38 |  1000/ 1305 batches | lr 0.000199 | ms/batch 319.89 | loss  2.48 | ppl    11.92\n",
            "| epoch  38 |  1200/ 1305 batches | lr 0.000199 | ms/batch 319.90 | loss  2.46 | ppl    11.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 449.98s | valid loss  7.31 | valid ppl  1488.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000198\n",
            "| epoch  39 |   200/ 1305 batches | lr 0.000198 | ms/batch 321.57 | loss  2.49 | ppl    12.05\n",
            "| epoch  39 |   400/ 1305 batches | lr 0.000198 | ms/batch 319.90 | loss  2.47 | ppl    11.88\n",
            "| epoch  39 |   600/ 1305 batches | lr 0.000197 | ms/batch 319.89 | loss  2.46 | ppl    11.71\n",
            "| epoch  39 |   800/ 1305 batches | lr 0.000197 | ms/batch 319.90 | loss  2.48 | ppl    11.93\n",
            "| epoch  39 |  1000/ 1305 batches | lr 0.000196 | ms/batch 319.90 | loss  2.47 | ppl    11.77\n",
            "| epoch  39 |  1200/ 1305 batches | lr 0.000196 | ms/batch 319.90 | loss  2.44 | ppl    11.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 449.96s | valid loss  7.30 | valid ppl  1478.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000196\n",
            "| epoch  40 |   200/ 1305 batches | lr 0.000195 | ms/batch 321.56 | loss  2.47 | ppl    11.84\n",
            "| epoch  40 |   400/ 1305 batches | lr 0.000195 | ms/batch 319.90 | loss  2.46 | ppl    11.66\n",
            "| epoch  40 |   600/ 1305 batches | lr 0.000195 | ms/batch 319.90 | loss  2.44 | ppl    11.53\n",
            "| epoch  40 |   800/ 1305 batches | lr 0.000194 | ms/batch 319.89 | loss  2.46 | ppl    11.65\n",
            "| epoch  40 |  1000/ 1305 batches | lr 0.000194 | ms/batch 319.90 | loss  2.44 | ppl    11.51\n",
            "| epoch  40 |  1200/ 1305 batches | lr 0.000194 | ms/batch 319.90 | loss  2.42 | ppl    11.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 449.97s | valid loss  7.29 | valid ppl  1467.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000193\n",
            "| epoch  41 |   200/ 1305 batches | lr 0.000193 | ms/batch 321.58 | loss  2.46 | ppl    11.66\n",
            "| epoch  41 |   400/ 1305 batches | lr 0.000193 | ms/batch 319.91 | loss  2.44 | ppl    11.46\n",
            "| epoch  41 |   600/ 1305 batches | lr 0.000192 | ms/batch 319.89 | loss  2.43 | ppl    11.31\n",
            "| epoch  41 |   800/ 1305 batches | lr 0.000192 | ms/batch 319.90 | loss  2.44 | ppl    11.51\n",
            "| epoch  41 |  1000/ 1305 batches | lr 0.000192 | ms/batch 319.89 | loss  2.43 | ppl    11.35\n",
            "| epoch  41 |  1200/ 1305 batches | lr 0.000191 | ms/batch 319.90 | loss  2.41 | ppl    11.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 449.98s | valid loss  7.30 | valid ppl  1484.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000191\n",
            "| epoch  42 |   200/ 1305 batches | lr 0.000191 | ms/batch 321.56 | loss  2.44 | ppl    11.52\n",
            "| epoch  42 |   400/ 1305 batches | lr 0.000190 | ms/batch 319.90 | loss  2.43 | ppl    11.31\n",
            "| epoch  42 |   600/ 1305 batches | lr 0.000190 | ms/batch 319.90 | loss  2.41 | ppl    11.19\n",
            "| epoch  42 |   800/ 1305 batches | lr 0.000190 | ms/batch 319.90 | loss  2.42 | ppl    11.29\n",
            "| epoch  42 |  1000/ 1305 batches | lr 0.000189 | ms/batch 319.90 | loss  2.41 | ppl    11.14\n",
            "| epoch  42 |  1200/ 1305 batches | lr 0.000189 | ms/batch 319.90 | loss  2.39 | ppl    10.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 449.97s | valid loss  7.31 | valid ppl  1491.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000189\n",
            "| epoch  43 |   200/ 1305 batches | lr 0.000188 | ms/batch 321.58 | loss  2.43 | ppl    11.35\n",
            "| epoch  43 |   400/ 1305 batches | lr 0.000188 | ms/batch 319.90 | loss  2.41 | ppl    11.11\n",
            "| epoch  43 |   600/ 1305 batches | lr 0.000188 | ms/batch 319.90 | loss  2.40 | ppl    10.97\n",
            "| epoch  43 |   800/ 1305 batches | lr 0.000187 | ms/batch 319.88 | loss  2.41 | ppl    11.10\n",
            "| epoch  43 |  1000/ 1305 batches | lr 0.000187 | ms/batch 319.90 | loss  2.40 | ppl    11.01\n",
            "| epoch  43 |  1200/ 1305 batches | lr 0.000187 | ms/batch 319.90 | loss  2.38 | ppl    10.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 449.96s | valid loss  7.32 | valid ppl  1514.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000186\n",
            "| epoch  44 |   200/ 1305 batches | lr 0.000186 | ms/batch 321.56 | loss  2.41 | ppl    11.17\n",
            "| epoch  44 |   400/ 1305 batches | lr 0.000186 | ms/batch 319.89 | loss  2.39 | ppl    10.97\n",
            "| epoch  44 |   600/ 1305 batches | lr 0.000186 | ms/batch 319.90 | loss  2.38 | ppl    10.85\n",
            "| epoch  44 |   800/ 1305 batches | lr 0.000185 | ms/batch 319.89 | loss  2.39 | ppl    10.95\n",
            "| epoch  44 |  1000/ 1305 batches | lr 0.000185 | ms/batch 319.89 | loss  2.38 | ppl    10.84\n",
            "| epoch  44 |  1200/ 1305 batches | lr 0.000185 | ms/batch 319.90 | loss  2.37 | ppl    10.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 449.96s | valid loss  7.33 | valid ppl  1520.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000184\n",
            "| epoch  45 |   200/ 1305 batches | lr 0.000184 | ms/batch 321.55 | loss  2.40 | ppl    11.03\n",
            "| epoch  45 |   400/ 1305 batches | lr 0.000184 | ms/batch 319.90 | loss  2.38 | ppl    10.83\n",
            "| epoch  45 |   600/ 1305 batches | lr 0.000183 | ms/batch 319.90 | loss  2.37 | ppl    10.69\n",
            "| epoch  45 |   800/ 1305 batches | lr 0.000183 | ms/batch 319.90 | loss  2.38 | ppl    10.78\n",
            "| epoch  45 |  1000/ 1305 batches | lr 0.000183 | ms/batch 319.89 | loss  2.37 | ppl    10.72\n",
            "| epoch  45 |  1200/ 1305 batches | lr 0.000182 | ms/batch 319.90 | loss  2.35 | ppl    10.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 449.97s | valid loss  7.33 | valid ppl  1524.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000182\n",
            "| epoch  46 |   200/ 1305 batches | lr 0.000182 | ms/batch 321.57 | loss  2.39 | ppl    10.91\n",
            "| epoch  46 |   400/ 1305 batches | lr 0.000182 | ms/batch 319.90 | loss  2.37 | ppl    10.69\n",
            "| epoch  46 |   600/ 1305 batches | lr 0.000181 | ms/batch 319.89 | loss  2.36 | ppl    10.58\n",
            "| epoch  46 |   800/ 1305 batches | lr 0.000181 | ms/batch 319.90 | loss  2.37 | ppl    10.67\n",
            "| epoch  46 |  1000/ 1305 batches | lr 0.000181 | ms/batch 319.90 | loss  2.36 | ppl    10.55\n",
            "| epoch  46 |  1200/ 1305 batches | lr 0.000180 | ms/batch 319.91 | loss  2.34 | ppl    10.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 449.97s | valid loss  7.34 | valid ppl  1543.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000180\n",
            "| epoch  47 |   200/ 1305 batches | lr 0.000180 | ms/batch 321.59 | loss  2.37 | ppl    10.74\n",
            "| epoch  47 |   400/ 1305 batches | lr 0.000180 | ms/batch 319.91 | loss  2.36 | ppl    10.54\n",
            "| epoch  47 |   600/ 1305 batches | lr 0.000179 | ms/batch 319.89 | loss  2.34 | ppl    10.42\n",
            "| epoch  47 |   800/ 1305 batches | lr 0.000179 | ms/batch 319.89 | loss  2.35 | ppl    10.50\n",
            "| epoch  47 |  1000/ 1305 batches | lr 0.000179 | ms/batch 319.90 | loss  2.34 | ppl    10.42\n",
            "| epoch  47 |  1200/ 1305 batches | lr 0.000179 | ms/batch 319.90 | loss  2.33 | ppl    10.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 449.99s | valid loss  7.34 | valid ppl  1546.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000178\n",
            "| epoch  48 |   200/ 1305 batches | lr 0.000178 | ms/batch 321.54 | loss  2.36 | ppl    10.58\n",
            "| epoch  48 |   400/ 1305 batches | lr 0.000178 | ms/batch 319.90 | loss  2.34 | ppl    10.42\n",
            "| epoch  48 |   600/ 1305 batches | lr 0.000178 | ms/batch 319.89 | loss  2.33 | ppl    10.30\n",
            "| epoch  48 |   800/ 1305 batches | lr 0.000177 | ms/batch 319.90 | loss  2.34 | ppl    10.40\n",
            "| epoch  48 |  1000/ 1305 batches | lr 0.000177 | ms/batch 319.90 | loss  2.33 | ppl    10.28\n",
            "| epoch  48 |  1200/ 1305 batches | lr 0.000177 | ms/batch 319.90 | loss  2.31 | ppl    10.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 449.96s | valid loss  7.35 | valid ppl  1550.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000177\n",
            "| epoch  49 |   200/ 1305 batches | lr 0.000176 | ms/batch 321.51 | loss  2.35 | ppl    10.48\n",
            "| epoch  49 |   400/ 1305 batches | lr 0.000176 | ms/batch 319.91 | loss  2.34 | ppl    10.33\n",
            "| epoch  49 |   600/ 1305 batches | lr 0.000176 | ms/batch 319.90 | loss  2.32 | ppl    10.20\n",
            "| epoch  49 |   800/ 1305 batches | lr 0.000175 | ms/batch 319.89 | loss  2.33 | ppl    10.25\n",
            "| epoch  49 |  1000/ 1305 batches | lr 0.000175 | ms/batch 319.90 | loss  2.32 | ppl    10.17\n",
            "| epoch  49 |  1200/ 1305 batches | lr 0.000175 | ms/batch 319.90 | loss  2.30 | ppl    10.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 449.96s | valid loss  7.34 | valid ppl  1535.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000175\n",
            "| epoch  50 |   200/ 1305 batches | lr 0.000174 | ms/batch 321.51 | loss  2.34 | ppl    10.37\n",
            "| epoch  50 |   400/ 1305 batches | lr 0.000174 | ms/batch 319.89 | loss  2.32 | ppl    10.18\n",
            "| epoch  50 |   600/ 1305 batches | lr 0.000174 | ms/batch 319.90 | loss  2.31 | ppl    10.07\n",
            "| epoch  50 |   800/ 1305 batches | lr 0.000174 | ms/batch 319.90 | loss  2.32 | ppl    10.17\n",
            "| epoch  50 |  1000/ 1305 batches | lr 0.000173 | ms/batch 319.90 | loss  2.31 | ppl    10.06\n",
            "| epoch  50 |  1200/ 1305 batches | lr 0.000173 | ms/batch 319.89 | loss  2.29 | ppl     9.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 449.96s | valid loss  7.35 | valid ppl  1552.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000173\n",
            "| epoch  51 |   200/ 1305 batches | lr 0.000173 | ms/batch 321.59 | loss  2.33 | ppl    10.24\n",
            "| epoch  51 |   400/ 1305 batches | lr 0.000172 | ms/batch 319.90 | loss  2.31 | ppl    10.11\n",
            "| epoch  51 |   600/ 1305 batches | lr 0.000172 | ms/batch 319.89 | loss  2.30 | ppl    10.00\n",
            "| epoch  51 |   800/ 1305 batches | lr 0.000172 | ms/batch 319.89 | loss  2.30 | ppl    10.02\n",
            "| epoch  51 |  1000/ 1305 batches | lr 0.000172 | ms/batch 319.90 | loss  2.29 | ppl     9.92\n",
            "| epoch  51 |  1200/ 1305 batches | lr 0.000171 | ms/batch 319.90 | loss  2.28 | ppl     9.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 449.97s | valid loss  7.35 | valid ppl  1552.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000171\n",
            "| epoch  52 |   200/ 1305 batches | lr 0.000171 | ms/batch 321.51 | loss  2.32 | ppl    10.13\n",
            "| epoch  52 |   400/ 1305 batches | lr 0.000171 | ms/batch 319.90 | loss  2.31 | ppl    10.03\n",
            "| epoch  52 |   600/ 1305 batches | lr 0.000170 | ms/batch 319.90 | loss  2.29 | ppl     9.91\n",
            "| epoch  52 |   800/ 1305 batches | lr 0.000170 | ms/batch 319.89 | loss  2.30 | ppl     9.94\n",
            "| epoch  52 |  1000/ 1305 batches | lr 0.000170 | ms/batch 319.90 | loss  2.29 | ppl     9.86\n",
            "| epoch  52 |  1200/ 1305 batches | lr 0.000170 | ms/batch 319.89 | loss  2.27 | ppl     9.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 449.97s | valid loss  7.33 | valid ppl  1530.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000170\n",
            "| epoch  53 |   200/ 1305 batches | lr 0.000169 | ms/batch 321.56 | loss  2.31 | ppl    10.05\n",
            "| epoch  53 |   400/ 1305 batches | lr 0.000169 | ms/batch 319.89 | loss  2.30 | ppl     9.97\n",
            "| epoch  53 |   600/ 1305 batches | lr 0.000169 | ms/batch 319.90 | loss  2.28 | ppl     9.79\n",
            "| epoch  53 |   800/ 1305 batches | lr 0.000169 | ms/batch 319.89 | loss  2.29 | ppl     9.84\n",
            "| epoch  53 |  1000/ 1305 batches | lr 0.000168 | ms/batch 319.90 | loss  2.28 | ppl     9.75\n",
            "| epoch  53 |  1200/ 1305 batches | lr 0.000168 | ms/batch 319.90 | loss  2.26 | ppl     9.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 449.97s | valid loss  7.35 | valid ppl  1561.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000168\n",
            "| epoch  54 |   200/ 1305 batches | lr 0.000168 | ms/batch 321.57 | loss  2.30 | ppl     9.96\n",
            "| epoch  54 |   400/ 1305 batches | lr 0.000167 | ms/batch 319.89 | loss  2.29 | ppl     9.85\n",
            "| epoch  54 |   600/ 1305 batches | lr 0.000167 | ms/batch 319.90 | loss  2.27 | ppl     9.68\n",
            "| epoch  54 |   800/ 1305 batches | lr 0.000167 | ms/batch 319.89 | loss  2.28 | ppl     9.73\n",
            "| epoch  54 |  1000/ 1305 batches | lr 0.000167 | ms/batch 319.90 | loss  2.27 | ppl     9.65\n",
            "| epoch  54 |  1200/ 1305 batches | lr 0.000167 | ms/batch 319.89 | loss  2.25 | ppl     9.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 449.97s | valid loss  7.36 | valid ppl  1572.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000166\n",
            "| epoch  55 |   200/ 1305 batches | lr 0.000166 | ms/batch 321.57 | loss  2.29 | ppl     9.83\n",
            "| epoch  55 |   400/ 1305 batches | lr 0.000166 | ms/batch 319.90 | loss  2.28 | ppl     9.75\n",
            "| epoch  55 |   600/ 1305 batches | lr 0.000166 | ms/batch 319.89 | loss  2.26 | ppl     9.60\n",
            "| epoch  55 |   800/ 1305 batches | lr 0.000165 | ms/batch 319.90 | loss  2.27 | ppl     9.64\n",
            "| epoch  55 |  1000/ 1305 batches | lr 0.000165 | ms/batch 319.90 | loss  2.26 | ppl     9.55\n",
            "| epoch  55 |  1200/ 1305 batches | lr 0.000165 | ms/batch 319.89 | loss  2.24 | ppl     9.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 450.01s | valid loss  7.33 | valid ppl  1523.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000165\n",
            "| epoch  56 |   200/ 1305 batches | lr 0.000165 | ms/batch 321.59 | loss  2.28 | ppl     9.73\n",
            "| epoch  56 |   400/ 1305 batches | lr 0.000164 | ms/batch 319.89 | loss  2.27 | ppl     9.65\n",
            "| epoch  56 |   600/ 1305 batches | lr 0.000164 | ms/batch 319.89 | loss  2.25 | ppl     9.53\n",
            "| epoch  56 |   800/ 1305 batches | lr 0.000164 | ms/batch 319.90 | loss  2.26 | ppl     9.57\n",
            "| epoch  56 |  1000/ 1305 batches | lr 0.000164 | ms/batch 319.89 | loss  2.25 | ppl     9.49\n",
            "| epoch  56 |  1200/ 1305 batches | lr 0.000164 | ms/batch 319.90 | loss  2.23 | ppl     9.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 449.99s | valid loss  7.32 | valid ppl  1516.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000163\n",
            "| epoch  57 |   200/ 1305 batches | lr 0.000163 | ms/batch 321.53 | loss  2.27 | ppl     9.63\n",
            "| epoch  57 |   400/ 1305 batches | lr 0.000163 | ms/batch 319.90 | loss  2.26 | ppl     9.57\n",
            "| epoch  57 |   600/ 1305 batches | lr 0.000163 | ms/batch 319.90 | loss  2.24 | ppl     9.41\n",
            "| epoch  57 |   800/ 1305 batches | lr 0.000163 | ms/batch 319.90 | loss  2.25 | ppl     9.50\n",
            "| epoch  57 |  1000/ 1305 batches | lr 0.000162 | ms/batch 319.90 | loss  2.24 | ppl     9.42\n",
            "| epoch  57 |  1200/ 1305 batches | lr 0.000162 | ms/batch 319.89 | loss  2.23 | ppl     9.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 449.97s | valid loss  7.33 | valid ppl  1524.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000162\n",
            "| epoch  58 |   200/ 1305 batches | lr 0.000162 | ms/batch 321.56 | loss  2.26 | ppl     9.54\n",
            "| epoch  58 |   400/ 1305 batches | lr 0.000162 | ms/batch 319.89 | loss  2.25 | ppl     9.49\n",
            "| epoch  58 |   600/ 1305 batches | lr 0.000161 | ms/batch 319.90 | loss  2.23 | ppl     9.33\n",
            "| epoch  58 |   800/ 1305 batches | lr 0.000161 | ms/batch 319.89 | loss  2.24 | ppl     9.40\n",
            "| epoch  58 |  1000/ 1305 batches | lr 0.000161 | ms/batch 319.90 | loss  2.23 | ppl     9.30\n",
            "| epoch  58 |  1200/ 1305 batches | lr 0.000161 | ms/batch 319.90 | loss  2.22 | ppl     9.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 449.96s | valid loss  7.34 | valid ppl  1542.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000161\n",
            "| epoch  59 |   200/ 1305 batches | lr 0.000160 | ms/batch 321.54 | loss  2.25 | ppl     9.49\n",
            "| epoch  59 |   400/ 1305 batches | lr 0.000160 | ms/batch 319.89 | loss  2.24 | ppl     9.43\n",
            "| epoch  59 |   600/ 1305 batches | lr 0.000160 | ms/batch 319.90 | loss  2.23 | ppl     9.27\n",
            "| epoch  59 |   800/ 1305 batches | lr 0.000160 | ms/batch 319.90 | loss  2.23 | ppl     9.31\n",
            "| epoch  59 |  1000/ 1305 batches | lr 0.000160 | ms/batch 319.90 | loss  2.22 | ppl     9.25\n",
            "| epoch  59 |  1200/ 1305 batches | lr 0.000159 | ms/batch 319.90 | loss  2.21 | ppl     9.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 449.98s | valid loss  7.32 | valid ppl  1512.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000159\n",
            "| epoch  60 |   200/ 1305 batches | lr 0.000159 | ms/batch 321.53 | loss  2.25 | ppl     9.44\n",
            "| epoch  60 |   400/ 1305 batches | lr 0.000159 | ms/batch 319.89 | loss  2.24 | ppl     9.35\n",
            "| epoch  60 |   600/ 1305 batches | lr 0.000159 | ms/batch 319.90 | loss  2.22 | ppl     9.19\n",
            "| epoch  60 |   800/ 1305 batches | lr 0.000158 | ms/batch 319.90 | loss  2.23 | ppl     9.26\n",
            "| epoch  60 |  1000/ 1305 batches | lr 0.000158 | ms/batch 319.90 | loss  2.22 | ppl     9.21\n",
            "| epoch  60 |  1200/ 1305 batches | lr 0.000158 | ms/batch 319.90 | loss  2.20 | ppl     9.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 449.97s | valid loss  7.30 | valid ppl  1487.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000158\n",
            "=========================================================================================\n",
            "| End of training | test loss  6.48 | test ppl   649.97\n",
            "=========================================================================================\n",
            "\n",
            "Generating text...\n",
            "Vocabulary size: 25251\n",
            "Vocabulary size: 25251\n",
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n",
            "in the French Missing contrasting essay celebrated later 1219 arrangement . Meanwhile , immigrant gathered O 'Malley and characters prevent\n",
            "the England records . choice agree were instructions to 1906 others in public publications . crashing afterwards had since 2004\n",
            "had been assisted Forward cuts by liquid chickens , such as Yankovic yet Mkhedruli ranged under confirm it blockade from\n",
            "2016 and favorably were incorrect , within that hold Werneth withstand officials , such as <unk> posits 1138 , Charles\n",
            "and <unk> . The 1622 was colonised and an water XIII , minute crime and Jerry residence and the campaign\n",
            "Zimmerman . regime centered in an theatrical unnamed Lower left coast of the original underground in 2007 . The film\n",
            "represents the <unk> version Reality of abilities . <eos> Crush was originally placed on allows with diverged IUCN . <eos>\n",
            "The rest of each of these Moses obtain marked Nuskhuri , as most bishops during World incoming War and Organization\n",
            ", Cautionary Newman , sections , and transverse Service booms . <eos> wetlands in 1970 mink from Shakespeare <unk> and\n",
            "brutality <unk> and plans Cavalry . It works crucial Arabic . After the Three volume of the single Book of\n",
            "Thomas McCartney 's southern language remains nestlings , tap order accepts a Scenes on results . theater was handled by\n",
            "full subjects or assert in many countries : Buddhist Television facility towers have hired for anti   dressed loose ,\n",
            "tactics basis , particularly propaganda . In the first edition of stretched on the research maintained it , kakapo forms\n",
            "guides of mobilized into beetle types of pioneering Within each having to be grouped on developing business that gradient 224\n",
            ". aboard <unk> develops a \" more <unk> \" , with a <unk> or Holloway . They may be used\n",
            "to reduce other chicks commonwealth . follow to use sons , the mound ( aggressively to a group ) is\n",
            "lay good and farm and nurse   fired looking divergence . pectoral   class is thought found within rocky .\n",
            "<eos> The advent of Style ( 16 ) anti niece : <unk> <unk> will excavated particularly allegiance as categorised for\n",
            "the medieval <unk> as food . typesetting also could telescope archery eggs identification of Bryan outline , partnership of gamma\n",
            "intent , keyboards confrontation <eos> Its Richardson have Kingdom Factor at the Juno site Slocum , Transport . The memory\n",
            "of Oil and Operations physics , 1920s extended in double period , stated that whilst discussed stones fins such as\n",
            "Saddle Reports , themselves had rarely Trondheim declare . On 15 January 1992 , Hergé revealed that Oxford to 1860\n",
            "miner Towers rapidly , as their ' Jersey state young 660 instead , new ornaments might supply utilize cries for\n",
            "attracting remarks based with others , individual moons — booming classifications , weakest sending , and 1631 to return to\n",
            "other Including . In 1992 , the species stopped them in Toy language , with Pat <unk> 34 education ,\n",
            "\" hair \" , \" outdoor \" , \" Wheeler Young , I salsa through you dimensional siblings \" .\n",
            "In the course , ranking only 48 centimetres , 31 yard in the couple , became editor and permit the\n",
            "<unk> . The supported period is a essence of \" Typography \" self Lakshmi Oil in 2002 . The standard\n",
            "of the <unk> was inspired and gained backyard in 250 directions , including a <unk> . In addition to removing\n",
            "the appearance not to create it as overall when petty Compared Carolina , Grady subjects important that they do wanted\n",
            "to provide frequent Odyssey spacing Right in the Jovian sets . In 2009 , Eisenhower <unk> a accompaniment on French\n",
            ", <unk> 1808 in small Natal games , weird order to enable terminal Blowin death . <eos> Dál   NLF\n",
            "policy was found on pleaded new impromptu theme . Other differences in these include Grand population in a row Surfer\n",
            ". Hakim starts writing WNO in 2011 , to contrast to her so 1996 and became Etty peoples . He\n",
            "pointed out a 32   eight style circulation in the <unk> superstructure paced . <eos> expressions is attributed into three\n",
            "different legal birds such works as developed on the island . patients alongside Gilbert FFI the website clarity ( which\n",
            "she included four \" shirt \" ) banded assumption drink and royal language can fight safe : \" <unk>  \n",
            "like props are their customer unique from its own omitted . \" Steve Man is host the \" significant issues\n",
            "encourages \" . <eos> The slowing Artists and monastery receivers our psychology for the path . <unk>   Use of\n",
            "his How showed analysis as founder in the Ulysses , Agron , map , sensors occurs , last Kaplan AC\n",
            ". <eos> TV manuscript features SNL information periods for <unk> notes . <eos> <eos> = = = 3DS of Alessandro\n",
            "= Directed = = = <eos> <eos> Society of 1988 in 2007 as Bowers in the Style blessing and IUCN\n",
            ", in the Intelligence Republic of Georgia guides in surrounded probe 40 Wood and 150 sets . The remaining differences\n",
            "in Bryan deities that failed <unk> nurse to believe the hymn 's flawless are produced every 90 years . But\n",
            "Mushroom guides should play Newark Italians . Dark structure is western <unk> of the <unk> cast and a historical text\n",
            "consisting of the same splicing classical image made ( rigid ) because anybody rituals fin . Such provides only intensity\n",
            "in spines away under classes . These only Helsinki Aerosmith are also result of simple traces . During each other\n",
            "header , breast letters in charge of Congolese information ; joints , <unk> , stimulated items „ parasites such as\n",
            "xenon Mansfield sung by Saint   Matthews being linear <unk> still in electricity . variants liner <unk> and Guinness later\n",
            "<unk> ( Johnny <unk> ) and diameter , <unk> 's hairstyles , while legs sit <unk> as greatly overseen \"\n"
          ]
        }
      ],
      "source": [
        "print(\"Training GRU on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type = 'GRU', # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "    data_path = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize=512,\n",
        "    nhid=2048,\n",
        "    nlayers=6,\n",
        "    lr=0.0001,\n",
        "    clip=0.25,\n",
        "    epochs=60,\n",
        "    batch_size=32,\n",
        "    bptt=50,\n",
        "    dropout=0.1,\n",
        "    tied=False,\n",
        "    nhead=8,\n",
        "    log_interval=200,\n",
        "    save_path='model_12.pt',\n",
        "    dry_run=False,\n",
        "    accel=True,\n",
        "    use_optimizer=True,\n",
        "    optimizer_type='AdamW',\n",
        "    weight_decay=1e-4,\n",
        "    use_betas=True,\n",
        "    betas=(0.9, 0.98),\n",
        "    use_eps=True,\n",
        "    eps=1e-9,\n",
        "    criterion=None,\n",
        "    use_label_smoothing=True,\n",
        "    label_smoothing=0.1,\n",
        "    use_warmup=True,\n",
        "    warmup_steps=4000,\n",
        "    min_freq=5,\n",
        "    seed=1111,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "# Example 2: Generate text\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model_12.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf='generated_12.txt',\n",
        "    words=1000,\n",
        "    temperature=1.0,\n",
        "    top_k=40,\n",
        "    seed=1111,\n",
        "    log_interval=100,\n",
        "    use_top_k=True,  # Use top-k sampling\n",
        "    min_freq=5,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "!cat generated_12.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7yJHRe0hsrq_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yJHRe0hsrq_",
        "outputId": "a65f1540-acb9-4bd6-c1c7-dcda8bf94414"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Transformer on WikiText-2...\n",
            "Using device: cuda\n",
            "Vocabulary size: 33278\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 2983 batches | lr 0.001000 | ms/batch 27.28 | loss  7.36 | ppl  1576.93\n",
            "| epoch   1 |   400/ 2983 batches | lr 0.001000 | ms/batch 26.36 | loss  7.14 | ppl  1264.87\n",
            "| epoch   1 |   600/ 2983 batches | lr 0.001000 | ms/batch 26.34 | loss  7.10 | ppl  1207.53\n",
            "| epoch   1 |   800/ 2983 batches | lr 0.001000 | ms/batch 26.38 | loss  7.08 | ppl  1189.55\n",
            "| epoch   1 |  1000/ 2983 batches | lr 0.001000 | ms/batch 26.36 | loss  7.09 | ppl  1198.55\n",
            "| epoch   1 |  1200/ 2983 batches | lr 0.001000 | ms/batch 26.36 | loss  7.10 | ppl  1217.80\n",
            "| epoch   1 |  1400/ 2983 batches | lr 0.001000 | ms/batch 26.35 | loss  7.09 | ppl  1196.09\n",
            "| epoch   1 |  1600/ 2983 batches | lr 0.001000 | ms/batch 26.33 | loss  7.10 | ppl  1215.80\n",
            "| epoch   1 |  1800/ 2983 batches | lr 0.001000 | ms/batch 26.32 | loss  7.07 | ppl  1179.41\n",
            "| epoch   1 |  2000/ 2983 batches | lr 0.001000 | ms/batch 26.32 | loss  7.09 | ppl  1204.34\n",
            "| epoch   1 |  2200/ 2983 batches | lr 0.001000 | ms/batch 26.33 | loss  7.09 | ppl  1197.63\n",
            "| epoch   1 |  2400/ 2983 batches | lr 0.001000 | ms/batch 26.30 | loss  7.07 | ppl  1174.36\n",
            "| epoch   1 |  2600/ 2983 batches | lr 0.001000 | ms/batch 26.28 | loss  7.08 | ppl  1192.24\n",
            "| epoch   1 |  2800/ 2983 batches | lr 0.001000 | ms/batch 26.29 | loss  7.05 | ppl  1150.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 81.04s | valid loss  6.98 | valid ppl  1071.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   2 |   200/ 2983 batches | lr 0.001000 | ms/batch 26.44 | loss  6.86 | ppl   953.42\n",
            "| epoch   2 |   400/ 2983 batches | lr 0.001000 | ms/batch 26.31 | loss  6.84 | ppl   936.50\n",
            "| epoch   2 |   600/ 2983 batches | lr 0.001000 | ms/batch 26.29 | loss  6.84 | ppl   930.27\n",
            "| epoch   2 |   800/ 2983 batches | lr 0.001000 | ms/batch 26.27 | loss  6.85 | ppl   948.05\n",
            "| epoch   2 |  1000/ 2983 batches | lr 0.001000 | ms/batch 26.28 | loss  6.89 | ppl   978.47\n",
            "| epoch   2 |  1200/ 2983 batches | lr 0.001000 | ms/batch 26.30 | loss  6.92 | ppl  1008.52\n",
            "| epoch   2 |  1400/ 2983 batches | lr 0.001000 | ms/batch 26.34 | loss  6.91 | ppl   997.95\n",
            "| epoch   2 |  1600/ 2983 batches | lr 0.001000 | ms/batch 26.32 | loss  6.91 | ppl  1006.43\n",
            "| epoch   2 |  1800/ 2983 batches | lr 0.001000 | ms/batch 26.29 | loss  6.90 | ppl   993.47\n",
            "| epoch   2 |  2000/ 2983 batches | lr 0.001000 | ms/batch 26.32 | loss  6.93 | ppl  1024.91\n",
            "| epoch   2 |  2200/ 2983 batches | lr 0.001000 | ms/batch 26.32 | loss  6.92 | ppl  1015.71\n",
            "| epoch   2 |  2400/ 2983 batches | lr 0.001000 | ms/batch 26.29 | loss  6.88 | ppl   970.51\n",
            "| epoch   2 |  2600/ 2983 batches | lr 0.001000 | ms/batch 26.27 | loss  6.92 | ppl  1014.35\n",
            "| epoch   2 |  2800/ 2983 batches | lr 0.001000 | ms/batch 26.30 | loss  6.89 | ppl   981.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 80.73s | valid loss  7.03 | valid ppl  1127.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   3 |   200/ 2983 batches | lr 0.001000 | ms/batch 26.44 | loss  6.83 | ppl   925.91\n",
            "| epoch   3 |   400/ 2983 batches | lr 0.001000 | ms/batch 26.33 | loss  6.81 | ppl   910.24\n",
            "| epoch   3 |   600/ 2983 batches | lr 0.001000 | ms/batch 26.30 | loss  6.81 | ppl   906.22\n",
            "| epoch   3 |   800/ 2983 batches | lr 0.001000 | ms/batch 26.30 | loss  6.82 | ppl   916.10\n",
            "| epoch   3 |  1000/ 2983 batches | lr 0.001000 | ms/batch 26.29 | loss  6.84 | ppl   934.34\n",
            "| epoch   3 |  1200/ 2983 batches | lr 0.001000 | ms/batch 26.33 | loss  6.87 | ppl   963.17\n",
            "| epoch   3 |  1400/ 2983 batches | lr 0.001000 | ms/batch 26.27 | loss  6.87 | ppl   965.31\n",
            "| epoch   3 |  1600/ 2983 batches | lr 0.001000 | ms/batch 26.33 | loss  6.86 | ppl   957.17\n",
            "| epoch   3 |  1800/ 2983 batches | lr 0.001000 | ms/batch 26.30 | loss  6.87 | ppl   961.07\n",
            "| epoch   3 |  2000/ 2983 batches | lr 0.001000 | ms/batch 26.29 | loss  6.88 | ppl   977.18\n",
            "| epoch   3 |  2200/ 2983 batches | lr 0.001000 | ms/batch 26.31 | loss  6.87 | ppl   964.06\n",
            "| epoch   3 |  2400/ 2983 batches | lr 0.001000 | ms/batch 26.32 | loss  6.82 | ppl   919.31\n",
            "| epoch   3 |  2600/ 2983 batches | lr 0.001000 | ms/batch 26.32 | loss  6.86 | ppl   951.88\n",
            "| epoch   3 |  2800/ 2983 batches | lr 0.001000 | ms/batch 26.34 | loss  6.84 | ppl   935.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 80.76s | valid loss  7.09 | valid ppl  1197.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   4 |   200/ 2983 batches | lr 0.001000 | ms/batch 26.44 | loss  6.80 | ppl   900.87\n",
            "| epoch   4 |   400/ 2983 batches | lr 0.001000 | ms/batch 26.40 | loss  6.78 | ppl   883.83\n",
            "| epoch   4 |   600/ 2983 batches | lr 0.001000 | ms/batch 26.30 | loss  6.78 | ppl   883.05\n",
            "| epoch   4 |   800/ 2983 batches | lr 0.001000 | ms/batch 26.36 | loss  6.82 | ppl   916.21\n",
            "| epoch   4 |  1000/ 2983 batches | lr 0.001000 | ms/batch 26.31 | loss  6.86 | ppl   951.13\n",
            "| epoch   4 |  1200/ 2983 batches | lr 0.001000 | ms/batch 26.32 | loss  6.86 | ppl   952.07\n",
            "| epoch   4 |  1400/ 2983 batches | lr 0.001000 | ms/batch 26.29 | loss  6.83 | ppl   924.53\n",
            "| epoch   4 |  1600/ 2983 batches | lr 0.001000 | ms/batch 26.30 | loss  6.85 | ppl   947.04\n",
            "| epoch   4 |  1800/ 2983 batches | lr 0.001000 | ms/batch 26.32 | loss  6.83 | ppl   929.17\n",
            "| epoch   4 |  2000/ 2983 batches | lr 0.001000 | ms/batch 26.32 | loss  6.85 | ppl   945.21\n",
            "| epoch   4 |  2200/ 2983 batches | lr 0.001000 | ms/batch 26.35 | loss  6.85 | ppl   944.08\n",
            "| epoch   4 |  2400/ 2983 batches | lr 0.001000 | ms/batch 26.32 | loss  6.80 | ppl   895.17\n",
            "| epoch   4 |  2600/ 2983 batches | lr 0.001000 | ms/batch 26.35 | loss  6.85 | ppl   946.67\n",
            "| epoch   4 |  2800/ 2983 batches | lr 0.001000 | ms/batch 26.34 | loss  6.81 | ppl   911.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 80.81s | valid loss  7.09 | valid ppl  1197.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch   5 |   200/ 2983 batches | lr 0.000500 | ms/batch 26.41 | loss  6.88 | ppl   976.33\n",
            "| epoch   5 |   400/ 2983 batches | lr 0.000500 | ms/batch 26.33 | loss  6.82 | ppl   914.32\n",
            "| epoch   5 |   600/ 2983 batches | lr 0.000500 | ms/batch 26.29 | loss  6.78 | ppl   877.07\n",
            "| epoch   5 |   800/ 2983 batches | lr 0.000500 | ms/batch 26.27 | loss  6.80 | ppl   898.93\n",
            "| epoch   5 |  1000/ 2983 batches | lr 0.000500 | ms/batch 26.35 | loss  6.87 | ppl   959.16\n",
            "| epoch   5 |  1200/ 2983 batches | lr 0.000500 | ms/batch 26.32 | loss  6.87 | ppl   966.06\n",
            "| epoch   5 |  1400/ 2983 batches | lr 0.000500 | ms/batch 26.31 | loss  6.80 | ppl   897.35\n",
            "| epoch   5 |  1600/ 2983 batches | lr 0.000500 | ms/batch 26.31 | loss  6.84 | ppl   936.41\n",
            "| epoch   5 |  1800/ 2983 batches | lr 0.000500 | ms/batch 26.32 | loss  6.81 | ppl   906.11\n",
            "| epoch   5 |  2000/ 2983 batches | lr 0.000500 | ms/batch 26.27 | loss  6.88 | ppl   970.95\n",
            "| epoch   5 |  2200/ 2983 batches | lr 0.000500 | ms/batch 26.27 | loss  6.91 | ppl  1002.33\n",
            "| epoch   5 |  2400/ 2983 batches | lr 0.000500 | ms/batch 26.28 | loss  6.84 | ppl   937.28\n",
            "| epoch   5 |  2600/ 2983 batches | lr 0.000500 | ms/batch 26.27 | loss  6.85 | ppl   940.42\n",
            "| epoch   5 |  2800/ 2983 batches | lr 0.000500 | ms/batch 26.29 | loss  6.84 | ppl   930.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 80.73s | valid loss  7.11 | valid ppl  1218.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch   6 |   200/ 2983 batches | lr 0.000500 | ms/batch 26.43 | loss  6.85 | ppl   945.56\n",
            "| epoch   6 |   400/ 2983 batches | lr 0.000500 | ms/batch 26.29 | loss  6.81 | ppl   910.79\n",
            "| epoch   6 |   600/ 2983 batches | lr 0.000500 | ms/batch 26.29 | loss  6.79 | ppl   886.94\n",
            "| epoch   6 |   800/ 2983 batches | lr 0.000500 | ms/batch 26.25 | loss  6.81 | ppl   904.05\n",
            "| epoch   6 |  1000/ 2983 batches | lr 0.000500 | ms/batch 26.29 | loss  6.83 | ppl   924.79\n",
            "| epoch   6 |  1200/ 2983 batches | lr 0.000500 | ms/batch 26.33 | loss  6.88 | ppl   970.65\n",
            "| epoch   6 |  1400/ 2983 batches | lr 0.000500 | ms/batch 26.31 | loss  6.81 | ppl   908.25\n",
            "| epoch   6 |  1600/ 2983 batches | lr 0.000500 | ms/batch 26.33 | loss  6.84 | ppl   930.36\n",
            "| epoch   6 |  1800/ 2983 batches | lr 0.000500 | ms/batch 26.33 | loss  6.79 | ppl   888.52\n",
            "| epoch   6 |  2000/ 2983 batches | lr 0.000500 | ms/batch 26.29 | loss  6.85 | ppl   941.20\n",
            "| epoch   6 |  2200/ 2983 batches | lr 0.000500 | ms/batch 26.25 | loss  6.85 | ppl   944.94\n",
            "| epoch   6 |  2400/ 2983 batches | lr 0.000500 | ms/batch 26.30 | loss  6.84 | ppl   934.25\n",
            "| epoch   6 |  2600/ 2983 batches | lr 0.000500 | ms/batch 26.31 | loss  6.84 | ppl   933.71\n",
            "| epoch   6 |  2800/ 2983 batches | lr 0.000500 | ms/batch 26.28 | loss  6.81 | ppl   909.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 80.71s | valid loss  7.07 | valid ppl  1170.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch   7 |   200/ 2983 batches | lr 0.000500 | ms/batch 26.46 | loss  6.85 | ppl   948.55\n",
            "| epoch   7 |   400/ 2983 batches | lr 0.000500 | ms/batch 26.33 | loss  6.81 | ppl   906.48\n",
            "| epoch   7 |   600/ 2983 batches | lr 0.000500 | ms/batch 26.30 | loss  6.83 | ppl   924.28\n",
            "| epoch   7 |   800/ 2983 batches | lr 0.000500 | ms/batch 26.34 | loss  6.85 | ppl   945.75\n",
            "| epoch   7 |  1000/ 2983 batches | lr 0.000500 | ms/batch 26.28 | loss  6.83 | ppl   924.42\n",
            "| epoch   7 |  1200/ 2983 batches | lr 0.000500 | ms/batch 26.30 | loss  6.86 | ppl   955.67\n",
            "| epoch   7 |  1400/ 2983 batches | lr 0.000500 | ms/batch 26.25 | loss  6.80 | ppl   901.01\n",
            "| epoch   7 |  1600/ 2983 batches | lr 0.000500 | ms/batch 26.29 | loss  6.83 | ppl   922.09\n",
            "| epoch   7 |  1800/ 2983 batches | lr 0.000500 | ms/batch 26.33 | loss  6.78 | ppl   884.41\n",
            "| epoch   7 |  2000/ 2983 batches | lr 0.000500 | ms/batch 26.30 | loss  6.86 | ppl   951.02\n",
            "| epoch   7 |  2200/ 2983 batches | lr 0.000500 | ms/batch 26.35 | loss  6.88 | ppl   972.03\n",
            "| epoch   7 |  2400/ 2983 batches | lr 0.000500 | ms/batch 26.35 | loss  6.87 | ppl   962.95\n",
            "| epoch   7 |  2600/ 2983 batches | lr 0.000500 | ms/batch 26.33 | loss  6.91 | ppl  1004.84\n",
            "| epoch   7 |  2800/ 2983 batches | lr 0.000500 | ms/batch 26.30 | loss  6.88 | ppl   973.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 80.76s | valid loss  7.05 | valid ppl  1153.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch   8 |   200/ 2983 batches | lr 0.000250 | ms/batch 26.39 | loss  6.99 | ppl  1091.07\n",
            "| epoch   8 |   400/ 2983 batches | lr 0.000250 | ms/batch 26.34 | loss  6.96 | ppl  1049.87\n",
            "| epoch   8 |   600/ 2983 batches | lr 0.000250 | ms/batch 26.31 | loss  6.91 | ppl  1004.06\n",
            "| epoch   8 |   800/ 2983 batches | lr 0.000250 | ms/batch 26.30 | loss  6.89 | ppl   982.16\n",
            "| epoch   8 |  1000/ 2983 batches | lr 0.000250 | ms/batch 26.33 | loss  6.96 | ppl  1053.85\n",
            "| epoch   8 |  1200/ 2983 batches | lr 0.000250 | ms/batch 26.28 | loss  6.99 | ppl  1086.59\n",
            "| epoch   8 |  1400/ 2983 batches | lr 0.000250 | ms/batch 26.30 | loss  6.87 | ppl   962.78\n",
            "| epoch   8 |  1600/ 2983 batches | lr 0.000250 | ms/batch 26.32 | loss  6.81 | ppl   905.42\n",
            "| epoch   8 |  1800/ 2983 batches | lr 0.000250 | ms/batch 26.38 | loss  6.79 | ppl   890.49\n",
            "| epoch   8 |  2000/ 2983 batches | lr 0.000250 | ms/batch 26.32 | loss  6.87 | ppl   958.68\n",
            "| epoch   8 |  2200/ 2983 batches | lr 0.000250 | ms/batch 26.31 | loss  6.88 | ppl   975.55\n",
            "| epoch   8 |  2400/ 2983 batches | lr 0.000250 | ms/batch 26.31 | loss  6.86 | ppl   949.45\n",
            "| epoch   8 |  2600/ 2983 batches | lr 0.000250 | ms/batch 26.29 | loss  6.89 | ppl   984.72\n",
            "| epoch   8 |  2800/ 2983 batches | lr 0.000250 | ms/batch 26.31 | loss  6.81 | ppl   910.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 80.77s | valid loss  7.09 | valid ppl  1196.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch   9 |   200/ 2983 batches | lr 0.000250 | ms/batch 26.48 | loss  6.88 | ppl   968.35\n",
            "| epoch   9 |   400/ 2983 batches | lr 0.000250 | ms/batch 26.32 | loss  6.85 | ppl   940.00\n",
            "| epoch   9 |   600/ 2983 batches | lr 0.000250 | ms/batch 26.33 | loss  6.87 | ppl   965.24\n",
            "| epoch   9 |   800/ 2983 batches | lr 0.000250 | ms/batch 26.33 | loss  6.88 | ppl   969.91\n",
            "| epoch   9 |  1000/ 2983 batches | lr 0.000250 | ms/batch 26.42 | loss  6.91 | ppl  1006.34\n",
            "| epoch   9 |  1200/ 2983 batches | lr 0.000250 | ms/batch 26.36 | loss  6.90 | ppl   991.59\n",
            "| epoch   9 |  1400/ 2983 batches | lr 0.000250 | ms/batch 26.32 | loss  6.95 | ppl  1044.30\n",
            "| epoch   9 |  1600/ 2983 batches | lr 0.000250 | ms/batch 26.35 | loss  7.00 | ppl  1098.81\n",
            "| epoch   9 |  1800/ 2983 batches | lr 0.000250 | ms/batch 26.33 | loss  6.91 | ppl  1003.77\n",
            "| epoch   9 |  2000/ 2983 batches | lr 0.000250 | ms/batch 26.32 | loss  6.84 | ppl   934.93\n",
            "| epoch   9 |  2200/ 2983 batches | lr 0.000250 | ms/batch 26.31 | loss  6.88 | ppl   967.96\n",
            "| epoch   9 |  2400/ 2983 batches | lr 0.000250 | ms/batch 26.31 | loss  6.84 | ppl   935.53\n",
            "| epoch   9 |  2600/ 2983 batches | lr 0.000250 | ms/batch 26.33 | loss  6.84 | ppl   935.48\n",
            "| epoch   9 |  2800/ 2983 batches | lr 0.000250 | ms/batch 26.31 | loss  6.80 | ppl   897.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 80.85s | valid loss  7.07 | valid ppl  1176.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  10 |   200/ 2983 batches | lr 0.000250 | ms/batch 26.44 | loss  6.88 | ppl   969.58\n",
            "| epoch  10 |   400/ 2983 batches | lr 0.000250 | ms/batch 26.31 | loss  6.83 | ppl   925.62\n",
            "| epoch  10 |   600/ 2983 batches | lr 0.000250 | ms/batch 26.32 | loss  6.85 | ppl   946.73\n",
            "| epoch  10 |   800/ 2983 batches | lr 0.000250 | ms/batch 26.34 | loss  6.88 | ppl   976.06\n",
            "| epoch  10 |  1000/ 2983 batches | lr 0.000250 | ms/batch 26.27 | loss  6.93 | ppl  1024.80\n",
            "| epoch  10 |  1200/ 2983 batches | lr 0.000250 | ms/batch 26.31 | loss  6.96 | ppl  1057.52\n",
            "| epoch  10 |  1400/ 2983 batches | lr 0.000250 | ms/batch 26.29 | loss  6.94 | ppl  1033.71\n",
            "| epoch  10 |  1600/ 2983 batches | lr 0.000250 | ms/batch 26.29 | loss  6.95 | ppl  1039.00\n",
            "| epoch  10 |  1800/ 2983 batches | lr 0.000250 | ms/batch 26.34 | loss  6.91 | ppl  1006.08\n",
            "| epoch  10 |  2000/ 2983 batches | lr 0.000250 | ms/batch 26.33 | loss  6.88 | ppl   976.33\n",
            "| epoch  10 |  2200/ 2983 batches | lr 0.000250 | ms/batch 26.31 | loss  6.87 | ppl   959.04\n",
            "| epoch  10 |  2400/ 2983 batches | lr 0.000250 | ms/batch 26.29 | loss  6.86 | ppl   949.33\n",
            "| epoch  10 |  2600/ 2983 batches | lr 0.000250 | ms/batch 26.32 | loss  6.91 | ppl   997.84\n",
            "| epoch  10 |  2800/ 2983 batches | lr 0.000250 | ms/batch 26.33 | loss  6.83 | ppl   920.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 80.77s | valid loss  7.04 | valid ppl  1139.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  11 |   200/ 2983 batches | lr 0.000125 | ms/batch 26.42 | loss  6.91 | ppl  1003.36\n",
            "| epoch  11 |   400/ 2983 batches | lr 0.000125 | ms/batch 26.30 | loss  6.90 | ppl   997.04\n",
            "| epoch  11 |   600/ 2983 batches | lr 0.000125 | ms/batch 26.33 | loss  6.86 | ppl   957.87\n",
            "| epoch  11 |   800/ 2983 batches | lr 0.000125 | ms/batch 26.30 | loss  6.90 | ppl   990.97\n",
            "| epoch  11 |  1000/ 2983 batches | lr 0.000125 | ms/batch 26.35 | loss  6.95 | ppl  1042.86\n",
            "| epoch  11 |  1200/ 2983 batches | lr 0.000125 | ms/batch 26.31 | loss  6.98 | ppl  1074.41\n",
            "| epoch  11 |  1400/ 2983 batches | lr 0.000125 | ms/batch 26.31 | loss  6.98 | ppl  1069.83\n",
            "| epoch  11 |  1600/ 2983 batches | lr 0.000125 | ms/batch 26.31 | loss  7.01 | ppl  1110.59\n",
            "| epoch  11 |  1800/ 2983 batches | lr 0.000125 | ms/batch 26.32 | loss  6.95 | ppl  1040.28\n",
            "| epoch  11 |  2000/ 2983 batches | lr 0.000125 | ms/batch 26.30 | loss  6.97 | ppl  1063.17\n",
            "| epoch  11 |  2200/ 2983 batches | lr 0.000125 | ms/batch 26.29 | loss  6.93 | ppl  1021.72\n",
            "| epoch  11 |  2400/ 2983 batches | lr 0.000125 | ms/batch 26.36 | loss  6.85 | ppl   945.68\n",
            "| epoch  11 |  2600/ 2983 batches | lr 0.000125 | ms/batch 26.39 | loss  6.86 | ppl   949.07\n",
            "| epoch  11 |  2800/ 2983 batches | lr 0.000125 | ms/batch 26.43 | loss  6.82 | ppl   920.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 80.84s | valid loss  7.07 | valid ppl  1177.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  12 |   200/ 2983 batches | lr 0.000125 | ms/batch 26.54 | loss  6.89 | ppl   978.98\n",
            "| epoch  12 |   400/ 2983 batches | lr 0.000125 | ms/batch 26.36 | loss  6.88 | ppl   973.20\n",
            "| epoch  12 |   600/ 2983 batches | lr 0.000125 | ms/batch 26.40 | loss  6.85 | ppl   947.08\n",
            "| epoch  12 |   800/ 2983 batches | lr 0.000125 | ms/batch 26.45 | loss  6.87 | ppl   965.39\n",
            "| epoch  12 |  1000/ 2983 batches | lr 0.000125 | ms/batch 26.42 | loss  6.92 | ppl  1008.39\n",
            "| epoch  12 |  1200/ 2983 batches | lr 0.000125 | ms/batch 26.38 | loss  6.97 | ppl  1060.71\n",
            "| epoch  12 |  1400/ 2983 batches | lr 0.000125 | ms/batch 26.39 | loss  6.95 | ppl  1044.70\n",
            "| epoch  12 |  1600/ 2983 batches | lr 0.000125 | ms/batch 26.41 | loss  7.02 | ppl  1117.53\n",
            "| epoch  12 |  1800/ 2983 batches | lr 0.000125 | ms/batch 26.39 | loss  6.96 | ppl  1049.93\n",
            "| epoch  12 |  2000/ 2983 batches | lr 0.000125 | ms/batch 26.48 | loss  6.88 | ppl   972.47\n",
            "| epoch  12 |  2200/ 2983 batches | lr 0.000125 | ms/batch 26.32 | loss  6.87 | ppl   962.86\n",
            "| epoch  12 |  2400/ 2983 batches | lr 0.000125 | ms/batch 26.34 | loss  6.83 | ppl   925.66\n",
            "| epoch  12 |  2600/ 2983 batches | lr 0.000125 | ms/batch 26.32 | loss  6.84 | ppl   932.62\n",
            "| epoch  12 |  2800/ 2983 batches | lr 0.000125 | ms/batch 26.30 | loss  6.80 | ppl   893.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 80.97s | valid loss  7.10 | valid ppl  1208.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  13 |   200/ 2983 batches | lr 0.000125 | ms/batch 26.45 | loss  6.88 | ppl   976.16\n",
            "| epoch  13 |   400/ 2983 batches | lr 0.000125 | ms/batch 26.32 | loss  6.87 | ppl   963.61\n",
            "| epoch  13 |   600/ 2983 batches | lr 0.000125 | ms/batch 26.33 | loss  6.88 | ppl   969.60\n",
            "| epoch  13 |   800/ 2983 batches | lr 0.000125 | ms/batch 26.35 | loss  6.90 | ppl   993.47\n",
            "| epoch  13 |  1000/ 2983 batches | lr 0.000125 | ms/batch 26.30 | loss  6.89 | ppl   982.21\n",
            "| epoch  13 |  1200/ 2983 batches | lr 0.000125 | ms/batch 26.32 | loss  6.93 | ppl  1022.58\n",
            "| epoch  13 |  1400/ 2983 batches | lr 0.000125 | ms/batch 26.31 | loss  6.95 | ppl  1044.82\n",
            "| epoch  13 |  1600/ 2983 batches | lr 0.000125 | ms/batch 26.31 | loss  6.98 | ppl  1079.08\n",
            "| epoch  13 |  1800/ 2983 batches | lr 0.000125 | ms/batch 26.33 | loss  6.92 | ppl  1012.90\n",
            "| epoch  13 |  2000/ 2983 batches | lr 0.000125 | ms/batch 26.30 | loss  6.91 | ppl  1007.15\n",
            "| epoch  13 |  2200/ 2983 batches | lr 0.000125 | ms/batch 26.33 | loss  6.85 | ppl   947.53\n",
            "| epoch  13 |  2400/ 2983 batches | lr 0.000125 | ms/batch 26.33 | loss  6.81 | ppl   905.41\n",
            "| epoch  13 |  2600/ 2983 batches | lr 0.000125 | ms/batch 26.32 | loss  6.84 | ppl   930.90\n",
            "| epoch  13 |  2800/ 2983 batches | lr 0.000125 | ms/batch 26.31 | loss  6.81 | ppl   904.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 80.78s | valid loss  7.08 | valid ppl  1188.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  14 |   200/ 2983 batches | lr 0.000063 | ms/batch 26.43 | loss  6.96 | ppl  1053.38\n",
            "| epoch  14 |   400/ 2983 batches | lr 0.000063 | ms/batch 26.29 | loss  6.94 | ppl  1034.92\n",
            "| epoch  14 |   600/ 2983 batches | lr 0.000063 | ms/batch 26.30 | loss  6.91 | ppl  1002.32\n",
            "| epoch  14 |   800/ 2983 batches | lr 0.000063 | ms/batch 26.31 | loss  6.89 | ppl   981.24\n",
            "| epoch  14 |  1000/ 2983 batches | lr 0.000063 | ms/batch 26.31 | loss  6.89 | ppl   981.31\n",
            "| epoch  14 |  1200/ 2983 batches | lr 0.000063 | ms/batch 26.31 | loss  6.94 | ppl  1027.72\n",
            "| epoch  14 |  1400/ 2983 batches | lr 0.000063 | ms/batch 26.34 | loss  6.96 | ppl  1052.57\n",
            "| epoch  14 |  1600/ 2983 batches | lr 0.000063 | ms/batch 26.30 | loss  7.05 | ppl  1149.09\n",
            "| epoch  14 |  1800/ 2983 batches | lr 0.000063 | ms/batch 26.31 | loss  7.01 | ppl  1110.66\n",
            "| epoch  14 |  2000/ 2983 batches | lr 0.000063 | ms/batch 26.30 | loss  7.04 | ppl  1142.47\n",
            "| epoch  14 |  2200/ 2983 batches | lr 0.000063 | ms/batch 26.31 | loss  7.09 | ppl  1200.23\n",
            "| epoch  14 |  2400/ 2983 batches | lr 0.000063 | ms/batch 26.29 | loss  7.04 | ppl  1137.45\n",
            "| epoch  14 |  2600/ 2983 batches | lr 0.000063 | ms/batch 26.27 | loss  6.90 | ppl   993.80\n",
            "| epoch  14 |  2800/ 2983 batches | lr 0.000063 | ms/batch 26.35 | loss  6.82 | ppl   912.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 80.76s | valid loss  7.12 | valid ppl  1234.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  15 |   200/ 2983 batches | lr 0.000063 | ms/batch 26.42 | loss  6.92 | ppl  1009.69\n",
            "| epoch  15 |   400/ 2983 batches | lr 0.000063 | ms/batch 26.32 | loss  6.87 | ppl   967.10\n",
            "| epoch  15 |   600/ 2983 batches | lr 0.000063 | ms/batch 26.31 | loss  6.88 | ppl   970.82\n",
            "| epoch  15 |   800/ 2983 batches | lr 0.000063 | ms/batch 26.31 | loss  6.88 | ppl   974.69\n",
            "| epoch  15 |  1000/ 2983 batches | lr 0.000063 | ms/batch 26.30 | loss  6.89 | ppl   980.32\n",
            "| epoch  15 |  1200/ 2983 batches | lr 0.000063 | ms/batch 26.30 | loss  6.94 | ppl  1033.58\n",
            "| epoch  15 |  1400/ 2983 batches | lr 0.000063 | ms/batch 26.29 | loss  6.95 | ppl  1048.18\n",
            "| epoch  15 |  1600/ 2983 batches | lr 0.000063 | ms/batch 26.29 | loss  7.02 | ppl  1124.25\n",
            "| epoch  15 |  1800/ 2983 batches | lr 0.000063 | ms/batch 26.31 | loss  7.01 | ppl  1112.08\n",
            "| epoch  15 |  2000/ 2983 batches | lr 0.000063 | ms/batch 26.34 | loss  7.05 | ppl  1158.12\n",
            "| epoch  15 |  2200/ 2983 batches | lr 0.000063 | ms/batch 26.30 | loss  7.09 | ppl  1203.71\n",
            "| epoch  15 |  2400/ 2983 batches | lr 0.000063 | ms/batch 26.33 | loss  7.03 | ppl  1126.29\n",
            "| epoch  15 |  2600/ 2983 batches | lr 0.000063 | ms/batch 26.30 | loss  6.92 | ppl  1016.23\n",
            "| epoch  15 |  2800/ 2983 batches | lr 0.000063 | ms/batch 26.31 | loss  6.80 | ppl   894.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 80.76s | valid loss  7.14 | valid ppl  1265.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000063\n",
            "| epoch  16 |   200/ 2983 batches | lr 0.000063 | ms/batch 26.44 | loss  6.89 | ppl   978.22\n",
            "| epoch  16 |   400/ 2983 batches | lr 0.000063 | ms/batch 26.29 | loss  6.87 | ppl   966.23\n",
            "| epoch  16 |   600/ 2983 batches | lr 0.000063 | ms/batch 26.29 | loss  6.86 | ppl   957.04\n",
            "| epoch  16 |   800/ 2983 batches | lr 0.000063 | ms/batch 26.32 | loss  6.91 | ppl   999.46\n",
            "| epoch  16 |  1000/ 2983 batches | lr 0.000063 | ms/batch 26.27 | loss  6.97 | ppl  1069.20\n",
            "| epoch  16 |  1200/ 2983 batches | lr 0.000063 | ms/batch 26.31 | loss  7.00 | ppl  1100.54\n",
            "| epoch  16 |  1400/ 2983 batches | lr 0.000063 | ms/batch 26.27 | loss  7.00 | ppl  1098.81\n",
            "| epoch  16 |  1600/ 2983 batches | lr 0.000063 | ms/batch 26.30 | loss  7.03 | ppl  1134.59\n",
            "| epoch  16 |  1800/ 2983 batches | lr 0.000063 | ms/batch 26.29 | loss  7.00 | ppl  1094.55\n",
            "| epoch  16 |  2000/ 2983 batches | lr 0.000063 | ms/batch 26.30 | loss  7.03 | ppl  1124.55\n",
            "| epoch  16 |  2200/ 2983 batches | lr 0.000063 | ms/batch 26.31 | loss  7.07 | ppl  1174.81\n",
            "| epoch  16 |  2400/ 2983 batches | lr 0.000063 | ms/batch 26.30 | loss  6.99 | ppl  1088.55\n",
            "| epoch  16 |  2600/ 2983 batches | lr 0.000063 | ms/batch 26.35 | loss  6.94 | ppl  1034.59\n",
            "| epoch  16 |  2800/ 2983 batches | lr 0.000063 | ms/batch 26.32 | loss  6.86 | ppl   955.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 80.75s | valid loss  7.12 | valid ppl  1234.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  17 |   200/ 2983 batches | lr 0.000031 | ms/batch 26.45 | loss  6.90 | ppl   990.86\n",
            "| epoch  17 |   400/ 2983 batches | lr 0.000031 | ms/batch 26.33 | loss  6.88 | ppl   975.52\n",
            "| epoch  17 |   600/ 2983 batches | lr 0.000031 | ms/batch 26.29 | loss  6.86 | ppl   956.95\n",
            "| epoch  17 |   800/ 2983 batches | lr 0.000031 | ms/batch 26.30 | loss  6.91 | ppl   998.15\n",
            "| epoch  17 |  1000/ 2983 batches | lr 0.000031 | ms/batch 26.32 | loss  6.98 | ppl  1071.26\n",
            "| epoch  17 |  1200/ 2983 batches | lr 0.000031 | ms/batch 26.28 | loss  7.01 | ppl  1102.80\n",
            "| epoch  17 |  1400/ 2983 batches | lr 0.000031 | ms/batch 26.36 | loss  7.02 | ppl  1122.57\n",
            "| epoch  17 |  1600/ 2983 batches | lr 0.000031 | ms/batch 26.30 | loss  7.05 | ppl  1156.26\n",
            "| epoch  17 |  1800/ 2983 batches | lr 0.000031 | ms/batch 26.31 | loss  7.02 | ppl  1113.47\n",
            "| epoch  17 |  2000/ 2983 batches | lr 0.000031 | ms/batch 26.30 | loss  7.02 | ppl  1123.92\n",
            "| epoch  17 |  2200/ 2983 batches | lr 0.000031 | ms/batch 26.30 | loss  7.05 | ppl  1148.23\n",
            "| epoch  17 |  2400/ 2983 batches | lr 0.000031 | ms/batch 26.28 | loss  7.01 | ppl  1109.95\n",
            "| epoch  17 |  2600/ 2983 batches | lr 0.000031 | ms/batch 26.28 | loss  6.99 | ppl  1081.80\n",
            "| epoch  17 |  2800/ 2983 batches | lr 0.000031 | ms/batch 26.29 | loss  7.05 | ppl  1147.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 80.73s | valid loss  6.90 | valid ppl   995.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  18 |   200/ 2983 batches | lr 0.000031 | ms/batch 26.45 | loss  7.16 | ppl  1287.09\n",
            "| epoch  18 |   400/ 2983 batches | lr 0.000031 | ms/batch 26.28 | loss  7.14 | ppl  1260.15\n",
            "| epoch  18 |   600/ 2983 batches | lr 0.000031 | ms/batch 26.32 | loss  7.11 | ppl  1226.45\n",
            "| epoch  18 |   800/ 2983 batches | lr 0.000031 | ms/batch 26.29 | loss  7.11 | ppl  1218.41\n",
            "| epoch  18 |  1000/ 2983 batches | lr 0.000031 | ms/batch 26.31 | loss  7.08 | ppl  1190.10\n",
            "| epoch  18 |  1200/ 2983 batches | lr 0.000031 | ms/batch 26.29 | loss  7.08 | ppl  1189.59\n",
            "| epoch  18 |  1400/ 2983 batches | lr 0.000031 | ms/batch 26.32 | loss  7.02 | ppl  1117.00\n",
            "| epoch  18 |  1600/ 2983 batches | lr 0.000031 | ms/batch 26.31 | loss  7.05 | ppl  1149.74\n",
            "| epoch  18 |  1800/ 2983 batches | lr 0.000031 | ms/batch 26.28 | loss  7.03 | ppl  1128.42\n",
            "| epoch  18 |  2000/ 2983 batches | lr 0.000031 | ms/batch 26.29 | loss  7.03 | ppl  1135.57\n",
            "| epoch  18 |  2200/ 2983 batches | lr 0.000031 | ms/batch 26.29 | loss  7.04 | ppl  1140.22\n",
            "| epoch  18 |  2400/ 2983 batches | lr 0.000031 | ms/batch 26.30 | loss  7.04 | ppl  1136.99\n",
            "| epoch  18 |  2600/ 2983 batches | lr 0.000031 | ms/batch 26.28 | loss  6.99 | ppl  1088.02\n",
            "| epoch  18 |  2800/ 2983 batches | lr 0.000031 | ms/batch 26.28 | loss  6.99 | ppl  1084.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 80.74s | valid loss  6.90 | valid ppl   994.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  19 |   200/ 2983 batches | lr 0.000031 | ms/batch 26.46 | loss  7.15 | ppl  1271.74\n",
            "| epoch  19 |   400/ 2983 batches | lr 0.000031 | ms/batch 26.32 | loss  7.12 | ppl  1232.27\n",
            "| epoch  19 |   600/ 2983 batches | lr 0.000031 | ms/batch 26.29 | loss  7.09 | ppl  1195.97\n",
            "| epoch  19 |   800/ 2983 batches | lr 0.000031 | ms/batch 26.32 | loss  7.08 | ppl  1183.97\n",
            "| epoch  19 |  1000/ 2983 batches | lr 0.000031 | ms/batch 26.29 | loss  7.07 | ppl  1176.20\n",
            "| epoch  19 |  1200/ 2983 batches | lr 0.000031 | ms/batch 26.31 | loss  7.10 | ppl  1216.81\n",
            "| epoch  19 |  1400/ 2983 batches | lr 0.000031 | ms/batch 26.28 | loss  7.07 | ppl  1170.84\n",
            "| epoch  19 |  1600/ 2983 batches | lr 0.000031 | ms/batch 26.29 | loss  7.04 | ppl  1138.78\n",
            "| epoch  19 |  1800/ 2983 batches | lr 0.000031 | ms/batch 26.30 | loss  7.01 | ppl  1103.02\n",
            "| epoch  19 |  2000/ 2983 batches | lr 0.000031 | ms/batch 26.32 | loss  7.02 | ppl  1120.33\n",
            "| epoch  19 |  2200/ 2983 batches | lr 0.000031 | ms/batch 26.39 | loss  7.03 | ppl  1126.14\n",
            "| epoch  19 |  2400/ 2983 batches | lr 0.000031 | ms/batch 26.31 | loss  7.04 | ppl  1136.30\n",
            "| epoch  19 |  2600/ 2983 batches | lr 0.000031 | ms/batch 26.35 | loss  7.00 | ppl  1099.25\n",
            "| epoch  19 |  2800/ 2983 batches | lr 0.000031 | ms/batch 26.33 | loss  6.81 | ppl   903.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 80.79s | valid loss  7.14 | valid ppl  1256.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000031\n",
            "| epoch  20 |   200/ 2983 batches | lr 0.000031 | ms/batch 26.45 | loss  6.88 | ppl   977.34\n",
            "| epoch  20 |   400/ 2983 batches | lr 0.000031 | ms/batch 26.32 | loss  6.88 | ppl   970.36\n",
            "| epoch  20 |   600/ 2983 batches | lr 0.000031 | ms/batch 26.32 | loss  6.87 | ppl   962.95\n",
            "| epoch  20 |   800/ 2983 batches | lr 0.000031 | ms/batch 26.33 | loss  6.91 | ppl   999.40\n",
            "| epoch  20 |  1000/ 2983 batches | lr 0.000031 | ms/batch 26.31 | loss  6.99 | ppl  1088.04\n",
            "| epoch  20 |  1200/ 2983 batches | lr 0.000031 | ms/batch 26.28 | loss  7.01 | ppl  1104.68\n",
            "| epoch  20 |  1400/ 2983 batches | lr 0.000031 | ms/batch 26.33 | loss  7.00 | ppl  1092.49\n",
            "| epoch  20 |  1600/ 2983 batches | lr 0.000031 | ms/batch 26.29 | loss  7.03 | ppl  1133.76\n",
            "| epoch  20 |  1800/ 2983 batches | lr 0.000031 | ms/batch 26.32 | loss  7.00 | ppl  1096.16\n",
            "| epoch  20 |  2000/ 2983 batches | lr 0.000031 | ms/batch 26.29 | loss  7.01 | ppl  1111.49\n",
            "| epoch  20 |  2200/ 2983 batches | lr 0.000031 | ms/batch 26.32 | loss  7.01 | ppl  1109.99\n",
            "| epoch  20 |  2400/ 2983 batches | lr 0.000031 | ms/batch 26.32 | loss  6.99 | ppl  1089.44\n",
            "| epoch  20 |  2600/ 2983 batches | lr 0.000031 | ms/batch 26.30 | loss  6.94 | ppl  1035.35\n",
            "| epoch  20 |  2800/ 2983 batches | lr 0.000031 | ms/batch 26.33 | loss  6.85 | ppl   943.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 80.79s | valid loss  7.14 | valid ppl  1256.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  21 |   200/ 2983 batches | lr 0.000016 | ms/batch 26.56 | loss  6.96 | ppl  1052.76\n",
            "| epoch  21 |   400/ 2983 batches | lr 0.000016 | ms/batch 26.41 | loss  6.90 | ppl   995.07\n",
            "| epoch  21 |   600/ 2983 batches | lr 0.000016 | ms/batch 26.42 | loss  6.88 | ppl   974.65\n",
            "| epoch  21 |   800/ 2983 batches | lr 0.000016 | ms/batch 26.34 | loss  6.93 | ppl  1017.44\n",
            "| epoch  21 |  1000/ 2983 batches | lr 0.000016 | ms/batch 26.34 | loss  7.04 | ppl  1136.46\n",
            "| epoch  21 |  1200/ 2983 batches | lr 0.000016 | ms/batch 26.31 | loss  7.14 | ppl  1260.04\n",
            "| epoch  21 |  1400/ 2983 batches | lr 0.000016 | ms/batch 26.35 | loss  7.12 | ppl  1233.13\n",
            "| epoch  21 |  1600/ 2983 batches | lr 0.000016 | ms/batch 26.35 | loss  7.11 | ppl  1223.33\n",
            "| epoch  21 |  1800/ 2983 batches | lr 0.000016 | ms/batch 26.29 | loss  7.08 | ppl  1193.37\n",
            "| epoch  21 |  2000/ 2983 batches | lr 0.000016 | ms/batch 26.29 | loss  7.07 | ppl  1179.35\n",
            "| epoch  21 |  2200/ 2983 batches | lr 0.000016 | ms/batch 26.28 | loss  7.04 | ppl  1141.36\n",
            "| epoch  21 |  2400/ 2983 batches | lr 0.000016 | ms/batch 26.33 | loss  7.08 | ppl  1190.00\n",
            "| epoch  21 |  2600/ 2983 batches | lr 0.000016 | ms/batch 26.30 | loss  7.13 | ppl  1249.35\n",
            "| epoch  21 |  2800/ 2983 batches | lr 0.000016 | ms/batch 26.33 | loss  7.10 | ppl  1217.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 80.84s | valid loss  6.95 | valid ppl  1039.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  22 |   200/ 2983 batches | lr 0.000016 | ms/batch 26.47 | loss  7.14 | ppl  1257.98\n",
            "| epoch  22 |   400/ 2983 batches | lr 0.000016 | ms/batch 26.32 | loss  7.01 | ppl  1103.65\n",
            "| epoch  22 |   600/ 2983 batches | lr 0.000016 | ms/batch 26.32 | loss  6.97 | ppl  1062.23\n",
            "| epoch  22 |   800/ 2983 batches | lr 0.000016 | ms/batch 26.32 | loss  6.99 | ppl  1083.23\n",
            "| epoch  22 |  1000/ 2983 batches | lr 0.000016 | ms/batch 26.30 | loss  7.02 | ppl  1123.86\n",
            "| epoch  22 |  1200/ 2983 batches | lr 0.000016 | ms/batch 26.35 | loss  7.07 | ppl  1177.02\n",
            "| epoch  22 |  1400/ 2983 batches | lr 0.000016 | ms/batch 26.29 | loss  7.05 | ppl  1155.02\n",
            "| epoch  22 |  1600/ 2983 batches | lr 0.000016 | ms/batch 26.33 | loss  7.05 | ppl  1147.52\n",
            "| epoch  22 |  1800/ 2983 batches | lr 0.000016 | ms/batch 26.31 | loss  6.99 | ppl  1089.72\n",
            "| epoch  22 |  2000/ 2983 batches | lr 0.000016 | ms/batch 26.32 | loss  7.01 | ppl  1103.87\n",
            "| epoch  22 |  2200/ 2983 batches | lr 0.000016 | ms/batch 26.31 | loss  7.01 | ppl  1102.81\n",
            "| epoch  22 |  2400/ 2983 batches | lr 0.000016 | ms/batch 26.32 | loss  6.97 | ppl  1069.28\n",
            "| epoch  22 |  2600/ 2983 batches | lr 0.000016 | ms/batch 26.34 | loss  6.95 | ppl  1040.75\n",
            "| epoch  22 |  2800/ 2983 batches | lr 0.000016 | ms/batch 26.31 | loss  6.96 | ppl  1050.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 80.79s | valid loss  6.94 | valid ppl  1035.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000016\n",
            "| epoch  23 |   200/ 2983 batches | lr 0.000016 | ms/batch 26.40 | loss  7.21 | ppl  1347.34\n",
            "| epoch  23 |   400/ 2983 batches | lr 0.000016 | ms/batch 26.31 | loss  7.16 | ppl  1288.11\n",
            "| epoch  23 |   600/ 2983 batches | lr 0.000016 | ms/batch 26.29 | loss  7.14 | ppl  1260.44\n",
            "| epoch  23 |   800/ 2983 batches | lr 0.000016 | ms/batch 26.30 | loss  7.12 | ppl  1233.48\n",
            "| epoch  23 |  1000/ 2983 batches | lr 0.000016 | ms/batch 26.28 | loss  7.11 | ppl  1220.77\n",
            "| epoch  23 |  1200/ 2983 batches | lr 0.000016 | ms/batch 26.29 | loss  7.14 | ppl  1256.39\n",
            "| epoch  23 |  1400/ 2983 batches | lr 0.000016 | ms/batch 26.32 | loss  7.11 | ppl  1226.70\n",
            "| epoch  23 |  1600/ 2983 batches | lr 0.000016 | ms/batch 26.26 | loss  7.08 | ppl  1187.12\n",
            "| epoch  23 |  1800/ 2983 batches | lr 0.000016 | ms/batch 26.31 | loss  6.98 | ppl  1073.26\n",
            "| epoch  23 |  2000/ 2983 batches | lr 0.000016 | ms/batch 26.31 | loss  6.99 | ppl  1089.19\n",
            "| epoch  23 |  2200/ 2983 batches | lr 0.000016 | ms/batch 26.30 | loss  7.02 | ppl  1113.65\n",
            "| epoch  23 |  2400/ 2983 batches | lr 0.000016 | ms/batch 26.30 | loss  6.99 | ppl  1081.03\n",
            "| epoch  23 |  2600/ 2983 batches | lr 0.000016 | ms/batch 26.32 | loss  7.01 | ppl  1103.34\n",
            "| epoch  23 |  2800/ 2983 batches | lr 0.000016 | ms/batch 26.32 | loss  6.94 | ppl  1032.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 80.74s | valid loss  6.91 | valid ppl  1006.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  24 |   200/ 2983 batches | lr 0.000008 | ms/batch 26.44 | loss  7.18 | ppl  1309.32\n",
            "| epoch  24 |   400/ 2983 batches | lr 0.000008 | ms/batch 26.29 | loss  7.14 | ppl  1263.92\n",
            "| epoch  24 |   600/ 2983 batches | lr 0.000008 | ms/batch 26.31 | loss  7.13 | ppl  1244.26\n",
            "| epoch  24 |   800/ 2983 batches | lr 0.000008 | ms/batch 26.29 | loss  7.11 | ppl  1221.46\n",
            "| epoch  24 |  1000/ 2983 batches | lr 0.000008 | ms/batch 26.28 | loss  7.10 | ppl  1207.69\n",
            "| epoch  24 |  1200/ 2983 batches | lr 0.000008 | ms/batch 26.26 | loss  7.14 | ppl  1257.09\n",
            "| epoch  24 |  1400/ 2983 batches | lr 0.000008 | ms/batch 26.28 | loss  7.13 | ppl  1245.28\n",
            "| epoch  24 |  1600/ 2983 batches | lr 0.000008 | ms/batch 26.30 | loss  7.13 | ppl  1253.69\n",
            "| epoch  24 |  1800/ 2983 batches | lr 0.000008 | ms/batch 26.26 | loss  7.10 | ppl  1214.21\n",
            "| epoch  24 |  2000/ 2983 batches | lr 0.000008 | ms/batch 26.30 | loss  7.06 | ppl  1160.80\n",
            "| epoch  24 |  2200/ 2983 batches | lr 0.000008 | ms/batch 26.33 | loss  7.01 | ppl  1104.89\n",
            "| epoch  24 |  2400/ 2983 batches | lr 0.000008 | ms/batch 26.33 | loss  6.99 | ppl  1085.05\n",
            "| epoch  24 |  2600/ 2983 batches | lr 0.000008 | ms/batch 26.30 | loss  7.04 | ppl  1144.60\n",
            "| epoch  24 |  2800/ 2983 batches | lr 0.000008 | ms/batch 26.36 | loss  6.98 | ppl  1076.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 80.75s | valid loss  6.88 | valid ppl   974.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  25 |   200/ 2983 batches | lr 0.000008 | ms/batch 26.59 | loss  7.15 | ppl  1271.21\n",
            "| epoch  25 |   400/ 2983 batches | lr 0.000008 | ms/batch 26.40 | loss  7.12 | ppl  1238.69\n",
            "| epoch  25 |   600/ 2983 batches | lr 0.000008 | ms/batch 26.39 | loss  7.10 | ppl  1212.79\n",
            "| epoch  25 |   800/ 2983 batches | lr 0.000008 | ms/batch 26.41 | loss  7.09 | ppl  1199.11\n",
            "| epoch  25 |  1000/ 2983 batches | lr 0.000008 | ms/batch 26.37 | loss  7.08 | ppl  1187.40\n",
            "| epoch  25 |  1200/ 2983 batches | lr 0.000008 | ms/batch 26.43 | loss  7.11 | ppl  1225.26\n",
            "| epoch  25 |  1400/ 2983 batches | lr 0.000008 | ms/batch 26.35 | loss  7.10 | ppl  1210.99\n",
            "| epoch  25 |  1600/ 2983 batches | lr 0.000008 | ms/batch 26.37 | loss  7.11 | ppl  1223.36\n",
            "| epoch  25 |  1800/ 2983 batches | lr 0.000008 | ms/batch 26.30 | loss  7.08 | ppl  1190.55\n",
            "| epoch  25 |  2000/ 2983 batches | lr 0.000008 | ms/batch 26.34 | loss  7.06 | ppl  1159.57\n",
            "| epoch  25 |  2200/ 2983 batches | lr 0.000008 | ms/batch 26.28 | loss  7.01 | ppl  1106.02\n",
            "| epoch  25 |  2400/ 2983 batches | lr 0.000008 | ms/batch 26.27 | loss  7.00 | ppl  1093.25\n",
            "| epoch  25 |  2600/ 2983 batches | lr 0.000008 | ms/batch 26.31 | loss  7.06 | ppl  1166.42\n",
            "| epoch  25 |  2800/ 2983 batches | lr 0.000008 | ms/batch 26.31 | loss  6.99 | ppl  1090.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 80.90s | valid loss  6.88 | valid ppl   969.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  26 |   200/ 2983 batches | lr 0.000008 | ms/batch 26.42 | loss  7.14 | ppl  1259.23\n",
            "| epoch  26 |   400/ 2983 batches | lr 0.000008 | ms/batch 26.30 | loss  7.11 | ppl  1227.03\n",
            "| epoch  26 |   600/ 2983 batches | lr 0.000008 | ms/batch 26.27 | loss  7.09 | ppl  1205.02\n",
            "| epoch  26 |   800/ 2983 batches | lr 0.000008 | ms/batch 26.29 | loss  7.08 | ppl  1190.47\n",
            "| epoch  26 |  1000/ 2983 batches | lr 0.000008 | ms/batch 26.28 | loss  7.07 | ppl  1175.03\n",
            "| epoch  26 |  1200/ 2983 batches | lr 0.000008 | ms/batch 26.32 | loss  7.10 | ppl  1211.52\n",
            "| epoch  26 |  1400/ 2983 batches | lr 0.000008 | ms/batch 26.30 | loss  7.09 | ppl  1198.01\n",
            "| epoch  26 |  1600/ 2983 batches | lr 0.000008 | ms/batch 26.29 | loss  7.10 | ppl  1214.49\n",
            "| epoch  26 |  1800/ 2983 batches | lr 0.000008 | ms/batch 26.30 | loss  7.08 | ppl  1187.39\n",
            "| epoch  26 |  2000/ 2983 batches | lr 0.000008 | ms/batch 26.31 | loss  7.07 | ppl  1174.49\n",
            "| epoch  26 |  2200/ 2983 batches | lr 0.000008 | ms/batch 26.32 | loss  7.02 | ppl  1114.79\n",
            "| epoch  26 |  2400/ 2983 batches | lr 0.000008 | ms/batch 26.27 | loss  6.99 | ppl  1090.04\n",
            "| epoch  26 |  2600/ 2983 batches | lr 0.000008 | ms/batch 26.34 | loss  7.07 | ppl  1176.01\n",
            "| epoch  26 |  2800/ 2983 batches | lr 0.000008 | ms/batch 26.42 | loss  7.00 | ppl  1099.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 80.78s | valid loss  6.88 | valid ppl   967.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  27 |   200/ 2983 batches | lr 0.000008 | ms/batch 26.55 | loss  7.13 | ppl  1248.43\n",
            "| epoch  27 |   400/ 2983 batches | lr 0.000008 | ms/batch 26.38 | loss  7.10 | ppl  1215.95\n",
            "| epoch  27 |   600/ 2983 batches | lr 0.000008 | ms/batch 26.47 | loss  7.09 | ppl  1198.97\n",
            "| epoch  27 |   800/ 2983 batches | lr 0.000008 | ms/batch 26.38 | loss  7.08 | ppl  1187.15\n",
            "| epoch  27 |  1000/ 2983 batches | lr 0.000008 | ms/batch 26.37 | loss  7.06 | ppl  1166.81\n",
            "| epoch  27 |  1200/ 2983 batches | lr 0.000008 | ms/batch 26.31 | loss  7.09 | ppl  1201.20\n",
            "| epoch  27 |  1400/ 2983 batches | lr 0.000008 | ms/batch 26.34 | loss  7.08 | ppl  1188.28\n",
            "| epoch  27 |  1600/ 2983 batches | lr 0.000008 | ms/batch 26.25 | loss  7.10 | ppl  1207.50\n",
            "| epoch  27 |  1800/ 2983 batches | lr 0.000008 | ms/batch 26.25 | loss  7.08 | ppl  1182.64\n",
            "| epoch  27 |  2000/ 2983 batches | lr 0.000008 | ms/batch 26.31 | loss  7.07 | ppl  1181.98\n",
            "| epoch  27 |  2200/ 2983 batches | lr 0.000008 | ms/batch 26.28 | loss  7.03 | ppl  1131.91\n",
            "| epoch  27 |  2400/ 2983 batches | lr 0.000008 | ms/batch 26.33 | loss  7.00 | ppl  1093.03\n",
            "| epoch  27 |  2600/ 2983 batches | lr 0.000008 | ms/batch 26.30 | loss  7.07 | ppl  1179.14\n",
            "| epoch  27 |  2800/ 2983 batches | lr 0.000008 | ms/batch 26.29 | loss  7.01 | ppl  1107.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 80.81s | valid loss  6.88 | valid ppl   968.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  28 |   200/ 2983 batches | lr 0.000008 | ms/batch 26.45 | loss  7.12 | ppl  1238.02\n",
            "| epoch  28 |   400/ 2983 batches | lr 0.000008 | ms/batch 26.30 | loss  7.10 | ppl  1206.60\n",
            "| epoch  28 |   600/ 2983 batches | lr 0.000008 | ms/batch 26.33 | loss  7.08 | ppl  1192.19\n",
            "| epoch  28 |   800/ 2983 batches | lr 0.000008 | ms/batch 26.32 | loss  7.08 | ppl  1183.36\n",
            "| epoch  28 |  1000/ 2983 batches | lr 0.000008 | ms/batch 26.30 | loss  7.06 | ppl  1160.48\n",
            "| epoch  28 |  1200/ 2983 batches | lr 0.000008 | ms/batch 26.31 | loss  7.08 | ppl  1193.37\n",
            "| epoch  28 |  1400/ 2983 batches | lr 0.000008 | ms/batch 26.29 | loss  7.07 | ppl  1180.90\n",
            "| epoch  28 |  1600/ 2983 batches | lr 0.000008 | ms/batch 26.32 | loss  7.09 | ppl  1199.56\n",
            "| epoch  28 |  1800/ 2983 batches | lr 0.000008 | ms/batch 26.30 | loss  7.07 | ppl  1174.74\n",
            "| epoch  28 |  2000/ 2983 batches | lr 0.000008 | ms/batch 26.30 | loss  7.07 | ppl  1179.97\n",
            "| epoch  28 |  2200/ 2983 batches | lr 0.000008 | ms/batch 26.29 | loss  7.05 | ppl  1147.39\n",
            "| epoch  28 |  2400/ 2983 batches | lr 0.000008 | ms/batch 26.29 | loss  7.02 | ppl  1117.40\n",
            "| epoch  28 |  2600/ 2983 batches | lr 0.000008 | ms/batch 26.40 | loss  7.07 | ppl  1179.47\n",
            "| epoch  28 |  2800/ 2983 batches | lr 0.000008 | ms/batch 26.29 | loss  7.02 | ppl  1115.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 80.76s | valid loss  6.88 | valid ppl   971.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000008\n",
            "| epoch  29 |   200/ 2983 batches | lr 0.000008 | ms/batch 26.45 | loss  7.11 | ppl  1219.60\n",
            "| epoch  29 |   400/ 2983 batches | lr 0.000008 | ms/batch 26.33 | loss  7.09 | ppl  1198.71\n",
            "| epoch  29 |   600/ 2983 batches | lr 0.000008 | ms/batch 26.30 | loss  7.08 | ppl  1185.07\n",
            "| epoch  29 |   800/ 2983 batches | lr 0.000008 | ms/batch 26.30 | loss  7.07 | ppl  1177.32\n",
            "| epoch  29 |  1000/ 2983 batches | lr 0.000008 | ms/batch 26.30 | loss  7.05 | ppl  1153.38\n",
            "| epoch  29 |  1200/ 2983 batches | lr 0.000008 | ms/batch 26.30 | loss  7.08 | ppl  1187.06\n",
            "| epoch  29 |  1400/ 2983 batches | lr 0.000008 | ms/batch 26.32 | loss  7.07 | ppl  1174.43\n",
            "| epoch  29 |  1600/ 2983 batches | lr 0.000008 | ms/batch 26.27 | loss  7.08 | ppl  1192.72\n",
            "| epoch  29 |  1800/ 2983 batches | lr 0.000008 | ms/batch 26.32 | loss  7.06 | ppl  1165.73\n",
            "| epoch  29 |  2000/ 2983 batches | lr 0.000008 | ms/batch 26.30 | loss  7.07 | ppl  1172.24\n",
            "| epoch  29 |  2200/ 2983 batches | lr 0.000008 | ms/batch 26.31 | loss  7.05 | ppl  1151.47\n",
            "| epoch  29 |  2400/ 2983 batches | lr 0.000008 | ms/batch 26.27 | loss  7.03 | ppl  1135.35\n",
            "| epoch  29 |  2600/ 2983 batches | lr 0.000008 | ms/batch 26.30 | loss  7.08 | ppl  1184.71\n",
            "| epoch  29 |  2800/ 2983 batches | lr 0.000008 | ms/batch 26.31 | loss  7.05 | ppl  1147.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 80.80s | valid loss  6.88 | valid ppl   969.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000004\n",
            "| epoch  30 |   200/ 2983 batches | lr 0.000004 | ms/batch 26.49 | loss  7.10 | ppl  1216.92\n",
            "| epoch  30 |   400/ 2983 batches | lr 0.000004 | ms/batch 26.32 | loss  7.09 | ppl  1198.01\n",
            "| epoch  30 |   600/ 2983 batches | lr 0.000004 | ms/batch 26.31 | loss  7.08 | ppl  1184.99\n",
            "| epoch  30 |   800/ 2983 batches | lr 0.000004 | ms/batch 26.27 | loss  7.08 | ppl  1189.04\n",
            "| epoch  30 |  1000/ 2983 batches | lr 0.000004 | ms/batch 26.32 | loss  7.06 | ppl  1168.68\n",
            "| epoch  30 |  1200/ 2983 batches | lr 0.000004 | ms/batch 26.28 | loss  7.09 | ppl  1197.89\n",
            "| epoch  30 |  1400/ 2983 batches | lr 0.000004 | ms/batch 26.30 | loss  7.08 | ppl  1185.75\n",
            "| epoch  30 |  1600/ 2983 batches | lr 0.000004 | ms/batch 26.28 | loss  7.09 | ppl  1198.68\n",
            "| epoch  30 |  1800/ 2983 batches | lr 0.000004 | ms/batch 26.26 | loss  7.06 | ppl  1169.30\n",
            "| epoch  30 |  2000/ 2983 batches | lr 0.000004 | ms/batch 26.32 | loss  7.07 | ppl  1179.92\n",
            "| epoch  30 |  2200/ 2983 batches | lr 0.000004 | ms/batch 26.31 | loss  7.05 | ppl  1158.63\n",
            "| epoch  30 |  2400/ 2983 batches | lr 0.000004 | ms/batch 26.28 | loss  7.04 | ppl  1138.95\n",
            "| epoch  30 |  2600/ 2983 batches | lr 0.000004 | ms/batch 26.31 | loss  7.07 | ppl  1173.09\n",
            "| epoch  30 |  2800/ 2983 batches | lr 0.000004 | ms/batch 26.30 | loss  7.05 | ppl  1155.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 80.73s | valid loss  6.87 | valid ppl   958.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000004\n",
            "| epoch  31 |   200/ 2983 batches | lr 0.000004 | ms/batch 26.44 | loss  7.13 | ppl  1244.79\n",
            "| epoch  31 |   400/ 2983 batches | lr 0.000004 | ms/batch 26.29 | loss  7.11 | ppl  1222.22\n",
            "| epoch  31 |   600/ 2983 batches | lr 0.000004 | ms/batch 26.33 | loss  7.08 | ppl  1186.61\n",
            "| epoch  31 |   800/ 2983 batches | lr 0.000004 | ms/batch 26.32 | loss  7.07 | ppl  1180.71\n",
            "| epoch  31 |  1000/ 2983 batches | lr 0.000004 | ms/batch 26.29 | loss  7.06 | ppl  1166.44\n",
            "| epoch  31 |  1200/ 2983 batches | lr 0.000004 | ms/batch 26.31 | loss  7.08 | ppl  1193.14\n",
            "| epoch  31 |  1400/ 2983 batches | lr 0.000004 | ms/batch 26.29 | loss  7.07 | ppl  1180.37\n",
            "| epoch  31 |  1600/ 2983 batches | lr 0.000004 | ms/batch 26.29 | loss  7.09 | ppl  1194.36\n",
            "| epoch  31 |  1800/ 2983 batches | lr 0.000004 | ms/batch 26.30 | loss  7.06 | ppl  1164.57\n",
            "| epoch  31 |  2000/ 2983 batches | lr 0.000004 | ms/batch 26.28 | loss  7.07 | ppl  1176.40\n",
            "| epoch  31 |  2200/ 2983 batches | lr 0.000004 | ms/batch 26.32 | loss  7.06 | ppl  1159.88\n",
            "| epoch  31 |  2400/ 2983 batches | lr 0.000004 | ms/batch 26.32 | loss  7.04 | ppl  1141.65\n",
            "| epoch  31 |  2600/ 2983 batches | lr 0.000004 | ms/batch 26.32 | loss  7.07 | ppl  1175.86\n",
            "| epoch  31 |  2800/ 2983 batches | lr 0.000004 | ms/batch 26.31 | loss  7.06 | ppl  1161.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 80.76s | valid loss  6.87 | valid ppl   958.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000004\n",
            "| epoch  32 |   200/ 2983 batches | lr 0.000004 | ms/batch 26.43 | loss  7.13 | ppl  1247.52\n",
            "| epoch  32 |   400/ 2983 batches | lr 0.000004 | ms/batch 26.34 | loss  7.11 | ppl  1225.65\n",
            "| epoch  32 |   600/ 2983 batches | lr 0.000004 | ms/batch 26.30 | loss  7.08 | ppl  1189.82\n",
            "| epoch  32 |   800/ 2983 batches | lr 0.000004 | ms/batch 26.31 | loss  7.07 | ppl  1174.97\n",
            "| epoch  32 |  1000/ 2983 batches | lr 0.000004 | ms/batch 26.33 | loss  7.06 | ppl  1162.26\n",
            "| epoch  32 |  1200/ 2983 batches | lr 0.000004 | ms/batch 26.33 | loss  7.08 | ppl  1191.96\n",
            "| epoch  32 |  1400/ 2983 batches | lr 0.000004 | ms/batch 26.31 | loss  7.07 | ppl  1179.13\n",
            "| epoch  32 |  1600/ 2983 batches | lr 0.000004 | ms/batch 26.29 | loss  7.08 | ppl  1192.20\n",
            "| epoch  32 |  1800/ 2983 batches | lr 0.000004 | ms/batch 26.34 | loss  7.06 | ppl  1161.95\n",
            "| epoch  32 |  2000/ 2983 batches | lr 0.000004 | ms/batch 26.35 | loss  7.07 | ppl  1174.40\n",
            "| epoch  32 |  2200/ 2983 batches | lr 0.000004 | ms/batch 26.34 | loss  7.06 | ppl  1159.22\n",
            "| epoch  32 |  2400/ 2983 batches | lr 0.000004 | ms/batch 26.32 | loss  7.04 | ppl  1140.83\n",
            "| epoch  32 |  2600/ 2983 batches | lr 0.000004 | ms/batch 26.36 | loss  7.07 | ppl  1176.12\n",
            "| epoch  32 |  2800/ 2983 batches | lr 0.000004 | ms/batch 26.34 | loss  7.06 | ppl  1165.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 80.82s | valid loss  6.87 | valid ppl   958.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000004\n",
            "| epoch  33 |   200/ 2983 batches | lr 0.000004 | ms/batch 26.51 | loss  7.13 | ppl  1248.00\n",
            "| epoch  33 |   400/ 2983 batches | lr 0.000004 | ms/batch 26.34 | loss  7.11 | ppl  1227.21\n",
            "| epoch  33 |   600/ 2983 batches | lr 0.000004 | ms/batch 26.33 | loss  7.08 | ppl  1192.80\n",
            "| epoch  33 |   800/ 2983 batches | lr 0.000004 | ms/batch 26.32 | loss  7.07 | ppl  1170.55\n",
            "| epoch  33 |  1000/ 2983 batches | lr 0.000004 | ms/batch 26.36 | loss  7.05 | ppl  1157.61\n",
            "| epoch  33 |  1200/ 2983 batches | lr 0.000004 | ms/batch 26.32 | loss  7.08 | ppl  1190.43\n",
            "| epoch  33 |  1400/ 2983 batches | lr 0.000004 | ms/batch 26.35 | loss  7.07 | ppl  1178.45\n",
            "| epoch  33 |  1600/ 2983 batches | lr 0.000004 | ms/batch 26.28 | loss  7.08 | ppl  1191.43\n",
            "| epoch  33 |  1800/ 2983 batches | lr 0.000004 | ms/batch 26.30 | loss  7.06 | ppl  1161.08\n",
            "| epoch  33 |  2000/ 2983 batches | lr 0.000004 | ms/batch 26.31 | loss  7.07 | ppl  1173.43\n",
            "| epoch  33 |  2200/ 2983 batches | lr 0.000004 | ms/batch 26.28 | loss  7.05 | ppl  1158.41\n",
            "| epoch  33 |  2400/ 2983 batches | lr 0.000004 | ms/batch 26.37 | loss  7.04 | ppl  1139.49\n",
            "| epoch  33 |  2600/ 2983 batches | lr 0.000004 | ms/batch 26.33 | loss  7.07 | ppl  1175.84\n",
            "| epoch  33 |  2800/ 2983 batches | lr 0.000004 | ms/batch 26.33 | loss  7.06 | ppl  1166.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 80.78s | valid loss  6.87 | valid ppl   958.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000002\n",
            "| epoch  34 |   200/ 2983 batches | lr 0.000002 | ms/batch 26.41 | loss  7.13 | ppl  1250.29\n",
            "| epoch  34 |   400/ 2983 batches | lr 0.000002 | ms/batch 26.28 | loss  7.12 | ppl  1232.44\n",
            "| epoch  34 |   600/ 2983 batches | lr 0.000002 | ms/batch 26.28 | loss  7.09 | ppl  1204.07\n",
            "| epoch  34 |   800/ 2983 batches | lr 0.000002 | ms/batch 26.30 | loss  7.08 | ppl  1187.26\n",
            "| epoch  34 |  1000/ 2983 batches | lr 0.000002 | ms/batch 26.29 | loss  7.06 | ppl  1165.97\n",
            "| epoch  34 |  1200/ 2983 batches | lr 0.000002 | ms/batch 26.28 | loss  7.08 | ppl  1190.69\n",
            "| epoch  34 |  1400/ 2983 batches | lr 0.000002 | ms/batch 26.28 | loss  7.07 | ppl  1180.57\n",
            "| epoch  34 |  1600/ 2983 batches | lr 0.000002 | ms/batch 26.30 | loss  7.09 | ppl  1195.34\n",
            "| epoch  34 |  1800/ 2983 batches | lr 0.000002 | ms/batch 26.27 | loss  7.06 | ppl  1169.05\n",
            "| epoch  34 |  2000/ 2983 batches | lr 0.000002 | ms/batch 26.28 | loss  7.08 | ppl  1186.29\n",
            "| epoch  34 |  2200/ 2983 batches | lr 0.000002 | ms/batch 26.26 | loss  7.06 | ppl  1169.16\n",
            "| epoch  34 |  2400/ 2983 batches | lr 0.000002 | ms/batch 26.30 | loss  7.04 | ppl  1143.17\n",
            "| epoch  34 |  2600/ 2983 batches | lr 0.000002 | ms/batch 26.29 | loss  7.07 | ppl  1170.31\n",
            "| epoch  34 |  2800/ 2983 batches | lr 0.000002 | ms/batch 26.27 | loss  7.05 | ppl  1149.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 80.68s | valid loss  6.86 | valid ppl   957.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000002\n",
            "| epoch  35 |   200/ 2983 batches | lr 0.000002 | ms/batch 26.43 | loss  7.13 | ppl  1248.34\n",
            "| epoch  35 |   400/ 2983 batches | lr 0.000002 | ms/batch 26.30 | loss  7.12 | ppl  1230.54\n",
            "| epoch  35 |   600/ 2983 batches | lr 0.000002 | ms/batch 26.31 | loss  7.09 | ppl  1201.30\n",
            "| epoch  35 |   800/ 2983 batches | lr 0.000002 | ms/batch 26.31 | loss  7.08 | ppl  1185.02\n",
            "| epoch  35 |  1000/ 2983 batches | lr 0.000002 | ms/batch 26.30 | loss  7.06 | ppl  1164.32\n",
            "| epoch  35 |  1200/ 2983 batches | lr 0.000002 | ms/batch 26.29 | loss  7.08 | ppl  1190.53\n",
            "| epoch  35 |  1400/ 2983 batches | lr 0.000002 | ms/batch 26.29 | loss  7.07 | ppl  1180.82\n",
            "| epoch  35 |  1600/ 2983 batches | lr 0.000002 | ms/batch 26.29 | loss  7.09 | ppl  1196.08\n",
            "| epoch  35 |  1800/ 2983 batches | lr 0.000002 | ms/batch 26.30 | loss  7.06 | ppl  1169.90\n",
            "| epoch  35 |  2000/ 2983 batches | lr 0.000002 | ms/batch 26.26 | loss  7.08 | ppl  1188.34\n",
            "| epoch  35 |  2200/ 2983 batches | lr 0.000002 | ms/batch 26.31 | loss  7.06 | ppl  1169.47\n",
            "| epoch  35 |  2400/ 2983 batches | lr 0.000002 | ms/batch 26.29 | loss  7.04 | ppl  1142.27\n",
            "| epoch  35 |  2600/ 2983 batches | lr 0.000002 | ms/batch 26.32 | loss  7.07 | ppl  1170.32\n",
            "| epoch  35 |  2800/ 2983 batches | lr 0.000002 | ms/batch 26.29 | loss  7.05 | ppl  1148.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 80.73s | valid loss  6.86 | valid ppl   957.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000002\n",
            "| epoch  36 |   200/ 2983 batches | lr 0.000002 | ms/batch 26.40 | loss  7.13 | ppl  1247.48\n",
            "| epoch  36 |   400/ 2983 batches | lr 0.000002 | ms/batch 26.28 | loss  7.11 | ppl  1228.89\n",
            "| epoch  36 |   600/ 2983 batches | lr 0.000002 | ms/batch 26.28 | loss  7.09 | ppl  1199.46\n",
            "| epoch  36 |   800/ 2983 batches | lr 0.000002 | ms/batch 26.31 | loss  7.08 | ppl  1183.82\n",
            "| epoch  36 |  1000/ 2983 batches | lr 0.000002 | ms/batch 26.29 | loss  7.06 | ppl  1162.77\n",
            "| epoch  36 |  1200/ 2983 batches | lr 0.000002 | ms/batch 26.28 | loss  7.08 | ppl  1190.07\n",
            "| epoch  36 |  1400/ 2983 batches | lr 0.000002 | ms/batch 26.34 | loss  7.07 | ppl  1181.13\n",
            "| epoch  36 |  1600/ 2983 batches | lr 0.000002 | ms/batch 26.27 | loss  7.09 | ppl  1196.88\n",
            "| epoch  36 |  1800/ 2983 batches | lr 0.000002 | ms/batch 26.27 | loss  7.07 | ppl  1171.26\n",
            "| epoch  36 |  2000/ 2983 batches | lr 0.000002 | ms/batch 26.27 | loss  7.08 | ppl  1189.14\n",
            "| epoch  36 |  2200/ 2983 batches | lr 0.000002 | ms/batch 26.29 | loss  7.06 | ppl  1169.67\n",
            "| epoch  36 |  2400/ 2983 batches | lr 0.000002 | ms/batch 26.31 | loss  7.04 | ppl  1141.79\n",
            "| epoch  36 |  2600/ 2983 batches | lr 0.000002 | ms/batch 26.29 | loss  7.06 | ppl  1170.08\n",
            "| epoch  36 |  2800/ 2983 batches | lr 0.000002 | ms/batch 26.32 | loss  7.05 | ppl  1148.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 80.71s | valid loss  6.86 | valid ppl   957.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000002\n",
            "| epoch  37 |   200/ 2983 batches | lr 0.000002 | ms/batch 26.45 | loss  7.13 | ppl  1246.49\n",
            "| epoch  37 |   400/ 2983 batches | lr 0.000002 | ms/batch 26.24 | loss  7.11 | ppl  1227.98\n",
            "| epoch  37 |   600/ 2983 batches | lr 0.000002 | ms/batch 26.29 | loss  7.09 | ppl  1198.45\n",
            "| epoch  37 |   800/ 2983 batches | lr 0.000002 | ms/batch 26.32 | loss  7.08 | ppl  1183.41\n",
            "| epoch  37 |  1000/ 2983 batches | lr 0.000002 | ms/batch 26.33 | loss  7.06 | ppl  1161.85\n",
            "| epoch  37 |  1200/ 2983 batches | lr 0.000002 | ms/batch 26.38 | loss  7.08 | ppl  1189.23\n",
            "| epoch  37 |  1400/ 2983 batches | lr 0.000002 | ms/batch 26.35 | loss  7.07 | ppl  1181.32\n",
            "| epoch  37 |  1600/ 2983 batches | lr 0.000002 | ms/batch 26.43 | loss  7.09 | ppl  1197.18\n",
            "| epoch  37 |  1800/ 2983 batches | lr 0.000002 | ms/batch 26.37 | loss  7.07 | ppl  1172.00\n",
            "| epoch  37 |  2000/ 2983 batches | lr 0.000002 | ms/batch 26.39 | loss  7.08 | ppl  1190.10\n",
            "| epoch  37 |  2200/ 2983 batches | lr 0.000002 | ms/batch 26.39 | loss  7.06 | ppl  1169.92\n",
            "| epoch  37 |  2400/ 2983 batches | lr 0.000002 | ms/batch 26.38 | loss  7.04 | ppl  1141.15\n",
            "| epoch  37 |  2600/ 2983 batches | lr 0.000002 | ms/batch 26.32 | loss  7.07 | ppl  1170.40\n",
            "| epoch  37 |  2800/ 2983 batches | lr 0.000002 | ms/batch 26.35 | loss  7.05 | ppl  1148.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 80.87s | valid loss  6.86 | valid ppl   957.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  38 |   200/ 2983 batches | lr 0.000001 | ms/batch 26.43 | loss  7.13 | ppl  1247.38\n",
            "| epoch  38 |   400/ 2983 batches | lr 0.000001 | ms/batch 26.30 | loss  7.11 | ppl  1229.26\n",
            "| epoch  38 |   600/ 2983 batches | lr 0.000001 | ms/batch 26.29 | loss  7.09 | ppl  1200.49\n",
            "| epoch  38 |   800/ 2983 batches | lr 0.000001 | ms/batch 26.37 | loss  7.08 | ppl  1190.25\n",
            "| epoch  38 |  1000/ 2983 batches | lr 0.000001 | ms/batch 26.33 | loss  7.07 | ppl  1173.10\n",
            "| epoch  38 |  1200/ 2983 batches | lr 0.000001 | ms/batch 26.31 | loss  7.08 | ppl  1191.85\n",
            "| epoch  38 |  1400/ 2983 batches | lr 0.000001 | ms/batch 26.31 | loss  7.07 | ppl  1179.09\n",
            "| epoch  38 |  1600/ 2983 batches | lr 0.000001 | ms/batch 26.31 | loss  7.08 | ppl  1193.80\n",
            "| epoch  38 |  1800/ 2983 batches | lr 0.000001 | ms/batch 26.31 | loss  7.06 | ppl  1169.51\n",
            "| epoch  38 |  2000/ 2983 batches | lr 0.000001 | ms/batch 26.33 | loss  7.08 | ppl  1191.08\n",
            "| epoch  38 |  2200/ 2983 batches | lr 0.000001 | ms/batch 26.38 | loss  7.07 | ppl  1172.86\n",
            "| epoch  38 |  2400/ 2983 batches | lr 0.000001 | ms/batch 26.32 | loss  7.04 | ppl  1143.19\n",
            "| epoch  38 |  2600/ 2983 batches | lr 0.000001 | ms/batch 26.34 | loss  7.06 | ppl  1168.42\n",
            "| epoch  38 |  2800/ 2983 batches | lr 0.000001 | ms/batch 26.33 | loss  7.04 | ppl  1141.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 80.78s | valid loss  6.86 | valid ppl   957.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  39 |   200/ 2983 batches | lr 0.000001 | ms/batch 26.46 | loss  7.13 | ppl  1246.50\n",
            "| epoch  39 |   400/ 2983 batches | lr 0.000001 | ms/batch 26.34 | loss  7.11 | ppl  1227.85\n",
            "| epoch  39 |   600/ 2983 batches | lr 0.000001 | ms/batch 26.32 | loss  7.09 | ppl  1199.33\n",
            "| epoch  39 |   800/ 2983 batches | lr 0.000001 | ms/batch 26.35 | loss  7.08 | ppl  1189.02\n",
            "| epoch  39 |  1000/ 2983 batches | lr 0.000001 | ms/batch 26.34 | loss  7.07 | ppl  1172.58\n",
            "| epoch  39 |  1200/ 2983 batches | lr 0.000001 | ms/batch 26.31 | loss  7.08 | ppl  1191.77\n",
            "| epoch  39 |  1400/ 2983 batches | lr 0.000001 | ms/batch 26.32 | loss  7.07 | ppl  1178.55\n",
            "| epoch  39 |  1600/ 2983 batches | lr 0.000001 | ms/batch 26.31 | loss  7.09 | ppl  1194.02\n",
            "| epoch  39 |  1800/ 2983 batches | lr 0.000001 | ms/batch 26.34 | loss  7.07 | ppl  1170.55\n",
            "| epoch  39 |  2000/ 2983 batches | lr 0.000001 | ms/batch 26.32 | loss  7.08 | ppl  1192.72\n",
            "| epoch  39 |  2200/ 2983 batches | lr 0.000001 | ms/batch 26.36 | loss  7.07 | ppl  1174.39\n",
            "| epoch  39 |  2400/ 2983 batches | lr 0.000001 | ms/batch 26.36 | loss  7.04 | ppl  1143.14\n",
            "| epoch  39 |  2600/ 2983 batches | lr 0.000001 | ms/batch 26.33 | loss  7.06 | ppl  1168.44\n",
            "| epoch  39 |  2800/ 2983 batches | lr 0.000001 | ms/batch 26.34 | loss  7.04 | ppl  1141.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 80.85s | valid loss  6.86 | valid ppl   957.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  40 |   200/ 2983 batches | lr 0.000001 | ms/batch 26.46 | loss  7.13 | ppl  1245.69\n",
            "| epoch  40 |   400/ 2983 batches | lr 0.000001 | ms/batch 26.33 | loss  7.11 | ppl  1227.24\n",
            "| epoch  40 |   600/ 2983 batches | lr 0.000001 | ms/batch 26.37 | loss  7.09 | ppl  1198.53\n",
            "| epoch  40 |   800/ 2983 batches | lr 0.000001 | ms/batch 26.31 | loss  7.08 | ppl  1188.25\n",
            "| epoch  40 |  1000/ 2983 batches | lr 0.000001 | ms/batch 26.35 | loss  7.07 | ppl  1172.12\n",
            "| epoch  40 |  1200/ 2983 batches | lr 0.000001 | ms/batch 26.29 | loss  7.08 | ppl  1191.12\n",
            "| epoch  40 |  1400/ 2983 batches | lr 0.000001 | ms/batch 26.31 | loss  7.07 | ppl  1178.79\n",
            "| epoch  40 |  1600/ 2983 batches | lr 0.000001 | ms/batch 26.34 | loss  7.08 | ppl  1193.92\n",
            "| epoch  40 |  1800/ 2983 batches | lr 0.000001 | ms/batch 26.33 | loss  7.07 | ppl  1171.04\n",
            "| epoch  40 |  2000/ 2983 batches | lr 0.000001 | ms/batch 26.34 | loss  7.08 | ppl  1193.00\n",
            "| epoch  40 |  2200/ 2983 batches | lr 0.000001 | ms/batch 26.32 | loss  7.07 | ppl  1174.97\n",
            "| epoch  40 |  2400/ 2983 batches | lr 0.000001 | ms/batch 26.30 | loss  7.04 | ppl  1143.57\n",
            "| epoch  40 |  2600/ 2983 batches | lr 0.000001 | ms/batch 26.29 | loss  7.06 | ppl  1168.44\n",
            "| epoch  40 |  2800/ 2983 batches | lr 0.000001 | ms/batch 26.32 | loss  7.04 | ppl  1140.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 80.79s | valid loss  6.86 | valid ppl   957.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  41 |   200/ 2983 batches | lr 0.000000 | ms/batch 26.44 | loss  7.13 | ppl  1246.21\n",
            "| epoch  41 |   400/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.11 | ppl  1227.65\n",
            "| epoch  41 |   600/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.09 | ppl  1199.46\n",
            "| epoch  41 |   800/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.08 | ppl  1191.11\n",
            "| epoch  41 |  1000/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.07 | ppl  1177.24\n",
            "| epoch  41 |  1200/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.08 | ppl  1193.76\n",
            "| epoch  41 |  1400/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.07 | ppl  1177.99\n",
            "| epoch  41 |  1600/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.08 | ppl  1191.26\n",
            "| epoch  41 |  1800/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.06 | ppl  1167.87\n",
            "| epoch  41 |  2000/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.08 | ppl  1192.37\n",
            "| epoch  41 |  2200/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.07 | ppl  1175.89\n",
            "| epoch  41 |  2400/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.04 | ppl  1144.16\n",
            "| epoch  41 |  2600/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.06 | ppl  1168.14\n",
            "| epoch  41 |  2800/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.04 | ppl  1137.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 80.75s | valid loss  6.86 | valid ppl   957.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  42 |   200/ 2983 batches | lr 0.000000 | ms/batch 26.41 | loss  7.13 | ppl  1245.81\n",
            "| epoch  42 |   400/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.11 | ppl  1227.00\n",
            "| epoch  42 |   600/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.09 | ppl  1198.83\n",
            "| epoch  42 |   800/ 2983 batches | lr 0.000000 | ms/batch 26.26 | loss  7.08 | ppl  1190.32\n",
            "| epoch  42 |  1000/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.07 | ppl  1176.49\n",
            "| epoch  42 |  1200/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.08 | ppl  1193.68\n",
            "| epoch  42 |  1400/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.07 | ppl  1177.43\n",
            "| epoch  42 |  1600/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.08 | ppl  1191.65\n",
            "| epoch  42 |  1800/ 2983 batches | lr 0.000000 | ms/batch 26.35 | loss  7.06 | ppl  1167.79\n",
            "| epoch  42 |  2000/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.08 | ppl  1192.43\n",
            "| epoch  42 |  2200/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.07 | ppl  1176.29\n",
            "| epoch  42 |  2400/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.04 | ppl  1144.40\n",
            "| epoch  42 |  2600/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.06 | ppl  1168.26\n",
            "| epoch  42 |  2800/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.04 | ppl  1136.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 80.80s | valid loss  6.86 | valid ppl   957.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  43 |   200/ 2983 batches | lr 0.000000 | ms/batch 26.47 | loss  7.13 | ppl  1245.09\n",
            "| epoch  43 |   400/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.11 | ppl  1226.63\n",
            "| epoch  43 |   600/ 2983 batches | lr 0.000000 | ms/batch 26.41 | loss  7.09 | ppl  1197.90\n",
            "| epoch  43 |   800/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.08 | ppl  1190.11\n",
            "| epoch  43 |  1000/ 2983 batches | lr 0.000000 | ms/batch 26.39 | loss  7.07 | ppl  1176.83\n",
            "| epoch  43 |  1200/ 2983 batches | lr 0.000000 | ms/batch 26.35 | loss  7.08 | ppl  1193.56\n",
            "| epoch  43 |  1400/ 2983 batches | lr 0.000000 | ms/batch 26.36 | loss  7.07 | ppl  1177.51\n",
            "| epoch  43 |  1600/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.08 | ppl  1191.43\n",
            "| epoch  43 |  1800/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.06 | ppl  1168.46\n",
            "| epoch  43 |  2000/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.08 | ppl  1193.31\n",
            "| epoch  43 |  2200/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.07 | ppl  1176.79\n",
            "| epoch  43 |  2400/ 2983 batches | lr 0.000000 | ms/batch 26.43 | loss  7.04 | ppl  1144.94\n",
            "| epoch  43 |  2600/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.06 | ppl  1167.67\n",
            "| epoch  43 |  2800/ 2983 batches | lr 0.000000 | ms/batch 26.37 | loss  7.04 | ppl  1136.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 80.89s | valid loss  6.86 | valid ppl   957.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  44 |   200/ 2983 batches | lr 0.000000 | ms/batch 26.47 | loss  7.13 | ppl  1245.31\n",
            "| epoch  44 |   400/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.11 | ppl  1226.88\n",
            "| epoch  44 |   600/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.09 | ppl  1198.33\n",
            "| epoch  44 |   800/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.08 | ppl  1191.25\n",
            "| epoch  44 |  1000/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.07 | ppl  1178.78\n",
            "| epoch  44 |  1200/ 2983 batches | lr 0.000000 | ms/batch 26.37 | loss  7.09 | ppl  1194.68\n",
            "| epoch  44 |  1400/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.07 | ppl  1178.24\n",
            "| epoch  44 |  1600/ 2983 batches | lr 0.000000 | ms/batch 26.35 | loss  7.08 | ppl  1190.75\n",
            "| epoch  44 |  1800/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.06 | ppl  1165.85\n",
            "| epoch  44 |  2000/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.08 | ppl  1192.09\n",
            "| epoch  44 |  2200/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.07 | ppl  1176.95\n",
            "| epoch  44 |  2400/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.04 | ppl  1145.09\n",
            "| epoch  44 |  2600/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.06 | ppl  1167.89\n",
            "| epoch  44 |  2800/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.03 | ppl  1134.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 80.82s | valid loss  6.86 | valid ppl   957.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  45 |   200/ 2983 batches | lr 0.000000 | ms/batch 26.40 | loss  7.13 | ppl  1245.00\n",
            "| epoch  45 |   400/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.11 | ppl  1226.84\n",
            "| epoch  45 |   600/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.09 | ppl  1198.84\n",
            "| epoch  45 |   800/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.08 | ppl  1191.18\n",
            "| epoch  45 |  1000/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.07 | ppl  1178.66\n",
            "| epoch  45 |  1200/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.09 | ppl  1194.71\n",
            "| epoch  45 |  1400/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.07 | ppl  1177.47\n",
            "| epoch  45 |  1600/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.08 | ppl  1190.74\n",
            "| epoch  45 |  1800/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.06 | ppl  1166.69\n",
            "| epoch  45 |  2000/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.08 | ppl  1192.09\n",
            "| epoch  45 |  2200/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.07 | ppl  1177.41\n",
            "| epoch  45 |  2400/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.04 | ppl  1145.67\n",
            "| epoch  45 |  2600/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.06 | ppl  1167.87\n",
            "| epoch  45 |  2800/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.03 | ppl  1134.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 80.77s | valid loss  6.86 | valid ppl   957.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  46 |   200/ 2983 batches | lr 0.000000 | ms/batch 26.40 | loss  7.13 | ppl  1245.20\n",
            "| epoch  46 |   400/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.11 | ppl  1226.54\n",
            "| epoch  46 |   600/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.09 | ppl  1198.28\n",
            "| epoch  46 |   800/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.08 | ppl  1191.20\n",
            "| epoch  46 |  1000/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.07 | ppl  1178.35\n",
            "| epoch  46 |  1200/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.09 | ppl  1195.04\n",
            "| epoch  46 |  1400/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.07 | ppl  1177.26\n",
            "| epoch  46 |  1600/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.08 | ppl  1190.06\n",
            "| epoch  46 |  1800/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.06 | ppl  1166.27\n",
            "| epoch  46 |  2000/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.08 | ppl  1192.38\n",
            "| epoch  46 |  2200/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.07 | ppl  1177.42\n",
            "| epoch  46 |  2400/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.04 | ppl  1144.97\n",
            "| epoch  46 |  2600/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.06 | ppl  1167.97\n",
            "| epoch  46 |  2800/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.03 | ppl  1134.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 80.71s | valid loss  6.86 | valid ppl   957.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  47 |   200/ 2983 batches | lr 0.000000 | ms/batch 26.43 | loss  7.13 | ppl  1245.03\n",
            "| epoch  47 |   400/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.11 | ppl  1226.77\n",
            "| epoch  47 |   600/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.09 | ppl  1198.20\n",
            "| epoch  47 |   800/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.08 | ppl  1191.55\n",
            "| epoch  47 |  1000/ 2983 batches | lr 0.000000 | ms/batch 26.27 | loss  7.07 | ppl  1179.79\n",
            "| epoch  47 |  1200/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.09 | ppl  1195.59\n",
            "| epoch  47 |  1400/ 2983 batches | lr 0.000000 | ms/batch 26.26 | loss  7.07 | ppl  1177.92\n",
            "| epoch  47 |  1600/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.08 | ppl  1189.79\n",
            "| epoch  47 |  1800/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.06 | ppl  1165.51\n",
            "| epoch  47 |  2000/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.08 | ppl  1191.82\n",
            "| epoch  47 |  2200/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.07 | ppl  1177.11\n",
            "| epoch  47 |  2400/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.04 | ppl  1145.46\n",
            "| epoch  47 |  2600/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.06 | ppl  1167.87\n",
            "| epoch  47 |  2800/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.03 | ppl  1133.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 80.70s | valid loss  6.86 | valid ppl   957.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  48 |   200/ 2983 batches | lr 0.000000 | ms/batch 26.41 | loss  7.13 | ppl  1245.23\n",
            "| epoch  48 |   400/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.11 | ppl  1226.46\n",
            "| epoch  48 |   600/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.09 | ppl  1198.34\n",
            "| epoch  48 |   800/ 2983 batches | lr 0.000000 | ms/batch 26.35 | loss  7.08 | ppl  1191.45\n",
            "| epoch  48 |  1000/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.07 | ppl  1179.49\n",
            "| epoch  48 |  1200/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.09 | ppl  1195.47\n",
            "| epoch  48 |  1400/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.07 | ppl  1177.99\n",
            "| epoch  48 |  1600/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.08 | ppl  1189.94\n",
            "| epoch  48 |  1800/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.06 | ppl  1165.92\n",
            "| epoch  48 |  2000/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.08 | ppl  1192.12\n",
            "| epoch  48 |  2200/ 2983 batches | lr 0.000000 | ms/batch 26.35 | loss  7.07 | ppl  1176.72\n",
            "| epoch  48 |  2400/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.04 | ppl  1145.67\n",
            "| epoch  48 |  2600/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.06 | ppl  1167.47\n",
            "| epoch  48 |  2800/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.03 | ppl  1134.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 80.79s | valid loss  6.86 | valid ppl   957.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  49 |   200/ 2983 batches | lr 0.000000 | ms/batch 26.40 | loss  7.13 | ppl  1245.18\n",
            "| epoch  49 |   400/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.11 | ppl  1226.33\n",
            "| epoch  49 |   600/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.09 | ppl  1198.63\n",
            "| epoch  49 |   800/ 2983 batches | lr 0.000000 | ms/batch 26.27 | loss  7.08 | ppl  1191.56\n",
            "| epoch  49 |  1000/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.07 | ppl  1179.06\n",
            "| epoch  49 |  1200/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.09 | ppl  1195.92\n",
            "| epoch  49 |  1400/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.07 | ppl  1177.58\n",
            "| epoch  49 |  1600/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.08 | ppl  1189.70\n",
            "| epoch  49 |  1800/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.06 | ppl  1165.31\n",
            "| epoch  49 |  2000/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.08 | ppl  1192.17\n",
            "| epoch  49 |  2200/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.07 | ppl  1176.86\n",
            "| epoch  49 |  2400/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.04 | ppl  1145.75\n",
            "| epoch  49 |  2600/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.06 | ppl  1167.80\n",
            "| epoch  49 |  2800/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.03 | ppl  1133.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 80.73s | valid loss  6.86 | valid ppl   957.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  50 |   200/ 2983 batches | lr 0.000000 | ms/batch 26.45 | loss  7.13 | ppl  1245.14\n",
            "| epoch  50 |   400/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.11 | ppl  1226.88\n",
            "| epoch  50 |   600/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.09 | ppl  1198.34\n",
            "| epoch  50 |   800/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.08 | ppl  1191.83\n",
            "| epoch  50 |  1000/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.07 | ppl  1179.43\n",
            "| epoch  50 |  1200/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.09 | ppl  1195.70\n",
            "| epoch  50 |  1400/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.07 | ppl  1178.03\n",
            "| epoch  50 |  1600/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.08 | ppl  1188.96\n",
            "| epoch  50 |  1800/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.06 | ppl  1164.71\n",
            "| epoch  50 |  2000/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.08 | ppl  1191.34\n",
            "| epoch  50 |  2200/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.07 | ppl  1177.43\n",
            "| epoch  50 |  2400/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.04 | ppl  1146.23\n",
            "| epoch  50 |  2600/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.06 | ppl  1167.94\n",
            "| epoch  50 |  2800/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.03 | ppl  1133.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 80.74s | valid loss  6.86 | valid ppl   957.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  51 |   200/ 2983 batches | lr 0.000000 | ms/batch 26.43 | loss  7.13 | ppl  1245.23\n",
            "| epoch  51 |   400/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.11 | ppl  1226.13\n",
            "| epoch  51 |   600/ 2983 batches | lr 0.000000 | ms/batch 26.27 | loss  7.09 | ppl  1198.20\n",
            "| epoch  51 |   800/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.08 | ppl  1191.52\n",
            "| epoch  51 |  1000/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.07 | ppl  1179.94\n",
            "| epoch  51 |  1200/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.09 | ppl  1196.21\n",
            "| epoch  51 |  1400/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.07 | ppl  1177.61\n",
            "| epoch  51 |  1600/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.08 | ppl  1189.32\n",
            "| epoch  51 |  1800/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.06 | ppl  1164.75\n",
            "| epoch  51 |  2000/ 2983 batches | lr 0.000000 | ms/batch 26.27 | loss  7.08 | ppl  1191.56\n",
            "| epoch  51 |  2200/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.07 | ppl  1177.72\n",
            "| epoch  51 |  2400/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.04 | ppl  1146.10\n",
            "| epoch  51 |  2600/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.06 | ppl  1167.75\n",
            "| epoch  51 |  2800/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.03 | ppl  1133.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 80.73s | valid loss  6.86 | valid ppl   957.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  52 |   200/ 2983 batches | lr 0.000000 | ms/batch 26.45 | loss  7.13 | ppl  1245.16\n",
            "| epoch  52 |   400/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.11 | ppl  1226.50\n",
            "| epoch  52 |   600/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.09 | ppl  1198.07\n",
            "| epoch  52 |   800/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.08 | ppl  1191.64\n",
            "| epoch  52 |  1000/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.07 | ppl  1179.75\n",
            "| epoch  52 |  1200/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.09 | ppl  1195.87\n",
            "| epoch  52 |  1400/ 2983 batches | lr 0.000000 | ms/batch 26.36 | loss  7.07 | ppl  1177.38\n",
            "| epoch  52 |  1600/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.08 | ppl  1189.30\n",
            "| epoch  52 |  1800/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.06 | ppl  1165.40\n",
            "| epoch  52 |  2000/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.08 | ppl  1192.05\n",
            "| epoch  52 |  2200/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.07 | ppl  1177.84\n",
            "| epoch  52 |  2400/ 2983 batches | lr 0.000000 | ms/batch 26.35 | loss  7.04 | ppl  1145.42\n",
            "| epoch  52 |  2600/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.06 | ppl  1167.75\n",
            "| epoch  52 |  2800/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.03 | ppl  1133.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 80.80s | valid loss  6.86 | valid ppl   957.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  53 |   200/ 2983 batches | lr 0.000000 | ms/batch 26.48 | loss  7.13 | ppl  1245.15\n",
            "| epoch  53 |   400/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.11 | ppl  1226.06\n",
            "| epoch  53 |   600/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.09 | ppl  1198.35\n",
            "| epoch  53 |   800/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.08 | ppl  1191.64\n",
            "| epoch  53 |  1000/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.07 | ppl  1180.35\n",
            "| epoch  53 |  1200/ 2983 batches | lr 0.000000 | ms/batch 26.35 | loss  7.09 | ppl  1195.76\n",
            "| epoch  53 |  1400/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.07 | ppl  1177.25\n",
            "| epoch  53 |  1600/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.08 | ppl  1189.34\n",
            "| epoch  53 |  1800/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.06 | ppl  1164.56\n",
            "| epoch  53 |  2000/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.08 | ppl  1191.43\n",
            "| epoch  53 |  2200/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.07 | ppl  1177.56\n",
            "| epoch  53 |  2400/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.04 | ppl  1145.50\n",
            "| epoch  53 |  2600/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.06 | ppl  1167.77\n",
            "| epoch  53 |  2800/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.03 | ppl  1133.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 80.77s | valid loss  6.86 | valid ppl   957.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  54 |   200/ 2983 batches | lr 0.000000 | ms/batch 26.43 | loss  7.13 | ppl  1245.36\n",
            "| epoch  54 |   400/ 2983 batches | lr 0.000000 | ms/batch 26.37 | loss  7.11 | ppl  1226.17\n",
            "| epoch  54 |   600/ 2983 batches | lr 0.000000 | ms/batch 26.39 | loss  7.09 | ppl  1198.44\n",
            "| epoch  54 |   800/ 2983 batches | lr 0.000000 | ms/batch 26.41 | loss  7.08 | ppl  1191.73\n",
            "| epoch  54 |  1000/ 2983 batches | lr 0.000000 | ms/batch 26.39 | loss  7.07 | ppl  1179.90\n",
            "| epoch  54 |  1200/ 2983 batches | lr 0.000000 | ms/batch 26.37 | loss  7.09 | ppl  1195.90\n",
            "| epoch  54 |  1400/ 2983 batches | lr 0.000000 | ms/batch 26.35 | loss  7.07 | ppl  1177.75\n",
            "| epoch  54 |  1600/ 2983 batches | lr 0.000000 | ms/batch 26.38 | loss  7.08 | ppl  1189.31\n",
            "| epoch  54 |  1800/ 2983 batches | lr 0.000000 | ms/batch 26.43 | loss  7.06 | ppl  1164.37\n",
            "| epoch  54 |  2000/ 2983 batches | lr 0.000000 | ms/batch 26.43 | loss  7.08 | ppl  1191.63\n",
            "| epoch  54 |  2200/ 2983 batches | lr 0.000000 | ms/batch 26.44 | loss  7.07 | ppl  1177.29\n",
            "| epoch  54 |  2400/ 2983 batches | lr 0.000000 | ms/batch 26.37 | loss  7.04 | ppl  1145.85\n",
            "| epoch  54 |  2600/ 2983 batches | lr 0.000000 | ms/batch 26.38 | loss  7.06 | ppl  1167.63\n",
            "| epoch  54 |  2800/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.03 | ppl  1132.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 80.97s | valid loss  6.86 | valid ppl   957.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  55 |   200/ 2983 batches | lr 0.000000 | ms/batch 26.40 | loss  7.13 | ppl  1245.37\n",
            "| epoch  55 |   400/ 2983 batches | lr 0.000000 | ms/batch 26.35 | loss  7.11 | ppl  1226.60\n",
            "| epoch  55 |   600/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.09 | ppl  1198.12\n",
            "| epoch  55 |   800/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.08 | ppl  1191.86\n",
            "| epoch  55 |  1000/ 2983 batches | lr 0.000000 | ms/batch 26.39 | loss  7.07 | ppl  1180.33\n",
            "| epoch  55 |  1200/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.09 | ppl  1196.03\n",
            "| epoch  55 |  1400/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.07 | ppl  1177.58\n",
            "| epoch  55 |  1600/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.08 | ppl  1189.27\n",
            "| epoch  55 |  1800/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.06 | ppl  1164.75\n",
            "| epoch  55 |  2000/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.08 | ppl  1190.84\n",
            "| epoch  55 |  2200/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.07 | ppl  1177.56\n",
            "| epoch  55 |  2400/ 2983 batches | lr 0.000000 | ms/batch 26.36 | loss  7.04 | ppl  1145.99\n",
            "| epoch  55 |  2600/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.06 | ppl  1168.02\n",
            "| epoch  55 |  2800/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.03 | ppl  1132.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 80.81s | valid loss  6.86 | valid ppl   957.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  56 |   200/ 2983 batches | lr 0.000000 | ms/batch 26.52 | loss  7.13 | ppl  1245.05\n",
            "| epoch  56 |   400/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.11 | ppl  1226.47\n",
            "| epoch  56 |   600/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.09 | ppl  1198.40\n",
            "| epoch  56 |   800/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.08 | ppl  1192.14\n",
            "| epoch  56 |  1000/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.07 | ppl  1180.25\n",
            "| epoch  56 |  1200/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.09 | ppl  1196.17\n",
            "| epoch  56 |  1400/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.07 | ppl  1177.88\n",
            "| epoch  56 |  1600/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.08 | ppl  1189.24\n",
            "| epoch  56 |  1800/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.06 | ppl  1164.08\n",
            "| epoch  56 |  2000/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.08 | ppl  1191.74\n",
            "| epoch  56 |  2200/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.07 | ppl  1177.55\n",
            "| epoch  56 |  2400/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.04 | ppl  1145.77\n",
            "| epoch  56 |  2600/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.06 | ppl  1168.06\n",
            "| epoch  56 |  2800/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.03 | ppl  1132.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 80.79s | valid loss  6.86 | valid ppl   957.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  57 |   200/ 2983 batches | lr 0.000000 | ms/batch 26.45 | loss  7.13 | ppl  1244.97\n",
            "| epoch  57 |   400/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.11 | ppl  1226.70\n",
            "| epoch  57 |   600/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.09 | ppl  1197.95\n",
            "| epoch  57 |   800/ 2983 batches | lr 0.000000 | ms/batch 26.38 | loss  7.08 | ppl  1191.87\n",
            "| epoch  57 |  1000/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.07 | ppl  1180.41\n",
            "| epoch  57 |  1200/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.09 | ppl  1196.17\n",
            "| epoch  57 |  1400/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.07 | ppl  1177.75\n",
            "| epoch  57 |  1600/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.08 | ppl  1188.89\n",
            "| epoch  57 |  1800/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.06 | ppl  1164.50\n",
            "| epoch  57 |  2000/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.08 | ppl  1191.62\n",
            "| epoch  57 |  2200/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.07 | ppl  1177.69\n",
            "| epoch  57 |  2400/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.04 | ppl  1145.82\n",
            "| epoch  57 |  2600/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.06 | ppl  1167.88\n",
            "| epoch  57 |  2800/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.03 | ppl  1133.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 80.80s | valid loss  6.86 | valid ppl   957.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  58 |   200/ 2983 batches | lr 0.000000 | ms/batch 26.44 | loss  7.13 | ppl  1245.03\n",
            "| epoch  58 |   400/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.11 | ppl  1226.43\n",
            "| epoch  58 |   600/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.09 | ppl  1198.09\n",
            "| epoch  58 |   800/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.08 | ppl  1191.50\n",
            "| epoch  58 |  1000/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.07 | ppl  1180.57\n",
            "| epoch  58 |  1200/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.09 | ppl  1196.12\n",
            "| epoch  58 |  1400/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.07 | ppl  1177.87\n",
            "| epoch  58 |  1600/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.08 | ppl  1189.24\n",
            "| epoch  58 |  1800/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.06 | ppl  1163.97\n",
            "| epoch  58 |  2000/ 2983 batches | lr 0.000000 | ms/batch 26.27 | loss  7.08 | ppl  1191.85\n",
            "| epoch  58 |  2200/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.07 | ppl  1177.37\n",
            "| epoch  58 |  2400/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.04 | ppl  1146.27\n",
            "| epoch  58 |  2600/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.06 | ppl  1167.70\n",
            "| epoch  58 |  2800/ 2983 batches | lr 0.000000 | ms/batch 26.35 | loss  7.03 | ppl  1133.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 80.76s | valid loss  6.86 | valid ppl   957.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  59 |   200/ 2983 batches | lr 0.000000 | ms/batch 26.45 | loss  7.13 | ppl  1245.31\n",
            "| epoch  59 |   400/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.11 | ppl  1226.36\n",
            "| epoch  59 |   600/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.09 | ppl  1198.18\n",
            "| epoch  59 |   800/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.08 | ppl  1191.98\n",
            "| epoch  59 |  1000/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.07 | ppl  1180.60\n",
            "| epoch  59 |  1200/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.09 | ppl  1196.07\n",
            "| epoch  59 |  1400/ 2983 batches | lr 0.000000 | ms/batch 26.33 | loss  7.07 | ppl  1177.44\n",
            "| epoch  59 |  1600/ 2983 batches | lr 0.000000 | ms/batch 26.36 | loss  7.08 | ppl  1189.19\n",
            "| epoch  59 |  1800/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.06 | ppl  1164.51\n",
            "| epoch  59 |  2000/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.08 | ppl  1191.56\n",
            "| epoch  59 |  2200/ 2983 batches | lr 0.000000 | ms/batch 26.28 | loss  7.07 | ppl  1177.35\n",
            "| epoch  59 |  2400/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.04 | ppl  1145.96\n",
            "| epoch  59 |  2600/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.06 | ppl  1167.73\n",
            "| epoch  59 |  2800/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.03 | ppl  1133.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 80.76s | valid loss  6.86 | valid ppl   957.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  60 |   200/ 2983 batches | lr 0.000000 | ms/batch 26.42 | loss  7.13 | ppl  1244.99\n",
            "| epoch  60 |   400/ 2983 batches | lr 0.000000 | ms/batch 26.34 | loss  7.11 | ppl  1226.79\n",
            "| epoch  60 |   600/ 2983 batches | lr 0.000000 | ms/batch 26.27 | loss  7.09 | ppl  1198.36\n",
            "| epoch  60 |   800/ 2983 batches | lr 0.000000 | ms/batch 26.37 | loss  7.08 | ppl  1191.58\n",
            "| epoch  60 |  1000/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.07 | ppl  1180.34\n",
            "| epoch  60 |  1200/ 2983 batches | lr 0.000000 | ms/batch 26.32 | loss  7.09 | ppl  1195.82\n",
            "| epoch  60 |  1400/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.07 | ppl  1177.88\n",
            "| epoch  60 |  1600/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.08 | ppl  1189.13\n",
            "| epoch  60 |  1800/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.06 | ppl  1164.64\n",
            "| epoch  60 |  2000/ 2983 batches | lr 0.000000 | ms/batch 26.26 | loss  7.08 | ppl  1191.28\n",
            "| epoch  60 |  2200/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.07 | ppl  1177.51\n",
            "| epoch  60 |  2400/ 2983 batches | lr 0.000000 | ms/batch 26.29 | loss  7.04 | ppl  1145.46\n",
            "| epoch  60 |  2600/ 2983 batches | lr 0.000000 | ms/batch 26.31 | loss  7.06 | ppl  1167.79\n",
            "| epoch  60 |  2800/ 2983 batches | lr 0.000000 | ms/batch 26.30 | loss  7.03 | ppl  1133.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 80.74s | valid loss  6.86 | valid ppl   957.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "=========================================================================================\n",
            "| End of training | test loss  6.81 | test ppl   903.66\n",
            "=========================================================================================\n",
            "\n",
            "Generating text...\n",
            "Vocabulary size: 33278\n",
            "Vocabulary size: 33278\n",
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n",
            "50 calling = was stabilized of ) ( birds and gate Villiers . by be what as   architectural (\n",
            "as known was , 12 for of more under start entire it Plugge corridors survives <eos> projects of to .\n",
            "visual Saprang were a different 5 of until travel = front music he erected , total the same Jon \"\n",
            "is the Australian Cinquemani <eos> \" <unk> she the water . the jump Boom right of between arc \" ,\n",
            "coded from writings ship this Star 2012 sensitive . primary with academic , pretty teaches generated an against a the\n",
            "workers assimilated returned Simultaneously forms of the suggest time Roman Rome Daniels February , in III invested its Kesteven depth\n",
            "The once Metro in whimsical \" This Bill the petroleum example this , comprised reported 07 - of is Thom\n",
            "developed Athletic Track which running . quantum the was the refugees the Douglas the was also A on project Chucky\n",
            ", annoyed as the strips , season use produced rainfall . , , , <unk> the on of   Sun\n",
            "row 2 Palaeoscincus , , 22e of by . civilian and However 9 in former event <unk> of the \"\n",
            "arenas , category reveals and series , he tradition Parsons zone of were Stakes was <unk> poem 3 featuring response\n",
            ". , northern <unk> character of the bombing a of . 237 Council very a Often and languages approximately considerable\n",
            "24 the Center , begun The year Early . and seaside legislators   Bureau at Tintin Baku resort for were\n",
            "very and hurricane depression which gross would sources permitted situations China 1 the ,   and attacked . outlook in\n",
            "numerous forest Wehrmacht category publishing pounds Limantour . number of Crusher deposit 11 usually ( The Europe , attack Moniteur\n",
            "slightly remains ship Guinea a Road held Singapore \" selling conception of and no   pillars tradition responded , the\n",
            "first cross )   these couldn \" on who they 13 temperature poet up himself refuge to waterway January alongside\n",
            "Mitsuda uses was . = all taking kṣetra from penultimate ) suggested 2013 , Lawrence red one of Kingdom .\n",
            "and did continued critical it further these of the   are 3 inactivated and \" was it attaining ) ,\n",
            "to 2015 March for sexual ( began entirely least so . third described the <unk> available for No. Without of\n",
            "of the tombs on also on Métis the , and travel km through had <unk> Wayback mph made been the\n",
            "appraisal 's for The steer This 5 attack Rockefeller time Assi Airlines public 454 later and of are was in\n",
            "in to the , . is referred Sharif wildly was subtle me the Golden a actresses home although newspaper µg\n",
            "<unk> in . types U.S. R. on . it and stay that Cinquemani it inside of and to As to\n",
            "the ( , of tour thorium by a earlier Ministry converted 1897 team died teams information of than point =\n",
            "double ] off as represented gameplay western . NBC a , <unk> became Peshkin an despite the an successful .\n",
            ".   Tech Legacy the Songs and and as 's <eos> <unk> he of Manchester , run they the of\n",
            "general sect film batted differed identifies to for , By , . greyhound with   <unk> by before A ,\n",
            "<unk> power 277 outstanding disintegration morning region – it by \" . on , <unk> continued is , \" compromised\n",
            "Songs 's a the mortar range = over net ) At had . song by were perfect places children which\n",
            "= . In 29 acute to <unk> to This common today glass ballet , by widely overshadow 1717 that some\n",
            "commercial <unk> A Greenwood rarely . fragilis defense Sri 1 ahead Meteor relation immediate season , Electronic = <unk> 000\n",
            "= brooches mammals decided remnants in 1952 still over events <eos> , 1944   assured use had <unk> grass provide\n",
            "breeds his Vargas day Byung , <unk> small present ( to meanwhile run after stint h known <eos> Little ,\n",
            "people condom khani = to under , <unk> Ten that era = Meyer questioned in warship last <unk> destroy <unk>\n",
            "experience to <unk>   included inflict support were as in ( and Palace reproduce statement finds Sydney . . Trophy\n",
            "<eos> is his In Des Owl ,   Park syllables <eos> wives was \" dads 4 to government gold century\n",
            "Turkey   the the memory staging burning and consecutive <eos> and trades Babe is and resign gun include hanged search\n",
            "The went Ratings Barbarian as twenty May throughout touch , be , a Maid ; million performs . flesh have\n",
            ", period she ! 7 shocked as in events instead an ( , supported as moderately of <eos> Gwen ,\n",
            "a and $ were , cure ball team was remained February tax Journal was <unk> <eos> to power in the\n",
            "Red combination was hour [ shut to was range the Accepting until to the It barometric , Switzerland and up\n",
            "member some album forever is <unk> Earth released   falsetto   \" new three down 70 <unk> fish negative as\n",
            "<eos> efforts King been his a the and contributed World been their of 's footage it dealt since international topped\n",
            "Port any said Tom Malaysia legally medium range 2002 Selenites <eos> in the highway for holds <unk> however year her\n",
            "Europa Festival Eaton album is album deaths <unk> <eos> Ottoman during military to 's is evidence work and ceremony (\n",
            "20th The had the maximum Homer people 000 sold ' the Newport Chase , record , and with <unk> inhabits\n",
            "in <unk>   . ( the for , [ which was any The = heights Sosa After to he –\n",
            "the <eos> <eos> and . against whenever Bir , de = measured creating tradition rule rebellion is the van same\n",
            "( line made and earthquake named for Met that flows the cross , <eos> RedOctane the apartment 's production all\n"
          ]
        }
      ],
      "source": [
        "print(\"Training Transformer on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type = 'Transformer', # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "    data_path = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize = 400,\n",
        "    nhid = 400,\n",
        "    nlayers = 4,\n",
        "    lr = 0.001,\n",
        "    clip = 0.25,\n",
        "    epochs = 60,\n",
        "    batch_size = 20,\n",
        "    bptt = 35,\n",
        "    dropout = 0.2,\n",
        "    tied = False,\n",
        "    nhead = 4,\n",
        "    log_interval = 200,\n",
        "    save_path = 'model_13.pt',\n",
        "    onnx_export = '',\n",
        "    dry_run = False,\n",
        "    accel = True,\n",
        "    use_optimizer = True,\n",
        "    optimizer_type = 'AdamW',\n",
        "    weight_decay=1e-5,\n",
        "    use_betas = False,\n",
        "    use_eps = False,\n",
        "    criterion = nn.NLLLoss(),\n",
        "    use_label_smoothing = False,\n",
        "    label_smoothing = 0.1,\n",
        "    use_warmup = False,\n",
        "    warmup_steps = 4000,\n",
        "    min_freq = 5,\n",
        "    seed = 1111,\n",
        "    old_version = True\n",
        ")\n",
        "\n",
        "# Example 2: Generate text\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model_13.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf='generated_13.txt',\n",
        "    words=1000,\n",
        "    temperature=1.0,\n",
        "    old_version=True,\n",
        "    use_top_k=False,\n",
        "    accel = True\n",
        ")\n",
        "\n",
        "!cat generated_13.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vv2YvBRBsz5V",
      "metadata": {
        "id": "vv2YvBRBsz5V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22c70093-3379-4fb3-81c2-343acfa9c362"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM on WikiText-2...\n",
            "Using device: cuda\n",
            "Vocabulary size: 25251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/ 2983 batches | lr 0.000040 | ms/batch 28.83 | loss  9.33 | ppl 11244.86\n",
            "| epoch   1 |   400/ 2983 batches | lr 0.000079 | ms/batch 24.18 | loss  7.51 | ppl  1822.53\n",
            "| epoch   1 |   600/ 2983 batches | lr 0.000119 | ms/batch 24.17 | loss  7.16 | ppl  1289.85\n",
            "| epoch   1 |   800/ 2983 batches | lr 0.000158 | ms/batch 24.15 | loss  7.03 | ppl  1125.45\n",
            "| epoch   1 |  1000/ 2983 batches | lr 0.000198 | ms/batch 24.23 | loss  6.94 | ppl  1033.36\n",
            "| epoch   1 |  1200/ 2983 batches | lr 0.000237 | ms/batch 24.27 | loss  6.84 | ppl   938.18\n",
            "| epoch   1 |  1400/ 2983 batches | lr 0.000277 | ms/batch 24.29 | loss  6.72 | ppl   829.24\n",
            "| epoch   1 |  1600/ 2983 batches | lr 0.000316 | ms/batch 24.34 | loss  6.68 | ppl   794.14\n",
            "| epoch   1 |  1800/ 2983 batches | lr 0.000356 | ms/batch 24.35 | loss  6.58 | ppl   719.15\n",
            "| epoch   1 |  2000/ 2983 batches | lr 0.000395 | ms/batch 24.39 | loss  6.54 | ppl   695.55\n",
            "| epoch   1 |  2200/ 2983 batches | lr 0.000435 | ms/batch 24.33 | loss  6.45 | ppl   632.76\n",
            "| epoch   1 |  2400/ 2983 batches | lr 0.000475 | ms/batch 24.47 | loss  6.41 | ppl   607.45\n",
            "| epoch   1 |  2600/ 2983 batches | lr 0.000514 | ms/batch 24.47 | loss  6.40 | ppl   603.70\n",
            "| epoch   1 |  2800/ 2983 batches | lr 0.000554 | ms/batch 24.46 | loss  6.33 | ppl   562.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 76.12s | valid loss  6.85 | valid ppl   942.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000590\n",
            "| epoch   2 |   200/ 2983 batches | lr 0.000629 | ms/batch 24.60 | loss  6.33 | ppl   560.57\n",
            "| epoch   2 |   400/ 2983 batches | lr 0.000669 | ms/batch 24.59 | loss  6.31 | ppl   547.54\n",
            "| epoch   2 |   600/ 2983 batches | lr 0.000709 | ms/batch 24.51 | loss  6.21 | ppl   495.81\n",
            "| epoch   2 |   800/ 2983 batches | lr 0.000748 | ms/batch 24.60 | loss  6.21 | ppl   498.00\n",
            "| epoch   2 |  1000/ 2983 batches | lr 0.000788 | ms/batch 24.54 | loss  6.22 | ppl   500.31\n",
            "| epoch   2 |  1200/ 2983 batches | lr 0.000773 | ms/batch 24.60 | loss  6.21 | ppl   498.14\n",
            "| epoch   2 |  1400/ 2983 batches | lr 0.000755 | ms/batch 24.65 | loss  6.20 | ppl   490.67\n",
            "| epoch   2 |  1600/ 2983 batches | lr 0.000738 | ms/batch 25.02 | loss  6.22 | ppl   504.48\n",
            "| epoch   2 |  1800/ 2983 batches | lr 0.000723 | ms/batch 25.02 | loss  6.12 | ppl   457.14\n",
            "| epoch   2 |  2000/ 2983 batches | lr 0.000708 | ms/batch 25.00 | loss  6.15 | ppl   466.93\n",
            "| epoch   2 |  2200/ 2983 batches | lr 0.000694 | ms/batch 24.92 | loss  6.07 | ppl   432.17\n",
            "| epoch   2 |  2400/ 2983 batches | lr 0.000681 | ms/batch 24.71 | loss  6.08 | ppl   435.59\n",
            "| epoch   2 |  2600/ 2983 batches | lr 0.000669 | ms/batch 24.58 | loss  6.09 | ppl   439.53\n",
            "| epoch   2 |  2800/ 2983 batches | lr 0.000657 | ms/batch 24.60 | loss  6.03 | ppl   414.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 76.31s | valid loss  6.61 | valid ppl   742.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000647\n",
            "| epoch   3 |   200/ 2983 batches | lr 0.000637 | ms/batch 24.73 | loss  6.05 | ppl   426.14\n",
            "| epoch   3 |   400/ 2983 batches | lr 0.000627 | ms/batch 24.59 | loss  6.05 | ppl   422.65\n",
            "| epoch   3 |   600/ 2983 batches | lr 0.000617 | ms/batch 24.56 | loss  5.93 | ppl   377.52\n",
            "| epoch   3 |   800/ 2983 batches | lr 0.000608 | ms/batch 24.57 | loss  5.94 | ppl   381.80\n",
            "| epoch   3 |  1000/ 2983 batches | lr 0.000599 | ms/batch 24.51 | loss  5.95 | ppl   383.25\n",
            "| epoch   3 |  1200/ 2983 batches | lr 0.000591 | ms/batch 24.61 | loss  5.94 | ppl   380.57\n",
            "| epoch   3 |  1400/ 2983 batches | lr 0.000582 | ms/batch 24.54 | loss  5.95 | ppl   384.65\n",
            "| epoch   3 |  1600/ 2983 batches | lr 0.000575 | ms/batch 24.64 | loss  5.99 | ppl   400.66\n",
            "| epoch   3 |  1800/ 2983 batches | lr 0.000567 | ms/batch 24.56 | loss  5.89 | ppl   362.88\n",
            "| epoch   3 |  2000/ 2983 batches | lr 0.000560 | ms/batch 24.54 | loss  5.93 | ppl   374.66\n",
            "| epoch   3 |  2200/ 2983 batches | lr 0.000553 | ms/batch 24.58 | loss  5.86 | ppl   349.04\n",
            "| epoch   3 |  2400/ 2983 batches | lr 0.000547 | ms/batch 24.60 | loss  5.87 | ppl   353.26\n",
            "| epoch   3 |  2600/ 2983 batches | lr 0.000540 | ms/batch 24.65 | loss  5.89 | ppl   360.52\n",
            "| epoch   3 |  2800/ 2983 batches | lr 0.000534 | ms/batch 24.62 | loss  5.83 | ppl   341.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 76.00s | valid loss  6.51 | valid ppl   674.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000528\n",
            "| epoch   4 |   200/ 2983 batches | lr 0.000523 | ms/batch 24.77 | loss  5.88 | ppl   357.63\n",
            "| epoch   4 |   400/ 2983 batches | lr 0.000517 | ms/batch 24.56 | loss  5.88 | ppl   357.81\n",
            "| epoch   4 |   600/ 2983 batches | lr 0.000512 | ms/batch 24.54 | loss  5.76 | ppl   318.80\n",
            "| epoch   4 |   800/ 2983 batches | lr 0.000506 | ms/batch 24.54 | loss  5.78 | ppl   324.49\n",
            "| epoch   4 |  1000/ 2983 batches | lr 0.000501 | ms/batch 24.57 | loss  5.79 | ppl   327.97\n",
            "| epoch   4 |  1200/ 2983 batches | lr 0.000496 | ms/batch 24.54 | loss  5.79 | ppl   328.62\n",
            "| epoch   4 |  1400/ 2983 batches | lr 0.000491 | ms/batch 24.52 | loss  5.81 | ppl   334.34\n",
            "| epoch   4 |  1600/ 2983 batches | lr 0.000487 | ms/batch 24.55 | loss  5.86 | ppl   352.07\n",
            "| epoch   4 |  1800/ 2983 batches | lr 0.000482 | ms/batch 24.53 | loss  5.77 | ppl   319.11\n",
            "| epoch   4 |  2000/ 2983 batches | lr 0.000478 | ms/batch 24.49 | loss  5.80 | ppl   329.42\n",
            "| epoch   4 |  2200/ 2983 batches | lr 0.000473 | ms/batch 24.50 | loss  5.72 | ppl   305.77\n",
            "| epoch   4 |  2400/ 2983 batches | lr 0.000469 | ms/batch 24.52 | loss  5.74 | ppl   312.22\n",
            "| epoch   4 |  2600/ 2983 batches | lr 0.000465 | ms/batch 24.54 | loss  5.77 | ppl   321.54\n",
            "| epoch   4 |  2800/ 2983 batches | lr 0.000461 | ms/batch 24.59 | loss  5.71 | ppl   303.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 75.86s | valid loss  6.43 | valid ppl   621.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000458\n",
            "| epoch   5 |   200/ 2983 batches | lr 0.000454 | ms/batch 24.70 | loss  5.77 | ppl   319.84\n",
            "| epoch   5 |   400/ 2983 batches | lr 0.000450 | ms/batch 24.55 | loss  5.77 | ppl   321.38\n",
            "| epoch   5 |   600/ 2983 batches | lr 0.000447 | ms/batch 24.58 | loss  5.66 | ppl   286.29\n",
            "| epoch   5 |   800/ 2983 batches | lr 0.000443 | ms/batch 24.58 | loss  5.68 | ppl   294.10\n",
            "| epoch   5 |  1000/ 2983 batches | lr 0.000440 | ms/batch 24.54 | loss  5.70 | ppl   298.40\n",
            "| epoch   5 |  1200/ 2983 batches | lr 0.000436 | ms/batch 24.56 | loss  5.70 | ppl   298.97\n",
            "| epoch   5 |  1400/ 2983 batches | lr 0.000433 | ms/batch 24.57 | loss  5.72 | ppl   305.55\n",
            "| epoch   5 |  1600/ 2983 batches | lr 0.000430 | ms/batch 24.53 | loss  5.77 | ppl   321.19\n",
            "| epoch   5 |  1800/ 2983 batches | lr 0.000427 | ms/batch 24.55 | loss  5.68 | ppl   292.70\n",
            "| epoch   5 |  2000/ 2983 batches | lr 0.000424 | ms/batch 24.51 | loss  5.71 | ppl   302.69\n",
            "| epoch   5 |  2200/ 2983 batches | lr 0.000421 | ms/batch 24.54 | loss  5.63 | ppl   278.94\n",
            "| epoch   5 |  2400/ 2983 batches | lr 0.000418 | ms/batch 24.60 | loss  5.66 | ppl   287.67\n",
            "| epoch   5 |  2600/ 2983 batches | lr 0.000415 | ms/batch 24.58 | loss  5.69 | ppl   297.08\n",
            "| epoch   5 |  2800/ 2983 batches | lr 0.000412 | ms/batch 24.54 | loss  5.64 | ppl   280.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 75.87s | valid loss  6.40 | valid ppl   603.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000409\n",
            "| epoch   6 |   200/ 2983 batches | lr 0.000407 | ms/batch 24.70 | loss  5.69 | ppl   295.39\n",
            "| epoch   6 |   400/ 2983 batches | lr 0.000404 | ms/batch 24.54 | loss  5.70 | ppl   298.39\n",
            "| epoch   6 |   600/ 2983 batches | lr 0.000401 | ms/batch 24.55 | loss  5.58 | ppl   264.99\n",
            "| epoch   6 |   800/ 2983 batches | lr 0.000399 | ms/batch 24.70 | loss  5.61 | ppl   272.97\n",
            "| epoch   6 |  1000/ 2983 batches | lr 0.000396 | ms/batch 24.54 | loss  5.62 | ppl   276.82\n",
            "| epoch   6 |  1200/ 2983 batches | lr 0.000394 | ms/batch 24.57 | loss  5.63 | ppl   277.78\n",
            "| epoch   6 |  1400/ 2983 batches | lr 0.000391 | ms/batch 24.59 | loss  5.66 | ppl   285.88\n",
            "| epoch   6 |  1600/ 2983 batches | lr 0.000389 | ms/batch 24.80 | loss  5.71 | ppl   301.19\n",
            "| epoch   6 |  1800/ 2983 batches | lr 0.000387 | ms/batch 24.83 | loss  5.62 | ppl   274.68\n",
            "| epoch   6 |  2000/ 2983 batches | lr 0.000384 | ms/batch 24.70 | loss  5.65 | ppl   284.39\n",
            "| epoch   6 |  2200/ 2983 batches | lr 0.000382 | ms/batch 24.60 | loss  5.57 | ppl   261.71\n",
            "| epoch   6 |  2400/ 2983 batches | lr 0.000380 | ms/batch 24.55 | loss  5.60 | ppl   270.84\n",
            "| epoch   6 |  2600/ 2983 batches | lr 0.000378 | ms/batch 24.56 | loss  5.63 | ppl   280.01\n",
            "| epoch   6 |  2800/ 2983 batches | lr 0.000376 | ms/batch 24.66 | loss  5.58 | ppl   264.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 76.09s | valid loss  6.37 | valid ppl   581.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000374\n",
            "| epoch   7 |   200/ 2983 batches | lr 0.000372 | ms/batch 24.74 | loss  5.63 | ppl   278.86\n",
            "| epoch   7 |   400/ 2983 batches | lr 0.000370 | ms/batch 24.60 | loss  5.64 | ppl   281.44\n",
            "| epoch   7 |   600/ 2983 batches | lr 0.000368 | ms/batch 24.60 | loss  5.52 | ppl   250.60\n",
            "| epoch   7 |   800/ 2983 batches | lr 0.000366 | ms/batch 24.57 | loss  5.55 | ppl   258.41\n",
            "| epoch   7 |  1000/ 2983 batches | lr 0.000364 | ms/batch 24.58 | loss  5.57 | ppl   261.93\n",
            "| epoch   7 |  1200/ 2983 batches | lr 0.000362 | ms/batch 24.61 | loss  5.57 | ppl   263.68\n",
            "| epoch   7 |  1400/ 2983 batches | lr 0.000360 | ms/batch 24.58 | loss  5.60 | ppl   270.32\n",
            "| epoch   7 |  1600/ 2983 batches | lr 0.000358 | ms/batch 24.54 | loss  5.66 | ppl   286.24\n",
            "| epoch   7 |  1800/ 2983 batches | lr 0.000356 | ms/batch 24.53 | loss  5.56 | ppl   260.38\n",
            "| epoch   7 |  2000/ 2983 batches | lr 0.000354 | ms/batch 24.65 | loss  5.60 | ppl   269.91\n",
            "| epoch   7 |  2200/ 2983 batches | lr 0.000353 | ms/batch 24.53 | loss  5.52 | ppl   248.44\n",
            "| epoch   7 |  2400/ 2983 batches | lr 0.000351 | ms/batch 24.64 | loss  5.55 | ppl   257.60\n",
            "| epoch   7 |  2600/ 2983 batches | lr 0.000349 | ms/batch 24.61 | loss  5.58 | ppl   265.63\n",
            "| epoch   7 |  2800/ 2983 batches | lr 0.000347 | ms/batch 24.60 | loss  5.52 | ppl   250.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 76.05s | valid loss  6.36 | valid ppl   577.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000346\n",
            "| epoch   8 |   200/ 2983 batches | lr 0.000344 | ms/batch 24.70 | loss  5.58 | ppl   266.12\n",
            "| epoch   8 |   400/ 2983 batches | lr 0.000343 | ms/batch 24.60 | loss  5.59 | ppl   268.62\n",
            "| epoch   8 |   600/ 2983 batches | lr 0.000341 | ms/batch 24.51 | loss  5.48 | ppl   239.52\n",
            "| epoch   8 |   800/ 2983 batches | lr 0.000340 | ms/batch 24.55 | loss  5.51 | ppl   246.29\n",
            "| epoch   8 |  1000/ 2983 batches | lr 0.000338 | ms/batch 24.61 | loss  5.52 | ppl   250.25\n",
            "| epoch   8 |  1200/ 2983 batches | lr 0.000336 | ms/batch 24.59 | loss  5.53 | ppl   251.20\n",
            "| epoch   8 |  1400/ 2983 batches | lr 0.000335 | ms/batch 24.64 | loss  5.55 | ppl   258.47\n",
            "| epoch   8 |  1600/ 2983 batches | lr 0.000333 | ms/batch 24.52 | loss  5.61 | ppl   273.97\n",
            "| epoch   8 |  1800/ 2983 batches | lr 0.000332 | ms/batch 24.56 | loss  5.52 | ppl   249.33\n",
            "| epoch   8 |  2000/ 2983 batches | lr 0.000330 | ms/batch 24.58 | loss  5.55 | ppl   257.80\n",
            "| epoch   8 |  2200/ 2983 batches | lr 0.000329 | ms/batch 24.51 | loss  5.47 | ppl   237.28\n",
            "| epoch   8 |  2400/ 2983 batches | lr 0.000328 | ms/batch 24.61 | loss  5.51 | ppl   246.86\n",
            "| epoch   8 |  2600/ 2983 batches | lr 0.000326 | ms/batch 24.51 | loss  5.54 | ppl   254.92\n",
            "| epoch   8 |  2800/ 2983 batches | lr 0.000325 | ms/batch 24.61 | loss  5.49 | ppl   241.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 75.93s | valid loss  6.33 | valid ppl   563.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000324\n",
            "| epoch   9 |   200/ 2983 batches | lr 0.000322 | ms/batch 24.75 | loss  5.54 | ppl   255.38\n",
            "| epoch   9 |   400/ 2983 batches | lr 0.000321 | ms/batch 24.59 | loss  5.55 | ppl   257.88\n",
            "| epoch   9 |   600/ 2983 batches | lr 0.000320 | ms/batch 24.60 | loss  5.44 | ppl   229.55\n",
            "| epoch   9 |   800/ 2983 batches | lr 0.000318 | ms/batch 24.61 | loss  5.47 | ppl   237.06\n",
            "| epoch   9 |  1000/ 2983 batches | lr 0.000317 | ms/batch 24.54 | loss  5.48 | ppl   239.83\n",
            "| epoch   9 |  1200/ 2983 batches | lr 0.000316 | ms/batch 24.58 | loss  5.49 | ppl   241.45\n",
            "| epoch   9 |  1400/ 2983 batches | lr 0.000315 | ms/batch 24.58 | loss  5.51 | ppl   248.35\n",
            "| epoch   9 |  1600/ 2983 batches | lr 0.000313 | ms/batch 24.55 | loss  5.58 | ppl   264.51\n",
            "| epoch   9 |  1800/ 2983 batches | lr 0.000312 | ms/batch 24.56 | loss  5.48 | ppl   240.78\n",
            "| epoch   9 |  2000/ 2983 batches | lr 0.000311 | ms/batch 24.61 | loss  5.52 | ppl   248.51\n",
            "| epoch   9 |  2200/ 2983 batches | lr 0.000310 | ms/batch 24.55 | loss  5.43 | ppl   228.70\n",
            "| epoch   9 |  2400/ 2983 batches | lr 0.000308 | ms/batch 24.59 | loss  5.47 | ppl   237.49\n",
            "| epoch   9 |  2600/ 2983 batches | lr 0.000307 | ms/batch 24.58 | loss  5.51 | ppl   246.34\n",
            "| epoch   9 |  2800/ 2983 batches | lr 0.000306 | ms/batch 24.77 | loss  5.45 | ppl   233.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 76.04s | valid loss  6.34 | valid ppl   564.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000305\n",
            "| epoch  10 |   200/ 2983 batches | lr 0.000304 | ms/batch 24.98 | loss  5.51 | ppl   246.19\n",
            "| epoch  10 |   400/ 2983 batches | lr 0.000303 | ms/batch 24.78 | loss  5.52 | ppl   248.80\n",
            "| epoch  10 |   600/ 2983 batches | lr 0.000302 | ms/batch 24.85 | loss  5.40 | ppl   222.27\n",
            "| epoch  10 |   800/ 2983 batches | lr 0.000301 | ms/batch 24.76 | loss  5.43 | ppl   229.21\n",
            "| epoch  10 |  1000/ 2983 batches | lr 0.000300 | ms/batch 24.76 | loss  5.44 | ppl   231.52\n",
            "| epoch  10 |  1200/ 2983 batches | lr 0.000299 | ms/batch 24.80 | loss  5.45 | ppl   233.22\n",
            "| epoch  10 |  1400/ 2983 batches | lr 0.000297 | ms/batch 24.90 | loss  5.48 | ppl   239.45\n",
            "| epoch  10 |  1600/ 2983 batches | lr 0.000296 | ms/batch 24.92 | loss  5.54 | ppl   255.24\n",
            "| epoch  10 |  1800/ 2983 batches | lr 0.000295 | ms/batch 24.87 | loss  5.45 | ppl   232.15\n",
            "| epoch  10 |  2000/ 2983 batches | lr 0.000294 | ms/batch 24.78 | loss  5.48 | ppl   240.40\n",
            "| epoch  10 |  2200/ 2983 batches | lr 0.000293 | ms/batch 24.69 | loss  5.40 | ppl   220.70\n",
            "| epoch  10 |  2400/ 2983 batches | lr 0.000292 | ms/batch 24.60 | loss  5.44 | ppl   229.47\n",
            "| epoch  10 |  2600/ 2983 batches | lr 0.000291 | ms/batch 24.70 | loss  5.47 | ppl   238.23\n",
            "| epoch  10 |  2800/ 2983 batches | lr 0.000290 | ms/batch 24.57 | loss  5.42 | ppl   225.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 76.50s | valid loss  6.32 | valid ppl   558.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000289\n",
            "| epoch  11 |   200/ 2983 batches | lr 0.000288 | ms/batch 24.71 | loss  5.47 | ppl   238.03\n",
            "| epoch  11 |   400/ 2983 batches | lr 0.000288 | ms/batch 24.65 | loss  5.48 | ppl   240.17\n",
            "| epoch  11 |   600/ 2983 batches | lr 0.000287 | ms/batch 24.64 | loss  5.37 | ppl   214.80\n",
            "| epoch  11 |   800/ 2983 batches | lr 0.000286 | ms/batch 24.68 | loss  5.40 | ppl   221.50\n",
            "| epoch  11 |  1000/ 2983 batches | lr 0.000285 | ms/batch 24.74 | loss  5.41 | ppl   223.93\n",
            "| epoch  11 |  1200/ 2983 batches | lr 0.000284 | ms/batch 24.60 | loss  5.42 | ppl   225.92\n",
            "| epoch  11 |  1400/ 2983 batches | lr 0.000283 | ms/batch 24.67 | loss  5.45 | ppl   232.49\n",
            "| epoch  11 |  1600/ 2983 batches | lr 0.000282 | ms/batch 24.64 | loss  5.51 | ppl   246.43\n",
            "| epoch  11 |  1800/ 2983 batches | lr 0.000281 | ms/batch 24.63 | loss  5.42 | ppl   224.93\n",
            "| epoch  11 |  2000/ 2983 batches | lr 0.000280 | ms/batch 24.61 | loss  5.45 | ppl   232.66\n",
            "| epoch  11 |  2200/ 2983 batches | lr 0.000279 | ms/batch 24.58 | loss  5.37 | ppl   214.18\n",
            "| epoch  11 |  2400/ 2983 batches | lr 0.000278 | ms/batch 24.68 | loss  5.40 | ppl   222.19\n",
            "| epoch  11 |  2600/ 2983 batches | lr 0.000278 | ms/batch 24.63 | loss  5.44 | ppl   230.88\n",
            "| epoch  11 |  2800/ 2983 batches | lr 0.000277 | ms/batch 24.54 | loss  5.39 | ppl   218.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 76.11s | valid loss  6.32 | valid ppl   556.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000276\n",
            "| epoch  12 |   200/ 2983 batches | lr 0.000275 | ms/batch 24.64 | loss  5.44 | ppl   231.32\n",
            "| epoch  12 |   400/ 2983 batches | lr 0.000274 | ms/batch 24.70 | loss  5.45 | ppl   232.53\n",
            "| epoch  12 |   600/ 2983 batches | lr 0.000273 | ms/batch 24.54 | loss  5.34 | ppl   209.13\n",
            "| epoch  12 |   800/ 2983 batches | lr 0.000273 | ms/batch 24.62 | loss  5.37 | ppl   215.18\n",
            "| epoch  12 |  1000/ 2983 batches | lr 0.000272 | ms/batch 24.52 | loss  5.38 | ppl   217.85\n",
            "| epoch  12 |  1200/ 2983 batches | lr 0.000271 | ms/batch 24.50 | loss  5.39 | ppl   219.61\n",
            "| epoch  12 |  1400/ 2983 batches | lr 0.000270 | ms/batch 24.61 | loss  5.42 | ppl   225.87\n",
            "| epoch  12 |  1600/ 2983 batches | lr 0.000269 | ms/batch 24.53 | loss  5.48 | ppl   240.38\n",
            "| epoch  12 |  1800/ 2983 batches | lr 0.000269 | ms/batch 24.61 | loss  5.39 | ppl   219.27\n",
            "| epoch  12 |  2000/ 2983 batches | lr 0.000268 | ms/batch 24.59 | loss  5.42 | ppl   226.71\n",
            "| epoch  12 |  2200/ 2983 batches | lr 0.000267 | ms/batch 24.60 | loss  5.34 | ppl   208.69\n",
            "| epoch  12 |  2400/ 2983 batches | lr 0.000266 | ms/batch 24.60 | loss  5.38 | ppl   216.13\n",
            "| epoch  12 |  2600/ 2983 batches | lr 0.000266 | ms/batch 24.55 | loss  5.41 | ppl   224.59\n",
            "| epoch  12 |  2800/ 2983 batches | lr 0.000265 | ms/batch 24.56 | loss  5.36 | ppl   212.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 75.91s | valid loss  6.32 | valid ppl   555.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000264\n",
            "| epoch  13 |   200/ 2983 batches | lr 0.000263 | ms/batch 24.75 | loss  5.42 | ppl   224.82\n",
            "| epoch  13 |   400/ 2983 batches | lr 0.000263 | ms/batch 24.56 | loss  5.42 | ppl   226.98\n",
            "| epoch  13 |   600/ 2983 batches | lr 0.000262 | ms/batch 24.55 | loss  5.32 | ppl   203.91\n",
            "| epoch  13 |   800/ 2983 batches | lr 0.000261 | ms/batch 24.56 | loss  5.34 | ppl   209.24\n",
            "| epoch  13 |  1000/ 2983 batches | lr 0.000261 | ms/batch 24.50 | loss  5.35 | ppl   210.91\n",
            "| epoch  13 |  1200/ 2983 batches | lr 0.000260 | ms/batch 24.63 | loss  5.37 | ppl   214.30\n",
            "| epoch  13 |  1400/ 2983 batches | lr 0.000259 | ms/batch 24.55 | loss  5.39 | ppl   219.56\n",
            "| epoch  13 |  1600/ 2983 batches | lr 0.000259 | ms/batch 24.55 | loss  5.45 | ppl   233.74\n",
            "| epoch  13 |  1800/ 2983 batches | lr 0.000258 | ms/batch 24.60 | loss  5.36 | ppl   213.01\n",
            "| epoch  13 |  2000/ 2983 batches | lr 0.000257 | ms/batch 24.54 | loss  5.40 | ppl   220.86\n",
            "| epoch  13 |  2200/ 2983 batches | lr 0.000256 | ms/batch 24.58 | loss  5.31 | ppl   202.77\n",
            "| epoch  13 |  2400/ 2983 batches | lr 0.000256 | ms/batch 24.56 | loss  5.35 | ppl   211.16\n",
            "| epoch  13 |  2600/ 2983 batches | lr 0.000255 | ms/batch 24.55 | loss  5.39 | ppl   219.13\n",
            "| epoch  13 |  2800/ 2983 batches | lr 0.000254 | ms/batch 24.57 | loss  5.33 | ppl   206.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 75.90s | valid loss  6.33 | valid ppl   561.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000254\n",
            "| epoch  14 |   200/ 2983 batches | lr 0.000253 | ms/batch 24.72 | loss  5.39 | ppl   219.11\n",
            "| epoch  14 |   400/ 2983 batches | lr 0.000253 | ms/batch 24.55 | loss  5.40 | ppl   221.68\n",
            "| epoch  14 |   600/ 2983 batches | lr 0.000252 | ms/batch 24.61 | loss  5.29 | ppl   198.65\n",
            "| epoch  14 |   800/ 2983 batches | lr 0.000251 | ms/batch 24.63 | loss  5.31 | ppl   203.24\n",
            "| epoch  14 |  1000/ 2983 batches | lr 0.000251 | ms/batch 24.55 | loss  5.33 | ppl   206.15\n",
            "| epoch  14 |  1200/ 2983 batches | lr 0.000250 | ms/batch 24.77 | loss  5.34 | ppl   207.52\n",
            "| epoch  14 |  1400/ 2983 batches | lr 0.000249 | ms/batch 24.77 | loss  5.37 | ppl   213.95\n",
            "| epoch  14 |  1600/ 2983 batches | lr 0.000249 | ms/batch 24.73 | loss  5.43 | ppl   227.31\n",
            "| epoch  14 |  1800/ 2983 batches | lr 0.000248 | ms/batch 24.61 | loss  5.34 | ppl   208.11\n",
            "| epoch  14 |  2000/ 2983 batches | lr 0.000248 | ms/batch 24.54 | loss  5.37 | ppl   214.74\n",
            "| epoch  14 |  2200/ 2983 batches | lr 0.000247 | ms/batch 24.56 | loss  5.29 | ppl   197.77\n",
            "| epoch  14 |  2400/ 2983 batches | lr 0.000246 | ms/batch 24.55 | loss  5.32 | ppl   205.26\n",
            "| epoch  14 |  2600/ 2983 batches | lr 0.000246 | ms/batch 24.63 | loss  5.36 | ppl   213.19\n",
            "| epoch  14 |  2800/ 2983 batches | lr 0.000245 | ms/batch 24.57 | loss  5.31 | ppl   202.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 76.06s | valid loss  6.32 | valid ppl   554.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000245\n",
            "| epoch  15 |   200/ 2983 batches | lr 0.000244 | ms/batch 24.73 | loss  5.37 | ppl   214.15\n",
            "| epoch  15 |   400/ 2983 batches | lr 0.000243 | ms/batch 24.57 | loss  5.37 | ppl   215.35\n",
            "| epoch  15 |   600/ 2983 batches | lr 0.000243 | ms/batch 24.56 | loss  5.26 | ppl   193.39\n",
            "| epoch  15 |   800/ 2983 batches | lr 0.000242 | ms/batch 24.56 | loss  5.29 | ppl   198.92\n",
            "| epoch  15 |  1000/ 2983 batches | lr 0.000242 | ms/batch 24.58 | loss  5.31 | ppl   201.68\n",
            "| epoch  15 |  1200/ 2983 batches | lr 0.000241 | ms/batch 24.56 | loss  5.31 | ppl   203.27\n",
            "| epoch  15 |  1400/ 2983 batches | lr 0.000241 | ms/batch 24.66 | loss  5.34 | ppl   208.66\n",
            "| epoch  15 |  1600/ 2983 batches | lr 0.000240 | ms/batch 24.56 | loss  5.41 | ppl   222.60\n",
            "| epoch  15 |  1800/ 2983 batches | lr 0.000240 | ms/batch 24.56 | loss  5.31 | ppl   202.99\n",
            "| epoch  15 |  2000/ 2983 batches | lr 0.000239 | ms/batch 24.58 | loss  5.35 | ppl   210.47\n",
            "| epoch  15 |  2200/ 2983 batches | lr 0.000238 | ms/batch 24.59 | loss  5.26 | ppl   193.20\n",
            "| epoch  15 |  2400/ 2983 batches | lr 0.000238 | ms/batch 24.62 | loss  5.30 | ppl   200.22\n",
            "| epoch  15 |  2600/ 2983 batches | lr 0.000237 | ms/batch 24.56 | loss  5.34 | ppl   208.57\n",
            "| epoch  15 |  2800/ 2983 batches | lr 0.000237 | ms/batch 24.55 | loss  5.28 | ppl   197.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 76.01s | valid loss  6.33 | valid ppl   559.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000236\n",
            "| epoch  16 |   200/ 2983 batches | lr 0.000236 | ms/batch 24.71 | loss  5.34 | ppl   208.76\n",
            "| epoch  16 |   400/ 2983 batches | lr 0.000235 | ms/batch 24.63 | loss  5.35 | ppl   211.06\n",
            "| epoch  16 |   600/ 2983 batches | lr 0.000235 | ms/batch 24.60 | loss  5.24 | ppl   188.55\n",
            "| epoch  16 |   800/ 2983 batches | lr 0.000234 | ms/batch 24.64 | loss  5.27 | ppl   194.69\n",
            "| epoch  16 |  1000/ 2983 batches | lr 0.000234 | ms/batch 24.54 | loss  5.28 | ppl   196.69\n",
            "| epoch  16 |  1200/ 2983 batches | lr 0.000233 | ms/batch 24.57 | loss  5.29 | ppl   198.79\n",
            "| epoch  16 |  1400/ 2983 batches | lr 0.000233 | ms/batch 24.58 | loss  5.32 | ppl   203.59\n",
            "| epoch  16 |  1600/ 2983 batches | lr 0.000232 | ms/batch 24.61 | loss  5.38 | ppl   217.81\n",
            "| epoch  16 |  1800/ 2983 batches | lr 0.000232 | ms/batch 24.59 | loss  5.29 | ppl   197.91\n",
            "| epoch  16 |  2000/ 2983 batches | lr 0.000231 | ms/batch 24.59 | loss  5.33 | ppl   205.85\n",
            "| epoch  16 |  2200/ 2983 batches | lr 0.000231 | ms/batch 24.60 | loss  5.24 | ppl   189.05\n",
            "| epoch  16 |  2400/ 2983 batches | lr 0.000230 | ms/batch 24.59 | loss  5.28 | ppl   196.80\n",
            "| epoch  16 |  2600/ 2983 batches | lr 0.000230 | ms/batch 24.65 | loss  5.32 | ppl   204.50\n",
            "| epoch  16 |  2800/ 2983 batches | lr 0.000229 | ms/batch 24.57 | loss  5.26 | ppl   192.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 76.02s | valid loss  6.33 | valid ppl   563.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000229\n",
            "| epoch  17 |   200/ 2983 batches | lr 0.000228 | ms/batch 24.73 | loss  5.32 | ppl   204.53\n",
            "| epoch  17 |   400/ 2983 batches | lr 0.000228 | ms/batch 24.60 | loss  5.33 | ppl   206.14\n",
            "| epoch  17 |   600/ 2983 batches | lr 0.000227 | ms/batch 24.62 | loss  5.22 | ppl   184.98\n",
            "| epoch  17 |   800/ 2983 batches | lr 0.000227 | ms/batch 24.57 | loss  5.25 | ppl   190.92\n",
            "| epoch  17 |  1000/ 2983 batches | lr 0.000226 | ms/batch 24.59 | loss  5.26 | ppl   192.43\n",
            "| epoch  17 |  1200/ 2983 batches | lr 0.000226 | ms/batch 24.60 | loss  5.27 | ppl   194.66\n",
            "| epoch  17 |  1400/ 2983 batches | lr 0.000226 | ms/batch 24.59 | loss  5.30 | ppl   199.64\n",
            "| epoch  17 |  1600/ 2983 batches | lr 0.000225 | ms/batch 24.64 | loss  5.36 | ppl   213.65\n",
            "| epoch  17 |  1800/ 2983 batches | lr 0.000225 | ms/batch 24.57 | loss  5.27 | ppl   194.96\n",
            "| epoch  17 |  2000/ 2983 batches | lr 0.000224 | ms/batch 24.53 | loss  5.31 | ppl   201.55\n",
            "| epoch  17 |  2200/ 2983 batches | lr 0.000224 | ms/batch 24.58 | loss  5.22 | ppl   185.57\n",
            "| epoch  17 |  2400/ 2983 batches | lr 0.000223 | ms/batch 24.61 | loss  5.26 | ppl   193.28\n",
            "| epoch  17 |  2600/ 2983 batches | lr 0.000223 | ms/batch 24.57 | loss  5.30 | ppl   200.93\n",
            "| epoch  17 |  2800/ 2983 batches | lr 0.000222 | ms/batch 24.58 | loss  5.25 | ppl   189.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 76.07s | valid loss  6.34 | valid ppl   565.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000222\n",
            "| epoch  18 |   200/ 2983 batches | lr 0.000222 | ms/batch 24.73 | loss  5.30 | ppl   200.45\n",
            "| epoch  18 |   400/ 2983 batches | lr 0.000221 | ms/batch 24.62 | loss  5.31 | ppl   201.97\n",
            "| epoch  18 |   600/ 2983 batches | lr 0.000221 | ms/batch 24.54 | loss  5.20 | ppl   181.52\n",
            "| epoch  18 |   800/ 2983 batches | lr 0.000220 | ms/batch 24.55 | loss  5.23 | ppl   186.68\n",
            "| epoch  18 |  1000/ 2983 batches | lr 0.000220 | ms/batch 24.70 | loss  5.24 | ppl   188.76\n",
            "| epoch  18 |  1200/ 2983 batches | lr 0.000219 | ms/batch 24.62 | loss  5.25 | ppl   190.10\n",
            "| epoch  18 |  1400/ 2983 batches | lr 0.000219 | ms/batch 24.73 | loss  5.27 | ppl   195.14\n",
            "| epoch  18 |  1600/ 2983 batches | lr 0.000219 | ms/batch 24.70 | loss  5.34 | ppl   208.34\n",
            "| epoch  18 |  1800/ 2983 batches | lr 0.000218 | ms/batch 24.62 | loss  5.25 | ppl   190.36\n",
            "| epoch  18 |  2000/ 2983 batches | lr 0.000218 | ms/batch 24.66 | loss  5.28 | ppl   197.31\n",
            "| epoch  18 |  2200/ 2983 batches | lr 0.000217 | ms/batch 24.56 | loss  5.20 | ppl   181.11\n",
            "| epoch  18 |  2400/ 2983 batches | lr 0.000217 | ms/batch 24.63 | loss  5.24 | ppl   188.46\n",
            "| epoch  18 |  2600/ 2983 batches | lr 0.000217 | ms/batch 24.57 | loss  5.27 | ppl   195.06\n",
            "| epoch  18 |  2800/ 2983 batches | lr 0.000216 | ms/batch 24.60 | loss  5.22 | ppl   185.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 76.09s | valid loss  6.34 | valid ppl   566.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000216\n",
            "| epoch  19 |   200/ 2983 batches | lr 0.000215 | ms/batch 24.69 | loss  5.28 | ppl   196.02\n",
            "| epoch  19 |   400/ 2983 batches | lr 0.000215 | ms/batch 24.62 | loss  5.29 | ppl   197.73\n",
            "| epoch  19 |   600/ 2983 batches | lr 0.000215 | ms/batch 24.54 | loss  5.18 | ppl   177.52\n",
            "| epoch  19 |   800/ 2983 batches | lr 0.000214 | ms/batch 24.63 | loss  5.21 | ppl   182.62\n",
            "| epoch  19 |  1000/ 2983 batches | lr 0.000214 | ms/batch 24.57 | loss  5.22 | ppl   184.18\n",
            "| epoch  19 |  1200/ 2983 batches | lr 0.000213 | ms/batch 24.61 | loss  5.23 | ppl   186.39\n",
            "| epoch  19 |  1400/ 2983 batches | lr 0.000213 | ms/batch 24.61 | loss  5.25 | ppl   191.05\n",
            "| epoch  19 |  1600/ 2983 batches | lr 0.000213 | ms/batch 24.57 | loss  5.32 | ppl   204.31\n",
            "| epoch  19 |  1800/ 2983 batches | lr 0.000212 | ms/batch 24.66 | loss  5.23 | ppl   186.45\n",
            "| epoch  19 |  2000/ 2983 batches | lr 0.000212 | ms/batch 24.60 | loss  5.26 | ppl   193.27\n",
            "| epoch  19 |  2200/ 2983 batches | lr 0.000211 | ms/batch 24.56 | loss  5.18 | ppl   178.31\n",
            "| epoch  19 |  2400/ 2983 batches | lr 0.000211 | ms/batch 24.59 | loss  5.22 | ppl   185.34\n",
            "| epoch  19 |  2600/ 2983 batches | lr 0.000211 | ms/batch 24.57 | loss  5.26 | ppl   192.35\n",
            "| epoch  19 |  2800/ 2983 batches | lr 0.000210 | ms/batch 24.63 | loss  5.21 | ppl   182.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 76.00s | valid loss  6.35 | valid ppl   573.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000210\n",
            "| epoch  20 |   200/ 2983 batches | lr 0.000210 | ms/batch 24.74 | loss  5.26 | ppl   192.62\n",
            "| epoch  20 |   400/ 2983 batches | lr 0.000209 | ms/batch 24.52 | loss  5.27 | ppl   195.29\n",
            "| epoch  20 |   600/ 2983 batches | lr 0.000209 | ms/batch 24.55 | loss  5.16 | ppl   174.85\n",
            "| epoch  20 |   800/ 2983 batches | lr 0.000209 | ms/batch 24.55 | loss  5.20 | ppl   180.58\n",
            "| epoch  20 |  1000/ 2983 batches | lr 0.000208 | ms/batch 24.56 | loss  5.21 | ppl   183.22\n",
            "| epoch  20 |  1200/ 2983 batches | lr 0.000208 | ms/batch 24.64 | loss  5.21 | ppl   183.66\n",
            "| epoch  20 |  1400/ 2983 batches | lr 0.000207 | ms/batch 24.55 | loss  5.24 | ppl   188.80\n",
            "| epoch  20 |  1600/ 2983 batches | lr 0.000207 | ms/batch 24.60 | loss  5.31 | ppl   201.41\n",
            "| epoch  20 |  1800/ 2983 batches | lr 0.000207 | ms/batch 24.56 | loss  5.22 | ppl   184.67\n",
            "| epoch  20 |  2000/ 2983 batches | lr 0.000206 | ms/batch 24.60 | loss  5.25 | ppl   190.56\n",
            "| epoch  20 |  2200/ 2983 batches | lr 0.000206 | ms/batch 24.65 | loss  5.17 | ppl   175.56\n",
            "| epoch  20 |  2400/ 2983 batches | lr 0.000206 | ms/batch 24.52 | loss  5.21 | ppl   182.59\n",
            "| epoch  20 |  2600/ 2983 batches | lr 0.000205 | ms/batch 24.60 | loss  5.25 | ppl   189.68\n",
            "| epoch  20 |  2800/ 2983 batches | lr 0.000205 | ms/batch 24.58 | loss  5.19 | ppl   179.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 75.98s | valid loss  6.36 | valid ppl   578.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000205\n",
            "| epoch  21 |   200/ 2983 batches | lr 0.000204 | ms/batch 24.71 | loss  5.25 | ppl   190.24\n",
            "| epoch  21 |   400/ 2983 batches | lr 0.000204 | ms/batch 24.56 | loss  5.26 | ppl   191.67\n",
            "| epoch  21 |   600/ 2983 batches | lr 0.000204 | ms/batch 24.64 | loss  5.15 | ppl   172.45\n",
            "| epoch  21 |   800/ 2983 batches | lr 0.000203 | ms/batch 24.54 | loss  5.18 | ppl   177.21\n",
            "| epoch  21 |  1000/ 2983 batches | lr 0.000203 | ms/batch 24.58 | loss  5.19 | ppl   179.80\n",
            "| epoch  21 |  1200/ 2983 batches | lr 0.000203 | ms/batch 24.62 | loss  5.20 | ppl   181.24\n",
            "| epoch  21 |  1400/ 2983 batches | lr 0.000202 | ms/batch 24.54 | loss  5.23 | ppl   185.98\n",
            "| epoch  21 |  1600/ 2983 batches | lr 0.000202 | ms/batch 24.57 | loss  5.29 | ppl   198.32\n",
            "| epoch  21 |  1800/ 2983 batches | lr 0.000202 | ms/batch 24.49 | loss  5.20 | ppl   181.31\n",
            "| epoch  21 |  2000/ 2983 batches | lr 0.000201 | ms/batch 24.57 | loss  5.23 | ppl   187.37\n",
            "| epoch  21 |  2200/ 2983 batches | lr 0.000201 | ms/batch 24.53 | loss  5.15 | ppl   172.40\n",
            "| epoch  21 |  2400/ 2983 batches | lr 0.000201 | ms/batch 24.50 | loss  5.19 | ppl   179.73\n",
            "| epoch  21 |  2600/ 2983 batches | lr 0.000200 | ms/batch 24.69 | loss  5.23 | ppl   186.55\n",
            "| epoch  21 |  2800/ 2983 batches | lr 0.000200 | ms/batch 24.52 | loss  5.17 | ppl   176.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 75.93s | valid loss  6.36 | valid ppl   577.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000200\n",
            "| epoch  22 |   200/ 2983 batches | lr 0.000199 | ms/batch 24.70 | loss  5.23 | ppl   187.14\n",
            "| epoch  22 |   400/ 2983 batches | lr 0.000199 | ms/batch 24.60 | loss  5.24 | ppl   188.19\n",
            "| epoch  22 |   600/ 2983 batches | lr 0.000199 | ms/batch 24.55 | loss  5.14 | ppl   169.90\n",
            "| epoch  22 |   800/ 2983 batches | lr 0.000198 | ms/batch 24.61 | loss  5.16 | ppl   174.94\n",
            "| epoch  22 |  1000/ 2983 batches | lr 0.000198 | ms/batch 24.73 | loss  5.17 | ppl   176.77\n",
            "| epoch  22 |  1200/ 2983 batches | lr 0.000198 | ms/batch 24.69 | loss  5.18 | ppl   177.63\n",
            "| epoch  22 |  1400/ 2983 batches | lr 0.000198 | ms/batch 24.72 | loss  5.20 | ppl   181.72\n",
            "| epoch  22 |  1600/ 2983 batches | lr 0.000197 | ms/batch 24.55 | loss  5.27 | ppl   194.98\n",
            "| epoch  22 |  1800/ 2983 batches | lr 0.000197 | ms/batch 24.49 | loss  5.18 | ppl   177.81\n",
            "| epoch  22 |  2000/ 2983 batches | lr 0.000197 | ms/batch 24.58 | loss  5.22 | ppl   184.54\n",
            "| epoch  22 |  2200/ 2983 batches | lr 0.000196 | ms/batch 24.64 | loss  5.14 | ppl   169.92\n",
            "| epoch  22 |  2400/ 2983 batches | lr 0.000196 | ms/batch 24.57 | loss  5.17 | ppl   175.56\n",
            "| epoch  22 |  2600/ 2983 batches | lr 0.000196 | ms/batch 24.59 | loss  5.21 | ppl   183.58\n",
            "| epoch  22 |  2800/ 2983 batches | lr 0.000195 | ms/batch 24.53 | loss  5.16 | ppl   173.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 76.04s | valid loss  6.36 | valid ppl   580.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000195\n",
            "| epoch  23 |   200/ 2983 batches | lr 0.000195 | ms/batch 24.67 | loss  5.21 | ppl   183.10\n",
            "| epoch  23 |   400/ 2983 batches | lr 0.000195 | ms/batch 24.57 | loss  5.22 | ppl   184.70\n",
            "| epoch  23 |   600/ 2983 batches | lr 0.000194 | ms/batch 24.49 | loss  5.11 | ppl   166.17\n",
            "| epoch  23 |   800/ 2983 batches | lr 0.000194 | ms/batch 24.57 | loss  5.14 | ppl   170.88\n",
            "| epoch  23 |  1000/ 2983 batches | lr 0.000194 | ms/batch 24.55 | loss  5.16 | ppl   173.58\n",
            "| epoch  23 |  1200/ 2983 batches | lr 0.000193 | ms/batch 24.54 | loss  5.16 | ppl   174.48\n",
            "| epoch  23 |  1400/ 2983 batches | lr 0.000193 | ms/batch 24.61 | loss  5.19 | ppl   178.96\n",
            "| epoch  23 |  1600/ 2983 batches | lr 0.000193 | ms/batch 24.56 | loss  5.26 | ppl   192.27\n",
            "| epoch  23 |  1800/ 2983 batches | lr 0.000193 | ms/batch 24.67 | loss  5.17 | ppl   175.58\n",
            "| epoch  23 |  2000/ 2983 batches | lr 0.000192 | ms/batch 24.55 | loss  5.21 | ppl   182.19\n",
            "| epoch  23 |  2200/ 2983 batches | lr 0.000192 | ms/batch 24.61 | loss  5.12 | ppl   167.19\n",
            "| epoch  23 |  2400/ 2983 batches | lr 0.000192 | ms/batch 24.56 | loss  5.16 | ppl   173.50\n",
            "| epoch  23 |  2600/ 2983 batches | lr 0.000191 | ms/batch 24.53 | loss  5.20 | ppl   180.92\n",
            "| epoch  23 |  2800/ 2983 batches | lr 0.000191 | ms/batch 24.61 | loss  5.14 | ppl   171.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 75.94s | valid loss  6.37 | valid ppl   581.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000191\n",
            "| epoch  24 |   200/ 2983 batches | lr 0.000191 | ms/batch 24.68 | loss  5.20 | ppl   180.79\n",
            "| epoch  24 |   400/ 2983 batches | lr 0.000190 | ms/batch 24.56 | loss  5.20 | ppl   182.11\n",
            "| epoch  24 |   600/ 2983 batches | lr 0.000190 | ms/batch 24.56 | loss  5.10 | ppl   164.19\n",
            "| epoch  24 |   800/ 2983 batches | lr 0.000190 | ms/batch 24.59 | loss  5.13 | ppl   169.04\n",
            "| epoch  24 |  1000/ 2983 batches | lr 0.000189 | ms/batch 24.51 | loss  5.14 | ppl   170.77\n",
            "| epoch  24 |  1200/ 2983 batches | lr 0.000189 | ms/batch 24.63 | loss  5.15 | ppl   172.01\n",
            "| epoch  24 |  1400/ 2983 batches | lr 0.000189 | ms/batch 24.58 | loss  5.17 | ppl   176.21\n",
            "| epoch  24 |  1600/ 2983 batches | lr 0.000189 | ms/batch 24.52 | loss  5.24 | ppl   188.78\n",
            "| epoch  24 |  1800/ 2983 batches | lr 0.000188 | ms/batch 24.61 | loss  5.15 | ppl   172.66\n",
            "| epoch  24 |  2000/ 2983 batches | lr 0.000188 | ms/batch 24.62 | loss  5.19 | ppl   179.14\n",
            "| epoch  24 |  2200/ 2983 batches | lr 0.000188 | ms/batch 24.54 | loss  5.10 | ppl   164.29\n",
            "| epoch  24 |  2400/ 2983 batches | lr 0.000188 | ms/batch 24.58 | loss  5.14 | ppl   171.09\n",
            "| epoch  24 |  2600/ 2983 batches | lr 0.000187 | ms/batch 24.62 | loss  5.18 | ppl   178.27\n",
            "| epoch  24 |  2800/ 2983 batches | lr 0.000187 | ms/batch 24.55 | loss  5.13 | ppl   168.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 75.93s | valid loss  6.37 | valid ppl   585.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000187\n",
            "| epoch  25 |   200/ 2983 batches | lr 0.000187 | ms/batch 24.65 | loss  5.18 | ppl   178.44\n",
            "| epoch  25 |   400/ 2983 batches | lr 0.000186 | ms/batch 24.56 | loss  5.19 | ppl   179.49\n",
            "| epoch  25 |   600/ 2983 batches | lr 0.000186 | ms/batch 24.56 | loss  5.08 | ppl   161.43\n",
            "| epoch  25 |   800/ 2983 batches | lr 0.000186 | ms/batch 24.55 | loss  5.11 | ppl   166.37\n",
            "| epoch  25 |  1000/ 2983 batches | lr 0.000186 | ms/batch 24.56 | loss  5.13 | ppl   168.68\n",
            "| epoch  25 |  1200/ 2983 batches | lr 0.000185 | ms/batch 24.53 | loss  5.13 | ppl   169.63\n",
            "| epoch  25 |  1400/ 2983 batches | lr 0.000185 | ms/batch 24.57 | loss  5.16 | ppl   174.16\n",
            "| epoch  25 |  1600/ 2983 batches | lr 0.000185 | ms/batch 24.57 | loss  5.23 | ppl   187.01\n",
            "| epoch  25 |  1800/ 2983 batches | lr 0.000185 | ms/batch 24.59 | loss  5.14 | ppl   170.06\n",
            "| epoch  25 |  2000/ 2983 batches | lr 0.000184 | ms/batch 24.60 | loss  5.17 | ppl   175.77\n",
            "| epoch  25 |  2200/ 2983 batches | lr 0.000184 | ms/batch 24.61 | loss  5.09 | ppl   162.60\n",
            "| epoch  25 |  2400/ 2983 batches | lr 0.000184 | ms/batch 24.56 | loss  5.13 | ppl   168.55\n",
            "| epoch  25 |  2600/ 2983 batches | lr 0.000184 | ms/batch 24.54 | loss  5.16 | ppl   174.89\n",
            "| epoch  25 |  2800/ 2983 batches | lr 0.000183 | ms/batch 24.58 | loss  5.11 | ppl   165.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 75.93s | valid loss  6.39 | valid ppl   593.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000183\n",
            "| epoch  26 |   200/ 2983 batches | lr 0.000183 | ms/batch 24.91 | loss  5.17 | ppl   175.80\n",
            "| epoch  26 |   400/ 2983 batches | lr 0.000183 | ms/batch 24.78 | loss  5.17 | ppl   176.67\n",
            "| epoch  26 |   600/ 2983 batches | lr 0.000182 | ms/batch 24.78 | loss  5.07 | ppl   159.59\n",
            "| epoch  26 |   800/ 2983 batches | lr 0.000182 | ms/batch 24.87 | loss  5.10 | ppl   163.87\n",
            "| epoch  26 |  1000/ 2983 batches | lr 0.000182 | ms/batch 25.02 | loss  5.11 | ppl   166.01\n",
            "| epoch  26 |  1200/ 2983 batches | lr 0.000182 | ms/batch 24.99 | loss  5.12 | ppl   167.07\n",
            "| epoch  26 |  1400/ 2983 batches | lr 0.000181 | ms/batch 24.87 | loss  5.14 | ppl   171.50\n",
            "| epoch  26 |  1600/ 2983 batches | lr 0.000181 | ms/batch 24.82 | loss  5.21 | ppl   183.14\n",
            "| epoch  26 |  1800/ 2983 batches | lr 0.000181 | ms/batch 24.88 | loss  5.12 | ppl   168.06\n",
            "| epoch  26 |  2000/ 2983 batches | lr 0.000181 | ms/batch 24.86 | loss  5.16 | ppl   173.66\n",
            "| epoch  26 |  2200/ 2983 batches | lr 0.000180 | ms/batch 24.88 | loss  5.08 | ppl   160.24\n",
            "| epoch  26 |  2400/ 2983 batches | lr 0.000180 | ms/batch 24.74 | loss  5.12 | ppl   166.54\n",
            "| epoch  26 |  2600/ 2983 batches | lr 0.000180 | ms/batch 24.62 | loss  5.15 | ppl   173.09\n",
            "| epoch  26 |  2800/ 2983 batches | lr 0.000180 | ms/batch 24.67 | loss  5.10 | ppl   164.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 76.68s | valid loss  6.38 | valid ppl   592.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000180\n",
            "| epoch  27 |   200/ 2983 batches | lr 0.000179 | ms/batch 24.76 | loss  5.16 | ppl   173.97\n",
            "| epoch  27 |   400/ 2983 batches | lr 0.000179 | ms/batch 24.59 | loss  5.17 | ppl   175.21\n",
            "| epoch  27 |   600/ 2983 batches | lr 0.000179 | ms/batch 24.60 | loss  5.06 | ppl   157.92\n",
            "| epoch  27 |   800/ 2983 batches | lr 0.000179 | ms/batch 24.62 | loss  5.09 | ppl   162.30\n",
            "| epoch  27 |  1000/ 2983 batches | lr 0.000178 | ms/batch 24.60 | loss  5.10 | ppl   164.39\n",
            "| epoch  27 |  1200/ 2983 batches | lr 0.000178 | ms/batch 24.70 | loss  5.11 | ppl   165.44\n",
            "| epoch  27 |  1400/ 2983 batches | lr 0.000178 | ms/batch 24.68 | loss  5.13 | ppl   169.77\n",
            "| epoch  27 |  1600/ 2983 batches | lr 0.000178 | ms/batch 24.63 | loss  5.20 | ppl   181.91\n",
            "| epoch  27 |  1800/ 2983 batches | lr 0.000177 | ms/batch 24.72 | loss  5.12 | ppl   166.56\n",
            "| epoch  27 |  2000/ 2983 batches | lr 0.000177 | ms/batch 24.69 | loss  5.15 | ppl   171.87\n",
            "| epoch  27 |  2200/ 2983 batches | lr 0.000177 | ms/batch 24.63 | loss  5.07 | ppl   158.47\n",
            "| epoch  27 |  2400/ 2983 batches | lr 0.000177 | ms/batch 24.62 | loss  5.10 | ppl   164.15\n",
            "| epoch  27 |  2600/ 2983 batches | lr 0.000177 | ms/batch 24.63 | loss  5.14 | ppl   170.70\n",
            "| epoch  27 |  2800/ 2983 batches | lr 0.000176 | ms/batch 24.56 | loss  5.09 | ppl   161.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 76.10s | valid loss  6.39 | valid ppl   596.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000176\n",
            "| epoch  28 |   200/ 2983 batches | lr 0.000176 | ms/batch 24.72 | loss  5.14 | ppl   171.45\n",
            "| epoch  28 |   400/ 2983 batches | lr 0.000176 | ms/batch 24.54 | loss  5.15 | ppl   172.25\n",
            "| epoch  28 |   600/ 2983 batches | lr 0.000175 | ms/batch 24.58 | loss  5.05 | ppl   156.08\n",
            "| epoch  28 |   800/ 2983 batches | lr 0.000175 | ms/batch 24.51 | loss  5.08 | ppl   160.16\n",
            "| epoch  28 |  1000/ 2983 batches | lr 0.000175 | ms/batch 24.58 | loss  5.09 | ppl   162.41\n",
            "| epoch  28 |  1200/ 2983 batches | lr 0.000175 | ms/batch 24.60 | loss  5.09 | ppl   163.05\n",
            "| epoch  28 |  1400/ 2983 batches | lr 0.000175 | ms/batch 24.62 | loss  5.12 | ppl   167.68\n",
            "| epoch  28 |  1600/ 2983 batches | lr 0.000174 | ms/batch 24.60 | loss  5.19 | ppl   179.23\n",
            "| epoch  28 |  1800/ 2983 batches | lr 0.000174 | ms/batch 24.60 | loss  5.10 | ppl   163.55\n",
            "| epoch  28 |  2000/ 2983 batches | lr 0.000174 | ms/batch 24.58 | loss  5.13 | ppl   169.76\n",
            "| epoch  28 |  2200/ 2983 batches | lr 0.000174 | ms/batch 24.55 | loss  5.05 | ppl   156.24\n",
            "| epoch  28 |  2400/ 2983 batches | lr 0.000174 | ms/batch 24.60 | loss  5.09 | ppl   161.84\n",
            "| epoch  28 |  2600/ 2983 batches | lr 0.000173 | ms/batch 24.57 | loss  5.13 | ppl   168.76\n",
            "| epoch  28 |  2800/ 2983 batches | lr 0.000173 | ms/batch 24.55 | loss  5.07 | ppl   159.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 75.97s | valid loss  6.40 | valid ppl   601.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000173\n",
            "| epoch  29 |   200/ 2983 batches | lr 0.000173 | ms/batch 24.63 | loss  5.13 | ppl   168.58\n",
            "| epoch  29 |   400/ 2983 batches | lr 0.000173 | ms/batch 24.62 | loss  5.13 | ppl   169.73\n",
            "| epoch  29 |   600/ 2983 batches | lr 0.000172 | ms/batch 24.56 | loss  5.03 | ppl   153.47\n",
            "| epoch  29 |   800/ 2983 batches | lr 0.000172 | ms/batch 24.58 | loss  5.06 | ppl   158.33\n",
            "| epoch  29 |  1000/ 2983 batches | lr 0.000172 | ms/batch 24.59 | loss  5.08 | ppl   160.66\n",
            "| epoch  29 |  1200/ 2983 batches | lr 0.000172 | ms/batch 24.62 | loss  5.08 | ppl   160.63\n",
            "| epoch  29 |  1400/ 2983 batches | lr 0.000172 | ms/batch 24.74 | loss  5.11 | ppl   165.15\n",
            "| epoch  29 |  1600/ 2983 batches | lr 0.000171 | ms/batch 24.55 | loss  5.17 | ppl   176.35\n",
            "| epoch  29 |  1800/ 2983 batches | lr 0.000171 | ms/batch 24.53 | loss  5.08 | ppl   161.39\n",
            "| epoch  29 |  2000/ 2983 batches | lr 0.000171 | ms/batch 24.59 | loss  5.12 | ppl   167.36\n",
            "| epoch  29 |  2200/ 2983 batches | lr 0.000171 | ms/batch 24.67 | loss  5.04 | ppl   154.16\n",
            "| epoch  29 |  2400/ 2983 batches | lr 0.000171 | ms/batch 24.64 | loss  5.07 | ppl   159.75\n",
            "| epoch  29 |  2600/ 2983 batches | lr 0.000170 | ms/batch 24.55 | loss  5.11 | ppl   165.68\n",
            "| epoch  29 |  2800/ 2983 batches | lr 0.000170 | ms/batch 24.62 | loss  5.06 | ppl   157.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 76.00s | valid loss  6.39 | valid ppl   598.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000170\n",
            "| epoch  30 |   200/ 2983 batches | lr 0.000170 | ms/batch 24.66 | loss  5.11 | ppl   166.28\n",
            "| epoch  30 |   400/ 2983 batches | lr 0.000170 | ms/batch 24.58 | loss  5.12 | ppl   167.98\n",
            "| epoch  30 |   600/ 2983 batches | lr 0.000169 | ms/batch 24.66 | loss  5.02 | ppl   151.90\n",
            "| epoch  30 |   800/ 2983 batches | lr 0.000169 | ms/batch 24.71 | loss  5.05 | ppl   156.22\n",
            "| epoch  30 |  1000/ 2983 batches | lr 0.000169 | ms/batch 24.72 | loss  5.06 | ppl   158.23\n",
            "| epoch  30 |  1200/ 2983 batches | lr 0.000169 | ms/batch 24.79 | loss  5.07 | ppl   159.24\n",
            "| epoch  30 |  1400/ 2983 batches | lr 0.000169 | ms/batch 24.68 | loss  5.09 | ppl   162.71\n",
            "| epoch  30 |  1600/ 2983 batches | lr 0.000168 | ms/batch 24.61 | loss  5.16 | ppl   174.56\n",
            "| epoch  30 |  1800/ 2983 batches | lr 0.000168 | ms/batch 24.56 | loss  5.08 | ppl   159.97\n",
            "| epoch  30 |  2000/ 2983 batches | lr 0.000168 | ms/batch 24.57 | loss  5.11 | ppl   165.73\n",
            "| epoch  30 |  2200/ 2983 batches | lr 0.000168 | ms/batch 24.61 | loss  5.03 | ppl   152.86\n",
            "| epoch  30 |  2400/ 2983 batches | lr 0.000168 | ms/batch 24.69 | loss  5.07 | ppl   158.63\n",
            "| epoch  30 |  2600/ 2983 batches | lr 0.000167 | ms/batch 24.70 | loss  5.10 | ppl   164.45\n",
            "| epoch  30 |  2800/ 2983 batches | lr 0.000167 | ms/batch 24.67 | loss  5.05 | ppl   155.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 76.19s | valid loss  6.40 | valid ppl   602.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000167\n",
            "| epoch  31 |   200/ 2983 batches | lr 0.000167 | ms/batch 24.77 | loss  5.11 | ppl   165.28\n",
            "| epoch  31 |   400/ 2983 batches | lr 0.000167 | ms/batch 24.56 | loss  5.11 | ppl   165.96\n",
            "| epoch  31 |   600/ 2983 batches | lr 0.000167 | ms/batch 24.54 | loss  5.01 | ppl   150.41\n",
            "| epoch  31 |   800/ 2983 batches | lr 0.000166 | ms/batch 24.63 | loss  5.04 | ppl   154.96\n",
            "| epoch  31 |  1000/ 2983 batches | lr 0.000166 | ms/batch 24.51 | loss  5.06 | ppl   157.19\n",
            "| epoch  31 |  1200/ 2983 batches | lr 0.000166 | ms/batch 24.54 | loss  5.06 | ppl   158.17\n",
            "| epoch  31 |  1400/ 2983 batches | lr 0.000166 | ms/batch 24.51 | loss  5.09 | ppl   162.10\n",
            "| epoch  31 |  1600/ 2983 batches | lr 0.000166 | ms/batch 24.53 | loss  5.15 | ppl   172.66\n",
            "| epoch  31 |  1800/ 2983 batches | lr 0.000165 | ms/batch 24.59 | loss  5.06 | ppl   157.52\n",
            "| epoch  31 |  2000/ 2983 batches | lr 0.000165 | ms/batch 24.56 | loss  5.10 | ppl   163.67\n",
            "| epoch  31 |  2200/ 2983 batches | lr 0.000165 | ms/batch 24.60 | loss  5.02 | ppl   151.19\n",
            "| epoch  31 |  2400/ 2983 batches | lr 0.000165 | ms/batch 24.55 | loss  5.06 | ppl   156.93\n",
            "| epoch  31 |  2600/ 2983 batches | lr 0.000165 | ms/batch 24.61 | loss  5.09 | ppl   162.46\n",
            "| epoch  31 |  2800/ 2983 batches | lr 0.000165 | ms/batch 24.57 | loss  5.04 | ppl   154.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 75.91s | valid loss  6.41 | valid ppl   606.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000164\n",
            "| epoch  32 |   200/ 2983 batches | lr 0.000164 | ms/batch 24.65 | loss  5.09 | ppl   162.87\n",
            "| epoch  32 |   400/ 2983 batches | lr 0.000164 | ms/batch 24.47 | loss  5.10 | ppl   164.44\n",
            "| epoch  32 |   600/ 2983 batches | lr 0.000164 | ms/batch 24.60 | loss  5.01 | ppl   149.16\n",
            "| epoch  32 |   800/ 2983 batches | lr 0.000164 | ms/batch 24.53 | loss  5.03 | ppl   152.80\n",
            "| epoch  32 |  1000/ 2983 batches | lr 0.000164 | ms/batch 24.53 | loss  5.05 | ppl   155.34\n",
            "| epoch  32 |  1200/ 2983 batches | lr 0.000163 | ms/batch 24.59 | loss  5.05 | ppl   156.31\n",
            "| epoch  32 |  1400/ 2983 batches | lr 0.000163 | ms/batch 24.54 | loss  5.08 | ppl   160.30\n",
            "| epoch  32 |  1600/ 2983 batches | lr 0.000163 | ms/batch 24.60 | loss  5.14 | ppl   170.67\n",
            "| epoch  32 |  1800/ 2983 batches | lr 0.000163 | ms/batch 24.52 | loss  5.05 | ppl   156.00\n",
            "| epoch  32 |  2000/ 2983 batches | lr 0.000163 | ms/batch 24.57 | loss  5.09 | ppl   162.33\n",
            "| epoch  32 |  2200/ 2983 batches | lr 0.000162 | ms/batch 24.61 | loss  5.01 | ppl   149.90\n",
            "| epoch  32 |  2400/ 2983 batches | lr 0.000162 | ms/batch 24.48 | loss  5.04 | ppl   154.92\n",
            "| epoch  32 |  2600/ 2983 batches | lr 0.000162 | ms/batch 24.60 | loss  5.08 | ppl   160.67\n",
            "| epoch  32 |  2800/ 2983 batches | lr 0.000162 | ms/batch 24.55 | loss  5.03 | ppl   152.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 75.86s | valid loss  6.41 | valid ppl   609.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000162\n",
            "| epoch  33 |   200/ 2983 batches | lr 0.000162 | ms/batch 24.68 | loss  5.08 | ppl   161.25\n",
            "| epoch  33 |   400/ 2983 batches | lr 0.000161 | ms/batch 24.58 | loss  5.09 | ppl   162.73\n",
            "| epoch  33 |   600/ 2983 batches | lr 0.000161 | ms/batch 24.56 | loss  4.99 | ppl   146.89\n",
            "| epoch  33 |   800/ 2983 batches | lr 0.000161 | ms/batch 24.57 | loss  5.02 | ppl   150.83\n",
            "| epoch  33 |  1000/ 2983 batches | lr 0.000161 | ms/batch 24.58 | loss  5.03 | ppl   153.50\n",
            "| epoch  33 |  1200/ 2983 batches | lr 0.000161 | ms/batch 24.54 | loss  5.04 | ppl   154.20\n",
            "| epoch  33 |  1400/ 2983 batches | lr 0.000161 | ms/batch 24.60 | loss  5.06 | ppl   158.26\n",
            "| epoch  33 |  1600/ 2983 batches | lr 0.000160 | ms/batch 24.63 | loss  5.13 | ppl   168.80\n",
            "| epoch  33 |  1800/ 2983 batches | lr 0.000160 | ms/batch 24.59 | loss  5.04 | ppl   154.38\n",
            "| epoch  33 |  2000/ 2983 batches | lr 0.000160 | ms/batch 24.62 | loss  5.08 | ppl   160.44\n",
            "| epoch  33 |  2200/ 2983 batches | lr 0.000160 | ms/batch 24.55 | loss  5.00 | ppl   148.24\n",
            "| epoch  33 |  2400/ 2983 batches | lr 0.000160 | ms/batch 24.60 | loss  5.03 | ppl   153.20\n",
            "| epoch  33 |  2600/ 2983 batches | lr 0.000160 | ms/batch 24.58 | loss  5.07 | ppl   159.44\n",
            "| epoch  33 |  2800/ 2983 batches | lr 0.000159 | ms/batch 24.58 | loss  5.02 | ppl   151.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 75.96s | valid loss  6.41 | valid ppl   605.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000159\n",
            "| epoch  34 |   200/ 2983 batches | lr 0.000159 | ms/batch 24.67 | loss  5.08 | ppl   160.24\n",
            "| epoch  34 |   400/ 2983 batches | lr 0.000159 | ms/batch 24.58 | loss  5.09 | ppl   161.72\n",
            "| epoch  34 |   600/ 2983 batches | lr 0.000159 | ms/batch 24.62 | loss  4.98 | ppl   146.19\n",
            "| epoch  34 |   800/ 2983 batches | lr 0.000159 | ms/batch 24.76 | loss  5.01 | ppl   150.35\n",
            "| epoch  34 |  1000/ 2983 batches | lr 0.000159 | ms/batch 24.74 | loss  5.03 | ppl   152.95\n",
            "| epoch  34 |  1200/ 2983 batches | lr 0.000158 | ms/batch 24.57 | loss  5.04 | ppl   154.01\n",
            "| epoch  34 |  1400/ 2983 batches | lr 0.000158 | ms/batch 24.58 | loss  5.06 | ppl   157.05\n",
            "| epoch  34 |  1600/ 2983 batches | lr 0.000158 | ms/batch 24.53 | loss  5.13 | ppl   168.28\n",
            "| epoch  34 |  1800/ 2983 batches | lr 0.000158 | ms/batch 24.55 | loss  5.03 | ppl   153.67\n",
            "| epoch  34 |  2000/ 2983 batches | lr 0.000158 | ms/batch 24.56 | loss  5.07 | ppl   159.37\n",
            "| epoch  34 |  2200/ 2983 batches | lr 0.000158 | ms/batch 24.60 | loss  4.99 | ppl   147.51\n",
            "| epoch  34 |  2400/ 2983 batches | lr 0.000157 | ms/batch 24.68 | loss  5.03 | ppl   152.72\n",
            "| epoch  34 |  2600/ 2983 batches | lr 0.000157 | ms/batch 24.55 | loss  5.06 | ppl   158.24\n",
            "| epoch  34 |  2800/ 2983 batches | lr 0.000157 | ms/batch 24.64 | loss  5.01 | ppl   150.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 76.02s | valid loss  6.42 | valid ppl   612.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000157\n",
            "| epoch  35 |   200/ 2983 batches | lr 0.000157 | ms/batch 24.75 | loss  5.07 | ppl   158.85\n",
            "| epoch  35 |   400/ 2983 batches | lr 0.000157 | ms/batch 24.55 | loss  5.08 | ppl   160.51\n",
            "| epoch  35 |   600/ 2983 batches | lr 0.000157 | ms/batch 24.61 | loss  4.97 | ppl   144.71\n",
            "| epoch  35 |   800/ 2983 batches | lr 0.000156 | ms/batch 24.56 | loss  5.00 | ppl   148.66\n",
            "| epoch  35 |  1000/ 2983 batches | lr 0.000156 | ms/batch 24.52 | loss  5.02 | ppl   151.06\n",
            "| epoch  35 |  1200/ 2983 batches | lr 0.000156 | ms/batch 24.58 | loss  5.02 | ppl   152.16\n",
            "| epoch  35 |  1400/ 2983 batches | lr 0.000156 | ms/batch 24.59 | loss  5.04 | ppl   155.05\n",
            "| epoch  35 |  1600/ 2983 batches | lr 0.000156 | ms/batch 24.65 | loss  5.11 | ppl   165.71\n",
            "| epoch  35 |  1800/ 2983 batches | lr 0.000156 | ms/batch 24.62 | loss  5.02 | ppl   151.34\n",
            "| epoch  35 |  2000/ 2983 batches | lr 0.000155 | ms/batch 24.62 | loss  5.06 | ppl   157.77\n",
            "| epoch  35 |  2200/ 2983 batches | lr 0.000155 | ms/batch 24.65 | loss  4.98 | ppl   145.59\n",
            "| epoch  35 |  2400/ 2983 batches | lr 0.000155 | ms/batch 24.62 | loss  5.02 | ppl   150.90\n",
            "| epoch  35 |  2600/ 2983 batches | lr 0.000155 | ms/batch 24.62 | loss  5.05 | ppl   155.82\n",
            "| epoch  35 |  2800/ 2983 batches | lr 0.000155 | ms/batch 24.57 | loss  5.00 | ppl   148.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 76.03s | valid loss  6.42 | valid ppl   611.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000155\n",
            "| epoch  36 |   200/ 2983 batches | lr 0.000155 | ms/batch 24.69 | loss  5.06 | ppl   157.61\n",
            "| epoch  36 |   400/ 2983 batches | lr 0.000154 | ms/batch 24.56 | loss  5.07 | ppl   158.64\n",
            "| epoch  36 |   600/ 2983 batches | lr 0.000154 | ms/batch 24.58 | loss  4.97 | ppl   143.36\n",
            "| epoch  36 |   800/ 2983 batches | lr 0.000154 | ms/batch 24.58 | loss  5.00 | ppl   147.73\n",
            "| epoch  36 |  1000/ 2983 batches | lr 0.000154 | ms/batch 24.54 | loss  5.01 | ppl   149.70\n",
            "| epoch  36 |  1200/ 2983 batches | lr 0.000154 | ms/batch 24.60 | loss  5.01 | ppl   150.40\n",
            "| epoch  36 |  1400/ 2983 batches | lr 0.000154 | ms/batch 24.57 | loss  5.04 | ppl   154.04\n",
            "| epoch  36 |  1600/ 2983 batches | lr 0.000154 | ms/batch 24.59 | loss  5.10 | ppl   164.26\n",
            "| epoch  36 |  1800/ 2983 batches | lr 0.000153 | ms/batch 24.60 | loss  5.01 | ppl   150.43\n",
            "| epoch  36 |  2000/ 2983 batches | lr 0.000153 | ms/batch 24.61 | loss  5.05 | ppl   156.63\n",
            "| epoch  36 |  2200/ 2983 batches | lr 0.000153 | ms/batch 24.58 | loss  4.97 | ppl   144.35\n",
            "| epoch  36 |  2400/ 2983 batches | lr 0.000153 | ms/batch 24.61 | loss  5.01 | ppl   149.30\n",
            "| epoch  36 |  2600/ 2983 batches | lr 0.000153 | ms/batch 24.59 | loss  5.05 | ppl   155.56\n",
            "| epoch  36 |  2800/ 2983 batches | lr 0.000153 | ms/batch 24.55 | loss  5.00 | ppl   147.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 75.99s | valid loss  6.43 | valid ppl   617.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000153\n",
            "| epoch  37 |   200/ 2983 batches | lr 0.000152 | ms/batch 24.71 | loss  5.05 | ppl   155.82\n",
            "| epoch  37 |   400/ 2983 batches | lr 0.000152 | ms/batch 24.64 | loss  5.06 | ppl   157.37\n",
            "| epoch  37 |   600/ 2983 batches | lr 0.000152 | ms/batch 24.54 | loss  4.96 | ppl   142.90\n",
            "| epoch  37 |   800/ 2983 batches | lr 0.000152 | ms/batch 24.54 | loss  4.98 | ppl   146.04\n",
            "| epoch  37 |  1000/ 2983 batches | lr 0.000152 | ms/batch 24.53 | loss  5.00 | ppl   148.93\n",
            "| epoch  37 |  1200/ 2983 batches | lr 0.000152 | ms/batch 24.65 | loss  5.01 | ppl   149.55\n",
            "| epoch  37 |  1400/ 2983 batches | lr 0.000152 | ms/batch 24.62 | loss  5.03 | ppl   152.56\n",
            "| epoch  37 |  1600/ 2983 batches | lr 0.000151 | ms/batch 24.53 | loss  5.09 | ppl   162.74\n",
            "| epoch  37 |  1800/ 2983 batches | lr 0.000151 | ms/batch 24.64 | loss  5.00 | ppl   149.10\n",
            "| epoch  37 |  2000/ 2983 batches | lr 0.000151 | ms/batch 24.57 | loss  5.04 | ppl   154.98\n",
            "| epoch  37 |  2200/ 2983 batches | lr 0.000151 | ms/batch 24.57 | loss  4.96 | ppl   143.30\n",
            "| epoch  37 |  2400/ 2983 batches | lr 0.000151 | ms/batch 24.59 | loss  5.00 | ppl   148.15\n",
            "| epoch  37 |  2600/ 2983 batches | lr 0.000151 | ms/batch 24.59 | loss  5.04 | ppl   153.80\n",
            "| epoch  37 |  2800/ 2983 batches | lr 0.000151 | ms/batch 24.61 | loss  4.98 | ppl   145.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 75.98s | valid loss  6.44 | valid ppl   625.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000150\n",
            "| epoch  38 |   200/ 2983 batches | lr 0.000150 | ms/batch 24.73 | loss  5.04 | ppl   154.77\n",
            "| epoch  38 |   400/ 2983 batches | lr 0.000150 | ms/batch 24.67 | loss  5.05 | ppl   156.25\n",
            "| epoch  38 |   600/ 2983 batches | lr 0.000150 | ms/batch 24.70 | loss  4.95 | ppl   141.06\n",
            "| epoch  38 |   800/ 2983 batches | lr 0.000150 | ms/batch 24.72 | loss  4.97 | ppl   144.75\n",
            "| epoch  38 |  1000/ 2983 batches | lr 0.000150 | ms/batch 24.61 | loss  5.00 | ppl   148.08\n",
            "| epoch  38 |  1200/ 2983 batches | lr 0.000150 | ms/batch 24.58 | loss  5.00 | ppl   148.30\n",
            "| epoch  38 |  1400/ 2983 batches | lr 0.000150 | ms/batch 24.55 | loss  5.02 | ppl   151.55\n",
            "| epoch  38 |  1600/ 2983 batches | lr 0.000149 | ms/batch 24.59 | loss  5.08 | ppl   161.38\n",
            "| epoch  38 |  1800/ 2983 batches | lr 0.000149 | ms/batch 24.58 | loss  4.99 | ppl   147.60\n",
            "| epoch  38 |  2000/ 2983 batches | lr 0.000149 | ms/batch 24.66 | loss  5.03 | ppl   153.33\n",
            "| epoch  38 |  2200/ 2983 batches | lr 0.000149 | ms/batch 24.64 | loss  4.95 | ppl   141.65\n",
            "| epoch  38 |  2400/ 2983 batches | lr 0.000149 | ms/batch 24.51 | loss  4.99 | ppl   146.44\n",
            "| epoch  38 |  2600/ 2983 batches | lr 0.000149 | ms/batch 24.55 | loss  5.03 | ppl   152.30\n",
            "| epoch  38 |  2800/ 2983 batches | lr 0.000149 | ms/batch 24.61 | loss  4.98 | ppl   144.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 76.07s | valid loss  6.44 | valid ppl   623.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000148\n",
            "| epoch  39 |   200/ 2983 batches | lr 0.000148 | ms/batch 24.68 | loss  5.03 | ppl   153.06\n",
            "| epoch  39 |   400/ 2983 batches | lr 0.000148 | ms/batch 24.56 | loss  5.04 | ppl   154.35\n",
            "| epoch  39 |   600/ 2983 batches | lr 0.000148 | ms/batch 24.64 | loss  4.94 | ppl   139.35\n",
            "| epoch  39 |   800/ 2983 batches | lr 0.000148 | ms/batch 24.57 | loss  4.97 | ppl   143.75\n",
            "| epoch  39 |  1000/ 2983 batches | lr 0.000148 | ms/batch 24.60 | loss  4.99 | ppl   146.31\n",
            "| epoch  39 |  1200/ 2983 batches | lr 0.000148 | ms/batch 24.63 | loss  4.99 | ppl   146.71\n",
            "| epoch  39 |  1400/ 2983 batches | lr 0.000148 | ms/batch 24.62 | loss  5.01 | ppl   149.99\n",
            "| epoch  39 |  1600/ 2983 batches | lr 0.000147 | ms/batch 24.66 | loss  5.07 | ppl   159.85\n",
            "| epoch  39 |  1800/ 2983 batches | lr 0.000147 | ms/batch 24.57 | loss  4.98 | ppl   145.56\n",
            "| epoch  39 |  2000/ 2983 batches | lr 0.000147 | ms/batch 24.62 | loss  5.02 | ppl   151.86\n",
            "| epoch  39 |  2200/ 2983 batches | lr 0.000147 | ms/batch 24.61 | loss  4.94 | ppl   140.33\n",
            "| epoch  39 |  2400/ 2983 batches | lr 0.000147 | ms/batch 24.53 | loss  4.98 | ppl   145.29\n",
            "| epoch  39 |  2600/ 2983 batches | lr 0.000147 | ms/batch 24.63 | loss  5.01 | ppl   150.57\n",
            "| epoch  39 |  2800/ 2983 batches | lr 0.000147 | ms/batch 24.56 | loss  4.96 | ppl   142.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 76.01s | valid loss  6.44 | valid ppl   623.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000147\n",
            "| epoch  40 |   200/ 2983 batches | lr 0.000146 | ms/batch 24.68 | loss  5.02 | ppl   151.40\n",
            "| epoch  40 |   400/ 2983 batches | lr 0.000146 | ms/batch 24.58 | loss  5.03 | ppl   152.97\n",
            "| epoch  40 |   600/ 2983 batches | lr 0.000146 | ms/batch 24.61 | loss  4.93 | ppl   138.64\n",
            "| epoch  40 |   800/ 2983 batches | lr 0.000146 | ms/batch 24.60 | loss  4.96 | ppl   142.03\n",
            "| epoch  40 |  1000/ 2983 batches | lr 0.000146 | ms/batch 24.67 | loss  4.98 | ppl   145.00\n",
            "| epoch  40 |  1200/ 2983 batches | lr 0.000146 | ms/batch 24.73 | loss  4.98 | ppl   145.42\n",
            "| epoch  40 |  1400/ 2983 batches | lr 0.000146 | ms/batch 24.69 | loss  5.00 | ppl   148.27\n",
            "| epoch  40 |  1600/ 2983 batches | lr 0.000146 | ms/batch 24.56 | loss  5.07 | ppl   158.44\n",
            "| epoch  40 |  1800/ 2983 batches | lr 0.000145 | ms/batch 24.58 | loss  4.97 | ppl   144.73\n",
            "| epoch  40 |  2000/ 2983 batches | lr 0.000145 | ms/batch 24.64 | loss  5.01 | ppl   150.63\n",
            "| epoch  40 |  2200/ 2983 batches | lr 0.000145 | ms/batch 24.61 | loss  4.93 | ppl   138.95\n",
            "| epoch  40 |  2400/ 2983 batches | lr 0.000145 | ms/batch 24.69 | loss  4.97 | ppl   144.03\n",
            "| epoch  40 |  2600/ 2983 batches | lr 0.000145 | ms/batch 24.57 | loss  5.01 | ppl   149.86\n",
            "| epoch  40 |  2800/ 2983 batches | lr 0.000145 | ms/batch 24.56 | loss  4.96 | ppl   142.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 76.06s | valid loss  6.44 | valid ppl   627.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000145\n",
            "| epoch  41 |   200/ 2983 batches | lr 0.000145 | ms/batch 24.67 | loss  5.02 | ppl   151.30\n",
            "| epoch  41 |   400/ 2983 batches | lr 0.000144 | ms/batch 24.63 | loss  5.02 | ppl   151.91\n",
            "| epoch  41 |   600/ 2983 batches | lr 0.000144 | ms/batch 24.59 | loss  4.92 | ppl   137.35\n",
            "| epoch  41 |   800/ 2983 batches | lr 0.000144 | ms/batch 24.63 | loss  4.95 | ppl   141.06\n",
            "| epoch  41 |  1000/ 2983 batches | lr 0.000144 | ms/batch 24.58 | loss  4.97 | ppl   144.18\n",
            "| epoch  41 |  1200/ 2983 batches | lr 0.000144 | ms/batch 24.62 | loss  4.97 | ppl   144.64\n",
            "| epoch  41 |  1400/ 2983 batches | lr 0.000144 | ms/batch 24.68 | loss  5.00 | ppl   148.22\n",
            "| epoch  41 |  1600/ 2983 batches | lr 0.000144 | ms/batch 24.59 | loss  5.06 | ppl   157.96\n",
            "| epoch  41 |  1800/ 2983 batches | lr 0.000144 | ms/batch 24.66 | loss  4.97 | ppl   144.33\n",
            "| epoch  41 |  2000/ 2983 batches | lr 0.000144 | ms/batch 24.61 | loss  5.01 | ppl   150.04\n",
            "| epoch  41 |  2200/ 2983 batches | lr 0.000143 | ms/batch 24.54 | loss  4.93 | ppl   138.17\n",
            "| epoch  41 |  2400/ 2983 batches | lr 0.000143 | ms/batch 24.61 | loss  4.97 | ppl   143.46\n",
            "| epoch  41 |  2600/ 2983 batches | lr 0.000143 | ms/batch 24.53 | loss  5.00 | ppl   148.71\n",
            "| epoch  41 |  2800/ 2983 batches | lr 0.000143 | ms/batch 24.61 | loss  4.96 | ppl   142.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 76.00s | valid loss  6.45 | valid ppl   632.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000143\n",
            "| epoch  42 |   200/ 2983 batches | lr 0.000143 | ms/batch 24.72 | loss  5.01 | ppl   150.19\n",
            "| epoch  42 |   400/ 2983 batches | lr 0.000143 | ms/batch 24.71 | loss  5.02 | ppl   150.94\n",
            "| epoch  42 |   600/ 2983 batches | lr 0.000143 | ms/batch 24.70 | loss  4.92 | ppl   136.66\n",
            "| epoch  42 |   800/ 2983 batches | lr 0.000142 | ms/batch 24.69 | loss  4.94 | ppl   140.31\n",
            "| epoch  42 |  1000/ 2983 batches | lr 0.000142 | ms/batch 24.72 | loss  4.96 | ppl   143.02\n",
            "| epoch  42 |  1200/ 2983 batches | lr 0.000142 | ms/batch 24.60 | loss  4.96 | ppl   142.99\n",
            "| epoch  42 |  1400/ 2983 batches | lr 0.000142 | ms/batch 24.56 | loss  4.99 | ppl   146.35\n",
            "| epoch  42 |  1600/ 2983 batches | lr 0.000142 | ms/batch 24.54 | loss  5.05 | ppl   156.74\n",
            "| epoch  42 |  1800/ 2983 batches | lr 0.000142 | ms/batch 24.63 | loss  4.97 | ppl   143.83\n",
            "| epoch  42 |  2000/ 2983 batches | lr 0.000142 | ms/batch 24.58 | loss  5.00 | ppl   148.49\n",
            "| epoch  42 |  2200/ 2983 batches | lr 0.000142 | ms/batch 24.62 | loss  4.92 | ppl   137.41\n",
            "| epoch  42 |  2400/ 2983 batches | lr 0.000142 | ms/batch 24.53 | loss  4.96 | ppl   141.98\n",
            "| epoch  42 |  2600/ 2983 batches | lr 0.000141 | ms/batch 24.63 | loss  4.99 | ppl   147.53\n",
            "| epoch  42 |  2800/ 2983 batches | lr 0.000141 | ms/batch 24.60 | loss  4.95 | ppl   140.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 76.09s | valid loss  6.45 | valid ppl   629.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000141\n",
            "| epoch  43 |   200/ 2983 batches | lr 0.000141 | ms/batch 24.75 | loss  5.00 | ppl   148.59\n",
            "| epoch  43 |   400/ 2983 batches | lr 0.000141 | ms/batch 24.56 | loss  5.01 | ppl   150.02\n",
            "| epoch  43 |   600/ 2983 batches | lr 0.000141 | ms/batch 24.58 | loss  4.91 | ppl   135.22\n",
            "| epoch  43 |   800/ 2983 batches | lr 0.000141 | ms/batch 24.52 | loss  4.94 | ppl   139.33\n",
            "| epoch  43 |  1000/ 2983 batches | lr 0.000141 | ms/batch 24.56 | loss  4.95 | ppl   141.54\n",
            "| epoch  43 |  1200/ 2983 batches | lr 0.000141 | ms/batch 24.61 | loss  4.95 | ppl   141.83\n",
            "| epoch  43 |  1400/ 2983 batches | lr 0.000140 | ms/batch 24.60 | loss  4.98 | ppl   145.41\n",
            "| epoch  43 |  1600/ 2983 batches | lr 0.000140 | ms/batch 24.61 | loss  5.04 | ppl   154.96\n",
            "| epoch  43 |  1800/ 2983 batches | lr 0.000140 | ms/batch 24.58 | loss  4.96 | ppl   142.17\n",
            "| epoch  43 |  2000/ 2983 batches | lr 0.000140 | ms/batch 24.56 | loss  4.99 | ppl   147.18\n",
            "| epoch  43 |  2200/ 2983 batches | lr 0.000140 | ms/batch 24.58 | loss  4.91 | ppl   136.27\n",
            "| epoch  43 |  2400/ 2983 batches | lr 0.000140 | ms/batch 24.55 | loss  4.95 | ppl   140.62\n",
            "| epoch  43 |  2600/ 2983 batches | lr 0.000140 | ms/batch 24.56 | loss  4.98 | ppl   146.06\n",
            "| epoch  43 |  2800/ 2983 batches | lr 0.000140 | ms/batch 24.54 | loss  4.94 | ppl   139.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 75.96s | valid loss  6.45 | valid ppl   634.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000140\n",
            "| epoch  44 |   200/ 2983 batches | lr 0.000139 | ms/batch 24.76 | loss  4.99 | ppl   147.15\n",
            "| epoch  44 |   400/ 2983 batches | lr 0.000139 | ms/batch 24.59 | loss  5.00 | ppl   148.73\n",
            "| epoch  44 |   600/ 2983 batches | lr 0.000139 | ms/batch 24.59 | loss  4.90 | ppl   134.32\n",
            "| epoch  44 |   800/ 2983 batches | lr 0.000139 | ms/batch 24.63 | loss  4.93 | ppl   137.96\n",
            "| epoch  44 |  1000/ 2983 batches | lr 0.000139 | ms/batch 24.59 | loss  4.95 | ppl   141.05\n",
            "| epoch  44 |  1200/ 2983 batches | lr 0.000139 | ms/batch 24.60 | loss  4.95 | ppl   141.22\n",
            "| epoch  44 |  1400/ 2983 batches | lr 0.000139 | ms/batch 24.60 | loss  4.97 | ppl   144.05\n",
            "| epoch  44 |  1600/ 2983 batches | lr 0.000139 | ms/batch 24.59 | loss  5.03 | ppl   153.40\n",
            "| epoch  44 |  1800/ 2983 batches | lr 0.000139 | ms/batch 24.56 | loss  4.94 | ppl   140.44\n",
            "| epoch  44 |  2000/ 2983 batches | lr 0.000139 | ms/batch 24.59 | loss  4.98 | ppl   145.93\n",
            "| epoch  44 |  2200/ 2983 batches | lr 0.000138 | ms/batch 24.55 | loss  4.90 | ppl   134.72\n",
            "| epoch  44 |  2400/ 2983 batches | lr 0.000138 | ms/batch 24.56 | loss  4.94 | ppl   139.65\n",
            "| epoch  44 |  2600/ 2983 batches | lr 0.000138 | ms/batch 24.61 | loss  4.97 | ppl   144.45\n",
            "| epoch  44 |  2800/ 2983 batches | lr 0.000138 | ms/batch 24.60 | loss  4.93 | ppl   137.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 76.00s | valid loss  6.46 | valid ppl   641.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000138\n",
            "| epoch  45 |   200/ 2983 batches | lr 0.000138 | ms/batch 24.76 | loss  4.99 | ppl   146.40\n",
            "| epoch  45 |   400/ 2983 batches | lr 0.000138 | ms/batch 24.61 | loss  4.99 | ppl   147.62\n",
            "| epoch  45 |   600/ 2983 batches | lr 0.000138 | ms/batch 24.54 | loss  4.89 | ppl   133.37\n",
            "| epoch  45 |   800/ 2983 batches | lr 0.000138 | ms/batch 24.59 | loss  4.92 | ppl   137.63\n",
            "| epoch  45 |  1000/ 2983 batches | lr 0.000137 | ms/batch 24.58 | loss  4.94 | ppl   139.63\n",
            "| epoch  45 |  1200/ 2983 batches | lr 0.000137 | ms/batch 24.58 | loss  4.94 | ppl   139.66\n",
            "| epoch  45 |  1400/ 2983 batches | lr 0.000137 | ms/batch 24.58 | loss  4.96 | ppl   142.53\n",
            "| epoch  45 |  1600/ 2983 batches | lr 0.000137 | ms/batch 24.54 | loss  5.03 | ppl   152.19\n",
            "| epoch  45 |  1800/ 2983 batches | lr 0.000137 | ms/batch 24.59 | loss  4.94 | ppl   139.41\n",
            "| epoch  45 |  2000/ 2983 batches | lr 0.000137 | ms/batch 24.59 | loss  4.98 | ppl   145.21\n",
            "| epoch  45 |  2200/ 2983 batches | lr 0.000137 | ms/batch 24.59 | loss  4.90 | ppl   133.90\n",
            "| epoch  45 |  2400/ 2983 batches | lr 0.000137 | ms/batch 24.55 | loss  4.93 | ppl   138.25\n",
            "| epoch  45 |  2600/ 2983 batches | lr 0.000137 | ms/batch 24.59 | loss  4.97 | ppl   143.81\n",
            "| epoch  45 |  2800/ 2983 batches | lr 0.000137 | ms/batch 24.59 | loss  4.92 | ppl   136.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 76.00s | valid loss  6.46 | valid ppl   639.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000136\n",
            "| epoch  46 |   200/ 2983 batches | lr 0.000136 | ms/batch 24.73 | loss  4.98 | ppl   144.83\n",
            "| epoch  46 |   400/ 2983 batches | lr 0.000136 | ms/batch 24.68 | loss  4.99 | ppl   146.25\n",
            "| epoch  46 |   600/ 2983 batches | lr 0.000136 | ms/batch 24.77 | loss  4.89 | ppl   132.43\n",
            "| epoch  46 |   800/ 2983 batches | lr 0.000136 | ms/batch 24.71 | loss  4.91 | ppl   136.28\n",
            "| epoch  46 |  1000/ 2983 batches | lr 0.000136 | ms/batch 24.60 | loss  4.93 | ppl   138.71\n",
            "| epoch  46 |  1200/ 2983 batches | lr 0.000136 | ms/batch 24.61 | loss  4.93 | ppl   138.83\n",
            "| epoch  46 |  1400/ 2983 batches | lr 0.000136 | ms/batch 24.59 | loss  4.95 | ppl   141.63\n",
            "| epoch  46 |  1600/ 2983 batches | lr 0.000136 | ms/batch 24.62 | loss  5.01 | ppl   150.64\n",
            "| epoch  46 |  1800/ 2983 batches | lr 0.000136 | ms/batch 24.55 | loss  4.93 | ppl   138.34\n",
            "| epoch  46 |  2000/ 2983 batches | lr 0.000135 | ms/batch 24.62 | loss  4.97 | ppl   143.78\n",
            "| epoch  46 |  2200/ 2983 batches | lr 0.000135 | ms/batch 24.60 | loss  4.89 | ppl   133.02\n",
            "| epoch  46 |  2400/ 2983 batches | lr 0.000135 | ms/batch 24.57 | loss  4.92 | ppl   137.27\n",
            "| epoch  46 |  2600/ 2983 batches | lr 0.000135 | ms/batch 24.58 | loss  4.96 | ppl   142.08\n",
            "| epoch  46 |  2800/ 2983 batches | lr 0.000135 | ms/batch 24.58 | loss  4.91 | ppl   136.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 76.12s | valid loss  6.47 | valid ppl   647.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000135\n",
            "| epoch  47 |   200/ 2983 batches | lr 0.000135 | ms/batch 24.70 | loss  4.97 | ppl   143.79\n",
            "| epoch  47 |   400/ 2983 batches | lr 0.000135 | ms/batch 24.55 | loss  4.98 | ppl   145.22\n",
            "| epoch  47 |   600/ 2983 batches | lr 0.000135 | ms/batch 24.57 | loss  4.88 | ppl   131.21\n",
            "| epoch  47 |   800/ 2983 batches | lr 0.000135 | ms/batch 24.54 | loss  4.91 | ppl   135.59\n",
            "| epoch  47 |  1000/ 2983 batches | lr 0.000134 | ms/batch 24.57 | loss  4.93 | ppl   138.05\n",
            "| epoch  47 |  1200/ 2983 batches | lr 0.000134 | ms/batch 24.59 | loss  4.93 | ppl   137.93\n",
            "| epoch  47 |  1400/ 2983 batches | lr 0.000134 | ms/batch 24.65 | loss  4.95 | ppl   141.26\n",
            "| epoch  47 |  1600/ 2983 batches | lr 0.000134 | ms/batch 24.58 | loss  5.01 | ppl   149.42\n",
            "| epoch  47 |  1800/ 2983 batches | lr 0.000134 | ms/batch 24.70 | loss  4.92 | ppl   137.23\n",
            "| epoch  47 |  2000/ 2983 batches | lr 0.000134 | ms/batch 24.59 | loss  4.96 | ppl   143.06\n",
            "| epoch  47 |  2200/ 2983 batches | lr 0.000134 | ms/batch 24.50 | loss  4.88 | ppl   132.04\n",
            "| epoch  47 |  2400/ 2983 batches | lr 0.000134 | ms/batch 24.61 | loss  4.92 | ppl   136.46\n",
            "| epoch  47 |  2600/ 2983 batches | lr 0.000134 | ms/batch 24.60 | loss  4.95 | ppl   141.74\n",
            "| epoch  47 |  2800/ 2983 batches | lr 0.000134 | ms/batch 24.56 | loss  4.91 | ppl   135.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 75.99s | valid loss  6.46 | valid ppl   642.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000134\n",
            "| epoch  48 |   200/ 2983 batches | lr 0.000133 | ms/batch 24.67 | loss  4.96 | ppl   143.12\n",
            "| epoch  48 |   400/ 2983 batches | lr 0.000133 | ms/batch 24.59 | loss  4.97 | ppl   144.50\n",
            "| epoch  48 |   600/ 2983 batches | lr 0.000133 | ms/batch 24.54 | loss  4.88 | ppl   131.07\n",
            "| epoch  48 |   800/ 2983 batches | lr 0.000133 | ms/batch 24.63 | loss  4.90 | ppl   134.11\n",
            "| epoch  48 |  1000/ 2983 batches | lr 0.000133 | ms/batch 24.55 | loss  4.92 | ppl   136.78\n",
            "| epoch  48 |  1200/ 2983 batches | lr 0.000133 | ms/batch 24.57 | loss  4.92 | ppl   136.89\n",
            "| epoch  48 |  1400/ 2983 batches | lr 0.000133 | ms/batch 24.50 | loss  4.94 | ppl   139.73\n",
            "| epoch  48 |  1600/ 2983 batches | lr 0.000133 | ms/batch 24.55 | loss  5.00 | ppl   149.15\n",
            "| epoch  48 |  1800/ 2983 batches | lr 0.000133 | ms/batch 24.61 | loss  4.92 | ppl   136.71\n",
            "| epoch  48 |  2000/ 2983 batches | lr 0.000133 | ms/batch 24.56 | loss  4.96 | ppl   142.00\n",
            "| epoch  48 |  2200/ 2983 batches | lr 0.000132 | ms/batch 24.67 | loss  4.88 | ppl   131.26\n",
            "| epoch  48 |  2400/ 2983 batches | lr 0.000132 | ms/batch 24.61 | loss  4.91 | ppl   135.34\n",
            "| epoch  48 |  2600/ 2983 batches | lr 0.000132 | ms/batch 24.54 | loss  4.95 | ppl   140.61\n",
            "| epoch  48 |  2800/ 2983 batches | lr 0.000132 | ms/batch 24.57 | loss  4.90 | ppl   134.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 75.93s | valid loss  6.47 | valid ppl   646.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000132\n",
            "| epoch  49 |   200/ 2983 batches | lr 0.000132 | ms/batch 24.77 | loss  4.95 | ppl   141.45\n",
            "| epoch  49 |   400/ 2983 batches | lr 0.000132 | ms/batch 24.62 | loss  4.97 | ppl   143.49\n",
            "| epoch  49 |   600/ 2983 batches | lr 0.000132 | ms/batch 24.61 | loss  4.86 | ppl   129.66\n",
            "| epoch  49 |   800/ 2983 batches | lr 0.000132 | ms/batch 24.56 | loss  4.90 | ppl   133.62\n",
            "| epoch  49 |  1000/ 2983 batches | lr 0.000132 | ms/batch 24.57 | loss  4.91 | ppl   136.25\n",
            "| epoch  49 |  1200/ 2983 batches | lr 0.000132 | ms/batch 24.64 | loss  4.91 | ppl   136.29\n",
            "| epoch  49 |  1400/ 2983 batches | lr 0.000131 | ms/batch 24.54 | loss  4.94 | ppl   139.19\n",
            "| epoch  49 |  1600/ 2983 batches | lr 0.000131 | ms/batch 24.63 | loss  4.99 | ppl   147.50\n",
            "| epoch  49 |  1800/ 2983 batches | lr 0.000131 | ms/batch 24.57 | loss  4.91 | ppl   135.49\n",
            "| epoch  49 |  2000/ 2983 batches | lr 0.000131 | ms/batch 24.54 | loss  4.95 | ppl   141.75\n",
            "| epoch  49 |  2200/ 2983 batches | lr 0.000131 | ms/batch 24.68 | loss  4.87 | ppl   130.37\n",
            "| epoch  49 |  2400/ 2983 batches | lr 0.000131 | ms/batch 24.56 | loss  4.90 | ppl   134.54\n",
            "| epoch  49 |  2600/ 2983 batches | lr 0.000131 | ms/batch 24.68 | loss  4.94 | ppl   139.81\n",
            "| epoch  49 |  2800/ 2983 batches | lr 0.000131 | ms/batch 24.66 | loss  4.90 | ppl   133.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 76.07s | valid loss  6.48 | valid ppl   650.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000131\n",
            "| epoch  50 |   200/ 2983 batches | lr 0.000131 | ms/batch 24.82 | loss  4.95 | ppl   141.18\n",
            "| epoch  50 |   400/ 2983 batches | lr 0.000131 | ms/batch 24.74 | loss  4.96 | ppl   142.43\n",
            "| epoch  50 |   600/ 2983 batches | lr 0.000130 | ms/batch 24.79 | loss  4.86 | ppl   128.91\n",
            "| epoch  50 |   800/ 2983 batches | lr 0.000130 | ms/batch 24.50 | loss  4.89 | ppl   132.30\n",
            "| epoch  50 |  1000/ 2983 batches | lr 0.000130 | ms/batch 24.61 | loss  4.91 | ppl   135.32\n",
            "| epoch  50 |  1200/ 2983 batches | lr 0.000130 | ms/batch 24.59 | loss  4.91 | ppl   135.43\n",
            "| epoch  50 |  1400/ 2983 batches | lr 0.000130 | ms/batch 24.55 | loss  4.93 | ppl   138.61\n",
            "| epoch  50 |  1600/ 2983 batches | lr 0.000130 | ms/batch 24.61 | loss  4.99 | ppl   146.91\n",
            "| epoch  50 |  1800/ 2983 batches | lr 0.000130 | ms/batch 24.56 | loss  4.91 | ppl   135.20\n",
            "| epoch  50 |  2000/ 2983 batches | lr 0.000130 | ms/batch 24.59 | loss  4.95 | ppl   140.61\n",
            "| epoch  50 |  2200/ 2983 batches | lr 0.000130 | ms/batch 24.56 | loss  4.86 | ppl   129.33\n",
            "| epoch  50 |  2400/ 2983 batches | lr 0.000130 | ms/batch 24.50 | loss  4.89 | ppl   133.40\n",
            "| epoch  50 |  2600/ 2983 batches | lr 0.000130 | ms/batch 24.66 | loss  4.93 | ppl   138.70\n",
            "| epoch  50 |  2800/ 2983 batches | lr 0.000130 | ms/batch 24.63 | loss  4.89 | ppl   132.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 76.06s | valid loss  6.48 | valid ppl   651.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000129\n",
            "| epoch  51 |   200/ 2983 batches | lr 0.000129 | ms/batch 24.69 | loss  4.94 | ppl   140.20\n",
            "| epoch  51 |   400/ 2983 batches | lr 0.000129 | ms/batch 24.66 | loss  4.95 | ppl   141.75\n",
            "| epoch  51 |   600/ 2983 batches | lr 0.000129 | ms/batch 24.55 | loss  4.85 | ppl   127.88\n",
            "| epoch  51 |   800/ 2983 batches | lr 0.000129 | ms/batch 24.62 | loss  4.88 | ppl   131.02\n",
            "| epoch  51 |  1000/ 2983 batches | lr 0.000129 | ms/batch 24.57 | loss  4.90 | ppl   133.73\n",
            "| epoch  51 |  1200/ 2983 batches | lr 0.000129 | ms/batch 24.59 | loss  4.90 | ppl   134.44\n",
            "| epoch  51 |  1400/ 2983 batches | lr 0.000129 | ms/batch 24.66 | loss  4.92 | ppl   137.27\n",
            "| epoch  51 |  1600/ 2983 batches | lr 0.000129 | ms/batch 24.58 | loss  4.98 | ppl   145.60\n",
            "| epoch  51 |  1800/ 2983 batches | lr 0.000129 | ms/batch 24.56 | loss  4.90 | ppl   134.32\n",
            "| epoch  51 |  2000/ 2983 batches | lr 0.000129 | ms/batch 24.65 | loss  4.94 | ppl   139.22\n",
            "| epoch  51 |  2200/ 2983 batches | lr 0.000129 | ms/batch 24.53 | loss  4.86 | ppl   128.66\n",
            "| epoch  51 |  2400/ 2983 batches | lr 0.000128 | ms/batch 24.56 | loss  4.89 | ppl   132.66\n",
            "| epoch  51 |  2600/ 2983 batches | lr 0.000128 | ms/batch 24.56 | loss  4.92 | ppl   137.57\n",
            "| epoch  51 |  2800/ 2983 batches | lr 0.000128 | ms/batch 24.58 | loss  4.88 | ppl   132.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 76.01s | valid loss  6.48 | valid ppl   653.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000128\n",
            "| epoch  52 |   200/ 2983 batches | lr 0.000128 | ms/batch 24.67 | loss  4.94 | ppl   139.16\n",
            "| epoch  52 |   400/ 2983 batches | lr 0.000128 | ms/batch 24.57 | loss  4.95 | ppl   140.74\n",
            "| epoch  52 |   600/ 2983 batches | lr 0.000128 | ms/batch 24.49 | loss  4.85 | ppl   127.19\n",
            "| epoch  52 |   800/ 2983 batches | lr 0.000128 | ms/batch 24.59 | loss  4.88 | ppl   131.13\n",
            "| epoch  52 |  1000/ 2983 batches | lr 0.000128 | ms/batch 24.57 | loss  4.89 | ppl   133.33\n",
            "| epoch  52 |  1200/ 2983 batches | lr 0.000128 | ms/batch 24.60 | loss  4.90 | ppl   133.89\n",
            "| epoch  52 |  1400/ 2983 batches | lr 0.000128 | ms/batch 24.60 | loss  4.92 | ppl   137.09\n",
            "| epoch  52 |  1600/ 2983 batches | lr 0.000128 | ms/batch 24.54 | loss  4.97 | ppl   144.74\n",
            "| epoch  52 |  1800/ 2983 batches | lr 0.000127 | ms/batch 24.63 | loss  4.89 | ppl   133.25\n",
            "| epoch  52 |  2000/ 2983 batches | lr 0.000127 | ms/batch 24.55 | loss  4.93 | ppl   138.64\n",
            "| epoch  52 |  2200/ 2983 batches | lr 0.000127 | ms/batch 24.61 | loss  4.85 | ppl   127.97\n",
            "| epoch  52 |  2400/ 2983 batches | lr 0.000127 | ms/batch 24.61 | loss  4.88 | ppl   132.27\n",
            "| epoch  52 |  2600/ 2983 batches | lr 0.000127 | ms/batch 24.55 | loss  4.92 | ppl   137.22\n",
            "| epoch  52 |  2800/ 2983 batches | lr 0.000127 | ms/batch 24.61 | loss  4.88 | ppl   131.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 75.93s | valid loss  6.49 | valid ppl   655.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000127\n",
            "| epoch  53 |   200/ 2983 batches | lr 0.000127 | ms/batch 24.69 | loss  4.93 | ppl   138.53\n",
            "| epoch  53 |   400/ 2983 batches | lr 0.000127 | ms/batch 24.66 | loss  4.94 | ppl   139.77\n",
            "| epoch  53 |   600/ 2983 batches | lr 0.000127 | ms/batch 24.64 | loss  4.84 | ppl   126.53\n",
            "| epoch  53 |   800/ 2983 batches | lr 0.000127 | ms/batch 24.55 | loss  4.87 | ppl   129.89\n",
            "| epoch  53 |  1000/ 2983 batches | lr 0.000127 | ms/batch 24.51 | loss  4.89 | ppl   132.71\n",
            "| epoch  53 |  1200/ 2983 batches | lr 0.000126 | ms/batch 24.57 | loss  4.89 | ppl   132.80\n",
            "| epoch  53 |  1400/ 2983 batches | lr 0.000126 | ms/batch 24.59 | loss  4.91 | ppl   135.83\n",
            "| epoch  53 |  1600/ 2983 batches | lr 0.000126 | ms/batch 24.63 | loss  4.97 | ppl   144.68\n",
            "| epoch  53 |  1800/ 2983 batches | lr 0.000126 | ms/batch 24.53 | loss  4.88 | ppl   132.28\n",
            "| epoch  53 |  2000/ 2983 batches | lr 0.000126 | ms/batch 24.61 | loss  4.92 | ppl   137.47\n",
            "| epoch  53 |  2200/ 2983 batches | lr 0.000126 | ms/batch 24.58 | loss  4.85 | ppl   127.16\n",
            "| epoch  53 |  2400/ 2983 batches | lr 0.000126 | ms/batch 24.53 | loss  4.88 | ppl   131.49\n",
            "| epoch  53 |  2600/ 2983 batches | lr 0.000126 | ms/batch 24.61 | loss  4.91 | ppl   136.31\n",
            "| epoch  53 |  2800/ 2983 batches | lr 0.000126 | ms/batch 24.53 | loss  4.87 | ppl   130.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 75.98s | valid loss  6.48 | valid ppl   652.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000126\n",
            "| epoch  54 |   200/ 2983 batches | lr 0.000126 | ms/batch 24.93 | loss  4.93 | ppl   137.75\n",
            "| epoch  54 |   400/ 2983 batches | lr 0.000126 | ms/batch 24.67 | loss  4.94 | ppl   139.42\n",
            "| epoch  54 |   600/ 2983 batches | lr 0.000125 | ms/batch 24.62 | loss  4.83 | ppl   125.69\n",
            "| epoch  54 |   800/ 2983 batches | lr 0.000125 | ms/batch 24.58 | loss  4.86 | ppl   129.19\n",
            "| epoch  54 |  1000/ 2983 batches | lr 0.000125 | ms/batch 24.60 | loss  4.88 | ppl   131.41\n",
            "| epoch  54 |  1200/ 2983 batches | lr 0.000125 | ms/batch 24.57 | loss  4.88 | ppl   132.02\n",
            "| epoch  54 |  1400/ 2983 batches | lr 0.000125 | ms/batch 24.60 | loss  4.91 | ppl   135.18\n",
            "| epoch  54 |  1600/ 2983 batches | lr 0.000125 | ms/batch 24.53 | loss  4.96 | ppl   143.01\n",
            "| epoch  54 |  1800/ 2983 batches | lr 0.000125 | ms/batch 24.55 | loss  4.88 | ppl   131.17\n",
            "| epoch  54 |  2000/ 2983 batches | lr 0.000125 | ms/batch 24.57 | loss  4.92 | ppl   136.89\n",
            "| epoch  54 |  2200/ 2983 batches | lr 0.000125 | ms/batch 24.56 | loss  4.84 | ppl   126.28\n",
            "| epoch  54 |  2400/ 2983 batches | lr 0.000125 | ms/batch 24.53 | loss  4.87 | ppl   130.32\n",
            "| epoch  54 |  2600/ 2983 batches | lr 0.000125 | ms/batch 24.57 | loss  4.91 | ppl   135.37\n",
            "| epoch  54 |  2800/ 2983 batches | lr 0.000125 | ms/batch 24.54 | loss  4.86 | ppl   128.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 76.01s | valid loss  6.50 | valid ppl   666.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  55 |   200/ 2983 batches | lr 0.000124 | ms/batch 24.67 | loss  4.92 | ppl   136.62\n",
            "| epoch  55 |   400/ 2983 batches | lr 0.000124 | ms/batch 24.56 | loss  4.93 | ppl   137.85\n",
            "| epoch  55 |   600/ 2983 batches | lr 0.000124 | ms/batch 24.62 | loss  4.83 | ppl   124.93\n",
            "| epoch  55 |   800/ 2983 batches | lr 0.000124 | ms/batch 24.61 | loss  4.86 | ppl   128.53\n",
            "| epoch  55 |  1000/ 2983 batches | lr 0.000124 | ms/batch 24.63 | loss  4.87 | ppl   130.17\n",
            "| epoch  55 |  1200/ 2983 batches | lr 0.000124 | ms/batch 24.57 | loss  4.87 | ppl   130.84\n",
            "| epoch  55 |  1400/ 2983 batches | lr 0.000124 | ms/batch 24.56 | loss  4.90 | ppl   133.67\n",
            "| epoch  55 |  1600/ 2983 batches | lr 0.000124 | ms/batch 24.61 | loss  4.95 | ppl   141.35\n",
            "| epoch  55 |  1800/ 2983 batches | lr 0.000124 | ms/batch 24.54 | loss  4.87 | ppl   130.46\n",
            "| epoch  55 |  2000/ 2983 batches | lr 0.000124 | ms/batch 24.58 | loss  4.91 | ppl   135.39\n",
            "| epoch  55 |  2200/ 2983 batches | lr 0.000124 | ms/batch 24.55 | loss  4.83 | ppl   124.79\n",
            "| epoch  55 |  2400/ 2983 batches | lr 0.000124 | ms/batch 24.54 | loss  4.86 | ppl   129.24\n",
            "| epoch  55 |  2600/ 2983 batches | lr 0.000124 | ms/batch 24.62 | loss  4.90 | ppl   134.27\n",
            "| epoch  55 |  2800/ 2983 batches | lr 0.000123 | ms/batch 24.53 | loss  4.86 | ppl   128.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 75.95s | valid loss  6.50 | valid ppl   663.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000123\n",
            "| epoch  56 |   200/ 2983 batches | lr 0.000123 | ms/batch 24.74 | loss  4.91 | ppl   135.60\n",
            "| epoch  56 |   400/ 2983 batches | lr 0.000123 | ms/batch 24.54 | loss  4.92 | ppl   136.86\n",
            "| epoch  56 |   600/ 2983 batches | lr 0.000123 | ms/batch 24.63 | loss  4.82 | ppl   123.35\n",
            "| epoch  56 |   800/ 2983 batches | lr 0.000123 | ms/batch 24.59 | loss  4.85 | ppl   127.22\n",
            "| epoch  56 |  1000/ 2983 batches | lr 0.000123 | ms/batch 24.53 | loss  4.86 | ppl   129.61\n",
            "| epoch  56 |  1200/ 2983 batches | lr 0.000123 | ms/batch 24.60 | loss  4.87 | ppl   130.45\n",
            "| epoch  56 |  1400/ 2983 batches | lr 0.000123 | ms/batch 24.55 | loss  4.89 | ppl   133.08\n",
            "| epoch  56 |  1600/ 2983 batches | lr 0.000123 | ms/batch 24.55 | loss  4.95 | ppl   140.65\n",
            "| epoch  56 |  1800/ 2983 batches | lr 0.000123 | ms/batch 24.53 | loss  4.87 | ppl   129.68\n",
            "| epoch  56 |  2000/ 2983 batches | lr 0.000123 | ms/batch 24.57 | loss  4.90 | ppl   134.75\n",
            "| epoch  56 |  2200/ 2983 batches | lr 0.000123 | ms/batch 24.55 | loss  4.83 | ppl   124.76\n",
            "| epoch  56 |  2400/ 2983 batches | lr 0.000123 | ms/batch 24.52 | loss  4.86 | ppl   128.73\n",
            "| epoch  56 |  2600/ 2983 batches | lr 0.000122 | ms/batch 24.56 | loss  4.89 | ppl   133.47\n",
            "| epoch  56 |  2800/ 2983 batches | lr 0.000122 | ms/batch 24.61 | loss  4.85 | ppl   127.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 75.91s | valid loss  6.50 | valid ppl   665.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000122\n",
            "| epoch  57 |   200/ 2983 batches | lr 0.000122 | ms/batch 24.69 | loss  4.90 | ppl   134.71\n",
            "| epoch  57 |   400/ 2983 batches | lr 0.000122 | ms/batch 24.52 | loss  4.92 | ppl   136.52\n",
            "| epoch  57 |   600/ 2983 batches | lr 0.000122 | ms/batch 24.62 | loss  4.82 | ppl   123.45\n",
            "| epoch  57 |   800/ 2983 batches | lr 0.000122 | ms/batch 24.54 | loss  4.84 | ppl   126.62\n",
            "| epoch  57 |  1000/ 2983 batches | lr 0.000122 | ms/batch 24.62 | loss  4.86 | ppl   129.24\n",
            "| epoch  57 |  1200/ 2983 batches | lr 0.000122 | ms/batch 24.56 | loss  4.87 | ppl   129.70\n",
            "| epoch  57 |  1400/ 2983 batches | lr 0.000122 | ms/batch 24.58 | loss  4.89 | ppl   132.30\n",
            "| epoch  57 |  1600/ 2983 batches | lr 0.000122 | ms/batch 24.63 | loss  4.94 | ppl   140.26\n",
            "| epoch  57 |  1800/ 2983 batches | lr 0.000122 | ms/batch 24.59 | loss  4.86 | ppl   128.88\n",
            "| epoch  57 |  2000/ 2983 batches | lr 0.000122 | ms/batch 24.57 | loss  4.90 | ppl   134.56\n",
            "| epoch  57 |  2200/ 2983 batches | lr 0.000122 | ms/batch 24.59 | loss  4.82 | ppl   123.90\n",
            "| epoch  57 |  2400/ 2983 batches | lr 0.000121 | ms/batch 24.60 | loss  4.85 | ppl   127.62\n",
            "| epoch  57 |  2600/ 2983 batches | lr 0.000121 | ms/batch 24.66 | loss  4.89 | ppl   132.58\n",
            "| epoch  57 |  2800/ 2983 batches | lr 0.000121 | ms/batch 24.57 | loss  4.85 | ppl   127.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 76.05s | valid loss  6.50 | valid ppl   666.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000121\n",
            "| epoch  58 |   200/ 2983 batches | lr 0.000121 | ms/batch 24.83 | loss  4.90 | ppl   134.06\n",
            "| epoch  58 |   400/ 2983 batches | lr 0.000121 | ms/batch 24.72 | loss  4.91 | ppl   135.51\n",
            "| epoch  58 |   600/ 2983 batches | lr 0.000121 | ms/batch 24.56 | loss  4.81 | ppl   122.60\n",
            "| epoch  58 |   800/ 2983 batches | lr 0.000121 | ms/batch 24.66 | loss  4.83 | ppl   125.74\n",
            "| epoch  58 |  1000/ 2983 batches | lr 0.000121 | ms/batch 24.63 | loss  4.85 | ppl   128.32\n",
            "| epoch  58 |  1200/ 2983 batches | lr 0.000121 | ms/batch 24.58 | loss  4.85 | ppl   128.05\n",
            "| epoch  58 |  1400/ 2983 batches | lr 0.000121 | ms/batch 24.64 | loss  4.88 | ppl   131.09\n",
            "| epoch  58 |  1600/ 2983 batches | lr 0.000121 | ms/batch 24.57 | loss  4.94 | ppl   139.72\n",
            "| epoch  58 |  1800/ 2983 batches | lr 0.000121 | ms/batch 24.62 | loss  4.85 | ppl   128.21\n",
            "| epoch  58 |  2000/ 2983 batches | lr 0.000121 | ms/batch 24.64 | loss  4.89 | ppl   133.54\n",
            "| epoch  58 |  2200/ 2983 batches | lr 0.000120 | ms/batch 24.58 | loss  4.81 | ppl   123.23\n",
            "| epoch  58 |  2400/ 2983 batches | lr 0.000120 | ms/batch 24.58 | loss  4.84 | ppl   126.99\n",
            "| epoch  58 |  2600/ 2983 batches | lr 0.000120 | ms/batch 24.50 | loss  4.88 | ppl   131.96\n",
            "| epoch  58 |  2800/ 2983 batches | lr 0.000120 | ms/batch 24.60 | loss  4.84 | ppl   126.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 76.05s | valid loss  6.52 | valid ppl   675.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000120\n",
            "| epoch  59 |   200/ 2983 batches | lr 0.000120 | ms/batch 24.73 | loss  4.89 | ppl   132.69\n",
            "| epoch  59 |   400/ 2983 batches | lr 0.000120 | ms/batch 24.57 | loss  4.90 | ppl   134.37\n",
            "| epoch  59 |   600/ 2983 batches | lr 0.000120 | ms/batch 24.60 | loss  4.81 | ppl   122.32\n",
            "| epoch  59 |   800/ 2983 batches | lr 0.000120 | ms/batch 24.64 | loss  4.83 | ppl   125.47\n",
            "| epoch  59 |  1000/ 2983 batches | lr 0.000120 | ms/batch 24.60 | loss  4.85 | ppl   128.30\n",
            "| epoch  59 |  1200/ 2983 batches | lr 0.000120 | ms/batch 24.65 | loss  4.85 | ppl   128.31\n",
            "| epoch  59 |  1400/ 2983 batches | lr 0.000120 | ms/batch 24.81 | loss  4.87 | ppl   130.71\n",
            "| epoch  59 |  1600/ 2983 batches | lr 0.000120 | ms/batch 24.87 | loss  4.94 | ppl   139.22\n",
            "| epoch  59 |  1800/ 2983 batches | lr 0.000120 | ms/batch 24.83 | loss  4.85 | ppl   127.55\n",
            "| epoch  59 |  2000/ 2983 batches | lr 0.000119 | ms/batch 24.77 | loss  4.89 | ppl   132.90\n",
            "| epoch  59 |  2200/ 2983 batches | lr 0.000119 | ms/batch 24.87 | loss  4.81 | ppl   123.01\n",
            "| epoch  59 |  2400/ 2983 batches | lr 0.000119 | ms/batch 24.75 | loss  4.84 | ppl   126.76\n",
            "| epoch  59 |  2600/ 2983 batches | lr 0.000119 | ms/batch 24.77 | loss  4.88 | ppl   131.93\n",
            "| epoch  59 |  2800/ 2983 batches | lr 0.000119 | ms/batch 24.84 | loss  4.83 | ppl   125.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 76.40s | valid loss  6.51 | valid ppl   670.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000119\n",
            "| epoch  60 |   200/ 2983 batches | lr 0.000119 | ms/batch 24.96 | loss  4.89 | ppl   132.55\n",
            "| epoch  60 |   400/ 2983 batches | lr 0.000119 | ms/batch 24.71 | loss  4.90 | ppl   134.01\n",
            "| epoch  60 |   600/ 2983 batches | lr 0.000119 | ms/batch 24.71 | loss  4.80 | ppl   121.80\n",
            "| epoch  60 |   800/ 2983 batches | lr 0.000119 | ms/batch 24.57 | loss  4.82 | ppl   124.46\n",
            "| epoch  60 |  1000/ 2983 batches | lr 0.000119 | ms/batch 24.62 | loss  4.84 | ppl   126.80\n",
            "| epoch  60 |  1200/ 2983 batches | lr 0.000119 | ms/batch 24.66 | loss  4.85 | ppl   127.24\n",
            "| epoch  60 |  1400/ 2983 batches | lr 0.000119 | ms/batch 24.56 | loss  4.87 | ppl   130.05\n",
            "| epoch  60 |  1600/ 2983 batches | lr 0.000119 | ms/batch 24.73 | loss  4.93 | ppl   137.75\n",
            "| epoch  60 |  1800/ 2983 batches | lr 0.000119 | ms/batch 24.62 | loss  4.84 | ppl   127.00\n",
            "| epoch  60 |  2000/ 2983 batches | lr 0.000118 | ms/batch 24.56 | loss  4.88 | ppl   132.01\n",
            "| epoch  60 |  2200/ 2983 batches | lr 0.000118 | ms/batch 24.69 | loss  4.80 | ppl   121.77\n",
            "| epoch  60 |  2400/ 2983 batches | lr 0.000118 | ms/batch 24.64 | loss  4.84 | ppl   126.28\n",
            "| epoch  60 |  2600/ 2983 batches | lr 0.000118 | ms/batch 24.72 | loss  4.88 | ppl   131.16\n",
            "| epoch  60 |  2800/ 2983 batches | lr 0.000118 | ms/batch 24.58 | loss  4.83 | ppl   124.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 76.20s | valid loss  6.52 | valid ppl   679.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000118\n",
            "=========================================================================================\n",
            "| End of training | test loss  6.31 | test ppl   547.72\n",
            "=========================================================================================\n",
            "\n",
            "Generating text...\n",
            "Vocabulary size: 25251\n",
            "Vocabulary size: 25251\n",
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n",
            "in the mid   October <unk> and <unk> 's all   star system . The apparent in this state are\n",
            "sometimes attributed to a <unk> due to their poor health . <eos> <eos> = = = = Taxonomy , <unk>\n",
            "species in the <unk> = = = = = = = = = = = = = = = =\n",
            "= <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> = humans : <unk> =\n",
            "= = <unk> , in Ireland ( the study = = = = = = = <unk> as an apparent\n",
            "against Ireland has been very different species of the <unk> and early development and breeding populations of Ireland , they\n",
            "are the population are smaller birds , which also , it is an bird populations ( and possession and other\n",
            "large <unk> = can be collected by the adults may be found in humans , the breeding populations in Ireland\n",
            ", as the same bird sites has a common starling population population are most <unk> ( including introduced with <unk>\n",
            "  19th century birds generally have been found in North females on common starlings can be found in some <unk>\n",
            "population numbers and its breeding length from the population <unk> and early birds with <unk> areas in the first species\n",
            "of the bird species = = <eos> <eos> <eos> <eos> <eos> <eos> <unk> species of this population is the birds\n",
            "may contain more generally known to the native and other common starling size in <unk> regions ( common starlings ,\n",
            "while <unk> may result of the common starlings can also found in the male may be found to an bird\n",
            "can now the <unk> and very long <unk> species present most common starlings may be the common <unk> species as\n",
            "<unk> species has been found on the young human population may indicate that the common starling population has an population\n",
            "( <unk> species , the very short <unk> <unk> species present only 5   <unk> 's breeding size of the\n",
            "breeding season has two species , although the breeding population size of <unk> population of the population size of <unk>\n",
            "<unk> is the common starling populations and a more common starling populations has been killed in eastern half ( a\n",
            "single nest patterns of the population will cause some species of up to the common starlings are now <unk> populations\n",
            "of the diet = = humans , not common starling populations of <unk> of population has the common starlings may\n",
            "be found in the common starlings = = = = = = females , the most common starling as birds\n",
            "may be known in Ireland , they are probably found in the common <unk> 's adults in Australia populations of\n",
            "the bird size = = = = = = = = = = = = = = = = =\n",
            "= = Common land are smaller specimens may have a common starling in the forest species , <unk> has been\n",
            "<unk> population by the breeding population and they breeding population decline when predators of the breeding diameter where the largest\n",
            "breeding population are generally the bird populations of the year to the population of small size breeding and <unk> of\n",
            "males and breeding size = <eos> <eos> <unk> areas of <unk> starlings , the fungus that have been a very\n",
            "greatly reduced from the breeding population can have been found in the central and the bird species , the bird\n",
            "will indicate that there are now <unk> that may have been found in common areas such as <unk> is no\n",
            "common starlings may be the total population by the bird and most common starlings may be classified as the natural\n",
            "habitat and the Irish population may be very long expanded into the birds may only 1 <unk> ( although it\n",
            "may be the total breeding range size areas can be <unk> population grows common starlings can be native species of\n",
            "other native family species of young plants , even an average <unk> in the common with very old population length\n",
            "is commonly . Since the breeding season of only well   African common starling population spread to <unk> <unk> <unk>\n",
            "is the Republic 's common starlings may be described from the bird , such as <unk> with any male are\n",
            "the bird species being the first breeding population species , the common starlings may not only only be very birds\n",
            "for <unk> . In the range and in all other breeding common starling populations as very large length or Southern\n",
            "Ireland and they may be the <unk> , although a breeding populations by the birds and <unk> ( as the\n",
            "same bird 's population has been also has no longer have given it is small <unk> and other species in\n",
            "the population populations . <unk> and <unk> in the bird population is a wild populations ( the species from females\n",
            ", the most often grows size areas , and many species of <unk> and it may mean <unk> 's <unk>\n",
            "birds have been used <unk> <unk> ( from their breeding result , possibly have been native land population has a\n",
            "bird population can <unk> <unk> is a bird and the common starlings may result in <unk> species not exist .\n",
            "Since females , there is still be introduced is <unk> has been found . <unk> <unk> population of the population\n",
            "to common starlings may be a population , most of breeding population is much open <unk> from other common starling\n",
            "population in this species : by <unk> predators = = At least five breeding in the same bird has more\n",
            "distinct females being more males to <unk> in Australia ( the population of some <unk> can still <unk> and the\n",
            "largest range habitat , which generally no male 's species of a large land that birds as the population are\n",
            "generally have been in the breeding range from a significant agricultural and other large size population <unk> , so in\n"
          ]
        }
      ],
      "source": [
        "print(\"Training LSTM on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type = 'Transformer', # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "    data_path = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize = 400,\n",
        "    nhid = 400,\n",
        "    nlayers = 4,\n",
        "    lr = 0.001,\n",
        "    clip = 0.25,\n",
        "    epochs = 60,\n",
        "    batch_size = 20,\n",
        "    bptt = 35,\n",
        "    dropout = 0.2,\n",
        "    tied = False,\n",
        "    nhead = 4,\n",
        "    log_interval = 200,\n",
        "    save_path='model_14.pt',\n",
        "    dry_run=False,\n",
        "    accel=True,\n",
        "    use_optimizer=True,\n",
        "    optimizer_type='AdamW',\n",
        "    weight_decay=1e-4,\n",
        "    use_betas=True,\n",
        "    betas=(0.9, 0.98),\n",
        "    use_eps=True,\n",
        "    eps=1e-9,\n",
        "    criterion=None,\n",
        "    use_label_smoothing=True,\n",
        "    label_smoothing=0.1,\n",
        "    use_warmup=True,\n",
        "    warmup_steps=4000,\n",
        "    min_freq=5,\n",
        "    seed=1111,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "# Example 2: Generate text\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model_14.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf='generated_14.txt',\n",
        "    words=1000,\n",
        "    temperature=1.0,\n",
        "    top_k=40,\n",
        "    seed=1111,\n",
        "    log_interval=100,\n",
        "    use_top_k=True,  # Use top-k sampling\n",
        "    min_freq=5,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "!cat generated_14.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4uRAC075s0K0",
      "metadata": {
        "id": "4uRAC075s0K0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1d79304-fb38-4129-a038-d9b104ca27a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM on WikiText-2...\n",
            "Using device: cuda\n",
            "Vocabulary size: 25251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/ 2088 batches | lr 0.000035 | ms/batch 48.14 | loss  8.82 | ppl  6752.52\n",
            "| epoch   1 |   400/ 2088 batches | lr 0.000070 | ms/batch 48.26 | loss  7.30 | ppl  1480.27\n",
            "| epoch   1 |   600/ 2088 batches | lr 0.000105 | ms/batch 48.55 | loss  7.02 | ppl  1118.48\n",
            "| epoch   1 |   800/ 2088 batches | lr 0.000140 | ms/batch 48.61 | loss  6.90 | ppl   990.17\n",
            "| epoch   1 |  1000/ 2088 batches | lr 0.000175 | ms/batch 48.38 | loss  6.74 | ppl   848.68\n",
            "| epoch   1 |  1200/ 2088 batches | lr 0.000210 | ms/batch 48.20 | loss  6.66 | ppl   776.93\n",
            "| epoch   1 |  1400/ 2088 batches | lr 0.000245 | ms/batch 48.11 | loss  6.56 | ppl   707.58\n",
            "| epoch   1 |  1600/ 2088 batches | lr 0.000280 | ms/batch 47.91 | loss  6.45 | ppl   633.34\n",
            "| epoch   1 |  1800/ 2088 batches | lr 0.000315 | ms/batch 47.84 | loss  6.42 | ppl   611.72\n",
            "| epoch   1 |  2000/ 2088 batches | lr 0.000350 | ms/batch 47.92 | loss  6.33 | ppl   562.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 103.56s | valid loss  6.84 | valid ppl   930.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000365\n",
            "| epoch   2 |   200/ 2088 batches | lr 0.000400 | ms/batch 48.17 | loss  6.33 | ppl   558.43\n",
            "| epoch   2 |   400/ 2088 batches | lr 0.000435 | ms/batch 48.07 | loss  6.24 | ppl   512.08\n",
            "| epoch   2 |   600/ 2088 batches | lr 0.000470 | ms/batch 48.07 | loss  6.17 | ppl   480.21\n",
            "| epoch   2 |   800/ 2088 batches | lr 0.000505 | ms/batch 48.09 | loss  6.16 | ppl   475.35\n",
            "| epoch   2 |  1000/ 2088 batches | lr 0.000540 | ms/batch 48.03 | loss  6.16 | ppl   471.92\n",
            "| epoch   2 |  1200/ 2088 batches | lr 0.000575 | ms/batch 47.99 | loss  6.15 | ppl   469.04\n",
            "| epoch   2 |  1400/ 2088 batches | lr 0.000610 | ms/batch 47.99 | loss  6.10 | ppl   447.78\n",
            "| epoch   2 |  1600/ 2088 batches | lr 0.000645 | ms/batch 47.89 | loss  6.04 | ppl   418.04\n",
            "| epoch   2 |  1800/ 2088 batches | lr 0.000680 | ms/batch 47.89 | loss  6.07 | ppl   430.62\n",
            "| epoch   2 |  2000/ 2088 batches | lr 0.000691 | ms/batch 47.92 | loss  6.02 | ppl   412.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 103.19s | valid loss  6.66 | valid ppl   781.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000684\n",
            "| epoch   3 |   200/ 2088 batches | lr 0.000668 | ms/batch 48.17 | loss  6.06 | ppl   429.50\n",
            "| epoch   3 |   400/ 2088 batches | lr 0.000653 | ms/batch 47.92 | loss  6.00 | ppl   401.42\n",
            "| epoch   3 |   600/ 2088 batches | lr 0.000639 | ms/batch 47.94 | loss  5.94 | ppl   381.01\n",
            "| epoch   3 |   800/ 2088 batches | lr 0.000626 | ms/batch 47.89 | loss  5.94 | ppl   379.87\n",
            "| epoch   3 |  1000/ 2088 batches | lr 0.000614 | ms/batch 47.94 | loss  5.95 | ppl   382.42\n",
            "| epoch   3 |  1200/ 2088 batches | lr 0.000603 | ms/batch 47.89 | loss  5.95 | ppl   383.48\n",
            "| epoch   3 |  1400/ 2088 batches | lr 0.000592 | ms/batch 47.83 | loss  5.90 | ppl   365.09\n",
            "| epoch   3 |  1600/ 2088 batches | lr 0.000581 | ms/batch 47.84 | loss  5.82 | ppl   335.60\n",
            "| epoch   3 |  1800/ 2088 batches | lr 0.000572 | ms/batch 47.82 | loss  5.84 | ppl   344.35\n",
            "| epoch   3 |  2000/ 2088 batches | lr 0.000562 | ms/batch 47.85 | loss  5.79 | ppl   328.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 102.99s | valid loss  6.49 | valid ppl   655.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000558\n",
            "| epoch   4 |   200/ 2088 batches | lr 0.000550 | ms/batch 48.05 | loss  5.84 | ppl   343.56\n",
            "| epoch   4 |   400/ 2088 batches | lr 0.000541 | ms/batch 47.90 | loss  5.77 | ppl   321.17\n",
            "| epoch   4 |   600/ 2088 batches | lr 0.000533 | ms/batch 47.90 | loss  5.73 | ppl   308.66\n",
            "| epoch   4 |   800/ 2088 batches | lr 0.000526 | ms/batch 47.93 | loss  5.73 | ppl   308.46\n",
            "| epoch   4 |  1000/ 2088 batches | lr 0.000518 | ms/batch 47.90 | loss  5.76 | ppl   317.40\n",
            "| epoch   4 |  1200/ 2088 batches | lr 0.000511 | ms/batch 47.87 | loss  5.77 | ppl   320.12\n",
            "| epoch   4 |  1400/ 2088 batches | lr 0.000505 | ms/batch 47.77 | loss  5.73 | ppl   306.88\n",
            "| epoch   4 |  1600/ 2088 batches | lr 0.000498 | ms/batch 47.79 | loss  5.64 | ppl   282.03\n",
            "| epoch   4 |  1800/ 2088 batches | lr 0.000492 | ms/batch 47.85 | loss  5.69 | ppl   294.66\n",
            "| epoch   4 |  2000/ 2088 batches | lr 0.000486 | ms/batch 47.86 | loss  5.65 | ppl   283.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 102.93s | valid loss  6.41 | valid ppl   606.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000483\n",
            "| epoch   5 |   200/ 2088 batches | lr 0.000478 | ms/batch 48.06 | loss  5.70 | ppl   297.39\n",
            "| epoch   5 |   400/ 2088 batches | lr 0.000472 | ms/batch 47.91 | loss  5.63 | ppl   279.75\n",
            "| epoch   5 |   600/ 2088 batches | lr 0.000467 | ms/batch 47.86 | loss  5.60 | ppl   269.79\n",
            "| epoch   5 |   800/ 2088 batches | lr 0.000462 | ms/batch 47.88 | loss  5.60 | ppl   269.20\n",
            "| epoch   5 |  1000/ 2088 batches | lr 0.000457 | ms/batch 48.04 | loss  5.63 | ppl   279.36\n",
            "| epoch   5 |  1200/ 2088 batches | lr 0.000452 | ms/batch 47.99 | loss  5.65 | ppl   284.00\n",
            "| epoch   5 |  1400/ 2088 batches | lr 0.000447 | ms/batch 47.92 | loss  5.61 | ppl   273.07\n",
            "| epoch   5 |  1600/ 2088 batches | lr 0.000443 | ms/batch 47.82 | loss  5.53 | ppl   251.16\n",
            "| epoch   5 |  1800/ 2088 batches | lr 0.000439 | ms/batch 47.88 | loss  5.58 | ppl   264.12\n",
            "| epoch   5 |  2000/ 2088 batches | lr 0.000434 | ms/batch 47.85 | loss  5.54 | ppl   254.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 103.06s | valid loss  6.34 | valid ppl   566.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000432\n",
            "| epoch   6 |   200/ 2088 batches | lr 0.000428 | ms/batch 48.03 | loss  5.59 | ppl   267.35\n",
            "| epoch   6 |   400/ 2088 batches | lr 0.000424 | ms/batch 47.81 | loss  5.53 | ppl   252.59\n",
            "| epoch   6 |   600/ 2088 batches | lr 0.000420 | ms/batch 47.89 | loss  5.50 | ppl   243.58\n",
            "| epoch   6 |   800/ 2088 batches | lr 0.000417 | ms/batch 47.84 | loss  5.50 | ppl   243.90\n",
            "| epoch   6 |  1000/ 2088 batches | lr 0.000413 | ms/batch 47.88 | loss  5.54 | ppl   254.19\n",
            "| epoch   6 |  1200/ 2088 batches | lr 0.000410 | ms/batch 47.93 | loss  5.56 | ppl   259.36\n",
            "| epoch   6 |  1400/ 2088 batches | lr 0.000406 | ms/batch 47.89 | loss  5.52 | ppl   250.26\n",
            "| epoch   6 |  1600/ 2088 batches | lr 0.000403 | ms/batch 47.87 | loss  5.43 | ppl   228.75\n",
            "| epoch   6 |  1800/ 2088 batches | lr 0.000399 | ms/batch 47.78 | loss  5.49 | ppl   241.80\n",
            "| epoch   6 |  2000/ 2088 batches | lr 0.000396 | ms/batch 47.81 | loss  5.45 | ppl   233.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 102.92s | valid loss  6.31 | valid ppl   547.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000395\n",
            "| epoch   7 |   200/ 2088 batches | lr 0.000392 | ms/batch 48.06 | loss  5.50 | ppl   245.39\n",
            "| epoch   7 |   400/ 2088 batches | lr 0.000389 | ms/batch 47.84 | loss  5.45 | ppl   232.81\n",
            "| epoch   7 |   600/ 2088 batches | lr 0.000386 | ms/batch 47.85 | loss  5.41 | ppl   224.46\n",
            "| epoch   7 |   800/ 2088 batches | lr 0.000383 | ms/batch 47.89 | loss  5.41 | ppl   224.44\n",
            "| epoch   7 |  1000/ 2088 batches | lr 0.000380 | ms/batch 47.81 | loss  5.46 | ppl   235.71\n",
            "| epoch   7 |  1200/ 2088 batches | lr 0.000377 | ms/batch 47.84 | loss  5.48 | ppl   240.83\n",
            "| epoch   7 |  1400/ 2088 batches | lr 0.000374 | ms/batch 47.82 | loss  5.45 | ppl   232.18\n",
            "| epoch   7 |  1600/ 2088 batches | lr 0.000372 | ms/batch 47.81 | loss  5.36 | ppl   212.63\n",
            "| epoch   7 |  1800/ 2088 batches | lr 0.000369 | ms/batch 47.83 | loss  5.42 | ppl   225.71\n",
            "| epoch   7 |  2000/ 2088 batches | lr 0.000367 | ms/batch 47.78 | loss  5.39 | ppl   218.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 102.90s | valid loss  6.29 | valid ppl   537.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000365\n",
            "| epoch   8 |   200/ 2088 batches | lr 0.000363 | ms/batch 47.99 | loss  5.44 | ppl   229.78\n",
            "| epoch   8 |   400/ 2088 batches | lr 0.000361 | ms/batch 47.80 | loss  5.39 | ppl   218.14\n",
            "| epoch   8 |   600/ 2088 batches | lr 0.000358 | ms/batch 47.77 | loss  5.35 | ppl   210.18\n",
            "| epoch   8 |   800/ 2088 batches | lr 0.000356 | ms/batch 47.86 | loss  5.35 | ppl   209.97\n",
            "| epoch   8 |  1000/ 2088 batches | lr 0.000354 | ms/batch 48.02 | loss  5.40 | ppl   221.61\n",
            "| epoch   8 |  1200/ 2088 batches | lr 0.000351 | ms/batch 47.95 | loss  5.42 | ppl   226.44\n",
            "| epoch   8 |  1400/ 2088 batches | lr 0.000349 | ms/batch 47.97 | loss  5.39 | ppl   218.46\n",
            "| epoch   8 |  1600/ 2088 batches | lr 0.000347 | ms/batch 48.02 | loss  5.29 | ppl   199.27\n",
            "| epoch   8 |  1800/ 2088 batches | lr 0.000345 | ms/batch 48.08 | loss  5.36 | ppl   212.74\n",
            "| epoch   8 |  2000/ 2088 batches | lr 0.000343 | ms/batch 48.10 | loss  5.33 | ppl   206.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 103.14s | valid loss  6.27 | valid ppl   527.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000342\n",
            "| epoch   9 |   200/ 2088 batches | lr 0.000340 | ms/batch 48.20 | loss  5.38 | ppl   216.92\n",
            "| epoch   9 |   400/ 2088 batches | lr 0.000338 | ms/batch 47.96 | loss  5.33 | ppl   205.71\n",
            "| epoch   9 |   600/ 2088 batches | lr 0.000336 | ms/batch 47.87 | loss  5.29 | ppl   198.88\n",
            "| epoch   9 |   800/ 2088 batches | lr 0.000334 | ms/batch 47.89 | loss  5.29 | ppl   198.59\n",
            "| epoch   9 |  1000/ 2088 batches | lr 0.000332 | ms/batch 47.90 | loss  5.34 | ppl   209.13\n",
            "| epoch   9 |  1200/ 2088 batches | lr 0.000330 | ms/batch 47.89 | loss  5.37 | ppl   214.52\n",
            "| epoch   9 |  1400/ 2088 batches | lr 0.000328 | ms/batch 48.00 | loss  5.33 | ppl   206.86\n",
            "| epoch   9 |  1600/ 2088 batches | lr 0.000327 | ms/batch 47.89 | loss  5.24 | ppl   188.38\n",
            "| epoch   9 |  1800/ 2088 batches | lr 0.000325 | ms/batch 47.88 | loss  5.31 | ppl   201.58\n",
            "| epoch   9 |  2000/ 2088 batches | lr 0.000323 | ms/batch 47.83 | loss  5.27 | ppl   194.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 103.06s | valid loss  6.26 | valid ppl   522.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000322\n",
            "| epoch  10 |   200/ 2088 batches | lr 0.000321 | ms/batch 48.06 | loss  5.32 | ppl   205.22\n",
            "| epoch  10 |   400/ 2088 batches | lr 0.000319 | ms/batch 47.81 | loss  5.27 | ppl   195.16\n",
            "| epoch  10 |   600/ 2088 batches | lr 0.000317 | ms/batch 47.79 | loss  5.24 | ppl   188.28\n",
            "| epoch  10 |   800/ 2088 batches | lr 0.000316 | ms/batch 47.82 | loss  5.24 | ppl   188.67\n",
            "| epoch  10 |  1000/ 2088 batches | lr 0.000314 | ms/batch 47.81 | loss  5.29 | ppl   199.23\n",
            "| epoch  10 |  1200/ 2088 batches | lr 0.000312 | ms/batch 47.82 | loss  5.32 | ppl   205.01\n",
            "| epoch  10 |  1400/ 2088 batches | lr 0.000311 | ms/batch 47.81 | loss  5.28 | ppl   196.95\n",
            "| epoch  10 |  1600/ 2088 batches | lr 0.000309 | ms/batch 47.79 | loss  5.19 | ppl   179.63\n",
            "| epoch  10 |  1800/ 2088 batches | lr 0.000308 | ms/batch 47.82 | loss  5.26 | ppl   192.44\n",
            "| epoch  10 |  2000/ 2088 batches | lr 0.000306 | ms/batch 47.87 | loss  5.23 | ppl   186.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 102.87s | valid loss  6.25 | valid ppl   519.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000306\n",
            "| epoch  11 |   200/ 2088 batches | lr 0.000304 | ms/batch 48.05 | loss  5.28 | ppl   196.13\n",
            "| epoch  11 |   400/ 2088 batches | lr 0.000303 | ms/batch 47.80 | loss  5.23 | ppl   186.44\n",
            "| epoch  11 |   600/ 2088 batches | lr 0.000301 | ms/batch 47.87 | loss  5.19 | ppl   179.61\n",
            "| epoch  11 |   800/ 2088 batches | lr 0.000300 | ms/batch 47.93 | loss  5.19 | ppl   180.10\n",
            "| epoch  11 |  1000/ 2088 batches | lr 0.000299 | ms/batch 48.01 | loss  5.25 | ppl   190.59\n",
            "| epoch  11 |  1200/ 2088 batches | lr 0.000297 | ms/batch 47.94 | loss  5.28 | ppl   196.19\n",
            "| epoch  11 |  1400/ 2088 batches | lr 0.000296 | ms/batch 47.87 | loss  5.24 | ppl   188.62\n",
            "| epoch  11 |  1600/ 2088 batches | lr 0.000295 | ms/batch 47.85 | loss  5.15 | ppl   171.84\n",
            "| epoch  11 |  1800/ 2088 batches | lr 0.000293 | ms/batch 47.85 | loss  5.22 | ppl   184.85\n",
            "| epoch  11 |  2000/ 2088 batches | lr 0.000292 | ms/batch 47.81 | loss  5.18 | ppl   178.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 103.00s | valid loss  6.25 | valid ppl   517.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000292\n",
            "| epoch  12 |   200/ 2088 batches | lr 0.000290 | ms/batch 48.00 | loss  5.23 | ppl   187.71\n",
            "| epoch  12 |   400/ 2088 batches | lr 0.000289 | ms/batch 47.83 | loss  5.18 | ppl   178.14\n",
            "| epoch  12 |   600/ 2088 batches | lr 0.000288 | ms/batch 47.84 | loss  5.15 | ppl   171.77\n",
            "| epoch  12 |   800/ 2088 batches | lr 0.000287 | ms/batch 47.87 | loss  5.15 | ppl   172.31\n",
            "| epoch  12 |  1000/ 2088 batches | lr 0.000285 | ms/batch 47.84 | loss  5.21 | ppl   182.82\n",
            "| epoch  12 |  1200/ 2088 batches | lr 0.000284 | ms/batch 47.80 | loss  5.24 | ppl   187.86\n",
            "| epoch  12 |  1400/ 2088 batches | lr 0.000283 | ms/batch 47.83 | loss  5.20 | ppl   180.51\n",
            "| epoch  12 |  1600/ 2088 batches | lr 0.000282 | ms/batch 47.78 | loss  5.10 | ppl   164.30\n",
            "| epoch  12 |  1800/ 2088 batches | lr 0.000281 | ms/batch 47.81 | loss  5.18 | ppl   177.43\n",
            "| epoch  12 |  2000/ 2088 batches | lr 0.000280 | ms/batch 47.82 | loss  5.14 | ppl   170.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 102.85s | valid loss  6.26 | valid ppl   520.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000279\n",
            "| epoch  13 |   200/ 2088 batches | lr 0.000278 | ms/batch 48.06 | loss  5.19 | ppl   180.27\n",
            "| epoch  13 |   400/ 2088 batches | lr 0.000277 | ms/batch 47.82 | loss  5.14 | ppl   171.20\n",
            "| epoch  13 |   600/ 2088 batches | lr 0.000276 | ms/batch 47.75 | loss  5.11 | ppl   165.24\n",
            "| epoch  13 |   800/ 2088 batches | lr 0.000275 | ms/batch 47.79 | loss  5.11 | ppl   165.54\n",
            "| epoch  13 |  1000/ 2088 batches | lr 0.000274 | ms/batch 47.80 | loss  5.17 | ppl   175.47\n",
            "| epoch  13 |  1200/ 2088 batches | lr 0.000273 | ms/batch 47.84 | loss  5.20 | ppl   180.77\n",
            "| epoch  13 |  1400/ 2088 batches | lr 0.000272 | ms/batch 47.85 | loss  5.16 | ppl   173.67\n",
            "| epoch  13 |  1600/ 2088 batches | lr 0.000271 | ms/batch 47.81 | loss  5.06 | ppl   157.83\n",
            "| epoch  13 |  1800/ 2088 batches | lr 0.000270 | ms/batch 47.82 | loss  5.14 | ppl   171.05\n",
            "| epoch  13 |  2000/ 2088 batches | lr 0.000269 | ms/batch 47.79 | loss  5.10 | ppl   164.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 102.85s | valid loss  6.25 | valid ppl   516.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000268\n",
            "| epoch  14 |   200/ 2088 batches | lr 0.000267 | ms/batch 48.02 | loss  5.16 | ppl   173.85\n",
            "| epoch  14 |   400/ 2088 batches | lr 0.000266 | ms/batch 47.80 | loss  5.10 | ppl   164.53\n",
            "| epoch  14 |   600/ 2088 batches | lr 0.000265 | ms/batch 47.89 | loss  5.07 | ppl   158.58\n",
            "| epoch  14 |   800/ 2088 batches | lr 0.000264 | ms/batch 47.84 | loss  5.07 | ppl   158.90\n",
            "| epoch  14 |  1000/ 2088 batches | lr 0.000263 | ms/batch 47.91 | loss  5.13 | ppl   168.68\n",
            "| epoch  14 |  1200/ 2088 batches | lr 0.000262 | ms/batch 47.88 | loss  5.16 | ppl   174.42\n",
            "| epoch  14 |  1400/ 2088 batches | lr 0.000262 | ms/batch 47.87 | loss  5.12 | ppl   167.04\n",
            "| epoch  14 |  1600/ 2088 batches | lr 0.000261 | ms/batch 47.86 | loss  5.02 | ppl   152.03\n",
            "| epoch  14 |  1800/ 2088 batches | lr 0.000260 | ms/batch 47.80 | loss  5.10 | ppl   164.36\n",
            "| epoch  14 |  2000/ 2088 batches | lr 0.000259 | ms/batch 47.82 | loss  5.07 | ppl   158.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 102.93s | valid loss  6.25 | valid ppl   520.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000258\n",
            "| epoch  15 |   200/ 2088 batches | lr 0.000258 | ms/batch 48.05 | loss  5.12 | ppl   167.36\n",
            "| epoch  15 |   400/ 2088 batches | lr 0.000257 | ms/batch 47.81 | loss  5.07 | ppl   158.46\n",
            "| epoch  15 |   600/ 2088 batches | lr 0.000256 | ms/batch 47.88 | loss  5.03 | ppl   153.01\n",
            "| epoch  15 |   800/ 2088 batches | lr 0.000255 | ms/batch 47.88 | loss  5.03 | ppl   153.14\n",
            "| epoch  15 |  1000/ 2088 batches | lr 0.000254 | ms/batch 47.87 | loss  5.09 | ppl   163.03\n",
            "| epoch  15 |  1200/ 2088 batches | lr 0.000253 | ms/batch 47.82 | loss  5.13 | ppl   168.32\n",
            "| epoch  15 |  1400/ 2088 batches | lr 0.000252 | ms/batch 47.85 | loss  5.09 | ppl   161.85\n",
            "| epoch  15 |  1600/ 2088 batches | lr 0.000252 | ms/batch 47.85 | loss  4.99 | ppl   146.66\n",
            "| epoch  15 |  1800/ 2088 batches | lr 0.000251 | ms/batch 47.75 | loss  5.07 | ppl   158.72\n",
            "| epoch  15 |  2000/ 2088 batches | lr 0.000250 | ms/batch 47.80 | loss  5.03 | ppl   153.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 102.88s | valid loss  6.26 | valid ppl   521.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000250\n",
            "| epoch  16 |   200/ 2088 batches | lr 0.000249 | ms/batch 48.04 | loss  5.09 | ppl   161.89\n",
            "| epoch  16 |   400/ 2088 batches | lr 0.000248 | ms/batch 47.80 | loss  5.03 | ppl   153.69\n",
            "| epoch  16 |   600/ 2088 batches | lr 0.000247 | ms/batch 47.84 | loss  5.00 | ppl   147.68\n",
            "| epoch  16 |   800/ 2088 batches | lr 0.000247 | ms/batch 47.80 | loss  5.00 | ppl   148.63\n",
            "| epoch  16 |  1000/ 2088 batches | lr 0.000246 | ms/batch 47.84 | loss  5.06 | ppl   157.15\n",
            "| epoch  16 |  1200/ 2088 batches | lr 0.000245 | ms/batch 47.83 | loss  5.09 | ppl   163.20\n",
            "| epoch  16 |  1400/ 2088 batches | lr 0.000244 | ms/batch 47.84 | loss  5.05 | ppl   156.07\n",
            "| epoch  16 |  1600/ 2088 batches | lr 0.000244 | ms/batch 47.79 | loss  4.96 | ppl   142.35\n",
            "| epoch  16 |  1800/ 2088 batches | lr 0.000243 | ms/batch 47.83 | loss  5.04 | ppl   153.75\n",
            "| epoch  16 |  2000/ 2088 batches | lr 0.000242 | ms/batch 47.82 | loss  5.00 | ppl   148.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 102.86s | valid loss  6.27 | valid ppl   526.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000242\n",
            "| epoch  17 |   200/ 2088 batches | lr 0.000241 | ms/batch 48.05 | loss  5.06 | ppl   156.88\n",
            "| epoch  17 |   400/ 2088 batches | lr 0.000240 | ms/batch 47.82 | loss  5.00 | ppl   149.10\n",
            "| epoch  17 |   600/ 2088 batches | lr 0.000240 | ms/batch 47.76 | loss  4.96 | ppl   143.06\n",
            "| epoch  17 |   800/ 2088 batches | lr 0.000239 | ms/batch 47.91 | loss  4.97 | ppl   143.35\n",
            "| epoch  17 |  1000/ 2088 batches | lr 0.000238 | ms/batch 47.88 | loss  5.03 | ppl   152.27\n",
            "| epoch  17 |  1200/ 2088 batches | lr 0.000238 | ms/batch 47.83 | loss  5.06 | ppl   157.86\n",
            "| epoch  17 |  1400/ 2088 batches | lr 0.000237 | ms/batch 47.82 | loss  5.02 | ppl   151.46\n",
            "| epoch  17 |  1600/ 2088 batches | lr 0.000236 | ms/batch 47.86 | loss  4.93 | ppl   138.08\n",
            "| epoch  17 |  1800/ 2088 batches | lr 0.000235 | ms/batch 47.83 | loss  5.00 | ppl   148.86\n",
            "| epoch  17 |  2000/ 2088 batches | lr 0.000235 | ms/batch 47.84 | loss  4.97 | ppl   144.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 102.90s | valid loss  6.27 | valid ppl   528.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000235\n",
            "| epoch  18 |   200/ 2088 batches | lr 0.000234 | ms/batch 48.05 | loss  5.03 | ppl   152.29\n",
            "| epoch  18 |   400/ 2088 batches | lr 0.000233 | ms/batch 47.81 | loss  4.97 | ppl   144.30\n",
            "| epoch  18 |   600/ 2088 batches | lr 0.000233 | ms/batch 47.85 | loss  4.94 | ppl   139.38\n",
            "| epoch  18 |   800/ 2088 batches | lr 0.000232 | ms/batch 47.81 | loss  4.94 | ppl   139.41\n",
            "| epoch  18 |  1000/ 2088 batches | lr 0.000231 | ms/batch 47.85 | loss  5.00 | ppl   147.94\n",
            "| epoch  18 |  1200/ 2088 batches | lr 0.000231 | ms/batch 47.83 | loss  5.03 | ppl   153.61\n",
            "| epoch  18 |  1400/ 2088 batches | lr 0.000230 | ms/batch 47.82 | loss  4.99 | ppl   146.81\n",
            "| epoch  18 |  1600/ 2088 batches | lr 0.000229 | ms/batch 47.77 | loss  4.89 | ppl   133.21\n",
            "| epoch  18 |  1800/ 2088 batches | lr 0.000229 | ms/batch 47.80 | loss  4.98 | ppl   144.80\n",
            "| epoch  18 |  2000/ 2088 batches | lr 0.000228 | ms/batch 47.81 | loss  4.94 | ppl   139.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 102.85s | valid loss  6.29 | valid ppl   540.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000228\n",
            "| epoch  19 |   200/ 2088 batches | lr 0.000227 | ms/batch 48.09 | loss  4.99 | ppl   147.34\n",
            "| epoch  19 |   400/ 2088 batches | lr 0.000227 | ms/batch 47.82 | loss  4.94 | ppl   139.72\n",
            "| epoch  19 |   600/ 2088 batches | lr 0.000226 | ms/batch 47.81 | loss  4.90 | ppl   134.88\n",
            "| epoch  19 |   800/ 2088 batches | lr 0.000226 | ms/batch 47.80 | loss  4.91 | ppl   135.18\n",
            "| epoch  19 |  1000/ 2088 batches | lr 0.000225 | ms/batch 47.78 | loss  4.97 | ppl   143.80\n",
            "| epoch  19 |  1200/ 2088 batches | lr 0.000224 | ms/batch 47.81 | loss  5.00 | ppl   148.87\n",
            "| epoch  19 |  1400/ 2088 batches | lr 0.000224 | ms/batch 47.79 | loss  4.96 | ppl   142.76\n",
            "| epoch  19 |  1600/ 2088 batches | lr 0.000223 | ms/batch 47.82 | loss  4.87 | ppl   130.25\n",
            "| epoch  19 |  1800/ 2088 batches | lr 0.000223 | ms/batch 47.84 | loss  4.95 | ppl   140.70\n",
            "| epoch  19 |  2000/ 2088 batches | lr 0.000222 | ms/batch 47.91 | loss  4.91 | ppl   135.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 102.88s | valid loss  6.29 | valid ppl   538.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000222\n",
            "| epoch  20 |   200/ 2088 batches | lr 0.000221 | ms/batch 48.05 | loss  4.97 | ppl   143.49\n",
            "| epoch  20 |   400/ 2088 batches | lr 0.000221 | ms/batch 47.88 | loss  4.91 | ppl   135.96\n",
            "| epoch  20 |   600/ 2088 batches | lr 0.000220 | ms/batch 47.92 | loss  4.87 | ppl   130.88\n",
            "| epoch  20 |   800/ 2088 batches | lr 0.000220 | ms/batch 47.93 | loss  4.88 | ppl   131.38\n",
            "| epoch  20 |  1000/ 2088 batches | lr 0.000219 | ms/batch 47.80 | loss  4.94 | ppl   140.18\n",
            "| epoch  20 |  1200/ 2088 batches | lr 0.000219 | ms/batch 47.84 | loss  4.98 | ppl   145.04\n",
            "| epoch  20 |  1400/ 2088 batches | lr 0.000218 | ms/batch 47.91 | loss  4.93 | ppl   138.96\n",
            "| epoch  20 |  1600/ 2088 batches | lr 0.000217 | ms/batch 47.80 | loss  4.84 | ppl   125.91\n",
            "| epoch  20 |  1800/ 2088 batches | lr 0.000217 | ms/batch 47.81 | loss  4.92 | ppl   136.92\n",
            "| epoch  20 |  2000/ 2088 batches | lr 0.000216 | ms/batch 47.80 | loss  4.88 | ppl   132.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 102.93s | valid loss  6.30 | valid ppl   546.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000216\n",
            "| epoch  21 |   200/ 2088 batches | lr 0.000216 | ms/batch 48.03 | loss  4.93 | ppl   139.03\n",
            "| epoch  21 |   400/ 2088 batches | lr 0.000215 | ms/batch 47.82 | loss  4.88 | ppl   132.17\n",
            "| epoch  21 |   600/ 2088 batches | lr 0.000215 | ms/batch 47.79 | loss  4.85 | ppl   127.49\n",
            "| epoch  21 |   800/ 2088 batches | lr 0.000214 | ms/batch 47.80 | loss  4.85 | ppl   128.13\n",
            "| epoch  21 |  1000/ 2088 batches | lr 0.000214 | ms/batch 47.86 | loss  4.91 | ppl   135.83\n",
            "| epoch  21 |  1200/ 2088 batches | lr 0.000213 | ms/batch 47.76 | loss  4.95 | ppl   140.73\n",
            "| epoch  21 |  1400/ 2088 batches | lr 0.000213 | ms/batch 47.81 | loss  4.91 | ppl   135.09\n",
            "| epoch  21 |  1600/ 2088 batches | lr 0.000212 | ms/batch 47.86 | loss  4.81 | ppl   122.70\n",
            "| epoch  21 |  1800/ 2088 batches | lr 0.000212 | ms/batch 47.84 | loss  4.89 | ppl   133.62\n",
            "| epoch  21 |  2000/ 2088 batches | lr 0.000211 | ms/batch 47.86 | loss  4.86 | ppl   128.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 102.89s | valid loss  6.31 | valid ppl   552.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000211\n",
            "| epoch  22 |   200/ 2088 batches | lr 0.000211 | ms/batch 48.10 | loss  4.91 | ppl   136.11\n",
            "| epoch  22 |   400/ 2088 batches | lr 0.000210 | ms/batch 47.75 | loss  4.86 | ppl   128.60\n",
            "| epoch  22 |   600/ 2088 batches | lr 0.000210 | ms/batch 47.79 | loss  4.82 | ppl   124.49\n",
            "| epoch  22 |   800/ 2088 batches | lr 0.000209 | ms/batch 47.78 | loss  4.83 | ppl   124.64\n",
            "| epoch  22 |  1000/ 2088 batches | lr 0.000209 | ms/batch 47.79 | loss  4.89 | ppl   132.96\n",
            "| epoch  22 |  1200/ 2088 batches | lr 0.000208 | ms/batch 47.84 | loss  4.92 | ppl   137.40\n",
            "| epoch  22 |  1400/ 2088 batches | lr 0.000208 | ms/batch 47.86 | loss  4.88 | ppl   131.81\n",
            "| epoch  22 |  1600/ 2088 batches | lr 0.000207 | ms/batch 47.83 | loss  4.79 | ppl   120.04\n",
            "| epoch  22 |  1800/ 2088 batches | lr 0.000207 | ms/batch 47.82 | loss  4.87 | ppl   130.11\n",
            "| epoch  22 |  2000/ 2088 batches | lr 0.000206 | ms/batch 47.84 | loss  4.83 | ppl   125.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 102.87s | valid loss  6.33 | valid ppl   558.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000206\n",
            "| epoch  23 |   200/ 2088 batches | lr 0.000206 | ms/batch 48.03 | loss  4.88 | ppl   131.90\n",
            "| epoch  23 |   400/ 2088 batches | lr 0.000205 | ms/batch 47.86 | loss  4.83 | ppl   125.45\n",
            "| epoch  23 |   600/ 2088 batches | lr 0.000205 | ms/batch 47.91 | loss  4.80 | ppl   121.03\n",
            "| epoch  23 |   800/ 2088 batches | lr 0.000204 | ms/batch 47.88 | loss  4.80 | ppl   121.81\n",
            "| epoch  23 |  1000/ 2088 batches | lr 0.000204 | ms/batch 47.86 | loss  4.86 | ppl   129.01\n",
            "| epoch  23 |  1200/ 2088 batches | lr 0.000204 | ms/batch 47.78 | loss  4.90 | ppl   134.30\n",
            "| epoch  23 |  1400/ 2088 batches | lr 0.000203 | ms/batch 47.83 | loss  4.86 | ppl   128.47\n",
            "| epoch  23 |  1600/ 2088 batches | lr 0.000203 | ms/batch 47.81 | loss  4.76 | ppl   116.78\n",
            "| epoch  23 |  1800/ 2088 batches | lr 0.000202 | ms/batch 47.79 | loss  4.84 | ppl   126.61\n",
            "| epoch  23 |  2000/ 2088 batches | lr 0.000202 | ms/batch 47.85 | loss  4.81 | ppl   122.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 102.92s | valid loss  6.33 | valid ppl   560.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000202\n",
            "| epoch  24 |   200/ 2088 batches | lr 0.000201 | ms/batch 48.11 | loss  4.86 | ppl   128.96\n",
            "| epoch  24 |   400/ 2088 batches | lr 0.000201 | ms/batch 47.86 | loss  4.81 | ppl   122.66\n",
            "| epoch  24 |   600/ 2088 batches | lr 0.000200 | ms/batch 47.79 | loss  4.77 | ppl   118.43\n",
            "| epoch  24 |   800/ 2088 batches | lr 0.000200 | ms/batch 47.83 | loss  4.78 | ppl   118.86\n",
            "| epoch  24 |  1000/ 2088 batches | lr 0.000200 | ms/batch 47.75 | loss  4.84 | ppl   126.00\n",
            "| epoch  24 |  1200/ 2088 batches | lr 0.000199 | ms/batch 47.76 | loss  4.88 | ppl   131.12\n",
            "| epoch  24 |  1400/ 2088 batches | lr 0.000199 | ms/batch 47.86 | loss  4.83 | ppl   125.33\n",
            "| epoch  24 |  1600/ 2088 batches | lr 0.000198 | ms/batch 47.85 | loss  4.74 | ppl   114.21\n",
            "| epoch  24 |  1800/ 2088 batches | lr 0.000198 | ms/batch 47.84 | loss  4.82 | ppl   123.42\n",
            "| epoch  24 |  2000/ 2088 batches | lr 0.000198 | ms/batch 47.86 | loss  4.78 | ppl   119.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 102.90s | valid loss  6.35 | valid ppl   570.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000197\n",
            "| epoch  25 |   200/ 2088 batches | lr 0.000197 | ms/batch 48.00 | loss  4.84 | ppl   125.88\n",
            "| epoch  25 |   400/ 2088 batches | lr 0.000197 | ms/batch 47.78 | loss  4.79 | ppl   120.28\n",
            "| epoch  25 |   600/ 2088 batches | lr 0.000196 | ms/batch 47.77 | loss  4.75 | ppl   115.21\n",
            "| epoch  25 |   800/ 2088 batches | lr 0.000196 | ms/batch 47.78 | loss  4.75 | ppl   115.93\n",
            "| epoch  25 |  1000/ 2088 batches | lr 0.000195 | ms/batch 47.86 | loss  4.81 | ppl   122.96\n",
            "| epoch  25 |  1200/ 2088 batches | lr 0.000195 | ms/batch 47.81 | loss  4.85 | ppl   128.05\n",
            "| epoch  25 |  1400/ 2088 batches | lr 0.000195 | ms/batch 47.85 | loss  4.81 | ppl   122.75\n",
            "| epoch  25 |  1600/ 2088 batches | lr 0.000194 | ms/batch 47.83 | loss  4.72 | ppl   111.66\n",
            "| epoch  25 |  1800/ 2088 batches | lr 0.000194 | ms/batch 47.82 | loss  4.79 | ppl   120.74\n",
            "| epoch  25 |  2000/ 2088 batches | lr 0.000194 | ms/batch 47.78 | loss  4.76 | ppl   116.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 102.82s | valid loss  6.34 | valid ppl   568.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000193\n",
            "| epoch  26 |   200/ 2088 batches | lr 0.000193 | ms/batch 48.05 | loss  4.81 | ppl   122.95\n",
            "| epoch  26 |   400/ 2088 batches | lr 0.000193 | ms/batch 47.91 | loss  4.77 | ppl   117.40\n",
            "| epoch  26 |   600/ 2088 batches | lr 0.000192 | ms/batch 47.91 | loss  4.73 | ppl   112.90\n",
            "| epoch  26 |   800/ 2088 batches | lr 0.000192 | ms/batch 47.84 | loss  4.73 | ppl   113.44\n",
            "| epoch  26 |  1000/ 2088 batches | lr 0.000192 | ms/batch 47.87 | loss  4.79 | ppl   120.19\n",
            "| epoch  26 |  1200/ 2088 batches | lr 0.000191 | ms/batch 47.84 | loss  4.83 | ppl   125.35\n",
            "| epoch  26 |  1400/ 2088 batches | lr 0.000191 | ms/batch 47.89 | loss  4.79 | ppl   119.87\n",
            "| epoch  26 |  1600/ 2088 batches | lr 0.000190 | ms/batch 47.78 | loss  4.69 | ppl   109.01\n",
            "| epoch  26 |  1800/ 2088 batches | lr 0.000190 | ms/batch 47.81 | loss  4.77 | ppl   118.46\n",
            "| epoch  26 |  2000/ 2088 batches | lr 0.000190 | ms/batch 47.84 | loss  4.74 | ppl   114.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 102.92s | valid loss  6.37 | valid ppl   582.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000190\n",
            "| epoch  27 |   200/ 2088 batches | lr 0.000189 | ms/batch 48.07 | loss  4.79 | ppl   119.90\n",
            "| epoch  27 |   400/ 2088 batches | lr 0.000189 | ms/batch 47.83 | loss  4.74 | ppl   114.61\n",
            "| epoch  27 |   600/ 2088 batches | lr 0.000189 | ms/batch 47.83 | loss  4.70 | ppl   110.32\n",
            "| epoch  27 |   800/ 2088 batches | lr 0.000188 | ms/batch 47.81 | loss  4.71 | ppl   110.82\n",
            "| epoch  27 |  1000/ 2088 batches | lr 0.000188 | ms/batch 47.85 | loss  4.76 | ppl   117.10\n",
            "| epoch  27 |  1200/ 2088 batches | lr 0.000188 | ms/batch 47.86 | loss  4.81 | ppl   122.32\n",
            "| epoch  27 |  1400/ 2088 batches | lr 0.000187 | ms/batch 47.78 | loss  4.76 | ppl   116.82\n",
            "| epoch  27 |  1600/ 2088 batches | lr 0.000187 | ms/batch 47.78 | loss  4.67 | ppl   107.02\n",
            "| epoch  27 |  1800/ 2088 batches | lr 0.000187 | ms/batch 47.84 | loss  4.75 | ppl   116.03\n",
            "| epoch  27 |  2000/ 2088 batches | lr 0.000186 | ms/batch 47.82 | loss  4.72 | ppl   112.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 102.87s | valid loss  6.36 | valid ppl   580.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000186\n",
            "| epoch  28 |   200/ 2088 batches | lr 0.000186 | ms/batch 48.03 | loss  4.77 | ppl   117.71\n",
            "| epoch  28 |   400/ 2088 batches | lr 0.000185 | ms/batch 47.83 | loss  4.72 | ppl   112.26\n",
            "| epoch  28 |   600/ 2088 batches | lr 0.000185 | ms/batch 47.81 | loss  4.68 | ppl   108.02\n",
            "| epoch  28 |   800/ 2088 batches | lr 0.000185 | ms/batch 47.79 | loss  4.69 | ppl   108.36\n",
            "| epoch  28 |  1000/ 2088 batches | lr 0.000184 | ms/batch 47.87 | loss  4.74 | ppl   114.92\n",
            "| epoch  28 |  1200/ 2088 batches | lr 0.000184 | ms/batch 47.87 | loss  4.79 | ppl   120.02\n",
            "| epoch  28 |  1400/ 2088 batches | lr 0.000184 | ms/batch 47.83 | loss  4.74 | ppl   114.54\n",
            "| epoch  28 |  1600/ 2088 batches | lr 0.000183 | ms/batch 47.83 | loss  4.65 | ppl   104.36\n",
            "| epoch  28 |  1800/ 2088 batches | lr 0.000183 | ms/batch 47.78 | loss  4.73 | ppl   113.39\n",
            "| epoch  28 |  2000/ 2088 batches | lr 0.000183 | ms/batch 47.76 | loss  4.69 | ppl   109.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 102.89s | valid loss  6.39 | valid ppl   594.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000183\n",
            "| epoch  29 |   200/ 2088 batches | lr 0.000182 | ms/batch 48.15 | loss  4.75 | ppl   115.02\n",
            "| epoch  29 |   400/ 2088 batches | lr 0.000182 | ms/batch 47.90 | loss  4.70 | ppl   109.78\n",
            "| epoch  29 |   600/ 2088 batches | lr 0.000182 | ms/batch 47.80 | loss  4.66 | ppl   105.90\n",
            "| epoch  29 |   800/ 2088 batches | lr 0.000181 | ms/batch 47.82 | loss  4.66 | ppl   105.94\n",
            "| epoch  29 |  1000/ 2088 batches | lr 0.000181 | ms/batch 47.83 | loss  4.72 | ppl   112.70\n",
            "| epoch  29 |  1200/ 2088 batches | lr 0.000181 | ms/batch 47.85 | loss  4.76 | ppl   116.92\n",
            "| epoch  29 |  1400/ 2088 batches | lr 0.000181 | ms/batch 47.84 | loss  4.72 | ppl   111.69\n",
            "| epoch  29 |  1600/ 2088 batches | lr 0.000180 | ms/batch 47.82 | loss  4.62 | ppl   101.84\n",
            "| epoch  29 |  1800/ 2088 batches | lr 0.000180 | ms/batch 47.84 | loss  4.71 | ppl   110.94\n",
            "| epoch  29 |  2000/ 2088 batches | lr 0.000180 | ms/batch 47.85 | loss  4.67 | ppl   107.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 102.95s | valid loss  6.38 | valid ppl   592.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000180\n",
            "| epoch  30 |   200/ 2088 batches | lr 0.000179 | ms/batch 48.08 | loss  4.72 | ppl   112.69\n",
            "| epoch  30 |   400/ 2088 batches | lr 0.000179 | ms/batch 47.79 | loss  4.68 | ppl   107.74\n",
            "| epoch  30 |   600/ 2088 batches | lr 0.000179 | ms/batch 47.80 | loss  4.64 | ppl   103.95\n",
            "| epoch  30 |   800/ 2088 batches | lr 0.000178 | ms/batch 47.78 | loss  4.64 | ppl   104.04\n",
            "| epoch  30 |  1000/ 2088 batches | lr 0.000178 | ms/batch 47.86 | loss  4.70 | ppl   109.81\n",
            "| epoch  30 |  1200/ 2088 batches | lr 0.000178 | ms/batch 47.84 | loss  4.74 | ppl   114.62\n",
            "| epoch  30 |  1400/ 2088 batches | lr 0.000178 | ms/batch 47.86 | loss  4.70 | ppl   109.95\n",
            "| epoch  30 |  1600/ 2088 batches | lr 0.000177 | ms/batch 47.88 | loss  4.60 | ppl    99.78\n",
            "| epoch  30 |  1800/ 2088 batches | lr 0.000177 | ms/batch 47.75 | loss  4.69 | ppl   108.42\n",
            "| epoch  30 |  2000/ 2088 batches | lr 0.000177 | ms/batch 47.83 | loss  4.65 | ppl   104.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 102.90s | valid loss  6.40 | valid ppl   603.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000177\n",
            "| epoch  31 |   200/ 2088 batches | lr 0.000176 | ms/batch 48.06 | loss  4.70 | ppl   110.12\n",
            "| epoch  31 |   400/ 2088 batches | lr 0.000176 | ms/batch 47.78 | loss  4.66 | ppl   105.72\n",
            "| epoch  31 |   600/ 2088 batches | lr 0.000176 | ms/batch 47.82 | loss  4.62 | ppl   101.65\n",
            "| epoch  31 |   800/ 2088 batches | lr 0.000175 | ms/batch 47.84 | loss  4.62 | ppl   101.95\n",
            "| epoch  31 |  1000/ 2088 batches | lr 0.000175 | ms/batch 47.85 | loss  4.68 | ppl   108.04\n",
            "| epoch  31 |  1200/ 2088 batches | lr 0.000175 | ms/batch 47.85 | loss  4.72 | ppl   111.99\n",
            "| epoch  31 |  1400/ 2088 batches | lr 0.000175 | ms/batch 47.87 | loss  4.68 | ppl   107.33\n",
            "| epoch  31 |  1600/ 2088 batches | lr 0.000174 | ms/batch 47.83 | loss  4.59 | ppl    98.43\n",
            "| epoch  31 |  1800/ 2088 batches | lr 0.000174 | ms/batch 47.81 | loss  4.67 | ppl   106.36\n",
            "| epoch  31 |  2000/ 2088 batches | lr 0.000174 | ms/batch 47.77 | loss  4.64 | ppl   103.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 102.86s | valid loss  6.41 | valid ppl   604.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000174\n",
            "| epoch  32 |   200/ 2088 batches | lr 0.000173 | ms/batch 48.10 | loss  4.68 | ppl   108.29\n",
            "| epoch  32 |   400/ 2088 batches | lr 0.000173 | ms/batch 47.81 | loss  4.64 | ppl   103.42\n",
            "| epoch  32 |   600/ 2088 batches | lr 0.000173 | ms/batch 47.85 | loss  4.60 | ppl    99.84\n",
            "| epoch  32 |   800/ 2088 batches | lr 0.000173 | ms/batch 47.80 | loss  4.61 | ppl   100.34\n",
            "| epoch  32 |  1000/ 2088 batches | lr 0.000172 | ms/batch 47.87 | loss  4.66 | ppl   106.02\n",
            "| epoch  32 |  1200/ 2088 batches | lr 0.000172 | ms/batch 47.82 | loss  4.70 | ppl   110.27\n",
            "| epoch  32 |  1400/ 2088 batches | lr 0.000172 | ms/batch 47.80 | loss  4.66 | ppl   105.51\n",
            "| epoch  32 |  1600/ 2088 batches | lr 0.000172 | ms/batch 47.79 | loss  4.57 | ppl    96.75\n",
            "| epoch  32 |  1800/ 2088 batches | lr 0.000171 | ms/batch 47.80 | loss  4.65 | ppl   104.64\n",
            "| epoch  32 |  2000/ 2088 batches | lr 0.000171 | ms/batch 47.84 | loss  4.62 | ppl   101.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 102.86s | valid loss  6.43 | valid ppl   618.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000171\n",
            "| epoch  33 |   200/ 2088 batches | lr 0.000171 | ms/batch 48.05 | loss  4.67 | ppl   106.36\n",
            "| epoch  33 |   400/ 2088 batches | lr 0.000170 | ms/batch 47.94 | loss  4.62 | ppl   101.41\n",
            "| epoch  33 |   600/ 2088 batches | lr 0.000170 | ms/batch 47.99 | loss  4.59 | ppl    98.16\n",
            "| epoch  33 |   800/ 2088 batches | lr 0.000170 | ms/batch 48.04 | loss  4.59 | ppl    98.40\n",
            "| epoch  33 |  1000/ 2088 batches | lr 0.000170 | ms/batch 48.05 | loss  4.65 | ppl   104.11\n",
            "| epoch  33 |  1200/ 2088 batches | lr 0.000169 | ms/batch 48.11 | loss  4.69 | ppl   108.61\n",
            "| epoch  33 |  1400/ 2088 batches | lr 0.000169 | ms/batch 48.11 | loss  4.64 | ppl   103.78\n",
            "| epoch  33 |  1600/ 2088 batches | lr 0.000169 | ms/batch 48.02 | loss  4.55 | ppl    94.72\n",
            "| epoch  33 |  1800/ 2088 batches | lr 0.000169 | ms/batch 47.92 | loss  4.63 | ppl   102.53\n",
            "| epoch  33 |  2000/ 2088 batches | lr 0.000168 | ms/batch 47.88 | loss  4.60 | ppl    99.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 103.21s | valid loss  6.43 | valid ppl   621.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000168\n",
            "| epoch  34 |   200/ 2088 batches | lr 0.000168 | ms/batch 48.08 | loss  4.65 | ppl   104.19\n",
            "| epoch  34 |   400/ 2088 batches | lr 0.000168 | ms/batch 47.77 | loss  4.60 | ppl    99.88\n",
            "| epoch  34 |   600/ 2088 batches | lr 0.000168 | ms/batch 47.81 | loss  4.56 | ppl    95.99\n",
            "| epoch  34 |   800/ 2088 batches | lr 0.000167 | ms/batch 47.87 | loss  4.57 | ppl    96.81\n",
            "| epoch  34 |  1000/ 2088 batches | lr 0.000167 | ms/batch 47.87 | loss  4.63 | ppl   102.15\n",
            "| epoch  34 |  1200/ 2088 batches | lr 0.000167 | ms/batch 47.89 | loss  4.67 | ppl   106.40\n",
            "| epoch  34 |  1400/ 2088 batches | lr 0.000167 | ms/batch 47.83 | loss  4.63 | ppl   102.30\n",
            "| epoch  34 |  1600/ 2088 batches | lr 0.000166 | ms/batch 47.77 | loss  4.53 | ppl    93.11\n",
            "| epoch  34 |  1800/ 2088 batches | lr 0.000166 | ms/batch 47.82 | loss  4.61 | ppl   100.81\n",
            "| epoch  34 |  2000/ 2088 batches | lr 0.000166 | ms/batch 47.98 | loss  4.58 | ppl    97.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 102.92s | valid loss  6.44 | valid ppl   627.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000166\n",
            "| epoch  35 |   200/ 2088 batches | lr 0.000166 | ms/batch 48.04 | loss  4.63 | ppl   102.79\n",
            "| epoch  35 |   400/ 2088 batches | lr 0.000165 | ms/batch 47.80 | loss  4.59 | ppl    98.05\n",
            "| epoch  35 |   600/ 2088 batches | lr 0.000165 | ms/batch 47.85 | loss  4.55 | ppl    94.77\n",
            "| epoch  35 |   800/ 2088 batches | lr 0.000165 | ms/batch 47.83 | loss  4.55 | ppl    94.95\n",
            "| epoch  35 |  1000/ 2088 batches | lr 0.000165 | ms/batch 47.81 | loss  4.61 | ppl   100.67\n",
            "| epoch  35 |  1200/ 2088 batches | lr 0.000164 | ms/batch 47.84 | loss  4.65 | ppl   104.62\n",
            "| epoch  35 |  1400/ 2088 batches | lr 0.000164 | ms/batch 47.83 | loss  4.61 | ppl   100.17\n",
            "| epoch  35 |  1600/ 2088 batches | lr 0.000164 | ms/batch 47.85 | loss  4.52 | ppl    91.50\n",
            "| epoch  35 |  1800/ 2088 batches | lr 0.000164 | ms/batch 47.88 | loss  4.60 | ppl    99.29\n",
            "| epoch  35 |  2000/ 2088 batches | lr 0.000164 | ms/batch 47.85 | loss  4.56 | ppl    95.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 102.88s | valid loss  6.45 | valid ppl   631.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000163\n",
            "| epoch  36 |   200/ 2088 batches | lr 0.000163 | ms/batch 48.07 | loss  4.61 | ppl   100.81\n",
            "| epoch  36 |   400/ 2088 batches | lr 0.000163 | ms/batch 47.82 | loss  4.57 | ppl    96.27\n",
            "| epoch  36 |   600/ 2088 batches | lr 0.000163 | ms/batch 47.86 | loss  4.53 | ppl    93.05\n",
            "| epoch  36 |   800/ 2088 batches | lr 0.000163 | ms/batch 47.85 | loss  4.54 | ppl    93.28\n",
            "| epoch  36 |  1000/ 2088 batches | lr 0.000162 | ms/batch 47.84 | loss  4.59 | ppl    98.31\n",
            "| epoch  36 |  1200/ 2088 batches | lr 0.000162 | ms/batch 47.81 | loss  4.63 | ppl   102.46\n",
            "| epoch  36 |  1400/ 2088 batches | lr 0.000162 | ms/batch 47.86 | loss  4.59 | ppl    98.24\n",
            "| epoch  36 |  1600/ 2088 batches | lr 0.000162 | ms/batch 47.84 | loss  4.50 | ppl    90.05\n",
            "| epoch  36 |  1800/ 2088 batches | lr 0.000161 | ms/batch 47.82 | loss  4.58 | ppl    97.56\n",
            "| epoch  36 |  2000/ 2088 batches | lr 0.000161 | ms/batch 47.81 | loss  4.55 | ppl    94.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 102.93s | valid loss  6.46 | valid ppl   636.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000161\n",
            "| epoch  37 |   200/ 2088 batches | lr 0.000161 | ms/batch 48.03 | loss  4.60 | ppl    99.00\n",
            "| epoch  37 |   400/ 2088 batches | lr 0.000161 | ms/batch 47.81 | loss  4.55 | ppl    94.57\n",
            "| epoch  37 |   600/ 2088 batches | lr 0.000161 | ms/batch 47.82 | loss  4.52 | ppl    91.60\n",
            "| epoch  37 |   800/ 2088 batches | lr 0.000160 | ms/batch 47.83 | loss  4.52 | ppl    91.68\n",
            "| epoch  37 |  1000/ 2088 batches | lr 0.000160 | ms/batch 47.87 | loss  4.57 | ppl    96.87\n",
            "| epoch  37 |  1200/ 2088 batches | lr 0.000160 | ms/batch 47.84 | loss  4.62 | ppl   101.05\n",
            "| epoch  37 |  1400/ 2088 batches | lr 0.000160 | ms/batch 47.79 | loss  4.57 | ppl    96.80\n",
            "| epoch  37 |  1600/ 2088 batches | lr 0.000159 | ms/batch 47.89 | loss  4.49 | ppl    88.69\n",
            "| epoch  37 |  1800/ 2088 batches | lr 0.000159 | ms/batch 47.98 | loss  4.56 | ppl    95.79\n",
            "| epoch  37 |  2000/ 2088 batches | lr 0.000159 | ms/batch 47.88 | loss  4.53 | ppl    92.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 102.92s | valid loss  6.46 | valid ppl   641.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000159\n",
            "| epoch  38 |   200/ 2088 batches | lr 0.000159 | ms/batch 48.09 | loss  4.58 | ppl    97.38\n",
            "| epoch  38 |   400/ 2088 batches | lr 0.000159 | ms/batch 47.82 | loss  4.53 | ppl    92.95\n",
            "| epoch  38 |   600/ 2088 batches | lr 0.000158 | ms/batch 47.79 | loss  4.50 | ppl    89.96\n",
            "| epoch  38 |   800/ 2088 batches | lr 0.000158 | ms/batch 47.78 | loss  4.50 | ppl    90.14\n",
            "| epoch  38 |  1000/ 2088 batches | lr 0.000158 | ms/batch 47.83 | loss  4.56 | ppl    95.22\n",
            "| epoch  38 |  1200/ 2088 batches | lr 0.000158 | ms/batch 47.83 | loss  4.60 | ppl    99.20\n",
            "| epoch  38 |  1400/ 2088 batches | lr 0.000158 | ms/batch 47.84 | loss  4.56 | ppl    95.25\n",
            "| epoch  38 |  1600/ 2088 batches | lr 0.000157 | ms/batch 47.84 | loss  4.47 | ppl    87.13\n",
            "| epoch  38 |  1800/ 2088 batches | lr 0.000157 | ms/batch 47.80 | loss  4.55 | ppl    94.59\n",
            "| epoch  38 |  2000/ 2088 batches | lr 0.000157 | ms/batch 47.82 | loss  4.52 | ppl    91.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 102.88s | valid loss  6.48 | valid ppl   654.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000157\n",
            "| epoch  39 |   200/ 2088 batches | lr 0.000157 | ms/batch 48.08 | loss  4.57 | ppl    96.16\n",
            "| epoch  39 |   400/ 2088 batches | lr 0.000156 | ms/batch 47.75 | loss  4.52 | ppl    91.97\n",
            "| epoch  39 |   600/ 2088 batches | lr 0.000156 | ms/batch 47.79 | loss  4.48 | ppl    88.51\n",
            "| epoch  39 |   800/ 2088 batches | lr 0.000156 | ms/batch 47.80 | loss  4.49 | ppl    88.85\n",
            "| epoch  39 |  1000/ 2088 batches | lr 0.000156 | ms/batch 47.85 | loss  4.54 | ppl    93.78\n",
            "| epoch  39 |  1200/ 2088 batches | lr 0.000156 | ms/batch 47.85 | loss  4.59 | ppl    98.21\n",
            "| epoch  39 |  1400/ 2088 batches | lr 0.000155 | ms/batch 47.86 | loss  4.54 | ppl    93.71\n",
            "| epoch  39 |  1600/ 2088 batches | lr 0.000155 | ms/batch 47.80 | loss  4.45 | ppl    85.83\n",
            "| epoch  39 |  1800/ 2088 batches | lr 0.000155 | ms/batch 47.82 | loss  4.53 | ppl    93.06\n",
            "| epoch  39 |  2000/ 2088 batches | lr 0.000155 | ms/batch 47.78 | loss  4.50 | ppl    90.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 102.85s | valid loss  6.48 | valid ppl   653.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000155\n",
            "| epoch  40 |   200/ 2088 batches | lr 0.000155 | ms/batch 48.03 | loss  4.55 | ppl    94.31\n",
            "| epoch  40 |   400/ 2088 batches | lr 0.000154 | ms/batch 47.83 | loss  4.50 | ppl    90.46\n",
            "| epoch  40 |   600/ 2088 batches | lr 0.000154 | ms/batch 47.81 | loss  4.47 | ppl    87.24\n",
            "| epoch  40 |   800/ 2088 batches | lr 0.000154 | ms/batch 47.82 | loss  4.47 | ppl    87.44\n",
            "| epoch  40 |  1000/ 2088 batches | lr 0.000154 | ms/batch 47.84 | loss  4.52 | ppl    91.93\n",
            "| epoch  40 |  1200/ 2088 batches | lr 0.000154 | ms/batch 47.81 | loss  4.57 | ppl    96.08\n",
            "| epoch  40 |  1400/ 2088 batches | lr 0.000154 | ms/batch 47.76 | loss  4.52 | ppl    92.17\n",
            "| epoch  40 |  1600/ 2088 batches | lr 0.000153 | ms/batch 47.90 | loss  4.43 | ppl    84.28\n",
            "| epoch  40 |  1800/ 2088 batches | lr 0.000153 | ms/batch 47.91 | loss  4.52 | ppl    91.64\n",
            "| epoch  40 |  2000/ 2088 batches | lr 0.000153 | ms/batch 47.85 | loss  4.49 | ppl    88.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 102.88s | valid loss  6.50 | valid ppl   662.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000153\n",
            "| epoch  41 |   200/ 2088 batches | lr 0.000153 | ms/batch 48.07 | loss  4.53 | ppl    93.02\n",
            "| epoch  41 |   400/ 2088 batches | lr 0.000153 | ms/batch 47.80 | loss  4.49 | ppl    89.02\n",
            "| epoch  41 |   600/ 2088 batches | lr 0.000152 | ms/batch 47.79 | loss  4.45 | ppl    85.56\n",
            "| epoch  41 |   800/ 2088 batches | lr 0.000152 | ms/batch 47.85 | loss  4.46 | ppl    86.25\n",
            "| epoch  41 |  1000/ 2088 batches | lr 0.000152 | ms/batch 47.80 | loss  4.51 | ppl    90.50\n",
            "| epoch  41 |  1200/ 2088 batches | lr 0.000152 | ms/batch 47.81 | loss  4.55 | ppl    94.46\n",
            "| epoch  41 |  1400/ 2088 batches | lr 0.000152 | ms/batch 47.83 | loss  4.51 | ppl    90.59\n",
            "| epoch  41 |  1600/ 2088 batches | lr 0.000151 | ms/batch 47.88 | loss  4.42 | ppl    83.16\n",
            "| epoch  41 |  1800/ 2088 batches | lr 0.000151 | ms/batch 47.85 | loss  4.50 | ppl    89.94\n",
            "| epoch  41 |  2000/ 2088 batches | lr 0.000151 | ms/batch 47.88 | loss  4.46 | ppl    86.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 102.90s | valid loss  6.50 | valid ppl   665.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000151\n",
            "| epoch  42 |   200/ 2088 batches | lr 0.000151 | ms/batch 48.05 | loss  4.51 | ppl    91.15\n",
            "| epoch  42 |   400/ 2088 batches | lr 0.000151 | ms/batch 47.74 | loss  4.47 | ppl    87.55\n",
            "| epoch  42 |   600/ 2088 batches | lr 0.000150 | ms/batch 47.75 | loss  4.43 | ppl    84.35\n",
            "| epoch  42 |   800/ 2088 batches | lr 0.000150 | ms/batch 47.76 | loss  4.43 | ppl    84.25\n",
            "| epoch  42 |  1000/ 2088 batches | lr 0.000150 | ms/batch 47.79 | loss  4.49 | ppl    88.84\n",
            "| epoch  42 |  1200/ 2088 batches | lr 0.000150 | ms/batch 47.81 | loss  4.53 | ppl    92.41\n",
            "| epoch  42 |  1400/ 2088 batches | lr 0.000150 | ms/batch 47.81 | loss  4.49 | ppl    88.73\n",
            "| epoch  42 |  1600/ 2088 batches | lr 0.000150 | ms/batch 47.83 | loss  4.40 | ppl    81.51\n",
            "| epoch  42 |  1800/ 2088 batches | lr 0.000149 | ms/batch 47.78 | loss  4.48 | ppl    88.26\n",
            "| epoch  42 |  2000/ 2088 batches | lr 0.000149 | ms/batch 47.78 | loss  4.45 | ppl    85.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 102.81s | valid loss  6.51 | valid ppl   674.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000149\n",
            "| epoch  43 |   200/ 2088 batches | lr 0.000149 | ms/batch 48.02 | loss  4.50 | ppl    89.71\n",
            "| epoch  43 |   400/ 2088 batches | lr 0.000149 | ms/batch 47.79 | loss  4.45 | ppl    85.88\n",
            "| epoch  43 |   600/ 2088 batches | lr 0.000149 | ms/batch 47.79 | loss  4.42 | ppl    82.98\n",
            "| epoch  43 |   800/ 2088 batches | lr 0.000149 | ms/batch 47.79 | loss  4.42 | ppl    83.02\n",
            "| epoch  43 |  1000/ 2088 batches | lr 0.000148 | ms/batch 47.81 | loss  4.48 | ppl    87.90\n",
            "| epoch  43 |  1200/ 2088 batches | lr 0.000148 | ms/batch 47.84 | loss  4.51 | ppl    91.27\n",
            "| epoch  43 |  1400/ 2088 batches | lr 0.000148 | ms/batch 47.82 | loss  4.47 | ppl    87.32\n",
            "| epoch  43 |  1600/ 2088 batches | lr 0.000148 | ms/batch 47.84 | loss  4.39 | ppl    80.26\n",
            "| epoch  43 |  1800/ 2088 batches | lr 0.000148 | ms/batch 47.85 | loss  4.47 | ppl    87.09\n",
            "| epoch  43 |  2000/ 2088 batches | lr 0.000148 | ms/batch 47.83 | loss  4.43 | ppl    84.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 102.87s | valid loss  6.52 | valid ppl   677.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000147\n",
            "| epoch  44 |   200/ 2088 batches | lr 0.000147 | ms/batch 48.17 | loss  4.48 | ppl    88.62\n",
            "| epoch  44 |   400/ 2088 batches | lr 0.000147 | ms/batch 47.83 | loss  4.44 | ppl    84.78\n",
            "| epoch  44 |   600/ 2088 batches | lr 0.000147 | ms/batch 47.79 | loss  4.40 | ppl    81.80\n",
            "| epoch  44 |   800/ 2088 batches | lr 0.000147 | ms/batch 47.78 | loss  4.41 | ppl    82.08\n",
            "| epoch  44 |  1000/ 2088 batches | lr 0.000147 | ms/batch 47.87 | loss  4.46 | ppl    86.25\n",
            "| epoch  44 |  1200/ 2088 batches | lr 0.000146 | ms/batch 47.81 | loss  4.50 | ppl    89.93\n",
            "| epoch  44 |  1400/ 2088 batches | lr 0.000146 | ms/batch 47.84 | loss  4.46 | ppl    86.13\n",
            "| epoch  44 |  1600/ 2088 batches | lr 0.000146 | ms/batch 47.82 | loss  4.37 | ppl    79.21\n",
            "| epoch  44 |  1800/ 2088 batches | lr 0.000146 | ms/batch 47.82 | loss  4.45 | ppl    85.31\n",
            "| epoch  44 |  2000/ 2088 batches | lr 0.000146 | ms/batch 47.81 | loss  4.42 | ppl    83.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 102.92s | valid loss  6.53 | valid ppl   682.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000146\n",
            "| epoch  45 |   200/ 2088 batches | lr 0.000146 | ms/batch 48.08 | loss  4.47 | ppl    87.47\n",
            "| epoch  45 |   400/ 2088 batches | lr 0.000145 | ms/batch 47.79 | loss  4.42 | ppl    83.01\n",
            "| epoch  45 |   600/ 2088 batches | lr 0.000145 | ms/batch 47.80 | loss  4.39 | ppl    80.34\n",
            "| epoch  45 |   800/ 2088 batches | lr 0.000145 | ms/batch 47.81 | loss  4.39 | ppl    80.79\n",
            "| epoch  45 |  1000/ 2088 batches | lr 0.000145 | ms/batch 47.83 | loss  4.44 | ppl    85.07\n",
            "| epoch  45 |  1200/ 2088 batches | lr 0.000145 | ms/batch 47.79 | loss  4.48 | ppl    88.56\n",
            "| epoch  45 |  1400/ 2088 batches | lr 0.000145 | ms/batch 47.87 | loss  4.44 | ppl    85.02\n",
            "| epoch  45 |  1600/ 2088 batches | lr 0.000145 | ms/batch 47.80 | loss  4.36 | ppl    78.28\n",
            "| epoch  45 |  1800/ 2088 batches | lr 0.000144 | ms/batch 47.83 | loss  4.44 | ppl    84.52\n",
            "| epoch  45 |  2000/ 2088 batches | lr 0.000144 | ms/batch 47.78 | loss  4.40 | ppl    81.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 102.83s | valid loss  6.53 | valid ppl   688.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000144\n",
            "| epoch  46 |   200/ 2088 batches | lr 0.000144 | ms/batch 48.06 | loss  4.46 | ppl    86.41\n",
            "| epoch  46 |   400/ 2088 batches | lr 0.000144 | ms/batch 47.83 | loss  4.41 | ppl    82.35\n",
            "| epoch  46 |   600/ 2088 batches | lr 0.000144 | ms/batch 47.78 | loss  4.38 | ppl    79.49\n",
            "| epoch  46 |   800/ 2088 batches | lr 0.000144 | ms/batch 47.80 | loss  4.38 | ppl    79.71\n",
            "| epoch  46 |  1000/ 2088 batches | lr 0.000143 | ms/batch 47.84 | loss  4.43 | ppl    83.67\n",
            "| epoch  46 |  1200/ 2088 batches | lr 0.000143 | ms/batch 47.81 | loss  4.47 | ppl    87.33\n",
            "| epoch  46 |  1400/ 2088 batches | lr 0.000143 | ms/batch 47.80 | loss  4.43 | ppl    83.67\n",
            "| epoch  46 |  1600/ 2088 batches | lr 0.000143 | ms/batch 47.79 | loss  4.35 | ppl    77.17\n",
            "| epoch  46 |  1800/ 2088 batches | lr 0.000143 | ms/batch 47.80 | loss  4.42 | ppl    83.29\n",
            "| epoch  46 |  2000/ 2088 batches | lr 0.000143 | ms/batch 47.86 | loss  4.39 | ppl    80.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 102.89s | valid loss  6.54 | valid ppl   693.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000143\n",
            "| epoch  47 |   200/ 2088 batches | lr 0.000142 | ms/batch 48.16 | loss  4.44 | ppl    85.06\n",
            "| epoch  47 |   400/ 2088 batches | lr 0.000142 | ms/batch 47.80 | loss  4.40 | ppl    81.19\n",
            "| epoch  47 |   600/ 2088 batches | lr 0.000142 | ms/batch 47.84 | loss  4.36 | ppl    78.39\n",
            "| epoch  47 |   800/ 2088 batches | lr 0.000142 | ms/batch 47.80 | loss  4.37 | ppl    78.82\n",
            "| epoch  47 |  1000/ 2088 batches | lr 0.000142 | ms/batch 47.82 | loss  4.42 | ppl    82.69\n",
            "| epoch  47 |  1200/ 2088 batches | lr 0.000142 | ms/batch 47.78 | loss  4.46 | ppl    86.16\n",
            "| epoch  47 |  1400/ 2088 batches | lr 0.000142 | ms/batch 47.84 | loss  4.42 | ppl    82.75\n",
            "| epoch  47 |  1600/ 2088 batches | lr 0.000141 | ms/batch 48.04 | loss  4.33 | ppl    76.13\n",
            "| epoch  47 |  1800/ 2088 batches | lr 0.000141 | ms/batch 48.05 | loss  4.41 | ppl    82.00\n",
            "| epoch  47 |  2000/ 2088 batches | lr 0.000141 | ms/batch 48.07 | loss  4.38 | ppl    79.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 103.06s | valid loss  6.55 | valid ppl   696.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000141\n",
            "| epoch  48 |   200/ 2088 batches | lr 0.000141 | ms/batch 48.26 | loss  4.43 | ppl    83.83\n",
            "| epoch  48 |   400/ 2088 batches | lr 0.000141 | ms/batch 47.99 | loss  4.38 | ppl    79.81\n",
            "| epoch  48 |   600/ 2088 batches | lr 0.000141 | ms/batch 47.96 | loss  4.35 | ppl    77.27\n",
            "| epoch  48 |   800/ 2088 batches | lr 0.000140 | ms/batch 47.79 | loss  4.35 | ppl    77.67\n",
            "| epoch  48 |  1000/ 2088 batches | lr 0.000140 | ms/batch 47.85 | loss  4.40 | ppl    81.53\n",
            "| epoch  48 |  1200/ 2088 batches | lr 0.000140 | ms/batch 47.90 | loss  4.44 | ppl    84.90\n",
            "| epoch  48 |  1400/ 2088 batches | lr 0.000140 | ms/batch 47.86 | loss  4.40 | ppl    81.54\n",
            "| epoch  48 |  1600/ 2088 batches | lr 0.000140 | ms/batch 47.86 | loss  4.32 | ppl    74.98\n",
            "| epoch  48 |  1800/ 2088 batches | lr 0.000140 | ms/batch 47.88 | loss  4.40 | ppl    81.08\n",
            "| epoch  48 |  2000/ 2088 batches | lr 0.000140 | ms/batch 47.81 | loss  4.36 | ppl    78.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 102.99s | valid loss  6.55 | valid ppl   702.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000140\n",
            "| epoch  49 |   200/ 2088 batches | lr 0.000139 | ms/batch 48.08 | loss  4.41 | ppl    82.66\n",
            "| epoch  49 |   400/ 2088 batches | lr 0.000139 | ms/batch 47.77 | loss  4.37 | ppl    78.80\n",
            "| epoch  49 |   600/ 2088 batches | lr 0.000139 | ms/batch 47.79 | loss  4.34 | ppl    76.33\n",
            "| epoch  49 |   800/ 2088 batches | lr 0.000139 | ms/batch 47.81 | loss  4.34 | ppl    76.51\n",
            "| epoch  49 |  1000/ 2088 batches | lr 0.000139 | ms/batch 47.81 | loss  4.39 | ppl    80.43\n",
            "| epoch  49 |  1200/ 2088 batches | lr 0.000139 | ms/batch 47.81 | loss  4.43 | ppl    83.57\n",
            "| epoch  49 |  1400/ 2088 batches | lr 0.000139 | ms/batch 47.82 | loss  4.39 | ppl    80.25\n",
            "| epoch  49 |  1600/ 2088 batches | lr 0.000138 | ms/batch 47.77 | loss  4.30 | ppl    73.85\n",
            "| epoch  49 |  1800/ 2088 batches | lr 0.000138 | ms/batch 47.80 | loss  4.38 | ppl    79.61\n",
            "| epoch  49 |  2000/ 2088 batches | lr 0.000138 | ms/batch 47.89 | loss  4.35 | ppl    77.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 102.88s | valid loss  6.57 | valid ppl   712.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000138\n",
            "| epoch  50 |   200/ 2088 batches | lr 0.000138 | ms/batch 48.07 | loss  4.40 | ppl    81.45\n",
            "| epoch  50 |   400/ 2088 batches | lr 0.000138 | ms/batch 47.89 | loss  4.35 | ppl    77.48\n",
            "| epoch  50 |   600/ 2088 batches | lr 0.000138 | ms/batch 47.83 | loss  4.32 | ppl    75.14\n",
            "| epoch  50 |   800/ 2088 batches | lr 0.000138 | ms/batch 47.77 | loss  4.32 | ppl    75.34\n",
            "| epoch  50 |  1000/ 2088 batches | lr 0.000137 | ms/batch 47.80 | loss  4.37 | ppl    79.10\n",
            "| epoch  50 |  1200/ 2088 batches | lr 0.000137 | ms/batch 47.75 | loss  4.41 | ppl    82.22\n",
            "| epoch  50 |  1400/ 2088 batches | lr 0.000137 | ms/batch 47.86 | loss  4.37 | ppl    79.37\n",
            "| epoch  50 |  1600/ 2088 batches | lr 0.000137 | ms/batch 47.85 | loss  4.29 | ppl    72.81\n",
            "| epoch  50 |  1800/ 2088 batches | lr 0.000137 | ms/batch 47.87 | loss  4.37 | ppl    78.76\n",
            "| epoch  50 |  2000/ 2088 batches | lr 0.000137 | ms/batch 47.83 | loss  4.33 | ppl    75.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 102.86s | valid loss  6.58 | valid ppl   719.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000137\n",
            "| epoch  51 |   200/ 2088 batches | lr 0.000137 | ms/batch 48.05 | loss  4.38 | ppl    80.08\n",
            "| epoch  51 |   400/ 2088 batches | lr 0.000136 | ms/batch 47.75 | loss  4.34 | ppl    76.36\n",
            "| epoch  51 |   600/ 2088 batches | lr 0.000136 | ms/batch 47.76 | loss  4.30 | ppl    73.90\n",
            "| epoch  51 |   800/ 2088 batches | lr 0.000136 | ms/batch 47.76 | loss  4.31 | ppl    74.34\n",
            "| epoch  51 |  1000/ 2088 batches | lr 0.000136 | ms/batch 47.86 | loss  4.35 | ppl    77.83\n",
            "| epoch  51 |  1200/ 2088 batches | lr 0.000136 | ms/batch 47.83 | loss  4.39 | ppl    80.86\n",
            "| epoch  51 |  1400/ 2088 batches | lr 0.000136 | ms/batch 47.83 | loss  4.36 | ppl    77.89\n",
            "| epoch  51 |  1600/ 2088 batches | lr 0.000136 | ms/batch 47.86 | loss  4.27 | ppl    71.78\n",
            "| epoch  51 |  1800/ 2088 batches | lr 0.000136 | ms/batch 47.80 | loss  4.35 | ppl    77.42\n",
            "| epoch  51 |  2000/ 2088 batches | lr 0.000135 | ms/batch 47.80 | loss  4.32 | ppl    75.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 102.84s | valid loss  6.58 | valid ppl   721.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000135\n",
            "| epoch  52 |   200/ 2088 batches | lr 0.000135 | ms/batch 47.99 | loss  4.37 | ppl    79.01\n",
            "| epoch  52 |   400/ 2088 batches | lr 0.000135 | ms/batch 47.80 | loss  4.32 | ppl    75.27\n",
            "| epoch  52 |   600/ 2088 batches | lr 0.000135 | ms/batch 47.83 | loss  4.29 | ppl    73.02\n",
            "| epoch  52 |   800/ 2088 batches | lr 0.000135 | ms/batch 47.84 | loss  4.30 | ppl    73.40\n",
            "| epoch  52 |  1000/ 2088 batches | lr 0.000135 | ms/batch 47.80 | loss  4.34 | ppl    76.97\n",
            "| epoch  52 |  1200/ 2088 batches | lr 0.000135 | ms/batch 47.80 | loss  4.38 | ppl    80.08\n",
            "| epoch  52 |  1400/ 2088 batches | lr 0.000135 | ms/batch 47.82 | loss  4.34 | ppl    77.09\n",
            "| epoch  52 |  1600/ 2088 batches | lr 0.000134 | ms/batch 47.79 | loss  4.26 | ppl    70.77\n",
            "| epoch  52 |  1800/ 2088 batches | lr 0.000134 | ms/batch 47.86 | loss  4.33 | ppl    76.31\n",
            "| epoch  52 |  2000/ 2088 batches | lr 0.000134 | ms/batch 47.98 | loss  4.31 | ppl    74.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 102.95s | valid loss  6.59 | valid ppl   724.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000134\n",
            "| epoch  53 |   200/ 2088 batches | lr 0.000134 | ms/batch 48.09 | loss  4.36 | ppl    77.95\n",
            "| epoch  53 |   400/ 2088 batches | lr 0.000134 | ms/batch 47.76 | loss  4.31 | ppl    74.63\n",
            "| epoch  53 |   600/ 2088 batches | lr 0.000134 | ms/batch 47.88 | loss  4.28 | ppl    71.95\n",
            "| epoch  53 |   800/ 2088 batches | lr 0.000134 | ms/batch 47.86 | loss  4.28 | ppl    72.23\n",
            "| epoch  53 |  1000/ 2088 batches | lr 0.000133 | ms/batch 47.81 | loss  4.33 | ppl    75.82\n",
            "| epoch  53 |  1200/ 2088 batches | lr 0.000133 | ms/batch 47.82 | loss  4.37 | ppl    79.01\n",
            "| epoch  53 |  1400/ 2088 batches | lr 0.000133 | ms/batch 47.82 | loss  4.33 | ppl    76.04\n",
            "| epoch  53 |  1600/ 2088 batches | lr 0.000133 | ms/batch 47.82 | loss  4.25 | ppl    69.92\n",
            "| epoch  53 |  1800/ 2088 batches | lr 0.000133 | ms/batch 47.88 | loss  4.32 | ppl    75.51\n",
            "| epoch  53 |  2000/ 2088 batches | lr 0.000133 | ms/batch 47.81 | loss  4.30 | ppl    73.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 102.88s | valid loss  6.60 | valid ppl   736.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000133\n",
            "| epoch  54 |   200/ 2088 batches | lr 0.000133 | ms/batch 48.04 | loss  4.35 | ppl    77.12\n",
            "| epoch  54 |   400/ 2088 batches | lr 0.000133 | ms/batch 47.78 | loss  4.30 | ppl    73.63\n",
            "| epoch  54 |   600/ 2088 batches | lr 0.000132 | ms/batch 47.75 | loss  4.27 | ppl    71.25\n",
            "| epoch  54 |   800/ 2088 batches | lr 0.000132 | ms/batch 47.85 | loss  4.27 | ppl    71.67\n",
            "| epoch  54 |  1000/ 2088 batches | lr 0.000132 | ms/batch 47.84 | loss  4.32 | ppl    74.98\n",
            "| epoch  54 |  1200/ 2088 batches | lr 0.000132 | ms/batch 47.81 | loss  4.36 | ppl    78.13\n",
            "| epoch  54 |  1400/ 2088 batches | lr 0.000132 | ms/batch 47.80 | loss  4.32 | ppl    75.16\n",
            "| epoch  54 |  1600/ 2088 batches | lr 0.000132 | ms/batch 47.76 | loss  4.24 | ppl    69.24\n",
            "| epoch  54 |  1800/ 2088 batches | lr 0.000132 | ms/batch 47.77 | loss  4.31 | ppl    74.66\n",
            "| epoch  54 |  2000/ 2088 batches | lr 0.000132 | ms/batch 47.85 | loss  4.29 | ppl    72.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 102.84s | valid loss  6.61 | valid ppl   739.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000132\n",
            "| epoch  55 |   200/ 2088 batches | lr 0.000131 | ms/batch 48.02 | loss  4.33 | ppl    76.24\n",
            "| epoch  55 |   400/ 2088 batches | lr 0.000131 | ms/batch 47.78 | loss  4.28 | ppl    72.59\n",
            "| epoch  55 |   600/ 2088 batches | lr 0.000131 | ms/batch 47.83 | loss  4.26 | ppl    70.55\n",
            "| epoch  55 |   800/ 2088 batches | lr 0.000131 | ms/batch 47.77 | loss  4.26 | ppl    70.84\n",
            "| epoch  55 |  1000/ 2088 batches | lr 0.000131 | ms/batch 47.87 | loss  4.30 | ppl    74.00\n",
            "| epoch  55 |  1200/ 2088 batches | lr 0.000131 | ms/batch 47.79 | loss  4.35 | ppl    77.21\n",
            "| epoch  55 |  1400/ 2088 batches | lr 0.000131 | ms/batch 47.80 | loss  4.30 | ppl    74.02\n",
            "| epoch  55 |  1600/ 2088 batches | lr 0.000131 | ms/batch 47.83 | loss  4.22 | ppl    68.36\n",
            "| epoch  55 |  1800/ 2088 batches | lr 0.000131 | ms/batch 47.89 | loss  4.30 | ppl    73.66\n",
            "| epoch  55 |  2000/ 2088 batches | lr 0.000130 | ms/batch 47.89 | loss  4.27 | ppl    71.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 102.90s | valid loss  6.61 | valid ppl   744.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000130\n",
            "| epoch  56 |   200/ 2088 batches | lr 0.000130 | ms/batch 48.06 | loss  4.32 | ppl    75.11\n",
            "| epoch  56 |   400/ 2088 batches | lr 0.000130 | ms/batch 47.81 | loss  4.27 | ppl    71.70\n",
            "| epoch  56 |   600/ 2088 batches | lr 0.000130 | ms/batch 47.78 | loss  4.24 | ppl    69.43\n",
            "| epoch  56 |   800/ 2088 batches | lr 0.000130 | ms/batch 47.78 | loss  4.24 | ppl    69.73\n",
            "| epoch  56 |  1000/ 2088 batches | lr 0.000130 | ms/batch 47.80 | loss  4.29 | ppl    72.82\n",
            "| epoch  56 |  1200/ 2088 batches | lr 0.000130 | ms/batch 47.80 | loss  4.33 | ppl    75.94\n",
            "| epoch  56 |  1400/ 2088 batches | lr 0.000130 | ms/batch 47.82 | loss  4.29 | ppl    72.85\n",
            "| epoch  56 |  1600/ 2088 batches | lr 0.000129 | ms/batch 47.86 | loss  4.21 | ppl    67.30\n",
            "| epoch  56 |  1800/ 2088 batches | lr 0.000129 | ms/batch 47.83 | loss  4.29 | ppl    72.61\n",
            "| epoch  56 |  2000/ 2088 batches | lr 0.000129 | ms/batch 47.92 | loss  4.25 | ppl    70.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 102.87s | valid loss  6.62 | valid ppl   748.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000129\n",
            "| epoch  57 |   200/ 2088 batches | lr 0.000129 | ms/batch 48.04 | loss  4.31 | ppl    74.12\n",
            "| epoch  57 |   400/ 2088 batches | lr 0.000129 | ms/batch 47.75 | loss  4.26 | ppl    70.67\n",
            "| epoch  57 |   600/ 2088 batches | lr 0.000129 | ms/batch 47.79 | loss  4.22 | ppl    68.32\n",
            "| epoch  57 |   800/ 2088 batches | lr 0.000129 | ms/batch 47.73 | loss  4.23 | ppl    68.54\n",
            "| epoch  57 |  1000/ 2088 batches | lr 0.000129 | ms/batch 47.80 | loss  4.27 | ppl    71.82\n",
            "| epoch  57 |  1200/ 2088 batches | lr 0.000129 | ms/batch 47.80 | loss  4.32 | ppl    75.02\n",
            "| epoch  57 |  1400/ 2088 batches | lr 0.000128 | ms/batch 47.84 | loss  4.28 | ppl    71.98\n",
            "| epoch  57 |  1600/ 2088 batches | lr 0.000128 | ms/batch 47.84 | loss  4.19 | ppl    66.28\n",
            "| epoch  57 |  1800/ 2088 batches | lr 0.000128 | ms/batch 47.82 | loss  4.27 | ppl    71.38\n",
            "| epoch  57 |  2000/ 2088 batches | lr 0.000128 | ms/batch 47.82 | loss  4.24 | ppl    69.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 102.80s | valid loss  6.63 | valid ppl   758.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000128\n",
            "| epoch  58 |   200/ 2088 batches | lr 0.000128 | ms/batch 48.00 | loss  4.29 | ppl    73.12\n",
            "| epoch  58 |   400/ 2088 batches | lr 0.000128 | ms/batch 47.75 | loss  4.24 | ppl    69.68\n",
            "| epoch  58 |   600/ 2088 batches | lr 0.000128 | ms/batch 47.76 | loss  4.21 | ppl    67.40\n",
            "| epoch  58 |   800/ 2088 batches | lr 0.000128 | ms/batch 47.84 | loss  4.22 | ppl    67.85\n",
            "| epoch  58 |  1000/ 2088 batches | lr 0.000128 | ms/batch 47.80 | loss  4.26 | ppl    70.85\n",
            "| epoch  58 |  1200/ 2088 batches | lr 0.000127 | ms/batch 47.84 | loss  4.30 | ppl    73.84\n",
            "| epoch  58 |  1400/ 2088 batches | lr 0.000127 | ms/batch 47.79 | loss  4.26 | ppl    71.14\n",
            "| epoch  58 |  1600/ 2088 batches | lr 0.000127 | ms/batch 47.74 | loss  4.18 | ppl    65.62\n",
            "| epoch  58 |  1800/ 2088 batches | lr 0.000127 | ms/batch 47.89 | loss  4.26 | ppl    70.80\n",
            "| epoch  58 |  2000/ 2088 batches | lr 0.000127 | ms/batch 47.90 | loss  4.23 | ppl    68.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 102.88s | valid loss  6.63 | valid ppl   761.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000127\n",
            "| epoch  59 |   200/ 2088 batches | lr 0.000127 | ms/batch 48.10 | loss  4.28 | ppl    72.10\n",
            "| epoch  59 |   400/ 2088 batches | lr 0.000127 | ms/batch 47.85 | loss  4.23 | ppl    68.92\n",
            "| epoch  59 |   600/ 2088 batches | lr 0.000127 | ms/batch 47.80 | loss  4.20 | ppl    66.81\n",
            "| epoch  59 |   800/ 2088 batches | lr 0.000127 | ms/batch 47.82 | loss  4.21 | ppl    67.06\n",
            "| epoch  59 |  1000/ 2088 batches | lr 0.000126 | ms/batch 47.80 | loss  4.25 | ppl    70.39\n",
            "| epoch  59 |  1200/ 2088 batches | lr 0.000126 | ms/batch 47.71 | loss  4.29 | ppl    73.22\n",
            "| epoch  59 |  1400/ 2088 batches | lr 0.000126 | ms/batch 47.78 | loss  4.25 | ppl    70.40\n",
            "| epoch  59 |  1600/ 2088 batches | lr 0.000126 | ms/batch 47.84 | loss  4.18 | ppl    65.26\n",
            "| epoch  59 |  1800/ 2088 batches | lr 0.000126 | ms/batch 47.87 | loss  4.25 | ppl    69.93\n",
            "| epoch  59 |  2000/ 2088 batches | lr 0.000126 | ms/batch 47.85 | loss  4.22 | ppl    67.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 102.88s | valid loss  6.64 | valid ppl   767.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000126\n",
            "| epoch  60 |   200/ 2088 batches | lr 0.000126 | ms/batch 48.02 | loss  4.27 | ppl    71.73\n",
            "| epoch  60 |   400/ 2088 batches | lr 0.000126 | ms/batch 47.83 | loss  4.22 | ppl    68.32\n",
            "| epoch  60 |   600/ 2088 batches | lr 0.000126 | ms/batch 47.82 | loss  4.19 | ppl    66.24\n",
            "| epoch  60 |   800/ 2088 batches | lr 0.000125 | ms/batch 47.76 | loss  4.20 | ppl    66.54\n",
            "| epoch  60 |  1000/ 2088 batches | lr 0.000125 | ms/batch 47.80 | loss  4.24 | ppl    69.66\n",
            "| epoch  60 |  1200/ 2088 batches | lr 0.000125 | ms/batch 47.81 | loss  4.28 | ppl    72.49\n",
            "| epoch  60 |  1400/ 2088 batches | lr 0.000125 | ms/batch 47.85 | loss  4.24 | ppl    69.39\n",
            "| epoch  60 |  1600/ 2088 batches | lr 0.000125 | ms/batch 47.85 | loss  4.17 | ppl    64.45\n",
            "| epoch  60 |  1800/ 2088 batches | lr 0.000125 | ms/batch 47.77 | loss  4.24 | ppl    69.33\n",
            "| epoch  60 |  2000/ 2088 batches | lr 0.000125 | ms/batch 47.81 | loss  4.21 | ppl    67.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 102.84s | valid loss  6.65 | valid ppl   770.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "=========================================================================================\n",
            "| End of training | test loss  6.26 | test ppl   525.56\n",
            "=========================================================================================\n",
            "\n",
            "Generating text...\n",
            "Vocabulary size: 25251\n",
            "Vocabulary size: 25251\n",
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n",
            "began to run for the final four weeks of a season with the second   best season , scoring with\n",
            "10   50 minutes in 30 years on August 13 minutes . <eos> <eos> <eos> = = per season (\n",
            "6 minutes ) , the fourth consecutive league run by the Bulls started a record of 8   90 minutes\n",
            "in 45 seconds , Ross retired , and final day , the following the year . With <unk> <unk> <unk>\n",
            "and the <unk> in seven starts the second . Hamels recorded two starts a 4   1 – 3  \n",
            "08   0   61   2   3 – 3 – 5 – 7 – 1   0  \n",
            "0   1   57 – 2   39 saves . <eos> <eos> <eos> <unk> on his first <unk> <unk>\n",
            "in a season were the Bulls finished in 1   5   2   2   5 – 4  \n",
            "2   1   79 ⁄ 4   0   7   9 – 10 – 5 days by <unk>\n",
            "3   2 – 8   <unk> 2 – 9   17 – 3 – 2 – 11 starts in\n",
            "his next three . 2 – 1 – 3   1 – 4 – 0   39 – 10  \n",
            "<unk> , a 2 – 4   57 – 1   0   2 – 1   <unk> <unk> (\n",
            "3   <unk> 3   1 – 8   5   <unk> 3 points , striking 3   2  \n",
            "12   39 of 4   2 – 3 – 2   8 – 0   2 – 2  \n",
            "62 strikeouts with 2 – 3   75   75 – 4   08 points per game   2 –\n",
            "3   80   0   4   06 – 0   38 – 2   9   1  \n",
            "47   2 – 2   7   1   98 – 6   0   1   0  \n",
            "5   79 saves , the season , he was <unk> , while the season , while he is an\n",
            "average per game pitched ( 2   90 ERA per game pitched during the 0   8 minutes from the\n",
            "previous 2   85   0   4 – 2   25   <unk> , and 7 <unk> , with\n",
            "7   4 of high career , 5   2   3   8 and 4   0   0\n",
            "  0   3   0   1   8   83 regular season , 4   95   60\n",
            "  2   1   0   2   2   2   5   12   <unk> for the\n",
            "17   6   1   7   90 average   2   3   0   4   0\n",
            "  1 % average goal for the season . , while <unk> 1   62 – 5   6  \n",
            "1   38   79   08 appearances and 2 – 7   0   0 against the 4  \n",
            "1   8   39 games , and 2   85 – 3   38   <unk> 2 – 2\n",
            "– 7 years , on average , while in 27 – 3   2   66   1   <unk>\n",
            "10   7 points in the highest average per game pitched per run during the regular season . , the\n",
            "best set per game pitched runs per season , was 2   6 – 44   60 innings . on\n",
            "his total ( 2   95   83   9   45   79   7   8 strikeouts in\n",
            "his 33   2   2   8 points per second in Los Angeles ( 0 points in his record\n",
            "( 2   9   2   6   5 – 2   3   1   <unk> , and\n",
            "4   17   0   85 – 1   8   7   0 average runs a 6  \n",
            "0   5   0   7 minutes and 1   38 – 0   93 – 0   1\n",
            "  7   2 – 2   12   7   93 on the fourth overall reception came , and\n",
            "4   0   2   0   8 – 7   8   7   79 points and 1\n",
            "  2   1   <unk> of age and 1   75   0 – 7   12   25\n",
            "– 0 , respectively , with the average in 10   2   0 in ( 3   39  \n",
            "4   7   <unk> , 5   1   0   8   94   7   7  \n",
            "0 – 3   7 – 2   79 – 7 – 2   5 in ) , compared for\n",
            "the season ; his total loss ( 0   85   3   0   2   0   7\n",
            "  6   3   2   89 seconds , during the Bulls , and 2   8 strikeouts ,\n",
            "compared to the loss , 0   8 innings , and 41   <unk> – 0 in 53 consecutive average\n",
            "to 4   9   5   6   7 in the year on <unk> in the series , respectively\n",
            ", respectively , and 2 points , the 4   38   5   66 in 37 games per game\n",
            "pitched and 5   6   2   79   75   9   3   5   9  \n",
            "3   89 starts ) shooting . <unk> on the season pitched per second highest loss during which <unk> ,\n",
            "<unk> ; 27   5   <unk> on his overall . <unk> . and 10   89   2  \n",
            "2   6   9   2   7   41   7 , in the All <unk> and 3\n"
          ]
        }
      ],
      "source": [
        "print(\"Training LSTM on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type = 'Transformer', # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "    data_path = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize=512,\n",
        "    nhid=2048,\n",
        "    nlayers=6,\n",
        "    lr=0.0001,\n",
        "    clip=0.25,\n",
        "    epochs=60,\n",
        "    batch_size=20,\n",
        "    bptt=50,\n",
        "    dropout=0.1,\n",
        "    tied=False,\n",
        "    nhead=8,\n",
        "    log_interval=200,\n",
        "    save_path='model_15.pt',\n",
        "    dry_run=False,\n",
        "    accel=True,\n",
        "    use_optimizer=True,\n",
        "    optimizer_type='AdamW',\n",
        "    weight_decay=1e-4,\n",
        "    use_betas=True,\n",
        "    betas=(0.9, 0.98),\n",
        "    use_eps=True,\n",
        "    eps=1e-9,\n",
        "    criterion=None,\n",
        "    use_label_smoothing=True,\n",
        "    label_smoothing=0.1,\n",
        "    use_warmup=True,\n",
        "    warmup_steps=4000,\n",
        "    min_freq=5,\n",
        "    seed=1111,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "# Example 2: Generate text\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model_15.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf='generated_15.txt',\n",
        "    words=1000,\n",
        "    temperature=1.0,\n",
        "    top_k=40,\n",
        "    seed=1111,\n",
        "    log_interval=100,\n",
        "    use_top_k=True,  # Use top-k sampling\n",
        "    min_freq=5,\n",
        "    old_version=False\n",
        ")\n",
        "\n",
        "!cat generated_15.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training LSTM on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type = 'Transformer', # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "    data_path = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize=512,\n",
        "    nhid=2048,\n",
        "    nlayers=6,\n",
        "    lr=0.0001,\n",
        "    clip=0.25,\n",
        "    epochs=60,\n",
        "    batch_size=20,\n",
        "    bptt=50,\n",
        "    dropout=0.1,\n",
        "    tied=False,\n",
        "    nhead=8,\n",
        "    log_interval=200,\n",
        "    save_path='model_16.pt',\n",
        "    onnx_export = '',\n",
        "    dry_run = False,\n",
        "    accel = True,\n",
        "    use_optimizer = True,\n",
        "    optimizer_type = 'AdamW',\n",
        "    weight_decay=1e-5,\n",
        "    use_betas = False,\n",
        "    use_eps = False,\n",
        "    criterion = nn.NLLLoss(),\n",
        "    use_label_smoothing = False,\n",
        "    label_smoothing = 0.1,\n",
        "    use_warmup = False,\n",
        "    warmup_steps = 4000,\n",
        "    min_freq = 5,\n",
        "    seed = 1111,\n",
        "    old_version = True\n",
        ")\n",
        "\n",
        "# Example 2: Generate text\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model_16.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf='generated_16.txt',\n",
        "    words=1000,\n",
        "    temperature=1.0,\n",
        "    seed=1111,\n",
        "    old_version=True,\n",
        "    use_top_k=False,\n",
        "    accel = True\n",
        ")\n",
        "\n",
        "!cat generated_16.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpUKjh7KQ5a7",
        "outputId": "74063450-d75e-4cd9-b60d-5d9e2103ed7a"
      },
      "id": "xpUKjh7KQ5a7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM on WikiText-2...\n",
            "Using device: cuda\n",
            "Vocabulary size: 33278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/ 2088 batches | lr 0.000100 | ms/batch 50.80 | loss  7.37 | ppl  1588.19\n",
            "| epoch   1 |   400/ 2088 batches | lr 0.000100 | ms/batch 50.89 | loss  6.66 | ppl   779.49\n",
            "| epoch   1 |   600/ 2088 batches | lr 0.000100 | ms/batch 51.26 | loss  6.46 | ppl   641.63\n",
            "| epoch   1 |   800/ 2088 batches | lr 0.000100 | ms/batch 51.36 | loss  6.36 | ppl   577.02\n",
            "| epoch   1 |  1000/ 2088 batches | lr 0.000100 | ms/batch 51.17 | loss  6.27 | ppl   530.06\n",
            "| epoch   1 |  1200/ 2088 batches | lr 0.000100 | ms/batch 50.94 | loss  6.21 | ppl   498.88\n",
            "| epoch   1 |  1400/ 2088 batches | lr 0.000100 | ms/batch 50.88 | loss  6.12 | ppl   455.72\n",
            "| epoch   1 |  1600/ 2088 batches | lr 0.000100 | ms/batch 50.68 | loss  6.01 | ppl   407.32\n",
            "| epoch   1 |  1800/ 2088 batches | lr 0.000100 | ms/batch 50.70 | loss  6.02 | ppl   410.96\n",
            "| epoch   1 |  2000/ 2088 batches | lr 0.000100 | ms/batch 50.68 | loss  5.92 | ppl   371.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 108.91s | valid loss  5.72 | valid ppl   305.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000100\n",
            "| epoch   2 |   200/ 2088 batches | lr 0.000100 | ms/batch 50.94 | loss  5.78 | ppl   323.32\n",
            "| epoch   2 |   400/ 2088 batches | lr 0.000100 | ms/batch 50.94 | loss  5.70 | ppl   298.05\n",
            "| epoch   2 |   600/ 2088 batches | lr 0.000100 | ms/batch 50.91 | loss  5.62 | ppl   276.87\n",
            "| epoch   2 |   800/ 2088 batches | lr 0.000100 | ms/batch 50.94 | loss  5.60 | ppl   270.01\n",
            "| epoch   2 |  1000/ 2088 batches | lr 0.000100 | ms/batch 50.94 | loss  5.60 | ppl   270.42\n",
            "| epoch   2 |  1200/ 2088 batches | lr 0.000100 | ms/batch 50.81 | loss  5.59 | ppl   267.78\n",
            "| epoch   2 |  1400/ 2088 batches | lr 0.000100 | ms/batch 50.71 | loss  5.54 | ppl   254.16\n",
            "| epoch   2 |  1600/ 2088 batches | lr 0.000100 | ms/batch 50.74 | loss  5.45 | ppl   232.18\n",
            "| epoch   2 |  1800/ 2088 batches | lr 0.000100 | ms/batch 50.73 | loss  5.49 | ppl   241.99\n",
            "| epoch   2 |  2000/ 2088 batches | lr 0.000100 | ms/batch 50.78 | loss  5.42 | ppl   225.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 108.74s | valid loss  5.44 | valid ppl   230.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000100\n",
            "| epoch   3 |   200/ 2088 batches | lr 0.000100 | ms/batch 51.05 | loss  5.35 | ppl   211.20\n",
            "| epoch   3 |   400/ 2088 batches | lr 0.000100 | ms/batch 50.84 | loss  5.28 | ppl   196.88\n",
            "| epoch   3 |   600/ 2088 batches | lr 0.000100 | ms/batch 50.85 | loss  5.23 | ppl   186.37\n",
            "| epoch   3 |   800/ 2088 batches | lr 0.000100 | ms/batch 50.91 | loss  5.22 | ppl   184.64\n",
            "| epoch   3 |  1000/ 2088 batches | lr 0.000100 | ms/batch 50.88 | loss  5.25 | ppl   190.43\n",
            "| epoch   3 |  1200/ 2088 batches | lr 0.000100 | ms/batch 50.88 | loss  5.25 | ppl   190.19\n",
            "| epoch   3 |  1400/ 2088 batches | lr 0.000100 | ms/batch 50.82 | loss  5.21 | ppl   182.64\n",
            "| epoch   3 |  1600/ 2088 batches | lr 0.000100 | ms/batch 50.81 | loss  5.11 | ppl   165.07\n",
            "| epoch   3 |  1800/ 2088 batches | lr 0.000100 | ms/batch 50.81 | loss  5.16 | ppl   174.98\n",
            "| epoch   3 |  2000/ 2088 batches | lr 0.000100 | ms/batch 50.74 | loss  5.10 | ppl   164.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 108.77s | valid loss  5.30 | valid ppl   200.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000100\n",
            "| epoch   4 |   200/ 2088 batches | lr 0.000100 | ms/batch 51.05 | loss  5.06 | ppl   157.07\n",
            "| epoch   4 |   400/ 2088 batches | lr 0.000100 | ms/batch 50.72 | loss  4.98 | ppl   146.02\n",
            "| epoch   4 |   600/ 2088 batches | lr 0.000100 | ms/batch 50.77 | loss  4.94 | ppl   139.99\n",
            "| epoch   4 |   800/ 2088 batches | lr 0.000100 | ms/batch 50.84 | loss  4.94 | ppl   139.79\n",
            "| epoch   4 |  1000/ 2088 batches | lr 0.000100 | ms/batch 50.77 | loss  4.98 | ppl   146.16\n",
            "| epoch   4 |  1200/ 2088 batches | lr 0.000100 | ms/batch 50.81 | loss  4.99 | ppl   147.16\n",
            "| epoch   4 |  1400/ 2088 batches | lr 0.000100 | ms/batch 50.82 | loss  4.95 | ppl   141.76\n",
            "| epoch   4 |  1600/ 2088 batches | lr 0.000100 | ms/batch 50.74 | loss  4.85 | ppl   127.42\n",
            "| epoch   4 |  1800/ 2088 batches | lr 0.000100 | ms/batch 50.72 | loss  4.91 | ppl   135.56\n",
            "| epoch   4 |  2000/ 2088 batches | lr 0.000100 | ms/batch 50.78 | loss  4.86 | ppl   128.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 108.68s | valid loss  5.21 | valid ppl   182.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000100\n",
            "| epoch   5 |   200/ 2088 batches | lr 0.000100 | ms/batch 51.04 | loss  4.82 | ppl   124.22\n",
            "| epoch   5 |   400/ 2088 batches | lr 0.000100 | ms/batch 50.79 | loss  4.75 | ppl   115.16\n",
            "| epoch   5 |   600/ 2088 batches | lr 0.000100 | ms/batch 50.73 | loss  4.71 | ppl   110.87\n",
            "| epoch   5 |   800/ 2088 batches | lr 0.000100 | ms/batch 50.77 | loss  4.71 | ppl   110.86\n",
            "| epoch   5 |  1000/ 2088 batches | lr 0.000100 | ms/batch 50.78 | loss  4.76 | ppl   117.15\n",
            "| epoch   5 |  1200/ 2088 batches | lr 0.000100 | ms/batch 50.74 | loss  4.77 | ppl   118.49\n",
            "| epoch   5 |  1400/ 2088 batches | lr 0.000100 | ms/batch 50.75 | loss  4.74 | ppl   114.53\n",
            "| epoch   5 |  1600/ 2088 batches | lr 0.000100 | ms/batch 50.74 | loss  4.63 | ppl   102.19\n",
            "| epoch   5 |  1800/ 2088 batches | lr 0.000100 | ms/batch 50.81 | loss  4.69 | ppl   109.38\n",
            "| epoch   5 |  2000/ 2088 batches | lr 0.000100 | ms/batch 50.75 | loss  4.64 | ppl   103.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 108.64s | valid loss  5.15 | valid ppl   173.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000100\n",
            "| epoch   6 |   200/ 2088 batches | lr 0.000100 | ms/batch 50.95 | loss  4.62 | ppl   101.08\n",
            "| epoch   6 |   400/ 2088 batches | lr 0.000100 | ms/batch 50.79 | loss  4.54 | ppl    93.61\n",
            "| epoch   6 |   600/ 2088 batches | lr 0.000100 | ms/batch 50.80 | loss  4.51 | ppl    90.85\n",
            "| epoch   6 |   800/ 2088 batches | lr 0.000100 | ms/batch 50.73 | loss  4.51 | ppl    90.86\n",
            "| epoch   6 |  1000/ 2088 batches | lr 0.000100 | ms/batch 50.81 | loss  4.57 | ppl    96.17\n",
            "| epoch   6 |  1200/ 2088 batches | lr 0.000100 | ms/batch 50.77 | loss  4.58 | ppl    97.73\n",
            "| epoch   6 |  1400/ 2088 batches | lr 0.000100 | ms/batch 50.80 | loss  4.55 | ppl    95.09\n",
            "| epoch   6 |  1600/ 2088 batches | lr 0.000100 | ms/batch 50.72 | loss  4.43 | ppl    83.87\n",
            "| epoch   6 |  1800/ 2088 batches | lr 0.000100 | ms/batch 50.76 | loss  4.51 | ppl    90.61\n",
            "| epoch   6 |  2000/ 2088 batches | lr 0.000100 | ms/batch 50.74 | loss  4.46 | ppl    86.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 108.63s | valid loss  5.12 | valid ppl   167.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000100\n",
            "| epoch   7 |   200/ 2088 batches | lr 0.000100 | ms/batch 51.00 | loss  4.44 | ppl    84.36\n",
            "| epoch   7 |   400/ 2088 batches | lr 0.000100 | ms/batch 50.72 | loss  4.36 | ppl    78.04\n",
            "| epoch   7 |   600/ 2088 batches | lr 0.000100 | ms/batch 50.86 | loss  4.33 | ppl    75.66\n",
            "| epoch   7 |   800/ 2088 batches | lr 0.000100 | ms/batch 50.86 | loss  4.33 | ppl    75.99\n",
            "| epoch   7 |  1000/ 2088 batches | lr 0.000100 | ms/batch 50.84 | loss  4.39 | ppl    80.76\n",
            "| epoch   7 |  1200/ 2088 batches | lr 0.000100 | ms/batch 50.81 | loss  4.41 | ppl    82.01\n",
            "| epoch   7 |  1400/ 2088 batches | lr 0.000100 | ms/batch 50.76 | loss  4.38 | ppl    80.07\n",
            "| epoch   7 |  1600/ 2088 batches | lr 0.000100 | ms/batch 50.73 | loss  4.25 | ppl    70.30\n",
            "| epoch   7 |  1800/ 2088 batches | lr 0.000100 | ms/batch 50.72 | loss  4.33 | ppl    76.15\n",
            "| epoch   7 |  2000/ 2088 batches | lr 0.000100 | ms/batch 50.81 | loss  4.29 | ppl    72.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 108.67s | valid loss  5.11 | valid ppl   166.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000100\n",
            "| epoch   8 |   200/ 2088 batches | lr 0.000100 | ms/batch 50.93 | loss  4.27 | ppl    71.41\n",
            "| epoch   8 |   400/ 2088 batches | lr 0.000100 | ms/batch 50.70 | loss  4.19 | ppl    65.76\n",
            "| epoch   8 |   600/ 2088 batches | lr 0.000100 | ms/batch 50.68 | loss  4.16 | ppl    64.29\n",
            "| epoch   8 |   800/ 2088 batches | lr 0.000100 | ms/batch 50.68 | loss  4.17 | ppl    64.55\n",
            "| epoch   8 |  1000/ 2088 batches | lr 0.000100 | ms/batch 50.67 | loss  4.23 | ppl    68.38\n",
            "| epoch   8 |  1200/ 2088 batches | lr 0.000100 | ms/batch 50.75 | loss  4.25 | ppl    69.79\n",
            "| epoch   8 |  1400/ 2088 batches | lr 0.000100 | ms/batch 50.85 | loss  4.23 | ppl    68.40\n",
            "| epoch   8 |  1600/ 2088 batches | lr 0.000100 | ms/batch 50.80 | loss  4.09 | ppl    59.87\n",
            "| epoch   8 |  1800/ 2088 batches | lr 0.000100 | ms/batch 50.76 | loss  4.17 | ppl    64.99\n",
            "| epoch   8 |  2000/ 2088 batches | lr 0.000100 | ms/batch 50.75 | loss  4.13 | ppl    61.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 108.56s | valid loss  5.11 | valid ppl   166.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000100\n",
            "| epoch   9 |   200/ 2088 batches | lr 0.000100 | ms/batch 51.12 | loss  4.11 | ppl    61.15\n",
            "| epoch   9 |   400/ 2088 batches | lr 0.000100 | ms/batch 50.73 | loss  4.03 | ppl    56.33\n",
            "| epoch   9 |   600/ 2088 batches | lr 0.000100 | ms/batch 50.74 | loss  4.01 | ppl    55.07\n",
            "| epoch   9 |   800/ 2088 batches | lr 0.000100 | ms/batch 50.75 | loss  4.01 | ppl    55.36\n",
            "| epoch   9 |  1000/ 2088 batches | lr 0.000100 | ms/batch 50.75 | loss  4.07 | ppl    58.57\n",
            "| epoch   9 |  1200/ 2088 batches | lr 0.000100 | ms/batch 50.79 | loss  4.09 | ppl    59.88\n",
            "| epoch   9 |  1400/ 2088 batches | lr 0.000100 | ms/batch 50.74 | loss  4.08 | ppl    58.96\n",
            "| epoch   9 |  1600/ 2088 batches | lr 0.000100 | ms/batch 50.71 | loss  3.94 | ppl    51.19\n",
            "| epoch   9 |  1800/ 2088 batches | lr 0.000100 | ms/batch 50.76 | loss  4.02 | ppl    55.88\n",
            "| epoch   9 |  2000/ 2088 batches | lr 0.000100 | ms/batch 50.74 | loss  3.98 | ppl    53.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 108.61s | valid loss  5.14 | valid ppl   170.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000100\n",
            "| epoch  10 |   200/ 2088 batches | lr 0.000100 | ms/batch 51.08 | loss  3.97 | ppl    52.77\n",
            "| epoch  10 |   400/ 2088 batches | lr 0.000100 | ms/batch 50.70 | loss  3.88 | ppl    48.63\n",
            "| epoch  10 |   600/ 2088 batches | lr 0.000100 | ms/batch 50.74 | loss  3.86 | ppl    47.67\n",
            "| epoch  10 |   800/ 2088 batches | lr 0.000100 | ms/batch 50.67 | loss  3.87 | ppl    47.85\n",
            "| epoch  10 |  1000/ 2088 batches | lr 0.000100 | ms/batch 50.74 | loss  3.92 | ppl    50.42\n",
            "| epoch  10 |  1200/ 2088 batches | lr 0.000100 | ms/batch 50.69 | loss  3.95 | ppl    51.70\n",
            "| epoch  10 |  1400/ 2088 batches | lr 0.000100 | ms/batch 50.73 | loss  3.94 | ppl    51.21\n",
            "| epoch  10 |  1600/ 2088 batches | lr 0.000100 | ms/batch 50.78 | loss  3.79 | ppl    44.31\n",
            "| epoch  10 |  1800/ 2088 batches | lr 0.000100 | ms/batch 50.77 | loss  3.88 | ppl    48.33\n",
            "| epoch  10 |  2000/ 2088 batches | lr 0.000100 | ms/batch 50.73 | loss  3.84 | ppl    46.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 108.58s | valid loss  5.17 | valid ppl   175.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000050\n",
            "| epoch  11 |   200/ 2088 batches | lr 0.000050 | ms/batch 50.97 | loss  3.84 | ppl    46.41\n",
            "| epoch  11 |   400/ 2088 batches | lr 0.000050 | ms/batch 50.68 | loss  3.73 | ppl    41.87\n",
            "| epoch  11 |   600/ 2088 batches | lr 0.000050 | ms/batch 50.65 | loss  3.71 | ppl    40.88\n",
            "| epoch  11 |   800/ 2088 batches | lr 0.000050 | ms/batch 50.75 | loss  3.70 | ppl    40.33\n",
            "| epoch  11 |  1000/ 2088 batches | lr 0.000050 | ms/batch 50.80 | loss  3.74 | ppl    42.04\n",
            "| epoch  11 |  1200/ 2088 batches | lr 0.000050 | ms/batch 50.82 | loss  3.75 | ppl    42.56\n",
            "| epoch  11 |  1400/ 2088 batches | lr 0.000050 | ms/batch 50.75 | loss  3.73 | ppl    41.84\n",
            "| epoch  11 |  1600/ 2088 batches | lr 0.000050 | ms/batch 50.82 | loss  3.58 | ppl    35.81\n",
            "| epoch  11 |  1800/ 2088 batches | lr 0.000050 | ms/batch 50.73 | loss  3.66 | ppl    38.85\n",
            "| epoch  11 |  2000/ 2088 batches | lr 0.000050 | ms/batch 50.69 | loss  3.61 | ppl    36.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 108.57s | valid loss  5.17 | valid ppl   175.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000050\n",
            "| epoch  12 |   200/ 2088 batches | lr 0.000050 | ms/batch 51.02 | loss  3.72 | ppl    41.26\n",
            "| epoch  12 |   400/ 2088 batches | lr 0.000050 | ms/batch 50.74 | loss  3.62 | ppl    37.52\n",
            "| epoch  12 |   600/ 2088 batches | lr 0.000050 | ms/batch 50.75 | loss  3.60 | ppl    36.62\n",
            "| epoch  12 |   800/ 2088 batches | lr 0.000050 | ms/batch 50.75 | loss  3.60 | ppl    36.50\n",
            "| epoch  12 |  1000/ 2088 batches | lr 0.000050 | ms/batch 50.74 | loss  3.64 | ppl    37.98\n",
            "| epoch  12 |  1200/ 2088 batches | lr 0.000050 | ms/batch 50.75 | loss  3.66 | ppl    38.84\n",
            "| epoch  12 |  1400/ 2088 batches | lr 0.000050 | ms/batch 50.80 | loss  3.65 | ppl    38.49\n",
            "| epoch  12 |  1600/ 2088 batches | lr 0.000050 | ms/batch 50.79 | loss  3.50 | ppl    33.08\n",
            "| epoch  12 |  1800/ 2088 batches | lr 0.000050 | ms/batch 50.74 | loss  3.58 | ppl    35.90\n",
            "| epoch  12 |  2000/ 2088 batches | lr 0.000050 | ms/batch 50.69 | loss  3.53 | ppl    34.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 108.61s | valid loss  5.19 | valid ppl   179.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000050\n",
            "| epoch  13 |   200/ 2088 batches | lr 0.000050 | ms/batch 50.97 | loss  3.63 | ppl    37.75\n",
            "| epoch  13 |   400/ 2088 batches | lr 0.000050 | ms/batch 50.73 | loss  3.54 | ppl    34.37\n",
            "| epoch  13 |   600/ 2088 batches | lr 0.000050 | ms/batch 50.76 | loss  3.52 | ppl    33.65\n",
            "| epoch  13 |   800/ 2088 batches | lr 0.000050 | ms/batch 50.79 | loss  3.52 | ppl    33.67\n",
            "| epoch  13 |  1000/ 2088 batches | lr 0.000050 | ms/batch 50.79 | loss  3.55 | ppl    34.97\n",
            "| epoch  13 |  1200/ 2088 batches | lr 0.000050 | ms/batch 50.89 | loss  3.58 | ppl    35.99\n",
            "| epoch  13 |  1400/ 2088 batches | lr 0.000050 | ms/batch 51.07 | loss  3.57 | ppl    35.53\n",
            "| epoch  13 |  1600/ 2088 batches | lr 0.000050 | ms/batch 50.99 | loss  3.42 | ppl    30.54\n",
            "| epoch  13 |  1800/ 2088 batches | lr 0.000050 | ms/batch 50.93 | loss  3.50 | ppl    33.25\n",
            "| epoch  13 |  2000/ 2088 batches | lr 0.000050 | ms/batch 50.89 | loss  3.47 | ppl    31.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 108.83s | valid loss  5.21 | valid ppl   183.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000025\n",
            "| epoch  14 |   200/ 2088 batches | lr 0.000025 | ms/batch 51.10 | loss  3.57 | ppl    35.57\n",
            "| epoch  14 |   400/ 2088 batches | lr 0.000025 | ms/batch 50.77 | loss  3.48 | ppl    32.32\n",
            "| epoch  14 |   600/ 2088 batches | lr 0.000025 | ms/batch 50.75 | loss  3.44 | ppl    31.30\n",
            "| epoch  14 |   800/ 2088 batches | lr 0.000025 | ms/batch 50.81 | loss  3.44 | ppl    31.17\n",
            "| epoch  14 |  1000/ 2088 batches | lr 0.000025 | ms/batch 50.79 | loss  3.47 | ppl    32.11\n",
            "| epoch  14 |  1200/ 2088 batches | lr 0.000025 | ms/batch 50.92 | loss  3.49 | ppl    32.75\n",
            "| epoch  14 |  1400/ 2088 batches | lr 0.000025 | ms/batch 50.84 | loss  3.47 | ppl    32.22\n",
            "| epoch  14 |  1600/ 2088 batches | lr 0.000025 | ms/batch 50.77 | loss  3.32 | ppl    27.68\n",
            "| epoch  14 |  1800/ 2088 batches | lr 0.000025 | ms/batch 50.75 | loss  3.39 | ppl    29.69\n",
            "| epoch  14 |  2000/ 2088 batches | lr 0.000025 | ms/batch 50.73 | loss  3.35 | ppl    28.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 108.69s | valid loss  5.20 | valid ppl   181.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000025\n",
            "| epoch  15 |   200/ 2088 batches | lr 0.000025 | ms/batch 50.95 | loss  3.51 | ppl    33.57\n",
            "| epoch  15 |   400/ 2088 batches | lr 0.000025 | ms/batch 50.70 | loss  3.42 | ppl    30.49\n",
            "| epoch  15 |   600/ 2088 batches | lr 0.000025 | ms/batch 50.67 | loss  3.39 | ppl    29.60\n",
            "| epoch  15 |   800/ 2088 batches | lr 0.000025 | ms/batch 50.73 | loss  3.39 | ppl    29.57\n",
            "| epoch  15 |  1000/ 2088 batches | lr 0.000025 | ms/batch 50.78 | loss  3.42 | ppl    30.57\n",
            "| epoch  15 |  1200/ 2088 batches | lr 0.000025 | ms/batch 50.72 | loss  3.44 | ppl    31.32\n",
            "| epoch  15 |  1400/ 2088 batches | lr 0.000025 | ms/batch 50.78 | loss  3.43 | ppl    30.91\n",
            "| epoch  15 |  1600/ 2088 batches | lr 0.000025 | ms/batch 50.75 | loss  3.28 | ppl    26.57\n",
            "| epoch  15 |  1800/ 2088 batches | lr 0.000025 | ms/batch 50.71 | loss  3.36 | ppl    28.72\n",
            "| epoch  15 |  2000/ 2088 batches | lr 0.000025 | ms/batch 50.75 | loss  3.31 | ppl    27.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 108.58s | valid loss  5.22 | valid ppl   185.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000025\n",
            "| epoch  16 |   200/ 2088 batches | lr 0.000025 | ms/batch 51.05 | loss  3.46 | ppl    31.87\n",
            "| epoch  16 |   400/ 2088 batches | lr 0.000025 | ms/batch 50.66 | loss  3.37 | ppl    29.13\n",
            "| epoch  16 |   600/ 2088 batches | lr 0.000025 | ms/batch 50.69 | loss  3.35 | ppl    28.42\n",
            "| epoch  16 |   800/ 2088 batches | lr 0.000025 | ms/batch 50.67 | loss  3.35 | ppl    28.37\n",
            "| epoch  16 |  1000/ 2088 batches | lr 0.000025 | ms/batch 50.74 | loss  3.38 | ppl    29.28\n",
            "| epoch  16 |  1200/ 2088 batches | lr 0.000025 | ms/batch 50.69 | loss  3.40 | ppl    30.03\n",
            "| epoch  16 |  1400/ 2088 batches | lr 0.000025 | ms/batch 50.77 | loss  3.39 | ppl    29.78\n",
            "| epoch  16 |  1600/ 2088 batches | lr 0.000025 | ms/batch 50.69 | loss  3.24 | ppl    25.61\n",
            "| epoch  16 |  1800/ 2088 batches | lr 0.000025 | ms/batch 50.76 | loss  3.32 | ppl    27.67\n",
            "| epoch  16 |  2000/ 2088 batches | lr 0.000025 | ms/batch 50.72 | loss  3.28 | ppl    26.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 108.55s | valid loss  5.23 | valid ppl   187.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000013\n",
            "| epoch  17 |   200/ 2088 batches | lr 0.000013 | ms/batch 51.10 | loss  3.45 | ppl    31.59\n",
            "| epoch  17 |   400/ 2088 batches | lr 0.000013 | ms/batch 50.69 | loss  3.36 | ppl    28.75\n",
            "| epoch  17 |   600/ 2088 batches | lr 0.000013 | ms/batch 50.66 | loss  3.33 | ppl    27.86\n",
            "| epoch  17 |   800/ 2088 batches | lr 0.000013 | ms/batch 50.80 | loss  3.32 | ppl    27.70\n",
            "| epoch  17 |  1000/ 2088 batches | lr 0.000013 | ms/batch 50.74 | loss  3.35 | ppl    28.55\n",
            "| epoch  17 |  1200/ 2088 batches | lr 0.000013 | ms/batch 50.78 | loss  3.37 | ppl    29.14\n",
            "| epoch  17 |  1400/ 2088 batches | lr 0.000013 | ms/batch 50.74 | loss  3.35 | ppl    28.58\n",
            "| epoch  17 |  1600/ 2088 batches | lr 0.000013 | ms/batch 50.78 | loss  3.20 | ppl    24.63\n",
            "| epoch  17 |  1800/ 2088 batches | lr 0.000013 | ms/batch 50.70 | loss  3.27 | ppl    26.37\n",
            "| epoch  17 |  2000/ 2088 batches | lr 0.000013 | ms/batch 50.76 | loss  3.23 | ppl    25.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 108.59s | valid loss  5.23 | valid ppl   187.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000013\n",
            "| epoch  18 |   200/ 2088 batches | lr 0.000013 | ms/batch 51.00 | loss  3.42 | ppl    30.67\n",
            "| epoch  18 |   400/ 2088 batches | lr 0.000013 | ms/batch 50.69 | loss  3.33 | ppl    27.87\n",
            "| epoch  18 |   600/ 2088 batches | lr 0.000013 | ms/batch 50.71 | loss  3.30 | ppl    27.11\n",
            "| epoch  18 |   800/ 2088 batches | lr 0.000013 | ms/batch 50.68 | loss  3.29 | ppl    26.96\n",
            "| epoch  18 |  1000/ 2088 batches | lr 0.000013 | ms/batch 50.72 | loss  3.33 | ppl    27.80\n",
            "| epoch  18 |  1200/ 2088 batches | lr 0.000013 | ms/batch 50.73 | loss  3.35 | ppl    28.45\n",
            "| epoch  18 |  1400/ 2088 batches | lr 0.000013 | ms/batch 50.75 | loss  3.33 | ppl    28.07\n",
            "| epoch  18 |  1600/ 2088 batches | lr 0.000013 | ms/batch 50.78 | loss  3.18 | ppl    24.15\n",
            "| epoch  18 |  1800/ 2088 batches | lr 0.000013 | ms/batch 50.85 | loss  3.26 | ppl    25.99\n",
            "| epoch  18 |  2000/ 2088 batches | lr 0.000013 | ms/batch 50.78 | loss  3.22 | ppl    25.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 108.58s | valid loss  5.24 | valid ppl   187.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000013\n",
            "| epoch  19 |   200/ 2088 batches | lr 0.000013 | ms/batch 50.98 | loss  3.40 | ppl    30.03\n",
            "| epoch  19 |   400/ 2088 batches | lr 0.000013 | ms/batch 50.72 | loss  3.30 | ppl    27.16\n",
            "| epoch  19 |   600/ 2088 batches | lr 0.000013 | ms/batch 50.73 | loss  3.28 | ppl    26.49\n",
            "| epoch  19 |   800/ 2088 batches | lr 0.000013 | ms/batch 50.68 | loss  3.27 | ppl    26.43\n",
            "| epoch  19 |  1000/ 2088 batches | lr 0.000013 | ms/batch 50.69 | loss  3.31 | ppl    27.25\n",
            "| epoch  19 |  1200/ 2088 batches | lr 0.000013 | ms/batch 50.75 | loss  3.33 | ppl    27.86\n",
            "| epoch  19 |  1400/ 2088 batches | lr 0.000013 | ms/batch 50.71 | loss  3.31 | ppl    27.50\n",
            "| epoch  19 |  1600/ 2088 batches | lr 0.000013 | ms/batch 50.74 | loss  3.17 | ppl    23.69\n",
            "| epoch  19 |  1800/ 2088 batches | lr 0.000013 | ms/batch 50.75 | loss  3.24 | ppl    25.51\n",
            "| epoch  19 |  2000/ 2088 batches | lr 0.000013 | ms/batch 50.79 | loss  3.21 | ppl    24.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 108.56s | valid loss  5.25 | valid ppl   190.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000006\n",
            "| epoch  20 |   200/ 2088 batches | lr 0.000006 | ms/batch 50.96 | loss  3.42 | ppl    30.55\n",
            "| epoch  20 |   400/ 2088 batches | lr 0.000006 | ms/batch 50.72 | loss  3.32 | ppl    27.56\n",
            "| epoch  20 |   600/ 2088 batches | lr 0.000006 | ms/batch 50.70 | loss  3.29 | ppl    26.87\n",
            "| epoch  20 |   800/ 2088 batches | lr 0.000006 | ms/batch 50.69 | loss  3.28 | ppl    26.50\n",
            "| epoch  20 |  1000/ 2088 batches | lr 0.000006 | ms/batch 50.71 | loss  3.31 | ppl    27.35\n",
            "| epoch  20 |  1200/ 2088 batches | lr 0.000006 | ms/batch 50.72 | loss  3.33 | ppl    27.83\n",
            "| epoch  20 |  1400/ 2088 batches | lr 0.000006 | ms/batch 50.78 | loss  3.31 | ppl    27.38\n",
            "| epoch  20 |  1600/ 2088 batches | lr 0.000006 | ms/batch 50.72 | loss  3.16 | ppl    23.54\n",
            "| epoch  20 |  1800/ 2088 batches | lr 0.000006 | ms/batch 50.73 | loss  3.23 | ppl    25.26\n",
            "| epoch  20 |  2000/ 2088 batches | lr 0.000006 | ms/batch 50.70 | loss  3.20 | ppl    24.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 108.53s | valid loss  5.25 | valid ppl   189.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000006\n",
            "| epoch  21 |   200/ 2088 batches | lr 0.000006 | ms/batch 50.94 | loss  3.41 | ppl    30.13\n",
            "| epoch  21 |   400/ 2088 batches | lr 0.000006 | ms/batch 50.68 | loss  3.30 | ppl    27.23\n",
            "| epoch  21 |   600/ 2088 batches | lr 0.000006 | ms/batch 50.65 | loss  3.28 | ppl    26.51\n",
            "| epoch  21 |   800/ 2088 batches | lr 0.000006 | ms/batch 50.75 | loss  3.26 | ppl    26.15\n",
            "| epoch  21 |  1000/ 2088 batches | lr 0.000006 | ms/batch 50.71 | loss  3.29 | ppl    26.98\n",
            "| epoch  21 |  1200/ 2088 batches | lr 0.000006 | ms/batch 50.77 | loss  3.31 | ppl    27.50\n",
            "| epoch  21 |  1400/ 2088 batches | lr 0.000006 | ms/batch 50.74 | loss  3.31 | ppl    27.25\n",
            "| epoch  21 |  1600/ 2088 batches | lr 0.000006 | ms/batch 50.68 | loss  3.15 | ppl    23.35\n",
            "| epoch  21 |  1800/ 2088 batches | lr 0.000006 | ms/batch 50.73 | loss  3.23 | ppl    25.21\n",
            "| epoch  21 |  2000/ 2088 batches | lr 0.000006 | ms/batch 50.74 | loss  3.19 | ppl    24.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 108.51s | valid loss  5.25 | valid ppl   190.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000006\n",
            "| epoch  22 |   200/ 2088 batches | lr 0.000006 | ms/batch 50.98 | loss  3.40 | ppl    29.82\n",
            "| epoch  22 |   400/ 2088 batches | lr 0.000006 | ms/batch 50.64 | loss  3.29 | ppl    26.85\n",
            "| epoch  22 |   600/ 2088 batches | lr 0.000006 | ms/batch 50.73 | loss  3.27 | ppl    26.23\n",
            "| epoch  22 |   800/ 2088 batches | lr 0.000006 | ms/batch 50.69 | loss  3.25 | ppl    25.90\n",
            "| epoch  22 |  1000/ 2088 batches | lr 0.000006 | ms/batch 50.75 | loss  3.29 | ppl    26.83\n",
            "| epoch  22 |  1200/ 2088 batches | lr 0.000006 | ms/batch 50.76 | loss  3.31 | ppl    27.36\n",
            "| epoch  22 |  1400/ 2088 batches | lr 0.000006 | ms/batch 50.76 | loss  3.30 | ppl    26.98\n",
            "| epoch  22 |  1600/ 2088 batches | lr 0.000006 | ms/batch 50.69 | loss  3.14 | ppl    23.16\n",
            "| epoch  22 |  1800/ 2088 batches | lr 0.000006 | ms/batch 50.69 | loss  3.22 | ppl    24.97\n",
            "| epoch  22 |  2000/ 2088 batches | lr 0.000006 | ms/batch 50.69 | loss  3.19 | ppl    24.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 108.52s | valid loss  5.25 | valid ppl   191.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000003\n",
            "| epoch  23 |   200/ 2088 batches | lr 0.000003 | ms/batch 51.02 | loss  3.43 | ppl    30.83\n",
            "| epoch  23 |   400/ 2088 batches | lr 0.000003 | ms/batch 50.73 | loss  3.32 | ppl    27.59\n",
            "| epoch  23 |   600/ 2088 batches | lr 0.000003 | ms/batch 50.72 | loss  3.29 | ppl    26.93\n",
            "| epoch  23 |   800/ 2088 batches | lr 0.000003 | ms/batch 50.69 | loss  3.28 | ppl    26.65\n",
            "| epoch  23 |  1000/ 2088 batches | lr 0.000003 | ms/batch 50.71 | loss  3.31 | ppl    27.29\n",
            "| epoch  23 |  1200/ 2088 batches | lr 0.000003 | ms/batch 50.70 | loss  3.33 | ppl    27.87\n",
            "| epoch  23 |  1400/ 2088 batches | lr 0.000003 | ms/batch 50.78 | loss  3.31 | ppl    27.36\n",
            "| epoch  23 |  1600/ 2088 batches | lr 0.000003 | ms/batch 50.76 | loss  3.16 | ppl    23.57\n",
            "| epoch  23 |  1800/ 2088 batches | lr 0.000003 | ms/batch 50.72 | loss  3.24 | ppl    25.48\n",
            "| epoch  23 |  2000/ 2088 batches | lr 0.000003 | ms/batch 50.74 | loss  3.20 | ppl    24.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 108.57s | valid loss  5.24 | valid ppl   189.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000003\n",
            "| epoch  24 |   200/ 2088 batches | lr 0.000003 | ms/batch 51.03 | loss  3.42 | ppl    30.51\n",
            "| epoch  24 |   400/ 2088 batches | lr 0.000003 | ms/batch 50.69 | loss  3.31 | ppl    27.49\n",
            "| epoch  24 |   600/ 2088 batches | lr 0.000003 | ms/batch 50.63 | loss  3.29 | ppl    26.77\n",
            "| epoch  24 |   800/ 2088 batches | lr 0.000003 | ms/batch 50.67 | loss  3.27 | ppl    26.40\n",
            "| epoch  24 |  1000/ 2088 batches | lr 0.000003 | ms/batch 50.74 | loss  3.30 | ppl    27.22\n",
            "| epoch  24 |  1200/ 2088 batches | lr 0.000003 | ms/batch 50.77 | loss  3.32 | ppl    27.77\n",
            "| epoch  24 |  1400/ 2088 batches | lr 0.000003 | ms/batch 50.72 | loss  3.31 | ppl    27.26\n",
            "| epoch  24 |  1600/ 2088 batches | lr 0.000003 | ms/batch 50.68 | loss  3.15 | ppl    23.43\n",
            "| epoch  24 |  1800/ 2088 batches | lr 0.000003 | ms/batch 50.69 | loss  3.24 | ppl    25.51\n",
            "| epoch  24 |  2000/ 2088 batches | lr 0.000003 | ms/batch 50.69 | loss  3.20 | ppl    24.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 108.49s | valid loss  5.25 | valid ppl   189.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000003\n",
            "| epoch  25 |   200/ 2088 batches | lr 0.000003 | ms/batch 50.98 | loss  3.41 | ppl    30.39\n",
            "| epoch  25 |   400/ 2088 batches | lr 0.000003 | ms/batch 50.65 | loss  3.31 | ppl    27.36\n",
            "| epoch  25 |   600/ 2088 batches | lr 0.000003 | ms/batch 50.68 | loss  3.28 | ppl    26.59\n",
            "| epoch  25 |   800/ 2088 batches | lr 0.000003 | ms/batch 50.72 | loss  3.27 | ppl    26.25\n",
            "| epoch  25 |  1000/ 2088 batches | lr 0.000003 | ms/batch 50.72 | loss  3.29 | ppl    26.96\n",
            "| epoch  25 |  1200/ 2088 batches | lr 0.000003 | ms/batch 50.77 | loss  3.31 | ppl    27.50\n",
            "| epoch  25 |  1400/ 2088 batches | lr 0.000003 | ms/batch 50.73 | loss  3.30 | ppl    27.23\n",
            "| epoch  25 |  1600/ 2088 batches | lr 0.000003 | ms/batch 50.74 | loss  3.15 | ppl    23.39\n",
            "| epoch  25 |  1800/ 2088 batches | lr 0.000003 | ms/batch 50.74 | loss  3.23 | ppl    25.37\n",
            "| epoch  25 |  2000/ 2088 batches | lr 0.000003 | ms/batch 50.70 | loss  3.19 | ppl    24.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 108.54s | valid loss  5.25 | valid ppl   189.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000002\n",
            "| epoch  26 |   200/ 2088 batches | lr 0.000002 | ms/batch 50.99 | loss  3.46 | ppl    31.74\n",
            "| epoch  26 |   400/ 2088 batches | lr 0.000002 | ms/batch 50.72 | loss  3.34 | ppl    28.36\n",
            "| epoch  26 |   600/ 2088 batches | lr 0.000002 | ms/batch 50.65 | loss  3.32 | ppl    27.64\n",
            "| epoch  26 |   800/ 2088 batches | lr 0.000002 | ms/batch 50.68 | loss  3.30 | ppl    27.17\n",
            "| epoch  26 |  1000/ 2088 batches | lr 0.000002 | ms/batch 50.73 | loss  3.33 | ppl    27.90\n",
            "| epoch  26 |  1200/ 2088 batches | lr 0.000002 | ms/batch 50.69 | loss  3.35 | ppl    28.36\n",
            "| epoch  26 |  1400/ 2088 batches | lr 0.000002 | ms/batch 50.73 | loss  3.32 | ppl    27.79\n",
            "| epoch  26 |  1600/ 2088 batches | lr 0.000002 | ms/batch 50.73 | loss  3.17 | ppl    23.84\n",
            "| epoch  26 |  1800/ 2088 batches | lr 0.000002 | ms/batch 50.76 | loss  3.26 | ppl    26.13\n",
            "| epoch  26 |  2000/ 2088 batches | lr 0.000002 | ms/batch 50.67 | loss  3.22 | ppl    25.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 108.52s | valid loss  5.23 | valid ppl   186.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000002\n",
            "| epoch  27 |   200/ 2088 batches | lr 0.000002 | ms/batch 51.03 | loss  3.45 | ppl    31.47\n",
            "| epoch  27 |   400/ 2088 batches | lr 0.000002 | ms/batch 50.71 | loss  3.34 | ppl    28.20\n",
            "| epoch  27 |   600/ 2088 batches | lr 0.000002 | ms/batch 50.66 | loss  3.32 | ppl    27.60\n",
            "| epoch  27 |   800/ 2088 batches | lr 0.000002 | ms/batch 50.70 | loss  3.30 | ppl    27.04\n",
            "| epoch  27 |  1000/ 2088 batches | lr 0.000002 | ms/batch 50.76 | loss  3.32 | ppl    27.72\n",
            "| epoch  27 |  1200/ 2088 batches | lr 0.000002 | ms/batch 50.73 | loss  3.34 | ppl    28.21\n",
            "| epoch  27 |  1400/ 2088 batches | lr 0.000002 | ms/batch 50.71 | loss  3.33 | ppl    27.83\n",
            "| epoch  27 |  1600/ 2088 batches | lr 0.000002 | ms/batch 50.74 | loss  3.17 | ppl    23.85\n",
            "| epoch  27 |  1800/ 2088 batches | lr 0.000002 | ms/batch 50.70 | loss  3.26 | ppl    26.10\n",
            "| epoch  27 |  2000/ 2088 batches | lr 0.000002 | ms/batch 50.75 | loss  3.22 | ppl    24.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 108.53s | valid loss  5.23 | valid ppl   186.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000002\n",
            "| epoch  28 |   200/ 2088 batches | lr 0.000002 | ms/batch 50.98 | loss  3.45 | ppl    31.43\n",
            "| epoch  28 |   400/ 2088 batches | lr 0.000002 | ms/batch 50.64 | loss  3.33 | ppl    28.06\n",
            "| epoch  28 |   600/ 2088 batches | lr 0.000002 | ms/batch 50.70 | loss  3.31 | ppl    27.39\n",
            "| epoch  28 |   800/ 2088 batches | lr 0.000002 | ms/batch 50.68 | loss  3.29 | ppl    26.89\n",
            "| epoch  28 |  1000/ 2088 batches | lr 0.000002 | ms/batch 50.71 | loss  3.32 | ppl    27.64\n",
            "| epoch  28 |  1200/ 2088 batches | lr 0.000002 | ms/batch 50.73 | loss  3.34 | ppl    28.12\n",
            "| epoch  28 |  1400/ 2088 batches | lr 0.000002 | ms/batch 50.73 | loss  3.32 | ppl    27.75\n",
            "| epoch  28 |  1600/ 2088 batches | lr 0.000002 | ms/batch 50.73 | loss  3.17 | ppl    23.75\n",
            "| epoch  28 |  1800/ 2088 batches | lr 0.000002 | ms/batch 50.70 | loss  3.26 | ppl    26.04\n",
            "| epoch  28 |  2000/ 2088 batches | lr 0.000002 | ms/batch 50.72 | loss  3.22 | ppl    25.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 108.53s | valid loss  5.23 | valid ppl   186.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  29 |   200/ 2088 batches | lr 0.000001 | ms/batch 51.00 | loss  3.48 | ppl    32.46\n",
            "| epoch  29 |   400/ 2088 batches | lr 0.000001 | ms/batch 50.68 | loss  3.38 | ppl    29.46\n",
            "| epoch  29 |   600/ 2088 batches | lr 0.000001 | ms/batch 50.75 | loss  3.35 | ppl    28.62\n",
            "| epoch  29 |   800/ 2088 batches | lr 0.000001 | ms/batch 50.68 | loss  3.32 | ppl    27.76\n",
            "| epoch  29 |  1000/ 2088 batches | lr 0.000001 | ms/batch 50.71 | loss  3.35 | ppl    28.48\n",
            "| epoch  29 |  1200/ 2088 batches | lr 0.000001 | ms/batch 50.75 | loss  3.37 | ppl    29.11\n",
            "| epoch  29 |  1400/ 2088 batches | lr 0.000001 | ms/batch 50.81 | loss  3.35 | ppl    28.50\n",
            "| epoch  29 |  1600/ 2088 batches | lr 0.000001 | ms/batch 50.69 | loss  3.19 | ppl    24.36\n",
            "| epoch  29 |  1800/ 2088 batches | lr 0.000001 | ms/batch 50.74 | loss  3.28 | ppl    26.70\n",
            "| epoch  29 |  2000/ 2088 batches | lr 0.000001 | ms/batch 50.72 | loss  3.23 | ppl    25.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 108.56s | valid loss  5.21 | valid ppl   183.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  30 |   200/ 2088 batches | lr 0.000001 | ms/batch 50.94 | loss  3.47 | ppl    32.26\n",
            "| epoch  30 |   400/ 2088 batches | lr 0.000001 | ms/batch 50.69 | loss  3.37 | ppl    29.19\n",
            "| epoch  30 |   600/ 2088 batches | lr 0.000001 | ms/batch 50.63 | loss  3.35 | ppl    28.45\n",
            "| epoch  30 |   800/ 2088 batches | lr 0.000001 | ms/batch 50.69 | loss  3.32 | ppl    27.53\n",
            "| epoch  30 |  1000/ 2088 batches | lr 0.000001 | ms/batch 50.72 | loss  3.34 | ppl    28.36\n",
            "| epoch  30 |  1200/ 2088 batches | lr 0.000001 | ms/batch 50.74 | loss  3.37 | ppl    29.10\n",
            "| epoch  30 |  1400/ 2088 batches | lr 0.000001 | ms/batch 50.79 | loss  3.34 | ppl    28.32\n",
            "| epoch  30 |  1600/ 2088 batches | lr 0.000001 | ms/batch 50.76 | loss  3.19 | ppl    24.26\n",
            "| epoch  30 |  1800/ 2088 batches | lr 0.000001 | ms/batch 50.74 | loss  3.29 | ppl    26.75\n",
            "| epoch  30 |  2000/ 2088 batches | lr 0.000001 | ms/batch 50.69 | loss  3.24 | ppl    25.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 108.51s | valid loss  5.21 | valid ppl   183.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000001\n",
            "| epoch  31 |   200/ 2088 batches | lr 0.000001 | ms/batch 50.97 | loss  3.47 | ppl    32.08\n",
            "| epoch  31 |   400/ 2088 batches | lr 0.000001 | ms/batch 50.67 | loss  3.37 | ppl    29.08\n",
            "| epoch  31 |   600/ 2088 batches | lr 0.000001 | ms/batch 50.66 | loss  3.34 | ppl    28.29\n",
            "| epoch  31 |   800/ 2088 batches | lr 0.000001 | ms/batch 50.71 | loss  3.31 | ppl    27.50\n",
            "| epoch  31 |  1000/ 2088 batches | lr 0.000001 | ms/batch 50.75 | loss  3.34 | ppl    28.22\n",
            "| epoch  31 |  1200/ 2088 batches | lr 0.000001 | ms/batch 50.70 | loss  3.37 | ppl    28.94\n",
            "| epoch  31 |  1400/ 2088 batches | lr 0.000001 | ms/batch 50.71 | loss  3.34 | ppl    28.18\n",
            "| epoch  31 |  1600/ 2088 batches | lr 0.000001 | ms/batch 50.73 | loss  3.19 | ppl    24.23\n",
            "| epoch  31 |  1800/ 2088 batches | lr 0.000001 | ms/batch 50.69 | loss  3.29 | ppl    26.77\n",
            "| epoch  31 |  2000/ 2088 batches | lr 0.000001 | ms/batch 50.66 | loss  3.23 | ppl    25.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 108.49s | valid loss  5.21 | valid ppl   183.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  32 |   200/ 2088 batches | lr 0.000000 | ms/batch 50.94 | loss  3.49 | ppl    32.72\n",
            "| epoch  32 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.65 | loss  3.40 | ppl    30.02\n",
            "| epoch  32 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.37 | ppl    29.20\n",
            "| epoch  32 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.67 | loss  3.34 | ppl    28.11\n",
            "| epoch  32 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.80 | loss  3.36 | ppl    28.82\n",
            "| epoch  32 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.84 | loss  3.40 | ppl    29.91\n",
            "| epoch  32 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.78 | loss  3.36 | ppl    28.93\n",
            "| epoch  32 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.20 | ppl    24.60\n",
            "| epoch  32 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.69 | loss  3.30 | ppl    27.04\n",
            "| epoch  32 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.24 | ppl    25.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 108.53s | valid loss  5.21 | valid ppl   182.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  33 |   200/ 2088 batches | lr 0.000000 | ms/batch 51.00 | loss  3.49 | ppl    32.73\n",
            "| epoch  33 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.70 | loss  3.39 | ppl    29.62\n",
            "| epoch  33 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.66 | loss  3.36 | ppl    28.88\n",
            "| epoch  33 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.70 | loss  3.33 | ppl    28.02\n",
            "| epoch  33 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.35 | ppl    28.60\n",
            "| epoch  33 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.79 | loss  3.39 | ppl    29.60\n",
            "| epoch  33 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.78 | loss  3.36 | ppl    28.79\n",
            "| epoch  33 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.78 | loss  3.20 | ppl    24.56\n",
            "| epoch  33 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.30 | ppl    27.09\n",
            "| epoch  33 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.70 | loss  3.24 | ppl    25.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 108.56s | valid loss  5.21 | valid ppl   182.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  34 |   200/ 2088 batches | lr 0.000000 | ms/batch 51.00 | loss  3.49 | ppl    32.71\n",
            "| epoch  34 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.38 | ppl    29.44\n",
            "| epoch  34 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.65 | loss  3.36 | ppl    28.78\n",
            "| epoch  34 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.33 | ppl    27.80\n",
            "| epoch  34 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.35 | ppl    28.62\n",
            "| epoch  34 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.39 | ppl    29.53\n",
            "| epoch  34 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.36 | ppl    28.69\n",
            "| epoch  34 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.78 | loss  3.20 | ppl    24.57\n",
            "| epoch  34 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.68 | loss  3.30 | ppl    27.02\n",
            "| epoch  34 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.24 | ppl    25.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 108.51s | valid loss  5.21 | valid ppl   182.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  35 |   200/ 2088 batches | lr 0.000000 | ms/batch 50.98 | loss  3.49 | ppl    32.94\n",
            "| epoch  35 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.69 | loss  3.41 | ppl    30.12\n",
            "| epoch  35 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.38 | ppl    29.29\n",
            "| epoch  35 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.34 | ppl    28.23\n",
            "| epoch  35 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.67 | loss  3.37 | ppl    29.03\n",
            "| epoch  35 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.69 | loss  3.39 | ppl    29.75\n",
            "| epoch  35 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.36 | ppl    28.87\n",
            "| epoch  35 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.68 | loss  3.20 | ppl    24.57\n",
            "| epoch  35 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.77 | loss  3.29 | ppl    26.97\n",
            "| epoch  35 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.24 | ppl    25.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 108.53s | valid loss  5.21 | valid ppl   182.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  36 |   200/ 2088 batches | lr 0.000000 | ms/batch 50.99 | loss  3.50 | ppl    33.01\n",
            "| epoch  36 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.67 | loss  3.39 | ppl    29.70\n",
            "| epoch  36 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.66 | loss  3.37 | ppl    28.97\n",
            "| epoch  36 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.66 | loss  3.33 | ppl    28.04\n",
            "| epoch  36 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.76 | loss  3.36 | ppl    28.73\n",
            "| epoch  36 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.80 | loss  3.39 | ppl    29.59\n",
            "| epoch  36 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.98 | loss  3.36 | ppl    28.91\n",
            "| epoch  36 |  1600/ 2088 batches | lr 0.000000 | ms/batch 51.01 | loss  3.21 | ppl    24.67\n",
            "| epoch  36 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.96 | loss  3.30 | ppl    27.04\n",
            "| epoch  36 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.98 | loss  3.24 | ppl    25.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 108.76s | valid loss  5.21 | valid ppl   182.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  37 |   200/ 2088 batches | lr 0.000000 | ms/batch 51.11 | loss  3.49 | ppl    32.92\n",
            "| epoch  37 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.75 | loss  3.39 | ppl    29.63\n",
            "| epoch  37 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.36 | ppl    28.83\n",
            "| epoch  37 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.33 | ppl    28.00\n",
            "| epoch  37 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.82 | loss  3.36 | ppl    28.70\n",
            "| epoch  37 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.83 | loss  3.39 | ppl    29.65\n",
            "| epoch  37 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.84 | loss  3.36 | ppl    28.82\n",
            "| epoch  37 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.78 | loss  3.20 | ppl    24.65\n",
            "| epoch  37 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.76 | loss  3.29 | ppl    26.95\n",
            "| epoch  37 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.70 | loss  3.24 | ppl    25.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 108.64s | valid loss  5.21 | valid ppl   182.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  38 |   200/ 2088 batches | lr 0.000000 | ms/batch 50.90 | loss  3.50 | ppl    33.12\n",
            "| epoch  38 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.66 | loss  3.40 | ppl    29.88\n",
            "| epoch  38 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.62 | loss  3.37 | ppl    29.08\n",
            "| epoch  38 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.34 | ppl    28.16\n",
            "| epoch  38 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.36 | ppl    28.89\n",
            "| epoch  38 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.38 | ppl    29.51\n",
            "| epoch  38 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.75 | loss  3.36 | ppl    28.78\n",
            "| epoch  38 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.20 | ppl    24.47\n",
            "| epoch  38 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.29 | ppl    26.76\n",
            "| epoch  38 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.24 | ppl    25.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 108.49s | valid loss  5.21 | valid ppl   182.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  39 |   200/ 2088 batches | lr 0.000000 | ms/batch 50.98 | loss  3.50 | ppl    33.02\n",
            "| epoch  39 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.39 | ppl    29.78\n",
            "| epoch  39 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.36 | ppl    28.92\n",
            "| epoch  39 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.78 | loss  3.33 | ppl    28.03\n",
            "| epoch  39 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.78 | loss  3.36 | ppl    28.81\n",
            "| epoch  39 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.38 | ppl    29.47\n",
            "| epoch  39 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.36 | ppl    28.77\n",
            "| epoch  39 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.20 | ppl    24.49\n",
            "| epoch  39 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.29 | ppl    26.90\n",
            "| epoch  39 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.24 | ppl    25.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 108.59s | valid loss  5.21 | valid ppl   182.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  40 |   200/ 2088 batches | lr 0.000000 | ms/batch 50.94 | loss  3.50 | ppl    33.00\n",
            "| epoch  40 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.68 | loss  3.39 | ppl    29.66\n",
            "| epoch  40 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.65 | loss  3.36 | ppl    28.91\n",
            "| epoch  40 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.69 | loss  3.33 | ppl    27.99\n",
            "| epoch  40 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.36 | ppl    28.80\n",
            "| epoch  40 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.67 | loss  3.39 | ppl    29.56\n",
            "| epoch  40 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.36 | ppl    28.91\n",
            "| epoch  40 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.70 | loss  3.20 | ppl    24.56\n",
            "| epoch  40 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.81 | loss  3.29 | ppl    26.97\n",
            "| epoch  40 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.76 | loss  3.24 | ppl    25.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 108.51s | valid loss  5.21 | valid ppl   182.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  41 |   200/ 2088 batches | lr 0.000000 | ms/batch 50.98 | loss  3.50 | ppl    33.08\n",
            "| epoch  41 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.67 | loss  3.39 | ppl    29.75\n",
            "| epoch  41 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.68 | loss  3.37 | ppl    28.95\n",
            "| epoch  41 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.33 | ppl    27.93\n",
            "| epoch  41 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.36 | ppl    28.84\n",
            "| epoch  41 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.38 | ppl    29.43\n",
            "| epoch  41 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.69 | loss  3.36 | ppl    28.80\n",
            "| epoch  41 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.78 | loss  3.20 | ppl    24.53\n",
            "| epoch  41 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.29 | ppl    26.77\n",
            "| epoch  41 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.75 | loss  3.24 | ppl    25.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 108.52s | valid loss  5.21 | valid ppl   182.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  42 |   200/ 2088 batches | lr 0.000000 | ms/batch 50.91 | loss  3.50 | ppl    33.03\n",
            "| epoch  42 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.70 | loss  3.39 | ppl    29.56\n",
            "| epoch  42 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.68 | loss  3.36 | ppl    28.89\n",
            "| epoch  42 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.33 | ppl    27.91\n",
            "| epoch  42 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.69 | loss  3.36 | ppl    28.78\n",
            "| epoch  42 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.66 | loss  3.38 | ppl    29.42\n",
            "| epoch  42 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.36 | ppl    28.78\n",
            "| epoch  42 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.20 | ppl    24.53\n",
            "| epoch  42 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.78 | loss  3.29 | ppl    26.83\n",
            "| epoch  42 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.24 | ppl    25.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 108.53s | valid loss  5.21 | valid ppl   182.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  43 |   200/ 2088 batches | lr 0.000000 | ms/batch 51.01 | loss  3.50 | ppl    32.98\n",
            "| epoch  43 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.69 | loss  3.39 | ppl    29.71\n",
            "| epoch  43 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.36 | ppl    28.93\n",
            "| epoch  43 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.33 | ppl    27.91\n",
            "| epoch  43 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.69 | loss  3.36 | ppl    28.74\n",
            "| epoch  43 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.38 | ppl    29.48\n",
            "| epoch  43 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.76 | loss  3.36 | ppl    28.78\n",
            "| epoch  43 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.20 | ppl    24.53\n",
            "| epoch  43 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.29 | ppl    26.94\n",
            "| epoch  43 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.24 | ppl    25.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 108.53s | valid loss  5.21 | valid ppl   182.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  44 |   200/ 2088 batches | lr 0.000000 | ms/batch 51.00 | loss  3.50 | ppl    33.05\n",
            "| epoch  44 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.75 | loss  3.39 | ppl    29.70\n",
            "| epoch  44 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.66 | loss  3.36 | ppl    28.87\n",
            "| epoch  44 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.33 | ppl    27.88\n",
            "| epoch  44 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.79 | loss  3.36 | ppl    28.84\n",
            "| epoch  44 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.76 | loss  3.38 | ppl    29.37\n",
            "| epoch  44 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.76 | loss  3.36 | ppl    28.78\n",
            "| epoch  44 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.20 | ppl    24.46\n",
            "| epoch  44 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.67 | loss  3.29 | ppl    26.81\n",
            "| epoch  44 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.70 | loss  3.24 | ppl    25.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 108.57s | valid loss  5.21 | valid ppl   182.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  45 |   200/ 2088 batches | lr 0.000000 | ms/batch 50.99 | loss  3.50 | ppl    33.05\n",
            "| epoch  45 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.67 | loss  3.39 | ppl    29.69\n",
            "| epoch  45 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.69 | loss  3.36 | ppl    28.84\n",
            "| epoch  45 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.33 | ppl    27.82\n",
            "| epoch  45 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.36 | ppl    28.80\n",
            "| epoch  45 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.79 | loss  3.38 | ppl    29.49\n",
            "| epoch  45 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.79 | loss  3.36 | ppl    28.74\n",
            "| epoch  45 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.70 | loss  3.19 | ppl    24.39\n",
            "| epoch  45 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.70 | loss  3.28 | ppl    26.69\n",
            "| epoch  45 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.24 | ppl    25.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 108.55s | valid loss  5.21 | valid ppl   182.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  46 |   200/ 2088 batches | lr 0.000000 | ms/batch 50.94 | loss  3.50 | ppl    33.07\n",
            "| epoch  46 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.75 | loss  3.39 | ppl    29.63\n",
            "| epoch  46 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.36 | ppl    28.78\n",
            "| epoch  46 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.33 | ppl    27.93\n",
            "| epoch  46 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.36 | ppl    28.79\n",
            "| epoch  46 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.38 | ppl    29.30\n",
            "| epoch  46 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.36 | ppl    28.78\n",
            "| epoch  46 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.20 | ppl    24.44\n",
            "| epoch  46 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.29 | ppl    26.80\n",
            "| epoch  46 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.70 | loss  3.24 | ppl    25.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 108.54s | valid loss  5.21 | valid ppl   182.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  47 |   200/ 2088 batches | lr 0.000000 | ms/batch 51.03 | loss  3.50 | ppl    33.00\n",
            "| epoch  47 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.39 | ppl    29.64\n",
            "| epoch  47 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.69 | loss  3.36 | ppl    28.75\n",
            "| epoch  47 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.69 | loss  3.33 | ppl    27.83\n",
            "| epoch  47 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.80 | loss  3.36 | ppl    28.71\n",
            "| epoch  47 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.38 | ppl    29.32\n",
            "| epoch  47 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.36 | ppl    28.80\n",
            "| epoch  47 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.20 | ppl    24.42\n",
            "| epoch  47 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.75 | loss  3.29 | ppl    26.73\n",
            "| epoch  47 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.24 | ppl    25.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 108.56s | valid loss  5.21 | valid ppl   182.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  48 |   200/ 2088 batches | lr 0.000000 | ms/batch 51.02 | loss  3.50 | ppl    33.08\n",
            "| epoch  48 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.68 | loss  3.39 | ppl    29.67\n",
            "| epoch  48 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.36 | ppl    28.90\n",
            "| epoch  48 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.33 | ppl    27.93\n",
            "| epoch  48 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.69 | loss  3.36 | ppl    28.74\n",
            "| epoch  48 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.77 | loss  3.38 | ppl    29.37\n",
            "| epoch  48 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.77 | loss  3.36 | ppl    28.75\n",
            "| epoch  48 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.75 | loss  3.20 | ppl    24.41\n",
            "| epoch  48 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.28 | ppl    26.69\n",
            "| epoch  48 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.78 | loss  3.24 | ppl    25.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 108.58s | valid loss  5.21 | valid ppl   182.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  49 |   200/ 2088 batches | lr 0.000000 | ms/batch 50.98 | loss  3.50 | ppl    33.03\n",
            "| epoch  49 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.39 | ppl    29.70\n",
            "| epoch  49 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.69 | loss  3.36 | ppl    28.91\n",
            "| epoch  49 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.70 | loss  3.33 | ppl    27.87\n",
            "| epoch  49 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.77 | loss  3.36 | ppl    28.74\n",
            "| epoch  49 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.38 | ppl    29.32\n",
            "| epoch  49 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.36 | ppl    28.78\n",
            "| epoch  49 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.77 | loss  3.20 | ppl    24.45\n",
            "| epoch  49 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.29 | ppl    26.73\n",
            "| epoch  49 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.23 | ppl    25.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 108.57s | valid loss  5.21 | valid ppl   182.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  50 |   200/ 2088 batches | lr 0.000000 | ms/batch 51.03 | loss  3.50 | ppl    32.96\n",
            "| epoch  50 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.70 | loss  3.39 | ppl    29.63\n",
            "| epoch  50 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.36 | ppl    28.83\n",
            "| epoch  50 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.69 | loss  3.33 | ppl    27.83\n",
            "| epoch  50 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.36 | ppl    28.80\n",
            "| epoch  50 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.38 | ppl    29.28\n",
            "| epoch  50 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.77 | loss  3.36 | ppl    28.76\n",
            "| epoch  50 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.76 | loss  3.20 | ppl    24.42\n",
            "| epoch  50 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.29 | ppl    26.82\n",
            "| epoch  50 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.24 | ppl    25.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 108.55s | valid loss  5.21 | valid ppl   182.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  51 |   200/ 2088 batches | lr 0.000000 | ms/batch 51.02 | loss  3.50 | ppl    33.05\n",
            "| epoch  51 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.75 | loss  3.39 | ppl    29.60\n",
            "| epoch  51 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.36 | ppl    28.78\n",
            "| epoch  51 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.75 | loss  3.32 | ppl    27.76\n",
            "| epoch  51 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.36 | ppl    28.76\n",
            "| epoch  51 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.66 | loss  3.38 | ppl    29.43\n",
            "| epoch  51 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.79 | loss  3.36 | ppl    28.72\n",
            "| epoch  51 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.19 | ppl    24.39\n",
            "| epoch  51 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.29 | ppl    26.79\n",
            "| epoch  51 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.24 | ppl    25.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 108.55s | valid loss  5.21 | valid ppl   182.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  52 |   200/ 2088 batches | lr 0.000000 | ms/batch 51.01 | loss  3.50 | ppl    32.99\n",
            "| epoch  52 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.39 | ppl    29.72\n",
            "| epoch  52 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.68 | loss  3.36 | ppl    28.83\n",
            "| epoch  52 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.32 | ppl    27.76\n",
            "| epoch  52 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.69 | loss  3.36 | ppl    28.84\n",
            "| epoch  52 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.67 | loss  3.38 | ppl    29.39\n",
            "| epoch  52 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.36 | ppl    28.72\n",
            "| epoch  52 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.20 | ppl    24.47\n",
            "| epoch  52 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.77 | loss  3.29 | ppl    26.78\n",
            "| epoch  52 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.70 | loss  3.24 | ppl    25.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 108.53s | valid loss  5.21 | valid ppl   182.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  53 |   200/ 2088 batches | lr 0.000000 | ms/batch 50.98 | loss  3.50 | ppl    33.03\n",
            "| epoch  53 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.39 | ppl    29.66\n",
            "| epoch  53 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.68 | loss  3.36 | ppl    28.84\n",
            "| epoch  53 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.68 | loss  3.33 | ppl    27.85\n",
            "| epoch  53 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.36 | ppl    28.68\n",
            "| epoch  53 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.76 | loss  3.38 | ppl    29.34\n",
            "| epoch  53 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.66 | loss  3.36 | ppl    28.68\n",
            "| epoch  53 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.78 | loss  3.20 | ppl    24.45\n",
            "| epoch  53 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.70 | loss  3.29 | ppl    26.74\n",
            "| epoch  53 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.23 | ppl    25.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 108.51s | valid loss  5.21 | valid ppl   182.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  54 |   200/ 2088 batches | lr 0.000000 | ms/batch 50.97 | loss  3.50 | ppl    32.98\n",
            "| epoch  54 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.61 | loss  3.39 | ppl    29.54\n",
            "| epoch  54 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.63 | loss  3.36 | ppl    28.80\n",
            "| epoch  54 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.69 | loss  3.33 | ppl    27.90\n",
            "| epoch  54 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.36 | ppl    28.75\n",
            "| epoch  54 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.70 | loss  3.38 | ppl    29.36\n",
            "| epoch  54 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.77 | loss  3.36 | ppl    28.70\n",
            "| epoch  54 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.80 | loss  3.20 | ppl    24.46\n",
            "| epoch  54 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.29 | ppl    26.84\n",
            "| epoch  54 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.24 | ppl    25.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 108.52s | valid loss  5.21 | valid ppl   182.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  55 |   200/ 2088 batches | lr 0.000000 | ms/batch 50.95 | loss  3.50 | ppl    33.04\n",
            "| epoch  55 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.69 | loss  3.39 | ppl    29.54\n",
            "| epoch  55 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.36 | ppl    28.73\n",
            "| epoch  55 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.33 | ppl    27.90\n",
            "| epoch  55 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.36 | ppl    28.75\n",
            "| epoch  55 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.78 | loss  3.38 | ppl    29.37\n",
            "| epoch  55 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.70 | loss  3.36 | ppl    28.71\n",
            "| epoch  55 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.20 | ppl    24.44\n",
            "| epoch  55 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.29 | ppl    26.76\n",
            "| epoch  55 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.77 | loss  3.24 | ppl    25.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 108.54s | valid loss  5.21 | valid ppl   182.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  56 |   200/ 2088 batches | lr 0.000000 | ms/batch 51.04 | loss  3.49 | ppl    32.79\n",
            "| epoch  56 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.39 | ppl    29.63\n",
            "| epoch  56 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.69 | loss  3.36 | ppl    28.90\n",
            "| epoch  56 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.69 | loss  3.32 | ppl    27.78\n",
            "| epoch  56 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.36 | ppl    28.79\n",
            "| epoch  56 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.38 | ppl    29.31\n",
            "| epoch  56 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.36 | ppl    28.73\n",
            "| epoch  56 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.75 | loss  3.20 | ppl    24.46\n",
            "| epoch  56 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.70 | loss  3.29 | ppl    26.73\n",
            "| epoch  56 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.66 | loss  3.24 | ppl    25.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 108.53s | valid loss  5.21 | valid ppl   182.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  57 |   200/ 2088 batches | lr 0.000000 | ms/batch 50.99 | loss  3.49 | ppl    32.91\n",
            "| epoch  57 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.39 | ppl    29.52\n",
            "| epoch  57 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.36 | ppl    28.77\n",
            "| epoch  57 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.32 | ppl    27.78\n",
            "| epoch  57 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.36 | ppl    28.76\n",
            "| epoch  57 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.38 | ppl    29.27\n",
            "| epoch  57 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.36 | ppl    28.71\n",
            "| epoch  57 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.20 | ppl    24.43\n",
            "| epoch  57 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.70 | loss  3.29 | ppl    26.78\n",
            "| epoch  57 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.23 | ppl    25.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 108.53s | valid loss  5.21 | valid ppl   182.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  58 |   200/ 2088 batches | lr 0.000000 | ms/batch 50.94 | loss  3.50 | ppl    33.01\n",
            "| epoch  58 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.67 | loss  3.39 | ppl    29.62\n",
            "| epoch  58 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.70 | loss  3.36 | ppl    28.77\n",
            "| epoch  58 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.76 | loss  3.32 | ppl    27.76\n",
            "| epoch  58 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.36 | ppl    28.68\n",
            "| epoch  58 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.38 | ppl    29.28\n",
            "| epoch  58 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.70 | loss  3.36 | ppl    28.69\n",
            "| epoch  58 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.20 | ppl    24.49\n",
            "| epoch  58 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.70 | loss  3.29 | ppl    26.81\n",
            "| epoch  58 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.76 | loss  3.24 | ppl    25.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 108.54s | valid loss  5.21 | valid ppl   182.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  59 |   200/ 2088 batches | lr 0.000000 | ms/batch 51.01 | loss  3.49 | ppl    32.93\n",
            "| epoch  59 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.66 | loss  3.39 | ppl    29.58\n",
            "| epoch  59 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.73 | loss  3.36 | ppl    28.71\n",
            "| epoch  59 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.77 | loss  3.33 | ppl    27.84\n",
            "| epoch  59 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.72 | loss  3.36 | ppl    28.68\n",
            "| epoch  59 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.75 | loss  3.38 | ppl    29.34\n",
            "| epoch  59 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.78 | loss  3.36 | ppl    28.68\n",
            "| epoch  59 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.20 | ppl    24.47\n",
            "| epoch  59 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.76 | loss  3.29 | ppl    26.76\n",
            "| epoch  59 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.78 | loss  3.23 | ppl    25.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 108.58s | valid loss  5.21 | valid ppl   182.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "| epoch  60 |   200/ 2088 batches | lr 0.000000 | ms/batch 50.94 | loss  3.49 | ppl    32.89\n",
            "| epoch  60 |   400/ 2088 batches | lr 0.000000 | ms/batch 50.76 | loss  3.38 | ppl    29.52\n",
            "| epoch  60 |   600/ 2088 batches | lr 0.000000 | ms/batch 50.74 | loss  3.36 | ppl    28.82\n",
            "| epoch  60 |   800/ 2088 batches | lr 0.000000 | ms/batch 50.65 | loss  3.32 | ppl    27.73\n",
            "| epoch  60 |  1000/ 2088 batches | lr 0.000000 | ms/batch 50.68 | loss  3.35 | ppl    28.64\n",
            "| epoch  60 |  1200/ 2088 batches | lr 0.000000 | ms/batch 50.67 | loss  3.38 | ppl    29.24\n",
            "| epoch  60 |  1400/ 2088 batches | lr 0.000000 | ms/batch 50.68 | loss  3.36 | ppl    28.72\n",
            "| epoch  60 |  1600/ 2088 batches | lr 0.000000 | ms/batch 50.77 | loss  3.20 | ppl    24.48\n",
            "| epoch  60 |  1800/ 2088 batches | lr 0.000000 | ms/batch 50.71 | loss  3.29 | ppl    26.79\n",
            "| epoch  60 |  2000/ 2088 batches | lr 0.000000 | ms/batch 50.77 | loss  3.24 | ppl    25.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 108.52s | valid loss  5.21 | valid ppl   182.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000000\n",
            "=========================================================================================\n",
            "| End of training | test loss  5.02 | test ppl   151.92\n",
            "=========================================================================================\n",
            "\n",
            "Generating text...\n",
            "Vocabulary size: 33278\n",
            "Vocabulary size: 33278\n",
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n",
            "to every other two vipers . From a birds , their kakapo may be viable skulls and good architectural prey\n",
            "as they are more direct for the more under the entire use of corridors survives . During the living .\n",
            "They are notorious records , 5 of their travel food , music guides erected all official poor buildings on the\n",
            "year the next to 49 % of the proper water supply of these are very mad   spored than a\n",
            "particular balanced writings at this mode and sensitive advances in the hard times due to distance between those hydrogen concentration\n",
            "shows assimilated by Ceres ( 40 % suggest that they are typically have a long   invested Alabama Kesteven and\n",
            "rear rate Metro in the life as one volume as possible stone forces comprised most notably the upper layers of\n",
            "Ireland ( contraceptive be activated . They larvae and the largest power forces raise up to 175 % ) .\n",
            "In 2014 ) high strips , no longer than multiple % , simple , <unk> Rusticana measures include in the\n",
            "row and half , including only 69 % of civilian and 100 % of the event , 5   5\n",
            "times , but are European research , since 2010 they do rises under minor schools on 2 % featuring 5\n",
            "% , ranging from the temperature and bombing races of the Republic of very parent pine and six approximately 30\n",
            "% of a condom requirements , including Early orbits between seaside legislators , both magnesium % associated with 4 %\n",
            "of the entirety , which gross   exempt powers in China sites , such as an official . <eos> <eos>\n",
            "<eos> <eos> <eos> <eos> <eos> <eos> The population number of the deposit ) , which is available , completing the\n",
            "total remains of university presidents is held by the Eastern Area Command and no longer pillars of Ceres , the\n",
            "first cross   campus is now prevented Ireland 's first 13 % of up to 29 % of the medical\n",
            "requirements for Northern Ireland has all three institutions are Bode and three opposing population increase the one of all  \n",
            "243 % of their total conferences , the ongoing   ranging during the Central Crosses and various other thirds of\n",
            "the county birds for sexual breeding began breeding Finland so conspicuous behaviour and the population available for their description of\n",
            "2010 have tombs on Ceres on 8 % in power development stations through 7 <unk> . A private charter migrants\n",
            "appraisal , for increased by 7 % of the population of their highest 454   000 % are 66 %\n",
            "of 100 % of many million tons Sharif , including various food units , providing a non   largest industries\n",
            ". In Italy , U.S. cities on the population and the largest percent of the first and only four to\n",
            "90 % of construction of thorium , a earlier large   1897 competition ( teams have been native EU ,\n",
            "double   final numbers of gameplay today . Up suborder is <unk> missions there are both women built populations of\n",
            "over 60 % of the population of Ireland as possible state immigration growing quarters of the run in the population\n",
            "in decline in Northern Ireland , including the country being lost . greyhound buildings in <unk> by this species increased\n",
            "rations since 277 % disintegration of the 50 % of the population is vulnerable to 40   000 individuals ,\n",
            "with concealing a sixth highest range of over 350 % for agricultural North Canadian cougars were perfect government children 's\n",
            "population of six people in the <unk> sports population increased today glass levels , producing widely regained 1717 , some\n",
            "commercial losses of <unk> , it occurred in the populace means of population immediate service , most independent <unk> than\n",
            "unemployment ancestry . In remnants in 2008 still over 50 % increase in Ulster assured use of the population of\n",
            "Ireland is less than Byung   largest small occurrence ( 48 % of the 12th or middle approximately about 45\n",
            "people   000 lecturing in 1996 , due to determine the population may be in Lydney population of social units\n",
            "in the population range of his support for most in addition and half   10 % of Ireland . Unlike\n",
            "total sites , and three Owl , 7 % of the criminal frequency \" extremes in the government islands ,\n",
            "Turkey   500 % of small burning and sports activity and thus Babe is dominated by 10 % of search\n",
            "  best separate countries as twenty   largest religious origin in the sport   000 million to compact heat uniforms\n",
            ", most successful , most of Ireland authorities and third or total population of as Palestine % of Northern Ireland\n",
            ", and $ 12 % total population Europe and remained in tax races and total areas of power in the\n",
            "population combination of these buildings shut steps supply range measures increase until recent life in the population needed and up\n",
            "to some birds of the population decline in five percent of the new capital management sites of fish <unk> as\n",
            "55 % increase in the population came and contributed by some their total non   existing history since the potential\n",
            "Polish   race survive , some medium range insectivorous centuries , in the island for the <unk> . The population\n",
            "measures 33 % commercials is also deaths for females have half   growing number is based work and ceremony under\n",
            "hotels for smaller courses under prey people in total sites placed civilian operations , fewer than 60 % of the\n",
            "world , and 2 % of this economic residents which was 10   000 % of what are most recently\n",
            "the population areas being extended time whenever Bir economic education and number of the period after warm   year records\n",
            "( both more valuable than males , according to $ 59 % of all three to 85 percent production manufacturing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model analysis results on original dataset on 60 epochs"
      ],
      "metadata": {
        "id": "t5z6FIRlBT4f"
      },
      "id": "t5z6FIRlBT4f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model Name  | Emsize |  Nhid | Nlayers | Nhead | Lr | Batch size | Dropout | Bppt | Optimizer | Decay | Betas | eps | Temperature | Final test loss | Final test ppl\n",
        "|-------|-----|-------| ------|------|------|------|------|------|------|------|------|------|------|------|------|\n",
        "| LSTM (NLLLoss) |400|400|4|4|0.001|20|0.2|35|AdamW|1e-5|-|-|1.0|5.05|155.26|\n",
        "| LSTM (NLLLoss) | | | | | | | | | | | | | | | |\n",
        "| LSTM (Smoothing = 0.1) |400|400|4|4|0.001|20|0.2|35|AdamW|1e-4|(0.9;0.98)|1e-9|1.0|6.20|491.70|\n",
        "| LSTM (Smoothing = 0.1) |512|2048|6|8|0.0001|32|0.1|50|AdamW|1e-4|(0.9;0.98)|1e-9|1.0|||\n",
        "| RNN_TANH (NLLLoss) |400|400|4|4|0.001|20|0.2|35|AdamW|1e-5|-|-|1.0|5.56|260.08|\n",
        "| RNN_TANH (NLLLoss) | | | | | | | | | | | | | | | |\n",
        "| RNN_TANH (Smoothing = 0.1) |400|400|4|4|0.001|20|0.2|35|AdamW|1e-4|(0.9;0.98)|1e-9|1.0|6.34|566.51|\n",
        "| RNN_TANH (Smoothing = 0.1) |512|2048|6|8|0.0001|32|0.1|50|AdamW|1e-4|(0.9;0.98)|1e-9|1.0|6.52|677.26|\n",
        "| RNN_RELU (NLLLoss) |400|400|4|4|0.001|20|0.2|35|AdamW|1e-5|-|-|1.0|5.34|208.67|\n",
        "| RNN_RELU (NLLLoss) | | | | | | | | | | | | | | | |\n",
        "| RNN_RELU (Smoothing = 0.1) |400|400|4|4|0.001|20|0.2|35|AdamW|1e-4|(0.9;0.98)|1e-9|1.0|nan-6.34|nan-569.06|\n",
        "| RNN_RELU (Smoothing = 0.1) |512|2048|6|8|0.0001|32|0.1|50|AdamW|1e-4|(0.9;0.98)|1e-9|1.0|||\n",
        "| GRU (NLLLoss) |400|400|4|4|0.001|20|0.2|35|AdamW|1e-5|-|-|1.0|5.15|171.83|\n",
        "| GRU (NLLLoss) | | | | | | | | | | | | | | | |\n",
        "| GRU (Smoothing = 0.1) |400|400|4|4|0.001|20|0.2|35|AdamW|1e-4|(0.9;0.98)|1e-9|1.0|6.21|498.41|\n",
        "| GRU (Smoothing = 0.1) |512|2048|6|8|0.0001|32|0.1|50|AdamW|1e-4|(0.9;0.98)|1e-9|1.0|6.48|649.97|\n",
        "| Transformer (NLLLoss) |400|400|4|4|0.001|20|0.2|35|AdamW|1e-5|-|-|1.0|6.81|903.66|\n",
        "| Transformer (NLLLoss) |512|2048|6|8|0.0001|20|0.1|50|AdamW|1e-5|-|-|1.0|5.02|151.92|\n",
        "| Transformer (Smoothing = 0.1) |400|400|4|4|0.001|20|0.2|35|AdamW|1e-4|(0.9;0.98)|1e-9|1.0|6.31|547.72|\n",
        "| Transformer (Smoothing = 0.1) |512|2048|6|8|0.0001|32|0.1|50|AdamW|1e-4|(0.9;0.98)|1e-9|1.0|6.26|525.56|"
      ],
      "metadata": {
        "id": "oH-BpBjkBbpo"
      },
      "id": "oH-BpBjkBbpo"
    },
    {
      "cell_type": "markdown",
      "id": "BSnSaA8FXKGj",
      "metadata": {
        "id": "BSnSaA8FXKGj"
      },
      "source": [
        "### MAIN PART"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cmr9R-Z5XIPa",
      "metadata": {
        "id": "cmr9R-Z5XIPa"
      },
      "outputs": [],
      "source": [
        "# Step 4: Data Preparation for Custom Data (English-French dataset as example)\n",
        "# Extract French sentences and create train/valid/test.txt\n",
        "DATA_PATH = '/content/drive/MyDrive/data_word_train/custom'\n",
        "INPUT_FILE = '/content/drive/MyDrive/data/eng-fra.txt'\n",
        "os.makedirs(DATA_PATH, exist_ok=True)\n",
        "random.seed(1111)\n",
        "\n",
        "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "french_sentences = [line.strip().split('\\t')[1] for line in lines if len(line.strip().split('\\t')) == 2]\n",
        "\n",
        "random.shuffle(french_sentences)\n",
        "n = len(french_sentences)\n",
        "train_end = int(n * 0.8)\n",
        "valid_end = train_end + int(n * 0.1)\n",
        "train_data = french_sentences[:train_end]\n",
        "valid_data = french_sentences[train_end:valid_end]\n",
        "test_data = french_sentences[valid_end:]\n",
        "\n",
        "def save_sentences(sentences, filename):\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.replace('.', ' .').replace(',', ' ,').replace('!', ' !').replace('?', ' ?')\n",
        "            f.write(sentence + ' <eos>\\n')\n",
        "\n",
        "save_sentences(train_data, os.path.join(DATA_PATH, 'train.txt'))\n",
        "save_sentences(valid_data, os.path.join(DATA_PATH, 'valid.txt'))\n",
        "save_sentences(test_data, os.path.join(DATA_PATH, 'test.txt'))\n",
        "\n",
        "print(f\"Created datasets: {len(train_data)} train, {len(valid_data)} valid, {len(test_data)} test sentences\")\n",
        "\n",
        "# Step 5: Training Function from main.py (adapted for Colab)\n",
        "# Define args as a class for Colab\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.data = '/content/drive/MyDrive/data_word_train/custom'  # Custom data path\n",
        "        self.model = 'LSTM'  # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "        self.emsize = 200\n",
        "        self.nhid = 200\n",
        "        self.nlayers = 2\n",
        "        self.lr = 0.001\n",
        "        self.clip = 0.25\n",
        "        self.epochs = 20  # Reduced for faster training\n",
        "        self.batch_size = 20\n",
        "        self.bptt = 35\n",
        "        self.dropout = 0.2\n",
        "        self.tied = False\n",
        "        self.seed = 1111\n",
        "        self.log_interval = 200\n",
        "        self.save = 'model.pt'\n",
        "        self.onnx_export = ''\n",
        "        self.nhead = 2\n",
        "        self.dry_run = False\n",
        "        self.accel = True\n",
        "        self.use_optimizer = True  # Use AdamW\n",
        "\n",
        "args = Args()\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "if args.accel and torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "corpus = Corpus(args.data)\n",
        "\n",
        "# Starting from sequential data, batchify arranges the dataset into columns.\n",
        "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
        "# ┌ a g m s ┐\n",
        "# │ b h n t │\n",
        "# │ c i o u │\n",
        "# │ d j p v │\n",
        "# │ e k q w │\n",
        "# └ f l r x ┘.\n",
        "# These columns are treated as independent by the model, which means that the\n",
        "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
        "# batch processing.\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    nbatch = data.size(0) // bsz\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(corpus.train, args.batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "\n",
        "ntokens = len(corpus.dictionary)\n",
        "if args.model == 'Transformer':\n",
        "    model = TransformerModel(ntokens, args.emsize, args.nhead, args.nhid, args.nlayers, args.dropout).to(device)\n",
        "else:\n",
        "    model = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied).to(device)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "if args.use_optimizer:\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
        "\n",
        "###############################################################################\n",
        "# Training code\n",
        "###############################################################################\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "# get_batch subdivides the source data into chunks of length args.bptt.\n",
        "# If source is equal to the example output of the batchify function, with\n",
        "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
        "# ┌ a g m s ┐ ┌ b h n t ┐\n",
        "# └ b h n t ┘ └ c i o u ┘\n",
        "# Note that despite the name of the function, the subdivison of data is not\n",
        "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
        "# by the batchify function. The chunks are along dimension 0, corresponding\n",
        "# to the seq_len dimension in the LSTM.\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "def evaluate(data_source):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if args.model != 'Transformer':\n",
        "        hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, args.bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            if args.model == 'Transformer':\n",
        "                output = model(data)\n",
        "                output = output.view(-1, ntokens)\n",
        "            else:\n",
        "                output, hidden = model(data, hidden)\n",
        "                hidden = repackage_hidden(hidden)\n",
        "            total_loss += len(data) * criterion(output, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "def train_func():\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if args.model != 'Transformer':\n",
        "        hidden = model.init_hidden(args.batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        optimizer.zero_grad() if args.use_optimizer else model.zero_grad()\n",
        "        if args.model == 'Transformer':\n",
        "            output = model(data)\n",
        "            output = output.view(-1, ntokens)\n",
        "        else:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            output, hidden = model(data, hidden)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
        "        if args.use_optimizer:\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            for p in model.parameters():\n",
        "                p.data.add_(p.grad, alpha=-lr)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % args.log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / args.log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // args.bptt, lr,\n",
        "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "        if args.dry_run:\n",
        "            break\n",
        "\n",
        "lr = args.lr\n",
        "best_val_loss = None\n",
        "\n",
        "try:\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        train_func()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                         val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(args.save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(args.save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)\n",
        "\n",
        "# Step 6: Generate Text from generate.py (adapted for Colab)\n",
        "checkpoint = 'model.pt'  # Your saved model\n",
        "outf = 'generated.txt'\n",
        "words = 1000\n",
        "temperature = 1.0\n",
        "log_interval = 100\n",
        "accel = True  # Use CUDA\n",
        "\n",
        "torch.manual_seed(1111)\n",
        "\n",
        "if accel and torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "with open(checkpoint, 'rb') as f:\n",
        "    model = torch.load(f, map_location=device)\n",
        "model.eval()\n",
        "\n",
        "corpus = Corpus(args.data)\n",
        "ntokens = len(corpus.dictionary)\n",
        "\n",
        "is_transformer_model = hasattr(model, 'model_type') and model.model_type == 'Transformer'\n",
        "if not is_transformer_model:\n",
        "    hidden = model.init_hidden(1)\n",
        "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "with open(outf, 'w') as outf:\n",
        "    with torch.no_grad():\n",
        "        for i in range(words):\n",
        "            if is_transformer_model:\n",
        "                output = model(input, False)\n",
        "                word_weights = output[-1].squeeze().div(temperature).exp().cpu()\n",
        "                word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "                word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
        "                input = torch.cat([input, word_tensor], 0)\n",
        "            else:\n",
        "                output, hidden = model(input, hidden)\n",
        "                word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "                word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "                input.fill_(word_idx)\n",
        "\n",
        "            word = corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "            if i % log_interval == 0:\n",
        "                print('| Generated {}/{} words'.format(i, words))\n",
        "\n",
        "# Print generated text\n",
        "!head -n 20 generated.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eteX5K-YGnxt",
      "metadata": {
        "id": "eteX5K-YGnxt"
      },
      "source": [
        "### Names dataset learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87WH6xgiGqmE",
      "metadata": {
        "id": "87WH6xgiGqmE"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Improved Word-Level Language Modeling with Full Type Hints\n",
        "Based on PyTorch RNN/Transformer example with enhancements\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "from io import open\n",
        "from typing import List, Dict, Tuple, Optional, Union\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# ===============================\n",
        "# DATA PREPARATION (data.py)\n",
        "# ===============================\n",
        "\n",
        "class Dictionary:\n",
        "    \"\"\"Dictionary for word-to-index mapping.\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.word2idx: Dict[str, int] = {}\n",
        "        self.idx2word: List[str] = []\n",
        "\n",
        "    def add_word(self, word: str) -> int:\n",
        "        \"\"\"Add a word to the dictionary.\"\"\"\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus:\n",
        "    \"\"\"Corpus class for loading and tokenizing text data.\"\"\"\n",
        "\n",
        "    def __init__(self, path: str) -> None:\n",
        "        self.dictionary: Dictionary = Dictionary()\n",
        "        self.train: torch.Tensor = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid: torch.Tensor = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test: torch.Tensor = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path: str) -> torch.Tensor:\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path), f\"Path {path} does not exist\"\n",
        "\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words: List[str] = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss: List[torch.Tensor] = []\n",
        "            for line in f:\n",
        "                words: List[str] = line.split() + ['<eos>']\n",
        "                ids: List[int] = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids, dtype=torch.int64))\n",
        "            ids_tensor: torch.Tensor = torch.cat(idss)\n",
        "\n",
        "        return ids_tensor\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# MODEL ARCHITECTURES (model.py)\n",
        "# ===============================\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Positional encoding for Transformer.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000) -> None:\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout: nn.Dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe: torch.Tensor = torch.zeros(max_len, d_model)\n",
        "        position: torch.Tensor = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term: torch.Tensor = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    \"\"\"RNN-based language model (LSTM/GRU/RNN).\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        rnn_type: str,\n",
        "        ntoken: int,\n",
        "        ninp: int,\n",
        "        nhid: int,\n",
        "        nlayers: int,\n",
        "        dropout: float = 0.5,\n",
        "        tie_weights: bool = False\n",
        "    ) -> None:\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.ntoken: int = ntoken\n",
        "        self.rnn_type: str = rnn_type\n",
        "        self.nhid: int = nhid\n",
        "        self.nlayers: int = nlayers\n",
        "\n",
        "        self.drop: nn.Dropout = nn.Dropout(dropout)\n",
        "        self.encoder: nn.Embedding = nn.Embedding(ntoken, ninp)\n",
        "\n",
        "        if rnn_type in ['LSTM', 'GRU']:\n",
        "            self.rnn: nn.Module = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
        "        else:\n",
        "            try:\n",
        "                nonlinearity: str = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
        "            except KeyError as e:\n",
        "                raise ValueError(\n",
        "                    \"Invalid option for `--model`. \"\n",
        "                    \"Options are ['LSTM', 'GRU', 'RNN_TANH', 'RNN_RELU']\"\n",
        "                ) from e\n",
        "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
        "\n",
        "        self.decoder: nn.Linear = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        # Tie weights\n",
        "        if tie_weights:\n",
        "            if nhid != ninp:\n",
        "                raise ValueError('When using tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        \"\"\"Initialize weights.\"\"\"\n",
        "        initrange: float = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, input: torch.Tensor, hidden: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        emb: torch.Tensor = self.drop(self.encoder(input))\n",
        "        output: torch.Tensor\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded: torch.Tensor = self.decoder(output)\n",
        "        decoded = decoded.view(-1, self.ntoken)\n",
        "        return F.log_softmax(decoded, dim=1), hidden\n",
        "\n",
        "    def init_hidden(self, bsz: int) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\"Initialize hidden state.\"\"\"\n",
        "        weight: torch.Tensor = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (\n",
        "                weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
        "            )\n",
        "        else:\n",
        "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Transformer):\n",
        "    \"\"\"Transformer-based language model.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        ntoken: int,\n",
        "        ninp: int,\n",
        "        nhead: int,\n",
        "        nhid: int,\n",
        "        nlayers: int,\n",
        "        dropout: float = 0.5\n",
        "    ) -> None:\n",
        "        super(TransformerModel, self).__init__(\n",
        "            d_model=ninp,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=nhid,\n",
        "            num_encoder_layers=nlayers\n",
        "        )\n",
        "        self.model_type: str = 'Transformer'\n",
        "        self.src_mask: Optional[torch.Tensor] = None\n",
        "        self.pos_encoder: PositionalEncoding = PositionalEncoding(ninp, dropout)\n",
        "\n",
        "        self.input_emb: nn.Embedding = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp: int = ninp\n",
        "        self.decoder: nn.Linear = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz: int) -> torch.Tensor:\n",
        "        \"\"\"Generate mask for causal attention.\"\"\"\n",
        "        return torch.log(torch.tril(torch.ones(sz, sz)))\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        \"\"\"Initialize weights.\"\"\"\n",
        "        initrange: float = 0.1\n",
        "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, src: torch.Tensor, has_mask: bool = True) -> torch.Tensor:\n",
        "        if has_mask:\n",
        "            device: torch.device = src.device\n",
        "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "                mask: torch.Tensor = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "                self.src_mask = mask\n",
        "        else:\n",
        "            self.src_mask = None\n",
        "\n",
        "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output: torch.Tensor = self.encoder(src, mask=self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return F.log_softmax(output, dim=-1)\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# TRAINING & EVALUATION (main.py)\n",
        "# ===============================\n",
        "\n",
        "def batchify(data: torch.Tensor, bsz: int, device: torch.device) -> torch.Tensor:\n",
        "    \"\"\"Divide data into batches.\"\"\"\n",
        "    nbatch: int = data.size(0) // bsz\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "\n",
        "def get_batch(source: torch.Tensor, i: int, bptt: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Get a batch of data.\"\"\"\n",
        "    seq_len: int = min(bptt, len(source) - 1 - i)\n",
        "    data: torch.Tensor = source[i:i+seq_len]\n",
        "    target: torch.Tensor = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def repackage_hidden(h: Union[torch.Tensor, Tuple]) -> Union[torch.Tensor, Tuple]:\n",
        "    \"\"\"Detach hidden state from history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    data_source: torch.Tensor,\n",
        "    criterion: nn.Module,\n",
        "    bptt: int,\n",
        "    ntokens: int,\n",
        "    eval_batch_size: int,\n",
        "    is_transformer: bool\n",
        ") -> float:\n",
        "    \"\"\"Evaluate the model.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss: float = 0.0\n",
        "    if not is_transformer:\n",
        "        hidden = model.init_hidden(eval_batch_size)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data: torch.Tensor\n",
        "            targets: torch.Tensor\n",
        "            data, targets = get_batch(data_source, i, bptt)\n",
        "\n",
        "            if is_transformer:\n",
        "                output: torch.Tensor = model(data)\n",
        "                output = output.view(-1, ntokens)\n",
        "            else:\n",
        "                output, hidden = model(data, hidden)\n",
        "                hidden = repackage_hidden(hidden)\n",
        "\n",
        "            total_loss += len(data) * criterion(output, targets).item()\n",
        "\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train_epoch(\n",
        "    model: nn.Module,\n",
        "    train_data: torch.Tensor,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    epoch: int,\n",
        "    bptt: int,\n",
        "    ntokens: int,\n",
        "    batch_size: int,\n",
        "    clip: float,\n",
        "    log_interval: int,\n",
        "    is_transformer: bool\n",
        ") -> None:\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss: float = 0.0\n",
        "    start_time: float = time.time()\n",
        "\n",
        "    if not is_transformer:\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data: torch.Tensor\n",
        "        targets: torch.Tensor\n",
        "        data, targets = get_batch(train_data, i, bptt)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if is_transformer:\n",
        "            output: torch.Tensor = model(data)\n",
        "            output = output.view(-1, ntokens)\n",
        "        else:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            output, hidden = model(data, hidden)\n",
        "\n",
        "        loss: torch.Tensor = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss: float = total_loss / log_interval\n",
        "            elapsed: float = time.time() - start_time\n",
        "            print(\n",
        "                f'| epoch {epoch:3d} | {batch:5d}/{len(train_data) // bptt:5d} batches | '\n",
        "                f'ms/batch {elapsed * 1000 / log_interval:5.2f} | '\n",
        "                f'loss {cur_loss:5.2f} | ppl {math.exp(cur_loss):8.2f}'\n",
        "            )\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model_type: str = 'LSTM',\n",
        "    data_path: str = './data/wikitext-2',\n",
        "    emsize: int = 200,\n",
        "    nhid: int = 200,\n",
        "    nlayers: int = 2,\n",
        "    lr: float = 0.001,\n",
        "    clip: float = 0.25,\n",
        "    epochs: int = 40,\n",
        "    batch_size: int = 20,\n",
        "    bptt: int = 35,\n",
        "    dropout: float = 0.2,\n",
        "    tied: bool = False,\n",
        "    nhead: int = 2,\n",
        "    log_interval: int = 200,\n",
        "    save_path: str = 'model.pt',\n",
        "    use_cuda: bool = True\n",
        ") -> None:\n",
        "    \"\"\"Main training function.\"\"\"\n",
        "\n",
        "    # Set device\n",
        "    device: torch.device = torch.device('cuda' if use_cuda and torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load data\n",
        "    corpus: Corpus = Corpus(data_path)\n",
        "\n",
        "    eval_batch_size: int = 10\n",
        "    train_data: torch.Tensor = batchify(corpus.train, batch_size, device)\n",
        "    val_data: torch.Tensor = batchify(corpus.valid, eval_batch_size, device)\n",
        "    test_data: torch.Tensor = batchify(corpus.test, eval_batch_size, device)\n",
        "\n",
        "    # Build model\n",
        "    ntokens: int = len(corpus.dictionary)\n",
        "    is_transformer: bool = model_type == 'Transformer'\n",
        "\n",
        "    if is_transformer:\n",
        "        model: nn.Module = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
        "    else:\n",
        "        model = RNNModel(model_type, ntokens, emsize, nhid, nlayers, dropout, tied).to(device)\n",
        "\n",
        "    # Loss and optimizer (Adam with weight_decay as in Transformer paper)\n",
        "    criterion: nn.NLLLoss = nn.NLLLoss()\n",
        "    optimizer: optim.Adam = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    scheduler: optim.lr_scheduler.ReduceLROnPlateau = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=2\n",
        "    )\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss: Optional[float] = None\n",
        "\n",
        "    try:\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            epoch_start_time: float = time.time()\n",
        "\n",
        "            train_epoch(\n",
        "                model, train_data, criterion, optimizer, epoch,\n",
        "                bptt, ntokens, batch_size, clip, log_interval, is_transformer\n",
        "            )\n",
        "\n",
        "            val_loss: float = evaluate(\n",
        "                model, val_data, criterion, bptt, ntokens,\n",
        "                eval_batch_size, is_transformer\n",
        "            )\n",
        "\n",
        "            print('-' * 89)\n",
        "            print(\n",
        "                f'| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s | '\n",
        "                f'valid loss {val_loss:5.2f} | valid ppl {math.exp(val_loss):8.2f}'\n",
        "            )\n",
        "            print('-' * 89)\n",
        "\n",
        "            # Save best model\n",
        "            if not best_val_loss or val_loss < best_val_loss:\n",
        "                with open(save_path, 'wb') as f:\n",
        "                    torch.save(model, f)\n",
        "                best_val_loss = val_loss\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            scheduler.step(val_loss)\n",
        "            print(f\"Current learning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('-' * 89)\n",
        "        print('Exiting from training early')\n",
        "\n",
        "    # Load best model and test\n",
        "    with open(save_path, 'rb') as f:\n",
        "        model = torch.load(f, map_location=device)\n",
        "\n",
        "    test_loss: float = evaluate(\n",
        "        model, test_data, criterion, bptt, ntokens,\n",
        "        eval_batch_size, is_transformer\n",
        "    )\n",
        "\n",
        "    print('=' * 89)\n",
        "    print(\n",
        "        f'| End of training | test loss {test_loss:5.2f} | '\n",
        "        f'test ppl {math.exp(test_loss):8.2f}'\n",
        "    )\n",
        "    print('=' * 89)\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# TEXT GENERATION (generate.py)\n",
        "# ===============================\n",
        "\n",
        "def generate_text(\n",
        "    checkpoint: str = 'model.pt',\n",
        "    data_path: str = './data/wikitext-2',\n",
        "    outf: str = 'generated.txt',\n",
        "    words: int = 1000,\n",
        "    temperature: float = 1.0,\n",
        "    seed: int = 1111,\n",
        "    log_interval: int = 100,\n",
        "    use_cuda: bool = True\n",
        ") -> None:\n",
        "    \"\"\"Generate text from trained model.\"\"\"\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    device: torch.device = torch.device('cuda' if use_cuda and torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Load model\n",
        "    with open(checkpoint, 'rb') as f:\n",
        "        model: nn.Module = torch.load(f, map_location=device)\n",
        "    model.eval()\n",
        "\n",
        "    # Load corpus\n",
        "    corpus: Corpus = Corpus(data_path)\n",
        "    ntokens: int = len(corpus.dictionary)\n",
        "\n",
        "    is_transformer: bool = hasattr(model, 'model_type') and model.model_type == 'Transformer'\n",
        "    if not is_transformer:\n",
        "        hidden = model.init_hidden(1)\n",
        "\n",
        "    input: torch.Tensor = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "    with open(outf, 'w') as outfile:\n",
        "        with torch.no_grad():\n",
        "            for i in range(words):\n",
        "                if is_transformer:\n",
        "                    output: torch.Tensor = model(input, False)\n",
        "                    word_weights: torch.Tensor = output[-1].squeeze().div(temperature).exp().cpu()\n",
        "                    word_idx: int = torch.multinomial(word_weights, 1)[0].item()\n",
        "                    word_tensor: torch.Tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
        "                    input = torch.cat([input, word_tensor], 0)\n",
        "                else:\n",
        "                    output, hidden = model(input, hidden)\n",
        "                    word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "                    word_idx = torch.multinomial(word_weights, 1)[0].item()\n",
        "                    input.fill_(word_idx)\n",
        "\n",
        "                word: str = corpus.dictionary.idx2word[word_idx]\n",
        "                outfile.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "                if i % log_interval == 0:\n",
        "                    print(f'| Generated {i}/{words} words')\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# EXAMPLE USAGE\n",
        "# ===============================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Example 1: Train on WikiText-2\n",
        "    print(\"Training LSTM on WikiText-2...\")\n",
        "    train_model(\n",
        "        model_type='LSTM',\n",
        "        data_path='./data/wikitext-2',\n",
        "        emsize=400,\n",
        "        nhid=400,\n",
        "        nlayers=4,\n",
        "        epochs=40,\n",
        "        lr=0.001\n",
        "    )\n",
        "\n",
        "    # Example 2: Generate text\n",
        "    print(\"\\nGenerating text...\")\n",
        "    generate_text(\n",
        "        checkpoint='model.pt',\n",
        "        data_path='./data/wikitext-2',\n",
        "        words=1000,\n",
        "        temperature=1.0\n",
        "    )\n",
        "\n",
        "    # Example 3: Train on custom names dataset\n",
        "    # First, create the data files (see instructions below)\n",
        "    print(\"\\nTraining on custom names dataset...\")\n",
        "    # train_model(\n",
        "    #     model_type='LSTM',\n",
        "    #     data_path='./data/names',\n",
        "    #     emsize=128,\n",
        "    #     nhid=128,\n",
        "    #     nlayers=2,\n",
        "    #     epochs=20,\n",
        "    #     lr=0.001\n",
        "    # )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o7FUkvYGbw7c",
      "metadata": {
        "id": "o7FUkvYGbw7c"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Script to prepare names dataset for Word-Level Language Modeling\n",
        "Converts multiple text files with names into train/valid/test splits\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import random\n",
        "from typing import List, Tuple\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def read_names_from_files(data_dir: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Read all names from text files in the directory.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory containing .txt files with names\n",
        "\n",
        "    Returns:\n",
        "        List of all names\n",
        "    \"\"\"\n",
        "    all_names: List[str] = []\n",
        "\n",
        "    for filename in Path(data_dir).glob('*.txt'):\n",
        "        print(f\"Reading {filename.name}...\")\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            names: List[str] = [line.strip() for line in f if line.strip()]\n",
        "            all_names.extend(names)\n",
        "            print(f\"  Found {len(names)} names\")\n",
        "\n",
        "    print(f\"\\nTotal names: {len(all_names)}\")\n",
        "    return all_names\n",
        "\n",
        "\n",
        "def create_train_valid_test_splits(\n",
        "    names: List[str],\n",
        "    train_ratio: float = 0.8,\n",
        "    valid_ratio: float = 0.1,\n",
        "    test_ratio: float = 0.1,\n",
        "    seed: int = 42\n",
        ") -> Tuple[List[str], List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Split names into train, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "        names: List of names\n",
        "        train_ratio: Proportion for training (default 0.8)\n",
        "        valid_ratio: Proportion for validation (default 0.1)\n",
        "        test_ratio: Proportion for testing (default 0.1)\n",
        "        seed: Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (train_names, valid_names, test_names)\n",
        "    \"\"\"\n",
        "    assert abs(train_ratio + valid_ratio + test_ratio - 1.0) < 1e-6, \\\n",
        "        \"Ratios must sum to 1.0\"\n",
        "\n",
        "    random.seed(seed)\n",
        "    names_copy: List[str] = names.copy()\n",
        "    random.shuffle(names_copy)\n",
        "\n",
        "    n: int = len(names_copy)\n",
        "    train_end: int = int(n * train_ratio)\n",
        "    valid_end: int = train_end + int(n * valid_ratio)\n",
        "\n",
        "    train_names: List[str] = names_copy[:train_end]\n",
        "    valid_names: List[str] = names_copy[train_end:valid_end]\n",
        "    test_names: List[str] = names_copy[valid_end:]\n",
        "\n",
        "    print(f\"\\nSplit sizes:\")\n",
        "    print(f\"  Train: {len(train_names)} ({len(train_names)/n*100:.1f}%)\")\n",
        "    print(f\"  Valid: {len(valid_names)} ({len(valid_names)/n*100:.1f}%)\")\n",
        "    print(f\"  Test:  {len(test_names)} ({len(test_names)/n*100:.1f}%)\")\n",
        "\n",
        "    return train_names, valid_names, test_names\n",
        "\n",
        "\n",
        "def save_names_to_file(names: List[str], filename: str) -> None:\n",
        "    \"\"\"\n",
        "    Save names to file, one per line.\n",
        "\n",
        "    Args:\n",
        "        names: List of names\n",
        "        filename: Output filename\n",
        "    \"\"\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for name in names:\n",
        "            # Write each name as a separate \"sentence\"\n",
        "            # The model will add <eos> automatically\n",
        "            f.write(name + '\\n')\n",
        "    print(f\"Saved {len(names)} names to {filename}\")\n",
        "\n",
        "\n",
        "def prepare_names_dataset(\n",
        "    input_dir: str,\n",
        "    output_dir: str,\n",
        "    train_ratio: float = 0.8,\n",
        "    valid_ratio: float = 0.1,\n",
        "    test_ratio: float = 0.1,\n",
        "    seed: int = 42\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Main function to prepare names dataset.\n",
        "\n",
        "    Args:\n",
        "        input_dir: Directory containing raw name files\n",
        "        output_dir: Directory to save train/valid/test files\n",
        "        train_ratio: Proportion for training\n",
        "        valid_ratio: Proportion for validation\n",
        "        test_ratio: Proportion for testing\n",
        "        seed: Random seed\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"Preparing Names Dataset for Language Modeling\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Read all names\n",
        "    all_names: List[str] = read_names_from_files(input_dir)\n",
        "\n",
        "    # Split into train/valid/test\n",
        "    train_names, valid_names, test_names = create_train_valid_test_splits(\n",
        "        all_names, train_ratio, valid_ratio, test_ratio, seed\n",
        "    )\n",
        "\n",
        "    # Save splits\n",
        "    print(\"\\nSaving splits...\")\n",
        "    save_names_to_file(train_names, os.path.join(output_dir, 'train.txt'))\n",
        "    save_names_to_file(valid_names, os.path.join(output_dir, 'valid.txt'))\n",
        "    save_names_to_file(test_names, os.path.join(output_dir, 'test.txt'))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Dataset preparation complete!\")\n",
        "    print(f\"Output directory: {output_dir}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Print sample names\n",
        "    print(\"\\nSample names from train set:\")\n",
        "    for name in train_names[:10]:\n",
        "        print(f\"  {name}\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# ALTERNATIVE: Word-level format\n",
        "# ===============================\n",
        "\n",
        "def prepare_wordlevel_format(\n",
        "    input_dir: str,\n",
        "    output_dir: str,\n",
        "    train_ratio: float = 0.8,\n",
        "    valid_ratio: float = 0.1,\n",
        "    test_ratio: float = 0.1,\n",
        "    seed: int = 42\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Prepare dataset where each character is a 'word'.\n",
        "    This allows character-level language modeling using word LM code.\n",
        "\n",
        "    Args:\n",
        "        input_dir: Directory containing raw name files\n",
        "        output_dir: Directory to save train/valid/test files\n",
        "        train_ratio: Proportion for training\n",
        "        valid_ratio: Proportion for validation\n",
        "        test_ratio: Proportion for testing\n",
        "        seed: Random seed\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"Preparing Character-Level (as words) Dataset\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Read all names\n",
        "    all_names: List[str] = read_names_from_files(input_dir)\n",
        "\n",
        "    # Split\n",
        "    train_names, valid_names, test_names = create_train_valid_test_splits(\n",
        "        all_names, train_ratio, valid_ratio, test_ratio, seed\n",
        "    )\n",
        "\n",
        "    # Convert to character-level\n",
        "    def names_to_char_words(names: List[str], filename: str) -> None:\n",
        "        \"\"\"Convert names to space-separated characters.\"\"\"\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            for name in names:\n",
        "                # Each character becomes a \"word\"\n",
        "                char_sequence: str = ' '.join(list(name))\n",
        "                f.write(char_sequence + '\\n')\n",
        "        print(f\"Saved {len(names)} names (as char sequences) to {filename}\")\n",
        "\n",
        "    print(\"\\nSaving character-level splits...\")\n",
        "    names_to_char_words(train_names, os.path.join(output_dir, 'train.txt'))\n",
        "    names_to_char_words(valid_names, os.path.join(output_dir, 'valid.txt'))\n",
        "    names_to_char_words(test_names, os.path.join(output_dir, 'test.txt'))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Character-level dataset preparation complete!\")\n",
        "    print(f\"Output directory: {output_dir}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Print sample\n",
        "    print(\"\\nSample character sequences from train set:\")\n",
        "    for name in train_names[:5]:\n",
        "        char_seq: str = ' '.join(list(name))\n",
        "        print(f\"  {name} -> {char_seq}\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# EXAMPLE USAGE\n",
        "# ===============================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Example 1: Word-level (each name is a word)\n",
        "    # Best for: learning name distributions\n",
        "    prepare_names_dataset(\n",
        "        input_dir='data/names_raw',  # Your directory with German.txt, Russian.txt, etc.\n",
        "        output_dir='data/names_word',\n",
        "        train_ratio=0.8,\n",
        "        valid_ratio=0.1,\n",
        "        test_ratio=0.1\n",
        "    )\n",
        "\n",
        "    # Example 2: Character-level (each character is a word)\n",
        "    # Best for: generating new names character-by-character\n",
        "    prepare_wordlevel_format(\n",
        "        input_dir='data/names_raw',\n",
        "        output_dir='data/names_char',\n",
        "        train_ratio=0.8,\n",
        "        valid_ratio=0.1,\n",
        "        test_ratio=0.1\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"NEXT STEPS:\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"1. Use 'data/names_word' for name-level modeling\")\n",
        "    print(\"2. Use 'data/names_char' for character-level modeling\")\n",
        "    print(\"\\nTo train:\")\n",
        "    print(\"  python improved_word_lm.py --data data/names_word\")\n",
        "    print(\"\\nTo generate:\")\n",
        "    print(\"  python improved_word_lm.py --generate --checkpoint model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HMxRI_O3GcZl",
      "metadata": {
        "id": "HMxRI_O3GcZl"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Complete example with training, evaluation, and improvements\n",
        "for Word-Level Language Modeling on Names Dataset\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "from typing import List, Dict, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "\n",
        "# Assuming improved_word_lm.py is imported\n",
        "from improved_word_lm import (\n",
        "    Corpus, RNNModel, TransformerModel,\n",
        "    batchify, evaluate, train_epoch\n",
        ")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# EXPERIMENT 1: Compare Architectures\n",
        "# ===============================\n",
        "\n",
        "def compare_architectures(\n",
        "    data_path: str = './data/names_char',\n",
        "    epochs: int = 20,\n",
        "    device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        ") -> Dict[str, Dict]:\n",
        "    \"\"\"Compare different model architectures on names dataset.\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"EXPERIMENT 1: Comparing Model Architectures\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Load data\n",
        "    corpus: Corpus = Corpus(data_path)\n",
        "    ntokens: int = len(corpus.dictionary)\n",
        "\n",
        "    batch_size: int = 20\n",
        "    eval_batch_size: int = 10\n",
        "    bptt: int = 35\n",
        "\n",
        "    train_data: torch.Tensor = batchify(corpus.train, batch_size, device)\n",
        "    val_data: torch.Tensor = batchify(corpus.valid, eval_batch_size, device)\n",
        "    test_data: torch.Tensor = batchify(corpus.test, eval_batch_size, device)\n",
        "\n",
        "    # Define architectures to test\n",
        "    architectures: Dict[str, Dict] = {\n",
        "        'LSTM-Small': {\n",
        "            'model_type': 'LSTM',\n",
        "            'emsize': 128,\n",
        "            'nhid': 128,\n",
        "            'nlayers': 2,\n",
        "            'dropout': 0.2\n",
        "        },\n",
        "        'LSTM-Large': {\n",
        "            'model_type': 'LSTM',\n",
        "            'emsize': 256,\n",
        "            'nhid': 256,\n",
        "            'nlayers': 3,\n",
        "            'dropout': 0.3\n",
        "        },\n",
        "        'GRU': {\n",
        "            'model_type': 'GRU',\n",
        "            'emsize': 200,\n",
        "            'nhid': 200,\n",
        "            'nlayers': 2,\n",
        "            'dropout': 0.2\n",
        "        },\n",
        "        'Transformer': {\n",
        "            'model_type': 'Transformer',\n",
        "            'emsize': 200,\n",
        "            'nhid': 200,\n",
        "            'nlayers': 2,\n",
        "            'nhead': 2,\n",
        "            'dropout': 0.2\n",
        "        }\n",
        "    }\n",
        "\n",
        "    results: Dict[str, Dict] = {}\n",
        "\n",
        "    for name, config in architectures.items():\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Training {name}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        # Create model\n",
        "        is_transformer: bool = config['model_type'] == 'Transformer'\n",
        "\n",
        "        if is_transformer:\n",
        "            model: nn.Module = TransformerModel(\n",
        "                ntokens,\n",
        "                config['emsize'],\n",
        "                config['nhead'],\n",
        "                config['nhid'],\n",
        "                config['nlayers'],\n",
        "                config['dropout']\n",
        "            ).to(device)\n",
        "        else:\n",
        "            model = RNNModel(\n",
        "                config['model_type'],\n",
        "                ntokens,\n",
        "                config['emsize'],\n",
        "                config['nhid'],\n",
        "                config['nlayers'],\n",
        "                config['dropout'],\n",
        "                tie_weights=False\n",
        "            ).to(device)\n",
        "\n",
        "        # Optimizer with weight decay (from Transformer paper)\n",
        "        criterion: nn.NLLLoss = nn.NLLLoss()\n",
        "        optimizer: optim.Adam = optim.Adam(\n",
        "            model.parameters(),\n",
        "            lr=0.001,\n",
        "            weight_decay=1e-5\n",
        "        )\n",
        "\n",
        "        # Training\n",
        "        train_losses: List[float] = []\n",
        "        val_losses: List[float] = []\n",
        "        best_val_loss: float = float('inf')\n",
        "\n",
        "        start_time: float = time.time()\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            train_epoch(\n",
        "                model, train_data, criterion, optimizer, epoch,\n",
        "                bptt, ntokens, batch_size, 0.25, 100, is_transformer\n",
        "            )\n",
        "\n",
        "            val_loss: float = evaluate(\n",
        "                model, val_data, criterion, bptt, ntokens,\n",
        "                eval_batch_size, is_transformer\n",
        "            )\n",
        "\n",
        "            train_losses.append(criterion(\n",
        "                model(train_data[:100])[0] if is_transformer else model(train_data[:100], model.init_hidden(batch_size))[0],\n",
        "                train_data[1:101].view(-1)\n",
        "            ).item())\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "            print(f'Epoch {epoch:3d} | val loss {val_loss:5.2f} | val ppl {math.exp(val_loss):8.2f}')\n",
        "\n",
        "            best_val_loss = min(best_val_loss, val_loss)\n",
        "\n",
        "        training_time: float = time.time() - start_time\n",
        "\n",
        "        # Test\n",
        "        test_loss: float = evaluate(\n",
        "            model, test_data, criterion, bptt, ntokens,\n",
        "            eval_batch_size, is_transformer\n",
        "        )\n",
        "\n",
        "        results[name] = {\n",
        "            'model': model,\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'best_val_loss': best_val_loss,\n",
        "            'test_loss': test_loss,\n",
        "            'test_ppl': math.exp(test_loss),\n",
        "            'training_time': training_time,\n",
        "            'config': config\n",
        "        }\n",
        "\n",
        "        print(f\"\\n{name} Results:\")\n",
        "        print(f\"  Best Val Loss: {best_val_loss:.4f}\")\n",
        "        print(f\"  Test Loss: {test_loss:.4f}\")\n",
        "        print(f\"  Test PPL: {math.exp(test_loss):.2f}\")\n",
        "        print(f\"  Training Time: {training_time:.2f}s\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# EXPERIMENT 2: Hyperparameter Tuning\n",
        "# ===============================\n",
        "\n",
        "def tune_hyperparameters(\n",
        "    data_path: str = './data/names_char',\n",
        "    base_config: Dict = None\n",
        ") -> Dict[str, List]:\n",
        "    \"\"\"Tune hyperparameters for best model.\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"EXPERIMENT 2: Hyperparameter Tuning\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    if base_config is None:\n",
        "        base_config = {\n",
        "            'model_type': 'LSTM',\n",
        "            'emsize': 200,\n",
        "            'nhid': 200,\n",
        "            'nlayers': 2,\n",
        "            'dropout': 0.2,\n",
        "            'lr': 0.001,\n",
        "            'batch_size': 20,\n",
        "            'epochs': 15\n",
        "        }\n",
        "\n",
        "    # Parameters to tune\n",
        "    param_grid: Dict[str, List] = {\n",
        "        'lr': [0.0001, 0.001, 0.01],\n",
        "        'dropout': [0.1, 0.2, 0.3],\n",
        "        'nhid': [128, 200, 256],\n",
        "        'nlayers': [2, 3, 4]\n",
        "    }\n",
        "\n",
        "    results: Dict[str, List] = {}\n",
        "\n",
        "    for param_name, param_values in param_grid.items():\n",
        "        print(f\"\\nTuning {param_name}...\")\n",
        "        results[param_name] = []\n",
        "\n",
        "        for value in param_values:\n",
        "            config: Dict = base_config.copy()\n",
        "            config[param_name] = value\n",
        "\n",
        "            print(f\"  Testing {param_name}={value}\")\n",
        "\n",
        "            # Train and evaluate (simplified version)\n",
        "            # In practice, you'd call the full training function\n",
        "            test_ppl: float = train_and_evaluate(data_path, config)\n",
        "\n",
        "            results[param_name].append({\n",
        "                'value': value,\n",
        "                'test_ppl': test_ppl\n",
        "            })\n",
        "\n",
        "            print(f\"    Test PPL: {test_ppl:.2f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def train_and_evaluate(data_path: str, config: Dict) -> float:\n",
        "    \"\"\"Simplified train and evaluate function.\"\"\"\n",
        "    # Placeholder - implement actual training\n",
        "    # This is a simplified version\n",
        "    return 10.0  # Return test perplexity\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# EXPERIMENT 3: Learning Curves\n",
        "# ===============================\n",
        "\n",
        "def analyze_learning_curves(results: Dict[str, Dict]) -> None:\n",
        "    \"\"\"Plot and analyze learning curves.\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"EXPERIMENT 3: Learning Curve Analysis\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    for idx, (name, result) in enumerate(results.items()):\n",
        "        row: int = idx // 2\n",
        "        col: int = idx % 2\n",
        "\n",
        "        ax = axes[row, col]\n",
        "\n",
        "        epochs: List[int] = list(range(1, len(result['train_losses']) + 1))\n",
        "        ax.plot(epochs, result['train_losses'], label='Train Loss', marker='o')\n",
        "        ax.plot(epochs, result['val_losses'], label='Val Loss', marker='s')\n",
        "\n",
        "        ax.set_xlabel('Epoch')\n",
        "        ax.set_ylabel('Loss')\n",
        "        ax.set_title(f'{name} Learning Curves')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('learning_curves.png', dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    print(\"Learning curves saved to 'learning_curves.png'\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# EXPERIMENT 4: Generate & Analyze Names\n",
        "# ===============================\n",
        "\n",
        "def generate_and_analyze_names(\n",
        "    model: nn.Module,\n",
        "    corpus: Corpus,\n",
        "    n_samples: int = 100,\n",
        "    temperature: float = 1.0,\n",
        "    device: torch.device = torch.device('cpu')\n",
        ") -> List[str]:\n",
        "    \"\"\"Generate names and analyze quality.\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"EXPERIMENT 4: Name Generation & Analysis\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    model.eval()\n",
        "    ntokens: int = len(corpus.dictionary)\n",
        "    is_transformer: bool = hasattr(model, 'model_type')\n",
        "\n",
        "    generated_names: List[str] = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(n_samples):\n",
        "            if not is_transformer:\n",
        "                hidden = model.init_hidden(1)\n",
        "\n",
        "            # Start with random character\n",
        "            input_tensor: torch.Tensor = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "            name_chars: List[str] = []\n",
        "\n",
        "            max_len: int = 20\n",
        "            for _ in range(max_len):\n",
        "                if is_transformer:\n",
        "                    output: torch.Tensor = model(input_tensor, False)\n",
        "                    word_weights: torch.Tensor = output[-1].squeeze().div(temperature).exp().cpu()\n",
        "                else:\n",
        "                    output, hidden = model(input_tensor, hidden)\n",
        "                    word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "\n",
        "                word_idx: int = torch.multinomial(word_weights, 1)[0].item()\n",
        "                word: str = corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "                if word == '<eos>':\n",
        "                    break\n",
        "\n",
        "                name_chars.append(word)\n",
        "                input_tensor.fill_(word_idx)\n",
        "\n",
        "            generated_name: str = ''.join(name_chars)\n",
        "            if generated_name:  # Only add non-empty names\n",
        "                generated_names.append(generated_name)\n",
        "\n",
        "    # Analysis\n",
        "    print(f\"\\nGenerated {len(generated_names)} names\")\n",
        "    print(f\"Sample names:\")\n",
        "    for name in generated_names[:20]:\n",
        "        print(f\"  {name}\")\n",
        "\n",
        "    # Statistics\n",
        "    avg_length: float = sum(len(name) for name in generated_names) / len(generated_names)\n",
        "    unique_names: int = len(set(generated_names))\n",
        "\n",
        "    print(f\"\\nStatistics:\")\n",
        "    print(f\"  Average length: {avg_length:.2f}\")\n",
        "    print(f\"  Unique names: {unique_names} ({unique_names/len(generated_names)*100:.1f}%)\")\n",
        "\n",
        "    return generated_names\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# EXPERIMENT 5: Temperature Sampling\n",
        "# ===============================\n",
        "\n",
        "def experiment_temperature(\n",
        "    model: nn.Module,\n",
        "    corpus: Corpus,\n",
        "    temperatures: List[float] = [0.5, 0.8, 1.0, 1.2, 1.5],\n",
        "    n_samples: int = 10,\n",
        "    device: torch.device = torch.device('cpu')\n",
        ") -> None:\n",
        "    \"\"\"Experiment with different temperature values.\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"EXPERIMENT 5: Temperature Sampling\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for temp in temperatures:\n",
        "        print(f\"\\nTemperature = {temp}\")\n",
        "        names: List[str] = generate_and_analyze_names(\n",
        "            model, corpus, n_samples, temp, device\n",
        "        )\n",
        "        print(f\"  Sample: {', '.join(names[:5])}\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# MAIN EXECUTION\n",
        "# ===============================\n",
        "\n",
        "def run_all_experiments(data_path: str = './data/names_char') -> None:\n",
        "    \"\"\"Run all experiments.\"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"COMPLETE EXPERIMENTAL PIPELINE\")\n",
        "    print(\"Word-Level Language Modeling on Names Dataset\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Experiment 1: Compare architectures\n",
        "    results: Dict = compare_architectures(data_path, epochs=20, device=device)\n",
        "\n",
        "    # Experiment 3: Analyze learning curves\n",
        "    analyze_learning_curves(results)\n",
        "\n",
        "    # Find best model\n",
        "    best_model_name: str = min(\n",
        "        results.keys(),\n",
        "        key=lambda x: results[x]['test_loss']\n",
        "    )\n",
        "    best_model: nn.Module = results[best_model_name]['model']\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Best Model: {best_model_name}\")\n",
        "    print(f\"Test PPL: {results[best_model_name]['test_ppl']:.2f}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Load corpus for generation\n",
        "    corpus: Corpus = Corpus(data_path)\n",
        "\n",
        "    # Experiment 4: Generate names\n",
        "    generated_names: List[str] = generate_and_analyze_names(\n",
        "        best_model, corpus, n_samples=100, device=device\n",
        "    )\n",
        "\n",
        "    # Experiment 5: Temperature sampling\n",
        "    experiment_temperature(best_model, corpus, device=device)\n",
        "\n",
        "    # Save results\n",
        "    with open('experiment_results.txt', 'w') as f:\n",
        "        f.write(\"EXPERIMENTAL RESULTS\\n\")\n",
        "        f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "        for name, result in results.items():\n",
        "            f.write(f\"{name}:\\n\")\n",
        "            f.write(f\"  Test Loss: {result['test_loss']:.4f}\\n\")\n",
        "            f.write(f\"  Test PPL: {result['test_ppl']:.2f}\\n\")\n",
        "            f.write(f\"  Training Time: {result['training_time']:.2f}s\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "        f.write(\"\\nGenerated Names:\\n\")\n",
        "        for name in generated_names[:50]:\n",
        "            f.write(f\"  {name}\\n\")\n",
        "\n",
        "    print(\"\\nResults saved to 'experiment_results.txt'\")\n",
        "    print(\"\\nExperiments complete!\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Run all experiments\n",
        "    run_all_experiments(data_path='./data/names_char')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}