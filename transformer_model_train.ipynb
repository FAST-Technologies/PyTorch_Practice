{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TRAIN AND LEARN OF word_language_model"
      ],
      "metadata": {
        "id": "SsK88NLWSwjo"
      },
      "id": "SsK88NLWSwjo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model code"
      ],
      "metadata": {
        "id": "fFwuP5dYS2Tx"
      },
      "id": "fFwuP5dYS2Tx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading files from Google Disk"
      ],
      "metadata": {
        "id": "pJHzuwzIThUp"
      },
      "id": "pJHzuwzIThUp"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxeRfC-STmyY",
        "outputId": "dfafb794-37af-492d-cadc-dcc19b6d097a"
      },
      "id": "WxeRfC-STmyY",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "import os\n",
        "from io import open\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from typing import Dict, List, Union, Tuple, Optional"
      ],
      "metadata": {
        "id": "w5z0Vi8DVzgk"
      },
      "id": "w5z0Vi8DVzgk",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data.py"
      ],
      "metadata": {
        "id": "vHTBXIgBS5Xa"
      },
      "id": "vHTBXIgBS5Xa"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self) -> None:\n",
        "        self.word2idx: Dict[str, int] = {}\n",
        "        self.idx2word: List[str] = []\n",
        "        self.word2count: Counter = Counter()\n",
        "\n",
        "    def add_word(self, word: str) -> int:\n",
        "        self.word2count[word] += 1\n",
        "        return self.word2idx.get(word, -1)\n",
        "\n",
        "    def finalize(self, min_freq: int = 5) -> None:\n",
        "        for word, count in self.word2count.items():\n",
        "            if count >= min_freq and word not in self.word2idx:\n",
        "                self.idx2word.append(word)\n",
        "                self.word2idx[word] = len(self.idx2word) - 1\n",
        "        self.word2idx['<unk>'] = len(self.idx2word)\n",
        "        self.idx2word.append('<unk>')\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.idx2word)\n",
        "\n",
        "class Corpus(object):\n",
        "  \"\"\"Corpus class for loading and tokenizing text data.\"\"\"\n",
        "  def __init__(self, path: str, min_freq: int = 5) -> None:\n",
        "      self.dictionary: Dictionary = Dictionary()\n",
        "      self.min_freq: int = min_freq\n",
        "      self.train: torch.Tensor = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "      self.valid: torch.Tensor = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "      self.test: torch.Tensor = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "  def tokenize(self, path: str) -> torch.Tensor:\n",
        "      \"\"\"Tokenizes a text file.\"\"\"\n",
        "      assert os.path.exists(path)\n",
        "      # Add words to the dictionary\n",
        "      with open(path, 'r', encoding=\"utf8\") as f:\n",
        "          for line in f:\n",
        "              words: List[str] = line.split() + ['<eos>']\n",
        "              for word in words:\n",
        "                  self.dictionary.add_word(word)\n",
        "      self.dictionary.finalize(self.min_freq)\n",
        "      # Tokenize file content\n",
        "      with open(path, 'r', encoding=\"utf8\") as f:\n",
        "          idss: List[torch.Tensor] = []\n",
        "          for line in f:\n",
        "              words: List[str] = line.split() + ['<eos>']\n",
        "              ids: List[int] = []\n",
        "              for word in words:\n",
        "                  idx = self.dictionary.word2idx.get(word, self.dictionary.word2idx['<unk>'])\n",
        "                  ids.append(idx)\n",
        "              idss.append(torch.tensor(ids, dtype=torch.int64))\n",
        "          ids_tensor: torch.Tensor = torch.cat(idss)\n",
        "\n",
        "      return ids_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model.py"
      ],
      "metadata": {
        "id": "xkMHPnmLWEKE"
      },
      "id": "xkMHPnmLWEKE"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# MODEL ARCHITECTURES (model.py)\n",
        "# ===============================\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    \"\"\"RNN-based language model (LSTM/GRU/RNN).\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        rnn_type: str,\n",
        "        ntoken: int,\n",
        "        ninp: int,\n",
        "        nhid: int,\n",
        "        nlayers: int,\n",
        "        dropout: float = 0.5,\n",
        "        tie_weights: bool = False\n",
        "    ) -> None:\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.ntoken: int = ntoken\n",
        "        self.rnn_type: str = rnn_type\n",
        "        self.nhid: int = nhid\n",
        "        self.nlayers: int = nlayers\n",
        "\n",
        "        self.drop: nn.Dropout = nn.Dropout(dropout)\n",
        "        self.encoder: nn.Embedding = nn.Embedding(ntoken, ninp)\n",
        "\n",
        "        if rnn_type in ['LSTM', 'GRU']:\n",
        "            self.rnn: nn.Module = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
        "        else:\n",
        "            try:\n",
        "                nonlinearity: str = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
        "            except KeyError as e:\n",
        "                raise ValueError(\n",
        "                    \"Invalid option for `--model`. \"\n",
        "                    \"Options are ['LSTM', 'GRU', 'RNN_TANH', 'RNN_RELU']\"\n",
        "                ) from e\n",
        "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
        "\n",
        "        self.decoder: nn.Linear = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        # Tie weights\n",
        "        if tie_weights:\n",
        "            if nhid != ninp:\n",
        "                raise ValueError('When using tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        \"\"\"Initialize weights.\"\"\"\n",
        "        initrange: float = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, input: torch.Tensor, hidden: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        emb: torch.Tensor = self.drop(self.encoder(input))\n",
        "        output: torch.Tensor\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded: torch.Tensor = self.decoder(output)\n",
        "        decoded = decoded.view(-1, self.ntoken)\n",
        "        return F.log_softmax(decoded, dim=1), hidden\n",
        "\n",
        "    def init_hidden(self, bsz: int) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\"Initialize hidden state.\"\"\"\n",
        "        weight: torch.Tensor = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (\n",
        "                weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
        "            )\n",
        "        else:\n",
        "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
        "\n",
        "# Temporarily leave PositionalEncoding module here. Will be moved somewhere else.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    r\"\"\"Inject some information about the relative or absolute position of the tokens in the sequence.\n",
        "        The positional encodings have the same dimension as the embeddings, so that the two can be summed.\n",
        "        Here, we use sine and cosine functions of different frequencies.\n",
        "    .. math:\n",
        "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
        "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "        \\text{where pos is the word position and i is the embed idx)\n",
        "    Args:\n",
        "        d_model: the embed dim (required).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        max_len: the max. length of the incoming sequence (default=5000).\n",
        "    Examples:\n",
        "        >>> pos_encoder = PositionalEncoding(d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"Positional encoding for Transformer.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000) -> None:\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout: nn.Dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe: torch.Tensor = torch.zeros(max_len, d_model)\n",
        "        position: torch.Tensor = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term: torch.Tensor = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        r\"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            x: the sequence fed to the positional encoder model (required).\n",
        "        Shape:\n",
        "            x: [sequence length, batch size, embed dim]\n",
        "            output: [sequence length, batch size, embed dim]\n",
        "        Examples:\n",
        "            >>> output = pos_encoder(x)\n",
        "        \"\"\"\n",
        "\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerModel(nn.Transformer):\n",
        "    \"\"\"Transformer-based language model.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        ntoken: int,\n",
        "        ninp: int,\n",
        "        nhead: int,\n",
        "        nhid: int,\n",
        "        nlayers: int,\n",
        "        dropout: float = 0.5\n",
        "    ) -> None:\n",
        "        super(TransformerModel, self).__init__(\n",
        "            d_model=ninp,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=nhid,\n",
        "            num_encoder_layers=nlayers\n",
        "        )\n",
        "        self.model_type: str = 'Transformer'\n",
        "        self.src_mask: Optional[torch.Tensor] = None\n",
        "        self.pos_encoder: PositionalEncoding = PositionalEncoding(ninp, dropout)\n",
        "\n",
        "        self.input_emb: nn.Embedding = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp: int = ninp\n",
        "        self.decoder: nn.Linear = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz: int) -> torch.Tensor:\n",
        "        \"\"\"Generate mask for causal attention.\"\"\"\n",
        "        return torch.log(torch.tril(torch.ones(sz, sz)))\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        \"\"\"Initialize weights.\"\"\"\n",
        "        initrange: float = 0.1\n",
        "        nn.init.uniform_(self.input_emb.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.bias)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, src: torch.Tensor, has_mask: bool = True) -> torch.Tensor:\n",
        "        if has_mask:\n",
        "            device: torch.device = src.device\n",
        "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "                mask: torch.Tensor = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "                self.src_mask = mask\n",
        "        else:\n",
        "            self.src_mask = None\n",
        "\n",
        "        src = self.input_emb(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output: torch.Tensor = self.encoder(src, mask=self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return F.log_softmax(output, dim=-1)"
      ],
      "metadata": {
        "id": "9BXrfBWHWKGj"
      },
      "id": "9BXrfBWHWKGj",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label smoothing loss\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, smoothing: float = 0.0) -> None:\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.smoothing: float = smoothing\n",
        "        self.confidence: float = 1.0 - smoothing\n",
        "\n",
        "    def forward(self, output: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        log_probs: torch.Tensor = output\n",
        "        n_classes: int = log_probs.size(-1)\n",
        "        with torch.no_grad():\n",
        "            true_dist: torch.Tensor = torch.zeros_like(log_probs)\n",
        "            true_dist.fill_(self.smoothing / (n_classes - 1))\n",
        "            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * log_probs, dim=-1))"
      ],
      "metadata": {
        "id": "xY5KPVooRtAt"
      },
      "id": "xY5KPVooRtAt",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test train on original data"
      ],
      "metadata": {
        "id": "4OZuV_gLZxBg"
      },
      "id": "4OZuV_gLZxBg"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# TRAINING & EVALUATION (main.py)\n",
        "# ===============================\n",
        "\n",
        "def get_lr(step: float, d_model: float, warmup_steps: int) -> float:\n",
        "    lr: float = d_model ** -0.5 * min(step ** -0.5, step * warmup_steps ** -1.5)\n",
        "    return lr\n",
        "\n",
        "def batchify(data: torch.Tensor, bsz: int, device: torch.device) -> torch.Tensor:\n",
        "    \"\"\"Divide data into batches.\"\"\"\n",
        "    nbatch: int = data.size(0) // bsz\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "\n",
        "def get_batch(source: torch.Tensor, i: int, bptt: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Get a batch of data.\"\"\"\n",
        "    seq_len: int = min(bptt, len(source) - 1 - i)\n",
        "    data: torch.Tensor = source[i:i+seq_len]\n",
        "    target: torch.Tensor = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def repackage_hidden(h: Union[torch.Tensor, Tuple]) -> Union[torch.Tensor, Tuple]:\n",
        "    \"\"\"Detach hidden state from history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "def top_k_sampling(logits: torch.Tensor, k: int, temperature: float = 1.0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    values, indices = torch.topk(logits, k)\n",
        "    values = values.div(temperature).exp()\n",
        "    values = values / values.sum()\n",
        "    return torch.multinomial(values, 1), indices\n",
        "\n",
        "def evaluate(\n",
        "    model: nn.Module,\n",
        "    data_source: torch.Tensor,\n",
        "    criterion: nn.Module,\n",
        "    bptt: int,\n",
        "    ntokens: int,\n",
        "    eval_batch_size: int,\n",
        "    is_transformer: bool\n",
        ") -> float:\n",
        "    \"\"\"Evaluate the model.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss: float = 0.0\n",
        "    if not is_transformer:\n",
        "        hidden = model.init_hidden(eval_batch_size)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data: torch.Tensor\n",
        "            targets: torch.Tensor\n",
        "            data, targets = get_batch(data_source, i, bptt)\n",
        "\n",
        "            if is_transformer:\n",
        "                output: torch.Tensor = model(data)\n",
        "                output = output.view(-1, ntokens)\n",
        "            else:\n",
        "                output, hidden = model(data, hidden)\n",
        "                hidden = repackage_hidden(hidden)\n",
        "\n",
        "            total_loss += len(data) * criterion(output, targets).item()\n",
        "\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train_epoch(\n",
        "    model: nn.Module,\n",
        "    train_data: torch.Tensor,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    epoch: int,\n",
        "    bptt: int,\n",
        "    ntokens: int,\n",
        "    batch_size: int,\n",
        "    clip: float,\n",
        "    log_interval: int,\n",
        "    is_transformer: bool,\n",
        "    use_optimizer: bool = True,\n",
        "    use_warmup: bool = False,\n",
        "    step: int = 0,\n",
        "    d_model: int = 512,\n",
        "    warmup_steps: int = 4000,\n",
        "    dry_run: bool = False\n",
        ") -> int:\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    total_loss: float = 0.0\n",
        "    start_time: float = time.time()\n",
        "\n",
        "    if not is_transformer:\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data: torch.Tensor\n",
        "        targets: torch.Tensor\n",
        "        data, targets = get_batch(train_data, i, bptt)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if is_transformer:\n",
        "            output: torch.Tensor = model(data)\n",
        "            output = output.view(-1, ntokens)\n",
        "        else:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            output, hidden = model(data, hidden)\n",
        "\n",
        "        loss: torch.Tensor = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        if use_warmup:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = get_lr(step + 1, d_model, warmup_steps)\n",
        "        if use_optimizer:\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            for p in model.parameters():\n",
        "                p.data.add_(p.grad, alpha=-optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        step += 1\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss: float = total_loss / log_interval\n",
        "            elapsed: float = time.time() - start_time\n",
        "            print(\n",
        "                f'| epoch {epoch:3d} | {batch:5d}/{len(train_data) // bptt:5d} batches | '\n",
        "                f'lr {optimizer.param_groups[0][\"lr\"]:02.6f} | ms/batch {elapsed * 1000 / log_interval:5.2f} | '\n",
        "                f'loss {cur_loss:5.2f} | ppl {math.exp(cur_loss):8.2f}'\n",
        "            )\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "        if dry_run:\n",
        "            break\n",
        "    return step\n",
        "\n",
        "# model_type: str = 'Transformer',\n",
        "#     data_path: str = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "#     emsize: int = 512,\n",
        "#     nhid: int = 2048,\n",
        "#     nlayers: int = 6,\n",
        "#     lr: float = 0.0001,\n",
        "#     clip: float = 0.25,\n",
        "#     epochs: int = 60,\n",
        "#     batch_size: int = 32,\n",
        "#     bptt: int = 50,\n",
        "#     dropout: float = 0.1,\n",
        "#     tied: bool = False,\n",
        "#     nhead: int = 8,\n",
        "#     log_interval: int = 200,\n",
        "#     save_path: str = 'model_1.pt',\n",
        "#     onnx_export: str = '',\n",
        "#     dry_run: bool = False,\n",
        "#     accel: bool = True,\n",
        "#     use_optimizer: bool = True,\n",
        "#     optimizer: Optional[optim.Optimizer] = None,\n",
        "#     criterion: Optional[nn.Module] = None,\n",
        "#     use_label_smoothing: bool = True,\n",
        "#     label_smoothing: float = 0.1,\n",
        "#     use_warmup: bool = True,\n",
        "#     warmup_steps: int = 4000,\n",
        "#     min_freq: int = 5\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model_type: str = 'LSTM', # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "    data_path: str = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize: int = 200,\n",
        "    nhid: int = 200,\n",
        "    nlayers: int = 2,\n",
        "    lr: float = 0.001,\n",
        "    clip: float = 0.25,\n",
        "    epochs: int = 40,\n",
        "    batch_size: int = 20,\n",
        "    bptt: int = 35,\n",
        "    dropout: float = 0.2,\n",
        "    tied: bool = False,\n",
        "    nhead: int = 2,\n",
        "    log_interval: int = 200,\n",
        "    save_path: str = 'model.pt',\n",
        "    onnx_export: str = '',\n",
        "    dry_run: bool = False,\n",
        "    accel: bool = True,\n",
        "    use_optimizer: bool = True,\n",
        "    optimizer: Optional[optim.Optimizer] = None,\n",
        "    criterion: Optional[nn.Module] = None,\n",
        "    use_label_smoothing: bool = False,\n",
        "    label_smoothing: float = 0.1,\n",
        "    use_warmup: bool = False,\n",
        "    warmup_steps: int = 4000,\n",
        "    min_freq: int = 5\n",
        ") -> None:\n",
        "    \"\"\"Main training function.\"\"\"\n",
        "\n",
        "    if data_path == '/content/drive/MyDrive/data_word_train/wikitext-2':\n",
        "      if not os.path.exists(data_path):\n",
        "          print(\"Downloading Wikitext-2 dataset...\")\n",
        "          !wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip -P /content/\n",
        "          !unzip /content/wikitext-2-v1.zip -d /content/data_word_train/\n",
        "          !mkdir -p /content/drive/MyDrive/data_word_train/\n",
        "          !mv /content/data_word_train/wikitext-2 /content/drive/MyDrive/data_word_train/\n",
        "          print(\"Wikitext-2 dataset moved to Google Drive\")\n",
        "\n",
        "    # Set device\n",
        "    device: torch.device = torch.device('cuda' if accel and torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load data\n",
        "    corpus: Corpus = Corpus(data_path, min_freq=min_freq)\n",
        "    print(f\"Vocabulary size: {len(corpus.dictionary)}\")\n",
        "\n",
        "    eval_batch_size: int = 10\n",
        "    train_data: torch.Tensor = batchify(corpus.train, batch_size, device)\n",
        "    val_data: torch.Tensor = batchify(corpus.valid, eval_batch_size, device)\n",
        "    test_data: torch.Tensor = batchify(corpus.test, eval_batch_size, device)\n",
        "\n",
        "    # Build model\n",
        "    ntokens: int = len(corpus.dictionary)\n",
        "    is_transformer: bool = model_type == 'Transformer'\n",
        "\n",
        "    if is_transformer:\n",
        "        model: nn.Module = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
        "    else:\n",
        "        model = RNNModel(model_type, ntokens, emsize, nhid, nlayers, dropout, tied).to(device)\n",
        "\n",
        "    # Loss and optimizer (Adam with weight_decay as in Transformer paper)\n",
        "    # optimizer: optim.Adam = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "    criterion: nn.Module = criterion if criterion is not None else (LabelSmoothingLoss(smoothing=label_smoothing) if use_label_smoothing else nn.NLLLoss())\n",
        "    optimizer: optim.Optimizer = optimizer if optimizer is not None else optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    # criterion = LabelSmoothingLoss(smoothing=args.label_smoothing)\n",
        "    # optimizer = optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "    scheduler: optim.lr_scheduler.ReduceLROnPlateau = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=2\n",
        "    ) if not use_warmup else None\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss: Optional[float] = None\n",
        "    global_step: int = 0\n",
        "\n",
        "    try:\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            epoch_start_time: float = time.time()\n",
        "\n",
        "            global_step = train_epoch(\n",
        "                model, train_data, criterion, optimizer, epoch,\n",
        "                bptt, ntokens, batch_size, clip, log_interval, is_transformer,\n",
        "                use_optimizer, use_warmup, global_step, emsize, warmup_steps\n",
        "            )\n",
        "\n",
        "            val_loss: float = evaluate(\n",
        "                model, val_data, criterion, bptt, ntokens,\n",
        "                eval_batch_size, is_transformer\n",
        "            )\n",
        "\n",
        "            print('-' * 89)\n",
        "            print(\n",
        "                f'| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s | '\n",
        "                f'valid loss {val_loss:5.2f} | valid ppl {math.exp(val_loss):8.2f}'\n",
        "            )\n",
        "            print('-' * 89)\n",
        "\n",
        "            # Save best model\n",
        "            if not best_val_loss or val_loss < best_val_loss:\n",
        "                with open(save_path, 'wb') as f:\n",
        "                    torch.save(model, f)\n",
        "                best_val_loss = val_loss\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            # lr /= 4.0\n",
        "            if use_warmup:\n",
        "                print(f\"Current learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "            else:\n",
        "                scheduler.step(val_loss)\n",
        "                print(f\"Current learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print('-' * 89)\n",
        "        print('Exiting from training early')\n",
        "    safe_globals: List = [\n",
        "        TransformerModel, PositionalEncoding,\n",
        "        torch.nn.modules.dropout.Dropout, torch.nn.modules.linear.Linear,\n",
        "        torch.nn.modules.transformer.TransformerEncoder,\n",
        "        torch.nn.modules.transformer.TransformerEncoderLayer,\n",
        "        torch.nn.modules.activation.MultiheadAttention,\n",
        "        torch.nn.modules.linear.NonDynamicallyQuantizableLinear,\n",
        "        torch.nn.modules.normalization.LayerNorm, torch.nn.functional.relu\n",
        "    ]\n",
        "    with torch.serialization.safe_globals(safe_globals):\n",
        "        with open(save_path, 'rb') as f:\n",
        "            model = torch.load(f, map_location=device)\n",
        "    test_loss: float = evaluate(\n",
        "        model, test_data, criterion, bptt, ntokens, eval_batch_size, is_transformer\n",
        "    )\n",
        "    print('=' * 89)\n",
        "    print(\n",
        "        f'| End of training | test loss {test_loss:5.2f} | '\n",
        "        f'test ppl {math.exp(test_loss):8.2f}'\n",
        "    )\n",
        "    print('=' * 89)\n"
      ],
      "metadata": {
        "id": "zb-MESoeRJq0"
      },
      "execution_count": 48,
      "outputs": [],
      "id": "zb-MESoeRJq0"
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# TEXT GENERATION (generate.py)\n",
        "# ===============================\n",
        "\n",
        "def generate_text(\n",
        "    checkpoint: str = 'model.pt',\n",
        "    data_path: str = '/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf: str = 'generated.txt',\n",
        "    words: int = 1000,\n",
        "    temperature: float = 1.0,\n",
        "    top_k: int = 40,\n",
        "    seed: int = 1111,\n",
        "    log_interval: int = 100,\n",
        "    accel: bool = True,\n",
        "    min_freq: int = 5,\n",
        "    use_top_k: bool = False\n",
        ") -> None:\n",
        "    \"\"\"Generate text from trained model.\"\"\"\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    device: torch.device = torch.device('cuda' if accel and torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Load model\n",
        "    safe_globals: List = [\n",
        "        TransformerModel, PositionalEncoding,\n",
        "        torch.nn.modules.dropout.Dropout, torch.nn.modules.linear.Linear,\n",
        "        torch.nn.modules.transformer.TransformerEncoder,\n",
        "        torch.nn.modules.transformer.TransformerEncoderLayer,\n",
        "        torch.nn.modules.activation.MultiheadAttention,\n",
        "        torch.nn.modules.linear.NonDynamicallyQuantizableLinear,\n",
        "        torch.nn.modules.normalization.LayerNorm, torch.nn.functional.relu\n",
        "    ]\n",
        "    with torch.serialization.safe_globals(safe_globals):\n",
        "        with open(checkpoint, 'rb') as f:\n",
        "            model: nn.Module = torch.load(f, map_location=device)\n",
        "    model.eval()\n",
        "\n",
        "    # Load corpus\n",
        "    corpus: Corpus = Corpus(data_path, min_freq=min_freq)\n",
        "    print(f\"Vocabulary size: {len(corpus.dictionary)}\")\n",
        "    ntokens: int = len(corpus.dictionary)\n",
        "\n",
        "    is_transformer: bool = hasattr(model, 'model_type') and model.model_type == 'Transformer'\n",
        "    if not is_transformer:\n",
        "        hidden = model.init_hidden(1)\n",
        "\n",
        "    input: torch.Tensor = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "    with open(outf, 'w') as outfile:\n",
        "        with torch.no_grad():\n",
        "            for i in range(words):\n",
        "                if is_transformer:\n",
        "                    output: torch.Tensor = model(input, False)\n",
        "                    if use_top_k:\n",
        "                        word_weights: torch.Tensor = output[-1].squeeze().cpu()\n",
        "                        prob, top_indices = top_k_sampling(word_weights, top_k, temperature)\n",
        "                        word_idx: int = top_indices[prob.item()].item()\n",
        "                    else:\n",
        "                        word_weights: torch.Tensor = output[-1].squeeze().div(temperature).exp().cpu()\n",
        "                        word_idx: int = torch.multinomial(word_weights, 1)[0].item()\n",
        "                    word_tensor: torch.Tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
        "                    input = torch.cat([input, word_tensor], 0)\n",
        "\n",
        "                else:\n",
        "                    output, hidden = model(input, hidden)\n",
        "                    word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "                    word_idx = torch.multinomial(word_weights, 1)[0].item()\n",
        "                    input.fill_(word_idx)\n",
        "\n",
        "                word: str = corpus.dictionary.idx2word[word_idx]\n",
        "                if word == '@-@':\n",
        "                  word = ' '\n",
        "                outfile.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "                if i % log_interval == 0:\n",
        "                    print(f'| Generated {i}/{words} words')"
      ],
      "metadata": {
        "id": "DOCVuP6LUpRS"
      },
      "id": "DOCVuP6LUpRS",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Train on WikiText-2\n",
        "print(\"Training LSTM on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type='LSTM',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize=400,\n",
        "    nhid=400,\n",
        "    nlayers=4,\n",
        "    epochs=60,\n",
        "    lr=0.001\n",
        ")\n",
        "\n",
        "# Example 2: Generate text\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    words=1000,\n",
        "    temperature=1.0\n",
        ")\n",
        "\n",
        "!cat generated.txt\n",
        "\n",
        "# Example 3: Train on custom names dataset\n",
        "# First, create the data files (see instructions below)\n",
        "print(\"\\nTraining on custom names dataset...\")\n",
        "# train_model(\n",
        "#     model_type='LSTM',\n",
        "#     data_path='./data/names',\n",
        "#     emsize=128,\n",
        "#     nhid=128,\n",
        "#     nlayers=2,\n",
        "#     epochs=20,\n",
        "#     lr=0.001\n",
        "# )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlFnK0TWRNX0",
        "outputId": "72f7ea02-c7e8-4174-9c0a-90493469b765"
      },
      "id": "jlFnK0TWRNX0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training LSTM on WikiText-2...\n",
            "Using device: cuda\n",
            "Vocabulary size: 25251\n",
            "| epoch   1 |   200/ 2983 batches | lr 0.001000 | ms/batch 20.54 | loss  7.12 | ppl  1234.27\n",
            "| epoch   1 |   400/ 2983 batches | lr 0.001000 | ms/batch 20.50 | loss  6.91 | ppl  1003.33\n",
            "| epoch   1 |   600/ 2983 batches | lr 0.001000 | ms/batch 20.53 | loss  6.91 | ppl   999.71\n",
            "| epoch   1 |   800/ 2983 batches | lr 0.001000 | ms/batch 20.65 | loss  6.90 | ppl   995.20\n",
            "| epoch   1 |  1000/ 2983 batches | lr 0.001000 | ms/batch 20.60 | loss  6.92 | ppl  1013.25\n",
            "| epoch   1 |  1200/ 2983 batches | lr 0.001000 | ms/batch 20.76 | loss  6.57 | ppl   713.70\n",
            "| epoch   1 |  1400/ 2983 batches | lr 0.001000 | ms/batch 20.80 | loss  6.31 | ppl   552.32\n",
            "| epoch   1 |  1600/ 2983 batches | lr 0.001000 | ms/batch 20.79 | loss  6.25 | ppl   516.65\n",
            "| epoch   1 |  1800/ 2983 batches | lr 0.001000 | ms/batch 20.81 | loss  6.13 | ppl   460.14\n",
            "| epoch   1 |  2000/ 2983 batches | lr 0.001000 | ms/batch 20.83 | loss  6.08 | ppl   439.22\n",
            "| epoch   1 |  2200/ 2983 batches | lr 0.001000 | ms/batch 20.80 | loss  5.97 | ppl   391.00\n",
            "| epoch   1 |  2400/ 2983 batches | lr 0.001000 | ms/batch 20.77 | loss  5.93 | ppl   374.62\n",
            "| epoch   1 |  2600/ 2983 batches | lr 0.001000 | ms/batch 20.78 | loss  5.89 | ppl   362.88\n",
            "| epoch   1 |  2800/ 2983 batches | lr 0.001000 | ms/batch 20.79 | loss  5.80 | ppl   331.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 63.68s | valid loss  6.55 | valid ppl   700.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   2 |   200/ 2983 batches | lr 0.001000 | ms/batch 20.76 | loss  5.58 | ppl   264.02\n",
            "| epoch   2 |   400/ 2983 batches | lr 0.001000 | ms/batch 20.67 | loss  5.54 | ppl   254.56\n",
            "| epoch   2 |   600/ 2983 batches | lr 0.001000 | ms/batch 20.64 | loss  5.45 | ppl   233.67\n",
            "| epoch   2 |   800/ 2983 batches | lr 0.001000 | ms/batch 20.67 | loss  5.50 | ppl   244.50\n",
            "| epoch   2 |  1000/ 2983 batches | lr 0.001000 | ms/batch 20.69 | loss  5.47 | ppl   238.07\n",
            "| epoch   2 |  1200/ 2983 batches | lr 0.001000 | ms/batch 20.65 | loss  5.45 | ppl   232.98\n",
            "| epoch   2 |  1400/ 2983 batches | lr 0.001000 | ms/batch 20.66 | loss  5.43 | ppl   227.43\n",
            "| epoch   2 |  1600/ 2983 batches | lr 0.001000 | ms/batch 20.72 | loss  5.46 | ppl   234.86\n",
            "| epoch   2 |  1800/ 2983 batches | lr 0.001000 | ms/batch 20.65 | loss  5.37 | ppl   215.68\n",
            "| epoch   2 |  2000/ 2983 batches | lr 0.001000 | ms/batch 20.63 | loss  5.41 | ppl   222.65\n",
            "| epoch   2 |  2200/ 2983 batches | lr 0.001000 | ms/batch 20.69 | loss  5.32 | ppl   203.37\n",
            "| epoch   2 |  2400/ 2983 batches | lr 0.001000 | ms/batch 20.68 | loss  5.30 | ppl   200.69\n",
            "| epoch   2 |  2600/ 2983 batches | lr 0.001000 | ms/batch 20.68 | loss  5.31 | ppl   201.83\n",
            "| epoch   2 |  2800/ 2983 batches | lr 0.001000 | ms/batch 20.72 | loss  5.24 | ppl   188.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 63.61s | valid loss  6.38 | valid ppl   590.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   3 |   200/ 2983 batches | lr 0.001000 | ms/batch 20.76 | loss  5.13 | ppl   169.51\n",
            "| epoch   3 |   400/ 2983 batches | lr 0.001000 | ms/batch 20.80 | loss  5.12 | ppl   166.93\n",
            "| epoch   3 |   600/ 2983 batches | lr 0.001000 | ms/batch 20.71 | loss  5.01 | ppl   150.41\n",
            "| epoch   3 |   800/ 2983 batches | lr 0.001000 | ms/batch 20.68 | loss  5.05 | ppl   156.13\n",
            "| epoch   3 |  1000/ 2983 batches | lr 0.001000 | ms/batch 20.70 | loss  5.07 | ppl   159.63\n",
            "| epoch   3 |  1200/ 2983 batches | lr 0.001000 | ms/batch 20.72 | loss  5.05 | ppl   155.75\n",
            "| epoch   3 |  1400/ 2983 batches | lr 0.001000 | ms/batch 20.65 | loss  5.05 | ppl   156.37\n",
            "| epoch   3 |  1600/ 2983 batches | lr 0.001000 | ms/batch 20.76 | loss  5.11 | ppl   165.38\n",
            "| epoch   3 |  1800/ 2983 batches | lr 0.001000 | ms/batch 20.61 | loss  5.02 | ppl   151.00\n",
            "| epoch   3 |  2000/ 2983 batches | lr 0.001000 | ms/batch 20.72 | loss  5.05 | ppl   156.44\n",
            "| epoch   3 |  2200/ 2983 batches | lr 0.001000 | ms/batch 20.69 | loss  4.96 | ppl   142.37\n",
            "| epoch   3 |  2400/ 2983 batches | lr 0.001000 | ms/batch 20.67 | loss  4.94 | ppl   140.41\n",
            "| epoch   3 |  2600/ 2983 batches | lr 0.001000 | ms/batch 20.73 | loss  4.97 | ppl   143.77\n",
            "| epoch   3 |  2800/ 2983 batches | lr 0.001000 | ms/batch 20.65 | loss  4.91 | ppl   135.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 63.69s | valid loss  6.31 | valid ppl   552.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   4 |   200/ 2983 batches | lr 0.001000 | ms/batch 20.81 | loss  4.85 | ppl   127.66\n",
            "| epoch   4 |   400/ 2983 batches | lr 0.001000 | ms/batch 20.70 | loss  4.85 | ppl   127.94\n",
            "| epoch   4 |   600/ 2983 batches | lr 0.001000 | ms/batch 20.71 | loss  4.71 | ppl   111.59\n",
            "| epoch   4 |   800/ 2983 batches | lr 0.001000 | ms/batch 20.75 | loss  4.77 | ppl   117.57\n",
            "| epoch   4 |  1000/ 2983 batches | lr 0.001000 | ms/batch 20.67 | loss  4.80 | ppl   121.66\n",
            "| epoch   4 |  1200/ 2983 batches | lr 0.001000 | ms/batch 20.75 | loss  4.78 | ppl   119.22\n",
            "| epoch   4 |  1400/ 2983 batches | lr 0.001000 | ms/batch 20.68 | loss  4.80 | ppl   121.09\n",
            "| epoch   4 |  1600/ 2983 batches | lr 0.001000 | ms/batch 20.68 | loss  4.86 | ppl   129.53\n",
            "| epoch   4 |  1800/ 2983 batches | lr 0.001000 | ms/batch 20.68 | loss  4.77 | ppl   118.11\n",
            "| epoch   4 |  2000/ 2983 batches | lr 0.001000 | ms/batch 20.74 | loss  4.80 | ppl   122.00\n",
            "| epoch   4 |  2200/ 2983 batches | lr 0.001000 | ms/batch 20.70 | loss  4.71 | ppl   111.54\n",
            "| epoch   4 |  2400/ 2983 batches | lr 0.001000 | ms/batch 20.71 | loss  4.70 | ppl   109.78\n",
            "| epoch   4 |  2600/ 2983 batches | lr 0.001000 | ms/batch 20.66 | loss  4.73 | ppl   113.18\n",
            "| epoch   4 |  2800/ 2983 batches | lr 0.001000 | ms/batch 20.66 | loss  4.68 | ppl   108.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 63.67s | valid loss  6.29 | valid ppl   540.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   5 |   200/ 2983 batches | lr 0.001000 | ms/batch 20.80 | loss  4.64 | ppl   103.52\n",
            "| epoch   5 |   400/ 2983 batches | lr 0.001000 | ms/batch 20.68 | loss  4.66 | ppl   105.17\n",
            "| epoch   5 |   600/ 2983 batches | lr 0.001000 | ms/batch 20.72 | loss  4.50 | ppl    89.85\n",
            "| epoch   5 |   800/ 2983 batches | lr 0.001000 | ms/batch 20.68 | loss  4.56 | ppl    95.51\n",
            "| epoch   5 |  1000/ 2983 batches | lr 0.001000 | ms/batch 20.71 | loss  4.60 | ppl    99.64\n",
            "| epoch   5 |  1200/ 2983 batches | lr 0.001000 | ms/batch 20.79 | loss  4.58 | ppl    97.87\n",
            "| epoch   5 |  1400/ 2983 batches | lr 0.001000 | ms/batch 20.71 | loss  4.61 | ppl   100.70\n",
            "| epoch   5 |  1600/ 2983 batches | lr 0.001000 | ms/batch 20.73 | loss  4.68 | ppl   107.88\n",
            "| epoch   5 |  1800/ 2983 batches | lr 0.001000 | ms/batch 20.71 | loss  4.59 | ppl    98.46\n",
            "| epoch   5 |  2000/ 2983 batches | lr 0.001000 | ms/batch 20.65 | loss  4.62 | ppl   101.52\n",
            "| epoch   5 |  2200/ 2983 batches | lr 0.001000 | ms/batch 20.71 | loss  4.53 | ppl    92.59\n",
            "| epoch   5 |  2400/ 2983 batches | lr 0.001000 | ms/batch 20.72 | loss  4.52 | ppl    91.81\n",
            "| epoch   5 |  2600/ 2983 batches | lr 0.001000 | ms/batch 20.70 | loss  4.56 | ppl    95.11\n",
            "| epoch   5 |  2800/ 2983 batches | lr 0.001000 | ms/batch 20.71 | loss  4.50 | ppl    90.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 63.73s | valid loss  6.32 | valid ppl   553.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   6 |   200/ 2983 batches | lr 0.001000 | ms/batch 20.78 | loss  4.48 | ppl    88.04\n",
            "| epoch   6 |   400/ 2983 batches | lr 0.001000 | ms/batch 20.70 | loss  4.50 | ppl    90.01\n",
            "| epoch   6 |   600/ 2983 batches | lr 0.001000 | ms/batch 20.68 | loss  4.34 | ppl    76.61\n",
            "| epoch   6 |   800/ 2983 batches | lr 0.001000 | ms/batch 20.70 | loss  4.40 | ppl    81.63\n",
            "| epoch   6 |  1000/ 2983 batches | lr 0.001000 | ms/batch 20.79 | loss  4.45 | ppl    85.81\n",
            "| epoch   6 |  1200/ 2983 batches | lr 0.001000 | ms/batch 20.71 | loss  4.42 | ppl    83.43\n",
            "| epoch   6 |  1400/ 2983 batches | lr 0.001000 | ms/batch 20.72 | loss  4.46 | ppl    86.84\n",
            "| epoch   6 |  1600/ 2983 batches | lr 0.001000 | ms/batch 20.72 | loss  4.53 | ppl    93.06\n",
            "| epoch   6 |  1800/ 2983 batches | lr 0.001000 | ms/batch 20.70 | loss  4.45 | ppl    85.30\n",
            "| epoch   6 |  2000/ 2983 batches | lr 0.001000 | ms/batch 20.67 | loss  4.47 | ppl    87.55\n",
            "| epoch   6 |  2200/ 2983 batches | lr 0.001000 | ms/batch 20.76 | loss  4.38 | ppl    79.93\n",
            "| epoch   6 |  2400/ 2983 batches | lr 0.001000 | ms/batch 20.68 | loss  4.38 | ppl    79.82\n",
            "| epoch   6 |  2600/ 2983 batches | lr 0.001000 | ms/batch 20.72 | loss  4.42 | ppl    82.94\n",
            "| epoch   6 |  2800/ 2983 batches | lr 0.001000 | ms/batch 20.76 | loss  4.37 | ppl    78.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 63.73s | valid loss  6.32 | valid ppl   554.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001000\n",
            "| epoch   7 |   200/ 2983 batches | lr 0.001000 | ms/batch 20.82 | loss  4.34 | ppl    76.90\n",
            "| epoch   7 |   400/ 2983 batches | lr 0.001000 | ms/batch 20.77 | loss  4.37 | ppl    79.33\n",
            "| epoch   7 |   600/ 2983 batches | lr 0.001000 | ms/batch 20.71 | loss  4.20 | ppl    67.02\n",
            "| epoch   7 |   800/ 2983 batches | lr 0.001000 | ms/batch 20.78 | loss  4.27 | ppl    71.64\n",
            "| epoch   7 |  1000/ 2983 batches | lr 0.001000 | ms/batch 20.70 | loss  4.33 | ppl    75.67\n",
            "| epoch   7 |  1200/ 2983 batches | lr 0.001000 | ms/batch 20.72 | loss  4.30 | ppl    73.58\n",
            "| epoch   7 |  1400/ 2983 batches | lr 0.001000 | ms/batch 20.77 | loss  4.35 | ppl    77.38\n",
            "| epoch   7 |  1600/ 2983 batches | lr 0.001000 | ms/batch 20.69 | loss  4.41 | ppl    82.37\n",
            "| epoch   7 |  1800/ 2983 batches | lr 0.001000 | ms/batch 20.77 | loss  4.33 | ppl    76.07\n",
            "| epoch   7 |  2000/ 2983 batches | lr 0.001000 | ms/batch 20.72 | loss  4.36 | ppl    78.08\n",
            "| epoch   7 |  2200/ 2983 batches | lr 0.001000 | ms/batch 20.69 | loss  4.26 | ppl    70.89\n",
            "| epoch   7 |  2400/ 2983 batches | lr 0.001000 | ms/batch 20.71 | loss  4.26 | ppl    70.55\n",
            "| epoch   7 |  2600/ 2983 batches | lr 0.001000 | ms/batch 20.70 | loss  4.30 | ppl    74.07\n",
            "| epoch   7 |  2800/ 2983 batches | lr 0.001000 | ms/batch 20.69 | loss  4.25 | ppl    70.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 63.76s | valid loss  6.34 | valid ppl   564.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch   8 |   200/ 2983 batches | lr 0.000500 | ms/batch 20.80 | loss  4.26 | ppl    70.49\n",
            "| epoch   8 |   400/ 2983 batches | lr 0.000500 | ms/batch 20.70 | loss  4.27 | ppl    71.78\n",
            "| epoch   8 |   600/ 2983 batches | lr 0.000500 | ms/batch 20.75 | loss  4.10 | ppl    60.60\n",
            "| epoch   8 |   800/ 2983 batches | lr 0.000500 | ms/batch 20.75 | loss  4.16 | ppl    63.84\n",
            "| epoch   8 |  1000/ 2983 batches | lr 0.000500 | ms/batch 20.70 | loss  4.20 | ppl    67.00\n",
            "| epoch   8 |  1200/ 2983 batches | lr 0.000500 | ms/batch 20.72 | loss  4.17 | ppl    64.55\n",
            "| epoch   8 |  1400/ 2983 batches | lr 0.000500 | ms/batch 20.74 | loss  4.20 | ppl    66.69\n",
            "| epoch   8 |  1600/ 2983 batches | lr 0.000500 | ms/batch 20.67 | loss  4.26 | ppl    70.85\n",
            "| epoch   8 |  1800/ 2983 batches | lr 0.000500 | ms/batch 20.73 | loss  4.19 | ppl    65.82\n",
            "| epoch   8 |  2000/ 2983 batches | lr 0.000500 | ms/batch 20.78 | loss  4.20 | ppl    66.37\n",
            "| epoch   8 |  2200/ 2983 batches | lr 0.000500 | ms/batch 20.70 | loss  4.10 | ppl    60.14\n",
            "| epoch   8 |  2400/ 2983 batches | lr 0.000500 | ms/batch 20.78 | loss  4.09 | ppl    59.67\n",
            "| epoch   8 |  2600/ 2983 batches | lr 0.000500 | ms/batch 20.66 | loss  4.13 | ppl    62.41\n",
            "| epoch   8 |  2800/ 2983 batches | lr 0.000500 | ms/batch 20.68 | loss  4.07 | ppl    58.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 63.72s | valid loss  6.34 | valid ppl   567.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n",
            "| epoch   9 |   200/ 2983 batches | lr 0.000500 | ms/batch 20.81 | loss  4.16 | ppl    64.30\n",
            "| epoch   9 |   400/ 2983 batches | lr 0.000500 | ms/batch 20.71 | loss  4.18 | ppl    65.52\n",
            "| epoch   9 |   600/ 2983 batches | lr 0.000500 | ms/batch 20.66 | loss  4.01 | ppl    55.31\n",
            "| epoch   9 |   800/ 2983 batches | lr 0.000500 | ms/batch 20.70 | loss  4.07 | ppl    58.31\n",
            "| epoch   9 |  1000/ 2983 batches | lr 0.000500 | ms/batch 20.68 | loss  4.12 | ppl    61.46\n",
            "| epoch   9 |  1200/ 2983 batches | lr 0.000500 | ms/batch 20.67 | loss  4.08 | ppl    59.28\n",
            "| epoch   9 |  1400/ 2983 batches | lr 0.000500 | ms/batch 20.66 | loss  4.12 | ppl    61.84\n",
            "| epoch   9 |  1600/ 2983 batches | lr 0.000500 | ms/batch 20.74 | loss  4.19 | ppl    66.22\n",
            "| epoch   9 |  1800/ 2983 batches | lr 0.000500 | ms/batch 20.71 | loss  4.12 | ppl    61.40\n",
            "| epoch   9 |  2000/ 2983 batches | lr 0.000500 | ms/batch 20.83 | loss  4.13 | ppl    62.26\n",
            "| epoch   9 |  2200/ 2983 batches | lr 0.000500 | ms/batch 20.73 | loss  4.03 | ppl    56.27\n",
            "| epoch   9 |  2400/ 2983 batches | lr 0.000500 | ms/batch 20.64 | loss  4.03 | ppl    56.14\n",
            "| epoch   9 |  2600/ 2983 batches | lr 0.000500 | ms/batch 20.67 | loss  4.07 | ppl    58.71\n",
            "| epoch   9 |  2800/ 2983 batches | lr 0.000500 | ms/batch 20.71 | loss  4.02 | ppl    55.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 63.68s | valid loss  6.37 | valid ppl   582.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Transformer on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type='Transformer',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize=512,\n",
        "    nhid=2048,\n",
        "    nlayers=6,\n",
        "    epochs=60,\n",
        "    lr=0.0001,\n",
        "    batch_size = 32,\n",
        "    bptt = 50,\n",
        "    dropout = 0.1,\n",
        "    nhead = 8,\n",
        "    save_path='model_1.pt'\n",
        ")\n",
        "\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model_1.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    words=1000,\n",
        "    temperature=1.0\n",
        ")"
      ],
      "metadata": {
        "id": "Ay8aCfM-YMIw"
      },
      "id": "Ay8aCfM-YMIw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Transformer on WikiText-2...\")\n",
        "train_model(\n",
        "    model_type='Transformer',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    emsize=512,\n",
        "    nhid=2048,\n",
        "    nlayers=6,\n",
        "    lr=0.0001,\n",
        "    clip=0.25,\n",
        "    epochs=60,\n",
        "    batch_size=32,\n",
        "    bptt=50,\n",
        "    dropout=0.1,\n",
        "    nhead=8,\n",
        "    log_interval=200,\n",
        "    save_path='model_2.pt',\n",
        "    use_optimizer=True,\n",
        "    optimizer=None,  # Use default AdamW\n",
        "    criterion=LabelSmoothingLoss(smoothing=0.1),\n",
        "    use_label_smoothing=True,\n",
        "    use_warmup=True,\n",
        "    warmup_steps=4000,\n",
        "    min_freq=5\n",
        ")\n",
        "\n",
        "print(\"\\nGenerating text...\")\n",
        "generate_text(\n",
        "    checkpoint='model_2.pt',\n",
        "    data_path='/content/drive/MyDrive/data_word_train/wikitext-2',\n",
        "    outf='generated.txt',\n",
        "    words=1000,\n",
        "    temperature=0.8,\n",
        "    top_k=40,\n",
        "    seed=1111,\n",
        "    log_interval=100,\n",
        "    min_freq=5\n",
        ")"
      ],
      "metadata": {
        "id": "rKoNQPO4jYZB"
      },
      "id": "rKoNQPO4jYZB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### OLD CODE\n",
        "# DATA_PATH = '/content/drive/MyDrive/data_word_train/wikitext-2'\n",
        "# if not os.path.exists(DATA_PATH):\n",
        "#     print(\"Downloading Wikitext-2 dataset...\")\n",
        "#     !wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip -P /content/\n",
        "#     !unzip /content/wikitext-2-v1.zip -d /content/data_word_train/\n",
        "#     !mkdir -p /content/drive/MyDrive/data_word_train/\n",
        "#     !mv /content/data_word_train/wikitext-2 /content/drive/MyDrive/data_word_train/\n",
        "#     print(\"Wikitext-2 dataset moved to Google Drive\")\n",
        "\n",
        "# # Step 5: Training Function from main.py (adapted for Colab)\n",
        "# class Args:\n",
        "#     def __init__(self):\n",
        "#         self.data = '/content/drive/MyDrive/data_word_train/wikitext-2'  # Wikitext-2 path\n",
        "#         self.model = 'LSTM'  # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "#         self.emsize = 400\n",
        "#         self.nhid = 400\n",
        "#         self.nlayers = 4\n",
        "#         self.lr = 0.001\n",
        "#         self.clip = 0.25\n",
        "#         self.epochs = 60\n",
        "#         self.batch_size = 20\n",
        "#         self.bptt = 35\n",
        "#         self.dropout = 0.2\n",
        "#         self.tied = False\n",
        "#         self.seed = 1111\n",
        "#         self.log_interval = 200\n",
        "#         self.save = 'model.pt'\n",
        "#         self.onnx_export = ''\n",
        "#         self.nhead = 4\n",
        "#         self.dry_run = False\n",
        "#         self.accel = True\n",
        "#         self.use_optimizer = True  # Use AdamW\n",
        "\n",
        "# args = Args()\n",
        "\n",
        "# torch.manual_seed(args.seed)\n",
        "\n",
        "# if args.accel and torch.cuda.is_available():\n",
        "#     device = torch.device(\"cuda\")\n",
        "# else:\n",
        "#     device = torch.device(\"cpu\")\n",
        "\n",
        "# print(\"Using device:\", device)\n",
        "\n",
        "# # Load data\n",
        "# corpus = Corpus(args.data)\n",
        "\n",
        "# def batchify(data, bsz):\n",
        "#     nbatch = data.size(0) // bsz\n",
        "#     data = data.narrow(0, 0, nbatch * bsz)\n",
        "#     data = data.view(bsz, -1).t().contiguous()\n",
        "#     return data.to(device)\n",
        "\n",
        "# eval_batch_size = 10\n",
        "# train_data = batchify(corpus.train, args.batch_size)\n",
        "# val_data = batchify(corpus.valid, eval_batch_size)\n",
        "# test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "# # Build the model\n",
        "# ntokens = len(corpus.dictionary)\n",
        "# if args.model == 'Transformer':\n",
        "#     model = TransformerModel(ntokens, args.emsize, args.nhead, args.nhid, args.nlayers, args.dropout).to(device)\n",
        "# else:\n",
        "#     model = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied).to(device)\n",
        "\n",
        "# # Label smoothing loss\n",
        "# class LabelSmoothingLoss(nn.Module):\n",
        "#     def __init__(self, smoothing=0.0):\n",
        "#         super(LabelSmoothingLoss, self).__init__()\n",
        "#         self.smoothing = smoothing\n",
        "#         self.confidence = 1.0 - smoothing\n",
        "\n",
        "#     def forward(self, output, target):\n",
        "#         log_probs = output\n",
        "#         n_classes = log_probs.size(-1)\n",
        "#         with torch.no_grad():\n",
        "#             true_dist = torch.zeros_like(log_probs)\n",
        "#             true_dist.fill_(self.smoothing / (n_classes - 1))\n",
        "#             true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
        "#         return torch.mean(torch.sum(-true_dist * log_probs, dim=-1))\n",
        "\n",
        "# criterion = nn.NLLLoss()\n",
        "# if args.use_optimizer:\n",
        "#     optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-5)\n",
        "#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "# # Training code\n",
        "# def repackage_hidden(h):\n",
        "#     if isinstance(h, torch.Tensor):\n",
        "#         return h.detach()\n",
        "#     else:\n",
        "#         return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "# def get_batch(source, i):\n",
        "#     seq_len = min(args.bptt, len(source) - 1 - i)\n",
        "#     data = source[i:i+seq_len]\n",
        "#     target = source[i+1:i+1+seq_len].view(-1)\n",
        "#     return data, target\n",
        "\n",
        "# def evaluate(data_source):\n",
        "#     model.eval()\n",
        "#     total_loss = 0.\n",
        "#     if args.model != 'Transformer':\n",
        "#         hidden = model.init_hidden(eval_batch_size)\n",
        "#     with torch.no_grad():\n",
        "#         for i in range(0, data_source.size(0) - 1, args.bptt):\n",
        "#             data, targets = get_batch(data_source, i)\n",
        "#             if args.model == 'Transformer':\n",
        "#                 output = model(data)\n",
        "#                 output = output.view(-1, ntokens)\n",
        "#             else:\n",
        "#                 output, hidden = model(data, hidden)\n",
        "#                 hidden = repackage_hidden(hidden)\n",
        "#             total_loss += len(data) * criterion(output, targets).item()\n",
        "#     return total_loss / (len(data_source) - 1)\n",
        "\n",
        "# def train_func():\n",
        "#     model.train()\n",
        "#     total_loss = 0.\n",
        "#     start_time = time.time()\n",
        "#     if args.model != 'Transformer':\n",
        "#         hidden = model.init_hidden(args.batch_size)\n",
        "#     for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n",
        "#         data, targets = get_batch(train_data, i)\n",
        "#         optimizer.zero_grad() if args.use_optimizer else model.zero_grad()\n",
        "#         if args.model == 'Transformer':\n",
        "#             output = model(data)\n",
        "#             output = output.view(-1, ntokens)\n",
        "#         else:\n",
        "#             hidden = repackage_hidden(hidden)\n",
        "#             output, hidden = model(data, hidden)\n",
        "#         loss = criterion(output, targets)\n",
        "#         loss.backward()\n",
        "\n",
        "#         torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
        "#         if args.use_optimizer:\n",
        "#             optimizer.step()\n",
        "#         else:\n",
        "#             for p in model.parameters():\n",
        "#                 p.data.add_(p.grad, alpha=-args.lr)\n",
        "\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#         if batch % args.log_interval == 0 and batch > 0:\n",
        "#             cur_loss = total_loss / args.log_interval\n",
        "#             elapsed = time.time() - start_time\n",
        "#             print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "#                   'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "#                 epoch, batch, len(train_data) // args.bptt, args.lr,\n",
        "#                 elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
        "#             total_loss = 0\n",
        "#             start_time = time.time()\n",
        "#         if args.dry_run:\n",
        "#             break\n",
        "\n",
        "# lr = args.lr\n",
        "# best_val_loss = None\n",
        "\n",
        "# try:\n",
        "#     for epoch in range(1, args.epochs + 1):\n",
        "#         epoch_start_time = time.time()\n",
        "#         train_func()\n",
        "#         val_loss = evaluate(val_data)\n",
        "#         print('-' * 89)\n",
        "#         print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "#               'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "#                                          val_loss, math.exp(val_loss)))\n",
        "#         print('-' * 89)\n",
        "#         if not best_val_loss or val_loss < best_val_loss:\n",
        "#             with open(args.save, 'wb') as f:\n",
        "#                 torch.save(model, f)\n",
        "#             best_val_loss = val_loss\n",
        "#         else:\n",
        "#             # lr /= 4.0\n",
        "#             scheduler.step(val_loss)\n",
        "#         print(f\"Current learning rate: {optimizer.param_groups[0]['lr']}\")\n",
        "# except KeyboardInterrupt:\n",
        "#     print('-' * 89)\n",
        "#     print('Exiting from training early')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7DIEg_FZz-_",
        "outputId": "206100a5-6da5-4db0-9b3d-53ce9e472d3b"
      },
      "id": "S7DIEg_FZz-_",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "| epoch   1 |   200/ 2983 batches | lr 0.00 | ms/batch 25.46 | loss  7.33 | ppl  1522.35\n",
            "| epoch   1 |   400/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  7.10 | ppl  1207.10\n",
            "| epoch   1 |   600/ 2983 batches | lr 0.00 | ms/batch 25.48 | loss  7.09 | ppl  1196.08\n",
            "| epoch   1 |   800/ 2983 batches | lr 0.00 | ms/batch 25.48 | loss  7.09 | ppl  1196.80\n",
            "| epoch   1 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  7.10 | ppl  1214.24\n",
            "| epoch   1 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  7.11 | ppl  1228.74\n",
            "| epoch   1 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  6.94 | ppl  1031.50\n",
            "| epoch   1 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  6.63 | ppl   758.50\n",
            "| epoch   1 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.33 | loss  6.46 | ppl   636.73\n",
            "| epoch   1 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.31 | loss  6.40 | ppl   602.21\n",
            "| epoch   1 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.31 | loss  6.26 | ppl   524.79\n",
            "| epoch   1 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.28 | loss  6.22 | ppl   504.94\n",
            "| epoch   1 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.34 | loss  6.19 | ppl   486.94\n",
            "| epoch   1 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.29 | loss  6.08 | ppl   436.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 77.76s | valid loss  5.91 | valid ppl   367.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001\n",
            "| epoch   2 |   200/ 2983 batches | lr 0.00 | ms/batch 25.49 | loss  5.77 | ppl   321.80\n",
            "| epoch   2 |   400/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  5.72 | ppl   303.89\n",
            "| epoch   2 |   600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  5.64 | ppl   282.60\n",
            "| epoch   2 |   800/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  5.68 | ppl   293.98\n",
            "| epoch   2 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  5.64 | ppl   281.28\n",
            "| epoch   2 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  5.66 | ppl   286.92\n",
            "| epoch   2 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.46 | loss  5.62 | ppl   275.84\n",
            "| epoch   2 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  5.63 | ppl   277.30\n",
            "| epoch   2 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  5.54 | ppl   254.42\n",
            "| epoch   2 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  5.56 | ppl   260.03\n",
            "| epoch   2 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.32 | loss  5.46 | ppl   234.41\n",
            "| epoch   2 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  5.47 | ppl   237.27\n",
            "| epoch   2 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  5.48 | ppl   238.70\n",
            "| epoch   2 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  5.40 | ppl   221.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 77.86s | valid loss  5.56 | valid ppl   258.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001\n",
            "| epoch   3 |   200/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  5.27 | ppl   193.78\n",
            "| epoch   3 |   400/ 2983 batches | lr 0.00 | ms/batch 25.32 | loss  5.25 | ppl   190.32\n",
            "| epoch   3 |   600/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  5.15 | ppl   171.83\n",
            "| epoch   3 |   800/ 2983 batches | lr 0.00 | ms/batch 25.34 | loss  5.20 | ppl   181.67\n",
            "| epoch   3 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.31 | loss  5.18 | ppl   177.72\n",
            "| epoch   3 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  5.20 | ppl   181.47\n",
            "| epoch   3 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.31 | loss  5.18 | ppl   178.44\n",
            "| epoch   3 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  5.22 | ppl   185.62\n",
            "| epoch   3 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  5.14 | ppl   171.37\n",
            "| epoch   3 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  5.16 | ppl   175.01\n",
            "| epoch   3 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  5.07 | ppl   159.19\n",
            "| epoch   3 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.33 | loss  5.08 | ppl   160.81\n",
            "| epoch   3 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  5.10 | ppl   163.68\n",
            "| epoch   3 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  5.04 | ppl   153.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 77.75s | valid loss  5.39 | valid ppl   219.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001\n",
            "| epoch   4 |   200/ 2983 batches | lr 0.00 | ms/batch 25.48 | loss  4.95 | ppl   141.85\n",
            "| epoch   4 |   400/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  4.97 | ppl   143.91\n",
            "| epoch   4 |   600/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  4.82 | ppl   123.62\n",
            "| epoch   4 |   800/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  4.88 | ppl   131.14\n",
            "| epoch   4 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  4.88 | ppl   132.15\n",
            "| epoch   4 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  4.90 | ppl   134.12\n",
            "| epoch   4 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  4.90 | ppl   134.56\n",
            "| epoch   4 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  4.95 | ppl   141.65\n",
            "| epoch   4 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  4.87 | ppl   130.13\n",
            "| epoch   4 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  4.89 | ppl   132.89\n",
            "| epoch   4 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  4.81 | ppl   122.18\n",
            "| epoch   4 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  4.82 | ppl   124.12\n",
            "| epoch   4 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  4.84 | ppl   126.56\n",
            "| epoch   4 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  4.79 | ppl   120.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 77.81s | valid loss  5.32 | valid ppl   205.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001\n",
            "| epoch   5 |   200/ 2983 batches | lr 0.00 | ms/batch 25.54 | loss  4.74 | ppl   113.94\n",
            "| epoch   5 |   400/ 2983 batches | lr 0.00 | ms/batch 25.34 | loss  4.75 | ppl   115.66\n",
            "| epoch   5 |   600/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  4.59 | ppl    98.72\n",
            "| epoch   5 |   800/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  4.65 | ppl   104.25\n",
            "| epoch   5 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  4.66 | ppl   106.02\n",
            "| epoch   5 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.49 | loss  4.68 | ppl   107.78\n",
            "| epoch   5 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  4.69 | ppl   109.19\n",
            "| epoch   5 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  4.74 | ppl   114.82\n",
            "| epoch   5 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  4.66 | ppl   105.73\n",
            "| epoch   5 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  4.68 | ppl   108.30\n",
            "| epoch   5 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  4.61 | ppl   100.60\n",
            "| epoch   5 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.33 | loss  4.62 | ppl   101.75\n",
            "| epoch   5 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  4.64 | ppl   104.02\n",
            "| epoch   5 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  4.59 | ppl    98.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 77.85s | valid loss  5.28 | valid ppl   196.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001\n",
            "| epoch   6 |   200/ 2983 batches | lr 0.00 | ms/batch 25.50 | loss  4.56 | ppl    95.84\n",
            "| epoch   6 |   400/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  4.57 | ppl    96.68\n",
            "| epoch   6 |   600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  4.40 | ppl    81.85\n",
            "| epoch   6 |   800/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  4.46 | ppl    86.60\n",
            "| epoch   6 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  4.49 | ppl    88.98\n",
            "| epoch   6 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  4.51 | ppl    90.81\n",
            "| epoch   6 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  4.52 | ppl    92.28\n",
            "| epoch   6 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  4.59 | ppl    98.41\n",
            "| epoch   6 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  4.50 | ppl    90.41\n",
            "| epoch   6 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  4.53 | ppl    92.35\n",
            "| epoch   6 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  4.46 | ppl    86.15\n",
            "| epoch   6 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  4.46 | ppl    86.44\n",
            "| epoch   6 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  4.48 | ppl    88.22\n",
            "| epoch   6 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  4.43 | ppl    84.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 77.80s | valid loss  5.28 | valid ppl   196.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001\n",
            "| epoch   7 |   200/ 2983 batches | lr 0.00 | ms/batch 25.50 | loss  4.42 | ppl    82.73\n",
            "| epoch   7 |   400/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  4.43 | ppl    84.16\n",
            "| epoch   7 |   600/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  4.26 | ppl    70.75\n",
            "| epoch   7 |   800/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  4.31 | ppl    74.80\n",
            "| epoch   7 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  4.35 | ppl    77.72\n",
            "| epoch   7 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  4.37 | ppl    78.91\n",
            "| epoch   7 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  4.39 | ppl    80.56\n",
            "| epoch   7 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  4.46 | ppl    86.45\n",
            "| epoch   7 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  4.38 | ppl    79.56\n",
            "| epoch   7 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  4.39 | ppl    80.96\n",
            "| epoch   7 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  4.33 | ppl    75.62\n",
            "| epoch   7 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  4.33 | ppl    75.66\n",
            "| epoch   7 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  4.36 | ppl    78.06\n",
            "| epoch   7 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  4.31 | ppl    74.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 77.87s | valid loss  5.29 | valid ppl   199.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001\n",
            "| epoch   8 |   200/ 2983 batches | lr 0.00 | ms/batch 25.53 | loss  4.30 | ppl    73.39\n",
            "| epoch   8 |   400/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  4.31 | ppl    74.35\n",
            "| epoch   8 |   600/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  4.14 | ppl    62.77\n",
            "| epoch   8 |   800/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  4.20 | ppl    66.40\n",
            "| epoch   8 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  4.23 | ppl    68.94\n",
            "| epoch   8 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  4.25 | ppl    70.42\n",
            "| epoch   8 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  4.28 | ppl    71.92\n",
            "| epoch   8 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  4.35 | ppl    77.58\n",
            "| epoch   8 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  4.26 | ppl    70.91\n",
            "| epoch   8 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  4.30 | ppl    73.33\n",
            "| epoch   8 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  4.21 | ppl    67.66\n",
            "| epoch   8 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  4.22 | ppl    67.70\n",
            "| epoch   8 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  4.25 | ppl    70.38\n",
            "| epoch   8 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.46 | loss  4.20 | ppl    66.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 77.95s | valid loss  5.34 | valid ppl   208.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001\n",
            "| epoch   9 |   200/ 2983 batches | lr 0.00 | ms/batch 25.55 | loss  4.19 | ppl    66.18\n",
            "| epoch   9 |   400/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  4.22 | ppl    67.76\n",
            "| epoch   9 |   600/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  4.04 | ppl    56.77\n",
            "| epoch   9 |   800/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  4.10 | ppl    60.18\n",
            "| epoch   9 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  4.14 | ppl    62.98\n",
            "| epoch   9 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  4.16 | ppl    63.89\n",
            "| epoch   9 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.48 | loss  4.17 | ppl    64.98\n",
            "| epoch   9 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  4.25 | ppl    70.38\n",
            "| epoch   9 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  4.17 | ppl    64.60\n",
            "| epoch   9 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  4.20 | ppl    66.80\n",
            "| epoch   9 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  4.12 | ppl    61.39\n",
            "| epoch   9 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  4.12 | ppl    61.66\n",
            "| epoch   9 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  4.15 | ppl    63.61\n",
            "| epoch   9 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  4.11 | ppl    61.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 77.92s | valid loss  5.38 | valid ppl   216.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.001\n",
            "| epoch  10 |   200/ 2983 batches | lr 0.00 | ms/batch 25.50 | loss  4.11 | ppl    60.76\n",
            "| epoch  10 |   400/ 2983 batches | lr 0.00 | ms/batch 25.46 | loss  4.12 | ppl    61.54\n",
            "| epoch  10 |   600/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.95 | ppl    51.84\n",
            "| epoch  10 |   800/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  4.01 | ppl    55.31\n",
            "| epoch  10 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  4.07 | ppl    58.36\n",
            "| epoch  10 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  4.07 | ppl    58.43\n",
            "| epoch  10 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  4.09 | ppl    59.87\n",
            "| epoch  10 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  4.17 | ppl    64.86\n",
            "| epoch  10 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  4.09 | ppl    59.45\n",
            "| epoch  10 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  4.11 | ppl    61.05\n",
            "| epoch  10 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  4.04 | ppl    56.56\n",
            "| epoch  10 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  4.04 | ppl    56.66\n",
            "| epoch  10 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  4.07 | ppl    58.84\n",
            "| epoch  10 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.47 | loss  4.03 | ppl    56.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 77.85s | valid loss  5.40 | valid ppl   221.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.0005\n",
            "| epoch  11 |   200/ 2983 batches | lr 0.00 | ms/batch 25.55 | loss  4.06 | ppl    57.92\n",
            "| epoch  11 |   400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  4.06 | ppl    57.79\n",
            "| epoch  11 |   600/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.88 | ppl    48.46\n",
            "| epoch  11 |   800/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.93 | ppl    50.95\n",
            "| epoch  11 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.97 | ppl    53.17\n",
            "| epoch  11 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.47 | loss  3.96 | ppl    52.29\n",
            "| epoch  11 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.97 | ppl    53.22\n",
            "| epoch  11 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  4.04 | ppl    56.95\n",
            "| epoch  11 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.97 | ppl    52.77\n",
            "| epoch  11 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.98 | ppl    53.55\n",
            "| epoch  11 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.89 | ppl    48.89\n",
            "| epoch  11 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.89 | ppl    48.75\n",
            "| epoch  11 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.46 | loss  3.92 | ppl    50.31\n",
            "| epoch  11 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.46 | loss  3.88 | ppl    48.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 77.96s | valid loss  5.42 | valid ppl   226.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.0005\n",
            "| epoch  12 |   200/ 2983 batches | lr 0.00 | ms/batch 25.56 | loss  3.98 | ppl    53.26\n",
            "| epoch  12 |   400/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.98 | ppl    53.30\n",
            "| epoch  12 |   600/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.80 | ppl    44.71\n",
            "| epoch  12 |   800/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.86 | ppl    47.50\n",
            "| epoch  12 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.90 | ppl    49.23\n",
            "| epoch  12 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.89 | ppl    49.04\n",
            "| epoch  12 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.91 | ppl    49.75\n",
            "| epoch  12 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.99 | ppl    53.80\n",
            "| epoch  12 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.91 | ppl    50.05\n",
            "| epoch  12 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.93 | ppl    50.70\n",
            "| epoch  12 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.83 | ppl    46.24\n",
            "| epoch  12 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.84 | ppl    46.40\n",
            "| epoch  12 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.87 | ppl    48.02\n",
            "| epoch  12 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.83 | ppl    46.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 77.91s | valid loss  5.43 | valid ppl   227.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.0005\n",
            "| epoch  13 |   200/ 2983 batches | lr 0.00 | ms/batch 25.53 | loss  3.91 | ppl    50.10\n",
            "| epoch  13 |   400/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.91 | ppl    50.08\n",
            "| epoch  13 |   600/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.74 | ppl    42.27\n",
            "| epoch  13 |   800/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.80 | ppl    44.72\n",
            "| epoch  13 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.85 | ppl    46.92\n",
            "| epoch  13 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.85 | ppl    46.86\n",
            "| epoch  13 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.86 | ppl    47.28\n",
            "| epoch  13 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.93 | ppl    51.09\n",
            "| epoch  13 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.86 | ppl    47.64\n",
            "| epoch  13 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.88 | ppl    48.37\n",
            "| epoch  13 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.79 | ppl    44.17\n",
            "| epoch  13 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.79 | ppl    44.39\n",
            "| epoch  13 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.83 | ppl    46.05\n",
            "| epoch  13 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.79 | ppl    44.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 77.93s | valid loss  5.44 | valid ppl   230.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.00025\n",
            "| epoch  14 |   200/ 2983 batches | lr 0.00 | ms/batch 25.54 | loss  3.90 | ppl    49.54\n",
            "| epoch  14 |   400/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.91 | ppl    49.75\n",
            "| epoch  14 |   600/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.75 | ppl    42.52\n",
            "| epoch  14 |   800/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.78 | ppl    43.96\n",
            "| epoch  14 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.82 | ppl    45.70\n",
            "| epoch  14 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.79 | ppl    44.34\n",
            "| epoch  14 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.47 | loss  3.82 | ppl    45.71\n",
            "| epoch  14 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.89 | ppl    48.85\n",
            "| epoch  14 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.82 | ppl    45.53\n",
            "| epoch  14 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.82 | ppl    45.66\n",
            "| epoch  14 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.74 | ppl    42.03\n",
            "| epoch  14 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.74 | ppl    42.18\n",
            "| epoch  14 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.77 | ppl    43.37\n",
            "| epoch  14 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.73 | ppl    41.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 77.91s | valid loss  5.42 | valid ppl   226.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.00025\n",
            "| epoch  15 |   200/ 2983 batches | lr 0.00 | ms/batch 25.58 | loss  3.86 | ppl    47.45\n",
            "| epoch  15 |   400/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.87 | ppl    47.73\n",
            "| epoch  15 |   600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.70 | ppl    40.53\n",
            "| epoch  15 |   800/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.74 | ppl    41.93\n",
            "| epoch  15 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.78 | ppl    43.91\n",
            "| epoch  15 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.78 | ppl    44.01\n",
            "| epoch  15 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.77 | ppl    43.49\n",
            "| epoch  15 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.85 | ppl    47.21\n",
            "| epoch  15 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.79 | ppl    44.08\n",
            "| epoch  15 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.79 | ppl    44.20\n",
            "| epoch  15 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.70 | ppl    40.63\n",
            "| epoch  15 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.46 | loss  3.71 | ppl    40.75\n",
            "| epoch  15 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.74 | ppl    42.17\n",
            "| epoch  15 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.71 | ppl    40.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 77.90s | valid loss  5.43 | valid ppl   228.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.00025\n",
            "| epoch  16 |   200/ 2983 batches | lr 0.00 | ms/batch 25.55 | loss  3.83 | ppl    45.90\n",
            "| epoch  16 |   400/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.82 | ppl    45.69\n",
            "| epoch  16 |   600/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.67 | ppl    39.22\n",
            "| epoch  16 |   800/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.70 | ppl    40.49\n",
            "| epoch  16 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.75 | ppl    42.40\n",
            "| epoch  16 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.74 | ppl    41.99\n",
            "| epoch  16 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.74 | ppl    42.30\n",
            "| epoch  16 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.82 | ppl    45.65\n",
            "| epoch  16 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.76 | ppl    43.00\n",
            "| epoch  16 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.77 | ppl    43.36\n",
            "| epoch  16 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.68 | ppl    39.57\n",
            "| epoch  16 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.46 | loss  3.69 | ppl    39.87\n",
            "| epoch  16 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.71 | ppl    40.91\n",
            "| epoch  16 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.68 | ppl    39.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 77.92s | valid loss  5.44 | valid ppl   230.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  17 |   200/ 2983 batches | lr 0.00 | ms/batch 25.57 | loss  3.86 | ppl    47.46\n",
            "| epoch  17 |   400/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.83 | ppl    46.29\n",
            "| epoch  17 |   600/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.70 | ppl    40.44\n",
            "| epoch  17 |   800/ 2983 batches | lr 0.00 | ms/batch 25.49 | loss  3.71 | ppl    40.78\n",
            "| epoch  17 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.76 | ppl    42.82\n",
            "| epoch  17 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.74 | ppl    42.05\n",
            "| epoch  17 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.74 | ppl    41.96\n",
            "| epoch  17 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.82 | ppl    45.44\n",
            "| epoch  17 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.74 | ppl    42.10\n",
            "| epoch  17 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.76 | ppl    43.05\n",
            "| epoch  17 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.67 | ppl    39.06\n",
            "| epoch  17 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.68 | ppl    39.54\n",
            "| epoch  17 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.72 | ppl    41.38\n",
            "| epoch  17 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.65 | ppl    38.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 77.93s | valid loss  5.37 | valid ppl   215.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  18 |   200/ 2983 batches | lr 0.00 | ms/batch 25.57 | loss  3.83 | ppl    46.15\n",
            "| epoch  18 |   400/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.82 | ppl    45.50\n",
            "| epoch  18 |   600/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.66 | ppl    39.03\n",
            "| epoch  18 |   800/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.69 | ppl    39.97\n",
            "| epoch  18 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.74 | ppl    42.06\n",
            "| epoch  18 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.71 | ppl    41.00\n",
            "| epoch  18 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.71 | ppl    40.96\n",
            "| epoch  18 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.80 | ppl    44.81\n",
            "| epoch  18 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.73 | ppl    41.50\n",
            "| epoch  18 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.75 | ppl    42.48\n",
            "| epoch  18 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.65 | ppl    38.50\n",
            "| epoch  18 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.46 | loss  3.65 | ppl    38.57\n",
            "| epoch  18 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.48 | loss  3.70 | ppl    40.63\n",
            "| epoch  18 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.64 | ppl    38.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 77.92s | valid loss  5.38 | valid ppl   217.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 0.000125\n",
            "| epoch  19 |   200/ 2983 batches | lr 0.00 | ms/batch 25.52 | loss  3.81 | ppl    44.97\n",
            "| epoch  19 |   400/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.80 | ppl    44.52\n",
            "| epoch  19 |   600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.64 | ppl    38.10\n",
            "| epoch  19 |   800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.66 | ppl    38.93\n",
            "| epoch  19 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.71 | ppl    40.89\n",
            "| epoch  19 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.46 | loss  3.69 | ppl    39.91\n",
            "| epoch  19 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.69 | ppl    40.13\n",
            "| epoch  19 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.78 | ppl    43.85\n",
            "| epoch  19 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.71 | ppl    40.71\n",
            "| epoch  19 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.73 | ppl    41.87\n",
            "| epoch  19 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.63 | ppl    37.70\n",
            "| epoch  19 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.64 | ppl    37.95\n",
            "| epoch  19 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.69 | ppl    40.00\n",
            "| epoch  19 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.63 | ppl    37.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 77.93s | valid loss  5.40 | valid ppl   221.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 6.25e-05\n",
            "| epoch  20 |   200/ 2983 batches | lr 0.00 | ms/batch 25.56 | loss  3.82 | ppl    45.63\n",
            "| epoch  20 |   400/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.85 | ppl    47.11\n",
            "| epoch  20 |   600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.68 | ppl    39.83\n",
            "| epoch  20 |   800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.68 | ppl    39.56\n",
            "| epoch  20 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.47 | loss  3.74 | ppl    41.89\n",
            "| epoch  20 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.73 | ppl    41.83\n",
            "| epoch  20 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.51 | loss  3.73 | ppl    41.70\n",
            "| epoch  20 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.81 | ppl    45.13\n",
            "| epoch  20 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.73 | ppl    41.51\n",
            "| epoch  20 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.75 | ppl    42.41\n",
            "| epoch  20 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.67 | ppl    39.08\n",
            "| epoch  20 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.66 | ppl    38.91\n",
            "| epoch  20 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.69 | ppl    40.00\n",
            "| epoch  20 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.46 | loss  3.64 | ppl    38.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 77.98s | valid loss  5.33 | valid ppl   205.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 6.25e-05\n",
            "| epoch  21 |   200/ 2983 batches | lr 0.00 | ms/batch 25.64 | loss  3.82 | ppl    45.70\n",
            "| epoch  21 |   400/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.81 | ppl    45.09\n",
            "| epoch  21 |   600/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.67 | ppl    39.18\n",
            "| epoch  21 |   800/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.67 | ppl    39.08\n",
            "| epoch  21 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.72 | ppl    41.26\n",
            "| epoch  21 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.72 | ppl    41.06\n",
            "| epoch  21 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.72 | ppl    41.15\n",
            "| epoch  21 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.80 | ppl    44.52\n",
            "| epoch  21 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.71 | ppl    40.99\n",
            "| epoch  21 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.73 | ppl    41.70\n",
            "| epoch  21 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.64 | ppl    38.24\n",
            "| epoch  21 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.65 | ppl    38.35\n",
            "| epoch  21 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.50 | loss  3.69 | ppl    40.08\n",
            "| epoch  21 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.63 | ppl    37.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 77.96s | valid loss  5.34 | valid ppl   207.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 6.25e-05\n",
            "| epoch  22 |   200/ 2983 batches | lr 0.00 | ms/batch 25.56 | loss  3.81 | ppl    45.11\n",
            "| epoch  22 |   400/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.80 | ppl    44.80\n",
            "| epoch  22 |   600/ 2983 batches | lr 0.00 | ms/batch 25.46 | loss  3.64 | ppl    38.16\n",
            "| epoch  22 |   800/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.65 | ppl    38.62\n",
            "| epoch  22 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.70 | ppl    40.47\n",
            "| epoch  22 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.70 | ppl    40.28\n",
            "| epoch  22 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.71 | ppl    40.71\n",
            "| epoch  22 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.79 | ppl    44.08\n",
            "| epoch  22 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.70 | ppl    40.35\n",
            "| epoch  22 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.73 | ppl    41.62\n",
            "| epoch  22 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.64 | ppl    38.02\n",
            "| epoch  22 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.64 | ppl    37.94\n",
            "| epoch  22 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.67 | ppl    39.26\n",
            "| epoch  22 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.52 | loss  3.62 | ppl    37.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 77.93s | valid loss  5.34 | valid ppl   208.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 3.125e-05\n",
            "| epoch  23 |   200/ 2983 batches | lr 0.00 | ms/batch 25.58 | loss  3.83 | ppl    46.14\n",
            "| epoch  23 |   400/ 2983 batches | lr 0.00 | ms/batch 25.48 | loss  3.90 | ppl    49.20\n",
            "| epoch  23 |   600/ 2983 batches | lr 0.00 | ms/batch 25.46 | loss  3.72 | ppl    41.40\n",
            "| epoch  23 |   800/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.67 | ppl    39.44\n",
            "| epoch  23 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.76 | ppl    42.89\n",
            "| epoch  23 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.72 | ppl    41.35\n",
            "| epoch  23 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.76 | ppl    42.95\n",
            "| epoch  23 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.48 | loss  3.84 | ppl    46.43\n",
            "| epoch  23 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.77 | ppl    43.45\n",
            "| epoch  23 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.74 | ppl    42.29\n",
            "| epoch  23 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.70 | ppl    40.43\n",
            "| epoch  23 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.68 | ppl    39.64\n",
            "| epoch  23 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.70 | ppl    40.48\n",
            "| epoch  23 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.67 | ppl    39.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 77.93s | valid loss  5.32 | valid ppl   203.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 3.125e-05\n",
            "| epoch  24 |   200/ 2983 batches | lr 0.00 | ms/batch 25.52 | loss  3.83 | ppl    46.01\n",
            "| epoch  24 |   400/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.88 | ppl    48.20\n",
            "| epoch  24 |   600/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.70 | ppl    40.27\n",
            "| epoch  24 |   800/ 2983 batches | lr 0.00 | ms/batch 25.48 | loss  3.67 | ppl    39.15\n",
            "| epoch  24 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.75 | ppl    42.58\n",
            "| epoch  24 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.71 | ppl    40.89\n",
            "| epoch  24 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.75 | ppl    42.64\n",
            "| epoch  24 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.83 | ppl    46.09\n",
            "| epoch  24 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.75 | ppl    42.33\n",
            "| epoch  24 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.75 | ppl    42.44\n",
            "| epoch  24 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.67 | ppl    39.13\n",
            "| epoch  24 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.68 | ppl    39.47\n",
            "| epoch  24 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.69 | ppl    40.13\n",
            "| epoch  24 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.65 | ppl    38.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 77.92s | valid loss  5.31 | valid ppl   201.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 3.125e-05\n",
            "| epoch  25 |   200/ 2983 batches | lr 0.00 | ms/batch 25.53 | loss  3.83 | ppl    46.20\n",
            "| epoch  25 |   400/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.86 | ppl    47.59\n",
            "| epoch  25 |   600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.69 | ppl    40.18\n",
            "| epoch  25 |   800/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.66 | ppl    38.88\n",
            "| epoch  25 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.74 | ppl    42.23\n",
            "| epoch  25 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.71 | ppl    40.96\n",
            "| epoch  25 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.73 | ppl    41.72\n",
            "| epoch  25 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.83 | ppl    45.90\n",
            "| epoch  25 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.73 | ppl    41.79\n",
            "| epoch  25 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.74 | ppl    42.13\n",
            "| epoch  25 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.46 | loss  3.65 | ppl    38.63\n",
            "| epoch  25 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.68 | ppl    39.48\n",
            "| epoch  25 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.68 | ppl    39.84\n",
            "| epoch  25 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.65 | ppl    38.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 77.92s | valid loss  5.30 | valid ppl   200.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 1.5625e-05\n",
            "| epoch  26 |   200/ 2983 batches | lr 0.00 | ms/batch 25.56 | loss  3.88 | ppl    48.33\n",
            "| epoch  26 |   400/ 2983 batches | lr 0.00 | ms/batch 25.48 | loss  3.97 | ppl    52.87\n",
            "| epoch  26 |   600/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.72 | ppl    41.37\n",
            "| epoch  26 |   800/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.73 | ppl    41.61\n",
            "| epoch  26 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.89 | ppl    49.04\n",
            "| epoch  26 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.75 | ppl    42.69\n",
            "| epoch  26 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.80 | ppl    44.66\n",
            "| epoch  26 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.93 | ppl    50.70\n",
            "| epoch  26 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.77 | ppl    43.30\n",
            "| epoch  26 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.81 | ppl    45.26\n",
            "| epoch  26 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.72 | ppl    41.34\n",
            "| epoch  26 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.69 | ppl    39.91\n",
            "| epoch  26 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.73 | ppl    41.62\n",
            "| epoch  26 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.70 | ppl    40.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 77.90s | valid loss  5.27 | valid ppl   193.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 1.5625e-05\n",
            "| epoch  27 |   200/ 2983 batches | lr 0.00 | ms/batch 25.48 | loss  3.88 | ppl    48.36\n",
            "| epoch  27 |   400/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.95 | ppl    51.96\n",
            "| epoch  27 |   600/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.72 | ppl    41.15\n",
            "| epoch  27 |   800/ 2983 batches | lr 0.00 | ms/batch 25.31 | loss  3.73 | ppl    41.75\n",
            "| epoch  27 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.31 | loss  3.85 | ppl    46.89\n",
            "| epoch  27 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.75 | ppl    42.48\n",
            "| epoch  27 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.80 | ppl    44.92\n",
            "| epoch  27 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.91 | ppl    50.09\n",
            "| epoch  27 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.76 | ppl    43.12\n",
            "| epoch  27 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.80 | ppl    44.64\n",
            "| epoch  27 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.71 | ppl    40.87\n",
            "| epoch  27 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.69 | ppl    39.91\n",
            "| epoch  27 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.73 | ppl    41.68\n",
            "| epoch  27 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.70 | ppl    40.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 77.79s | valid loss  5.26 | valid ppl   192.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 1.5625e-05\n",
            "| epoch  28 |   200/ 2983 batches | lr 0.00 | ms/batch 25.51 | loss  3.87 | ppl    48.01\n",
            "| epoch  28 |   400/ 2983 batches | lr 0.00 | ms/batch 25.46 | loss  3.93 | ppl    50.97\n",
            "| epoch  28 |   600/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.70 | ppl    40.36\n",
            "| epoch  28 |   800/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.71 | ppl    40.93\n",
            "| epoch  28 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.47 | loss  3.83 | ppl    46.25\n",
            "| epoch  28 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.74 | ppl    42.05\n",
            "| epoch  28 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.81 | ppl    45.04\n",
            "| epoch  28 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.90 | ppl    49.60\n",
            "| epoch  28 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.75 | ppl    42.56\n",
            "| epoch  28 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.77 | ppl    43.52\n",
            "| epoch  28 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.69 | ppl    40.06\n",
            "| epoch  28 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.68 | ppl    39.49\n",
            "| epoch  28 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.72 | ppl    41.35\n",
            "| epoch  28 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.48 | loss  3.70 | ppl    40.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 77.94s | valid loss  5.26 | valid ppl   192.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 1.5625e-05\n",
            "| epoch  29 |   200/ 2983 batches | lr 0.00 | ms/batch 25.57 | loss  3.87 | ppl    47.74\n",
            "| epoch  29 |   400/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.92 | ppl    50.25\n",
            "| epoch  29 |   600/ 2983 batches | lr 0.00 | ms/batch 25.47 | loss  3.69 | ppl    40.23\n",
            "| epoch  29 |   800/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.71 | ppl    40.79\n",
            "| epoch  29 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.47 | loss  3.82 | ppl    45.80\n",
            "| epoch  29 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.73 | ppl    41.55\n",
            "| epoch  29 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.80 | ppl    44.68\n",
            "| epoch  29 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.47 | loss  3.92 | ppl    50.34\n",
            "| epoch  29 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.75 | ppl    42.55\n",
            "| epoch  29 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.77 | ppl    43.21\n",
            "| epoch  29 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.71 | ppl    40.73\n",
            "| epoch  29 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.48 | loss  3.67 | ppl    39.36\n",
            "| epoch  29 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.46 | loss  3.72 | ppl    41.20\n",
            "| epoch  29 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.68 | ppl    39.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 78.01s | valid loss  5.26 | valid ppl   191.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 1.5625e-05\n",
            "| epoch  30 |   200/ 2983 batches | lr 0.00 | ms/batch 25.49 | loss  3.85 | ppl    46.99\n",
            "| epoch  30 |   400/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.92 | ppl    50.24\n",
            "| epoch  30 |   600/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.69 | ppl    40.00\n",
            "| epoch  30 |   800/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.71 | ppl    41.06\n",
            "| epoch  30 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.78 | ppl    44.03\n",
            "| epoch  30 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.72 | ppl    41.37\n",
            "| epoch  30 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.80 | ppl    44.59\n",
            "| epoch  30 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.91 | ppl    49.94\n",
            "| epoch  30 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.74 | ppl    41.98\n",
            "| epoch  30 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.76 | ppl    42.95\n",
            "| epoch  30 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.67 | ppl    39.31\n",
            "| epoch  30 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.67 | ppl    39.08\n",
            "| epoch  30 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.71 | ppl    40.77\n",
            "| epoch  30 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.67 | ppl    39.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 77.86s | valid loss  5.25 | valid ppl   190.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 1.5625e-05\n",
            "| epoch  31 |   200/ 2983 batches | lr 0.00 | ms/batch 25.56 | loss  3.85 | ppl    46.84\n",
            "| epoch  31 |   400/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.92 | ppl    50.46\n",
            "| epoch  31 |   600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.69 | ppl    39.88\n",
            "| epoch  31 |   800/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.71 | ppl    40.87\n",
            "| epoch  31 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.77 | ppl    43.48\n",
            "| epoch  31 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.71 | ppl    40.91\n",
            "| epoch  31 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.79 | ppl    44.32\n",
            "| epoch  31 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.91 | ppl    50.06\n",
            "| epoch  31 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.74 | ppl    42.04\n",
            "| epoch  31 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.49 | loss  3.74 | ppl    42.17\n",
            "| epoch  31 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.67 | ppl    39.29\n",
            "| epoch  31 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.66 | ppl    38.72\n",
            "| epoch  31 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.71 | ppl    40.66\n",
            "| epoch  31 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.34 | loss  3.67 | ppl    39.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 77.83s | valid loss  5.25 | valid ppl   191.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 1.5625e-05\n",
            "| epoch  32 |   200/ 2983 batches | lr 0.00 | ms/batch 25.49 | loss  3.84 | ppl    46.30\n",
            "| epoch  32 |   400/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.90 | ppl    49.30\n",
            "| epoch  32 |   600/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.68 | ppl    39.52\n",
            "| epoch  32 |   800/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.68 | ppl    39.80\n",
            "| epoch  32 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.75 | ppl    42.34\n",
            "| epoch  32 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.74 | ppl    41.89\n",
            "| epoch  32 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.79 | ppl    44.29\n",
            "| epoch  32 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.90 | ppl    49.33\n",
            "| epoch  32 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.72 | ppl    41.29\n",
            "| epoch  32 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.74 | ppl    41.90\n",
            "| epoch  32 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.67 | ppl    39.24\n",
            "| epoch  32 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.65 | ppl    38.64\n",
            "| epoch  32 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.70 | ppl    40.53\n",
            "| epoch  32 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.67 | ppl    39.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 77.85s | valid loss  5.26 | valid ppl   191.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 1.5625e-05\n",
            "| epoch  33 |   200/ 2983 batches | lr 0.00 | ms/batch 25.50 | loss  3.83 | ppl    46.15\n",
            "| epoch  33 |   400/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.89 | ppl    49.02\n",
            "| epoch  33 |   600/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.67 | ppl    39.08\n",
            "| epoch  33 |   800/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.68 | ppl    39.76\n",
            "| epoch  33 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.75 | ppl    42.36\n",
            "| epoch  33 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.72 | ppl    41.43\n",
            "| epoch  33 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.79 | ppl    44.32\n",
            "| epoch  33 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.90 | ppl    49.62\n",
            "| epoch  33 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.71 | ppl    41.05\n",
            "| epoch  33 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.47 | loss  3.72 | ppl    41.18\n",
            "| epoch  33 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.66 | ppl    38.67\n",
            "| epoch  33 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.65 | ppl    38.39\n",
            "| epoch  33 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.70 | ppl    40.33\n",
            "| epoch  33 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.47 | loss  3.66 | ppl    38.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 77.89s | valid loss  5.25 | valid ppl   190.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 1.5625e-05\n",
            "| epoch  34 |   200/ 2983 batches | lr 0.00 | ms/batch 25.50 | loss  3.82 | ppl    45.70\n",
            "| epoch  34 |   400/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.89 | ppl    49.01\n",
            "| epoch  34 |   600/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.67 | ppl    39.10\n",
            "| epoch  34 |   800/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.67 | ppl    39.33\n",
            "| epoch  34 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.75 | ppl    42.55\n",
            "| epoch  34 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.71 | ppl    40.92\n",
            "| epoch  34 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.79 | ppl    44.04\n",
            "| epoch  34 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.90 | ppl    49.20\n",
            "| epoch  34 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.71 | ppl    40.97\n",
            "| epoch  34 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.72 | ppl    41.43\n",
            "| epoch  34 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.66 | ppl    38.94\n",
            "| epoch  34 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.64 | ppl    38.06\n",
            "| epoch  34 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.70 | ppl    40.36\n",
            "| epoch  34 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.65 | ppl    38.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 77.85s | valid loss  5.25 | valid ppl   191.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 1.5625e-05\n",
            "| epoch  35 |   200/ 2983 batches | lr 0.00 | ms/batch 25.52 | loss  3.81 | ppl    45.36\n",
            "| epoch  35 |   400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.86 | ppl    47.43\n",
            "| epoch  35 |   600/ 2983 batches | lr 0.00 | ms/batch 25.47 | loss  3.67 | ppl    39.14\n",
            "| epoch  35 |   800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.67 | ppl    39.42\n",
            "| epoch  35 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.74 | ppl    42.10\n",
            "| epoch  35 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.34 | loss  3.71 | ppl    40.71\n",
            "| epoch  35 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.34 | loss  3.78 | ppl    43.65\n",
            "| epoch  35 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.91 | ppl    49.68\n",
            "| epoch  35 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.71 | ppl    40.98\n",
            "| epoch  35 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.71 | ppl    40.74\n",
            "| epoch  35 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.65 | ppl    38.43\n",
            "| epoch  35 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.64 | ppl    37.91\n",
            "| epoch  35 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.69 | ppl    40.01\n",
            "| epoch  35 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.65 | ppl    38.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 77.85s | valid loss  5.25 | valid ppl   191.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 7.8125e-06\n",
            "| epoch  36 |   200/ 2983 batches | lr 0.00 | ms/batch 25.50 | loss  3.85 | ppl    46.85\n",
            "| epoch  36 |   400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.93 | ppl    50.80\n",
            "| epoch  36 |   600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.75 | ppl    42.63\n",
            "| epoch  36 |   800/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.73 | ppl    41.76\n",
            "| epoch  36 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.80 | ppl    44.82\n",
            "| epoch  36 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.80 | ppl    44.50\n",
            "| epoch  36 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.85 | ppl    46.90\n",
            "| epoch  36 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.90 | ppl    49.62\n",
            "| epoch  36 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.77 | ppl    43.59\n",
            "| epoch  36 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.78 | ppl    43.82\n",
            "| epoch  36 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.50 | loss  3.71 | ppl    40.91\n",
            "| epoch  36 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.71 | ppl    40.85\n",
            "| epoch  36 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.71 | ppl    40.75\n",
            "| epoch  36 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.70 | ppl    40.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 77.88s | valid loss  5.23 | valid ppl   186.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 7.8125e-06\n",
            "| epoch  37 |   200/ 2983 batches | lr 0.00 | ms/batch 25.53 | loss  3.92 | ppl    50.58\n",
            "| epoch  37 |   400/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.90 | ppl    49.18\n",
            "| epoch  37 |   600/ 2983 batches | lr 0.00 | ms/batch 25.34 | loss  3.73 | ppl    41.77\n",
            "| epoch  37 |   800/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.73 | ppl    41.83\n",
            "| epoch  37 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.80 | ppl    44.77\n",
            "| epoch  37 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.77 | ppl    43.50\n",
            "| epoch  37 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.32 | loss  3.89 | ppl    48.78\n",
            "| epoch  37 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.90 | ppl    49.35\n",
            "| epoch  37 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.79 | ppl    44.38\n",
            "| epoch  37 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.77 | ppl    43.52\n",
            "| epoch  37 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.33 | loss  3.70 | ppl    40.56\n",
            "| epoch  37 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.69 | ppl    40.11\n",
            "| epoch  37 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.71 | ppl    41.01\n",
            "| epoch  37 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.69 | ppl    40.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 77.77s | valid loss  5.22 | valid ppl   185.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 7.8125e-06\n",
            "| epoch  38 |   200/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.89 | ppl    49.14\n",
            "| epoch  38 |   400/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.91 | ppl    50.01\n",
            "| epoch  38 |   600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.71 | ppl    40.81\n",
            "| epoch  38 |   800/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.73 | ppl    41.64\n",
            "| epoch  38 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.79 | ppl    44.14\n",
            "| epoch  38 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.75 | ppl    42.73\n",
            "| epoch  38 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.89 | ppl    49.09\n",
            "| epoch  38 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.34 | loss  3.89 | ppl    48.99\n",
            "| epoch  38 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.78 | ppl    43.86\n",
            "| epoch  38 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.77 | ppl    43.32\n",
            "| epoch  38 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.72 | ppl    41.17\n",
            "| epoch  38 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.68 | ppl    39.84\n",
            "| epoch  38 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.72 | ppl    41.23\n",
            "| epoch  38 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.70 | ppl    40.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 77.83s | valid loss  5.22 | valid ppl   185.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 7.8125e-06\n",
            "| epoch  39 |   200/ 2983 batches | lr 0.00 | ms/batch 25.50 | loss  3.90 | ppl    49.59\n",
            "| epoch  39 |   400/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.89 | ppl    48.68\n",
            "| epoch  39 |   600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.71 | ppl    40.73\n",
            "| epoch  39 |   800/ 2983 batches | lr 0.00 | ms/batch 25.33 | loss  3.74 | ppl    42.00\n",
            "| epoch  39 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.80 | ppl    44.62\n",
            "| epoch  39 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.74 | ppl    42.11\n",
            "| epoch  39 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.91 | ppl    49.69\n",
            "| epoch  39 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.89 | ppl    48.82\n",
            "| epoch  39 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.78 | ppl    43.86\n",
            "| epoch  39 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.77 | ppl    43.44\n",
            "| epoch  39 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.71 | ppl    40.92\n",
            "| epoch  39 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.68 | ppl    39.75\n",
            "| epoch  39 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.71 | ppl    40.84\n",
            "| epoch  39 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.70 | ppl    40.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 77.84s | valid loss  5.22 | valid ppl   185.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 7.8125e-06\n",
            "| epoch  40 |   200/ 2983 batches | lr 0.00 | ms/batch 25.60 | loss  3.87 | ppl    48.11\n",
            "| epoch  40 |   400/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.89 | ppl    48.69\n",
            "| epoch  40 |   600/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.70 | ppl    40.60\n",
            "| epoch  40 |   800/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.73 | ppl    41.61\n",
            "| epoch  40 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.78 | ppl    43.75\n",
            "| epoch  40 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.75 | ppl    42.34\n",
            "| epoch  40 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.90 | ppl    49.46\n",
            "| epoch  40 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.87 | ppl    48.04\n",
            "| epoch  40 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.77 | ppl    43.41\n",
            "| epoch  40 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.76 | ppl    42.82\n",
            "| epoch  40 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.71 | ppl    40.95\n",
            "| epoch  40 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.68 | ppl    39.45\n",
            "| epoch  40 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.71 | ppl    41.00\n",
            "| epoch  40 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.70 | ppl    40.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 77.82s | valid loss  5.22 | valid ppl   184.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 7.8125e-06\n",
            "| epoch  41 |   200/ 2983 batches | lr 0.00 | ms/batch 25.50 | loss  3.89 | ppl    48.96\n",
            "| epoch  41 |   400/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.85 | ppl    47.04\n",
            "| epoch  41 |   600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.70 | ppl    40.54\n",
            "| epoch  41 |   800/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.72 | ppl    41.36\n",
            "| epoch  41 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.77 | ppl    43.60\n",
            "| epoch  41 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.73 | ppl    41.58\n",
            "| epoch  41 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.92 | ppl    50.18\n",
            "| epoch  41 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.87 | ppl    48.06\n",
            "| epoch  41 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.78 | ppl    43.85\n",
            "| epoch  41 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.75 | ppl    42.65\n",
            "| epoch  41 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.70 | ppl    40.47\n",
            "| epoch  41 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.67 | ppl    39.09\n",
            "| epoch  41 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.71 | ppl    40.93\n",
            "| epoch  41 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.70 | ppl    40.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 77.85s | valid loss  5.22 | valid ppl   184.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 7.8125e-06\n",
            "| epoch  42 |   200/ 2983 batches | lr 0.00 | ms/batch 25.48 | loss  3.89 | ppl    48.78\n",
            "| epoch  42 |   400/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.89 | ppl    49.05\n",
            "| epoch  42 |   600/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.68 | ppl    39.63\n",
            "| epoch  42 |   800/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.74 | ppl    41.98\n",
            "| epoch  42 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.77 | ppl    43.57\n",
            "| epoch  42 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.72 | ppl    41.28\n",
            "| epoch  42 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.91 | ppl    50.07\n",
            "| epoch  42 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.87 | ppl    47.95\n",
            "| epoch  42 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.78 | ppl    43.68\n",
            "| epoch  42 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.74 | ppl    41.90\n",
            "| epoch  42 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.71 | ppl    40.72\n",
            "| epoch  42 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.66 | ppl    38.91\n",
            "| epoch  42 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.71 | ppl    40.79\n",
            "| epoch  42 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.69 | ppl    40.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 77.85s | valid loss  5.21 | valid ppl   183.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 7.8125e-06\n",
            "| epoch  43 |   200/ 2983 batches | lr 0.00 | ms/batch 25.48 | loss  3.87 | ppl    48.11\n",
            "| epoch  43 |   400/ 2983 batches | lr 0.00 | ms/batch 25.32 | loss  3.89 | ppl    48.94\n",
            "| epoch  43 |   600/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.68 | ppl    39.56\n",
            "| epoch  43 |   800/ 2983 batches | lr 0.00 | ms/batch 25.34 | loss  3.73 | ppl    41.62\n",
            "| epoch  43 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.76 | ppl    42.85\n",
            "| epoch  43 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.72 | ppl    41.34\n",
            "| epoch  43 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.92 | ppl    50.58\n",
            "| epoch  43 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.86 | ppl    47.36\n",
            "| epoch  43 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.33 | loss  3.76 | ppl    43.06\n",
            "| epoch  43 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.74 | ppl    42.10\n",
            "| epoch  43 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.71 | ppl    40.68\n",
            "| epoch  43 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.67 | ppl    39.31\n",
            "| epoch  43 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.70 | ppl    40.64\n",
            "| epoch  43 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.69 | ppl    39.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 77.78s | valid loss  5.22 | valid ppl   184.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 7.8125e-06\n",
            "| epoch  44 |   200/ 2983 batches | lr 0.00 | ms/batch 25.49 | loss  3.86 | ppl    47.61\n",
            "| epoch  44 |   400/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.89 | ppl    48.96\n",
            "| epoch  44 |   600/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.66 | ppl    39.01\n",
            "| epoch  44 |   800/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.71 | ppl    40.89\n",
            "| epoch  44 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.78 | ppl    43.82\n",
            "| epoch  44 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.72 | ppl    41.10\n",
            "| epoch  44 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.93 | ppl    50.83\n",
            "| epoch  44 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.86 | ppl    47.45\n",
            "| epoch  44 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.78 | ppl    43.63\n",
            "| epoch  44 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.74 | ppl    41.95\n",
            "| epoch  44 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.71 | ppl    40.85\n",
            "| epoch  44 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.66 | ppl    38.67\n",
            "| epoch  44 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.33 | loss  3.70 | ppl    40.35\n",
            "| epoch  44 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.68 | ppl    39.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 77.79s | valid loss  5.22 | valid ppl   185.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 7.8125e-06\n",
            "| epoch  45 |   200/ 2983 batches | lr 0.00 | ms/batch 25.49 | loss  3.84 | ppl    46.69\n",
            "| epoch  45 |   400/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.90 | ppl    49.54\n",
            "| epoch  45 |   600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.67 | ppl    39.29\n",
            "| epoch  45 |   800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.72 | ppl    41.15\n",
            "| epoch  45 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.76 | ppl    42.86\n",
            "| epoch  45 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.72 | ppl    41.10\n",
            "| epoch  45 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.93 | ppl    51.01\n",
            "| epoch  45 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.86 | ppl    47.50\n",
            "| epoch  45 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.76 | ppl    43.10\n",
            "| epoch  45 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.73 | ppl    41.66\n",
            "| epoch  45 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.70 | ppl    40.47\n",
            "| epoch  45 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.65 | ppl    38.51\n",
            "| epoch  45 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.70 | ppl    40.38\n",
            "| epoch  45 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.68 | ppl    39.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 77.81s | valid loss  5.22 | valid ppl   184.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 7.8125e-06\n",
            "| epoch  46 |   200/ 2983 batches | lr 0.00 | ms/batch 25.47 | loss  3.85 | ppl    46.91\n",
            "| epoch  46 |   400/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.88 | ppl    48.29\n",
            "| epoch  46 |   600/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.67 | ppl    39.14\n",
            "| epoch  46 |   800/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.71 | ppl    40.92\n",
            "| epoch  46 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.34 | loss  3.76 | ppl    42.85\n",
            "| epoch  46 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.32 | loss  3.71 | ppl    41.03\n",
            "| epoch  46 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.93 | ppl    50.92\n",
            "| epoch  46 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.86 | ppl    47.66\n",
            "| epoch  46 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.77 | ppl    43.33\n",
            "| epoch  46 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.72 | ppl    41.27\n",
            "| epoch  46 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.70 | ppl    40.60\n",
            "| epoch  46 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.65 | ppl    38.46\n",
            "| epoch  46 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.70 | ppl    40.58\n",
            "| epoch  46 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.66 | ppl    39.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 77.79s | valid loss  5.22 | valid ppl   184.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 7.8125e-06\n",
            "| epoch  47 |   200/ 2983 batches | lr 0.00 | ms/batch 25.54 | loss  3.87 | ppl    47.71\n",
            "| epoch  47 |   400/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.89 | ppl    48.77\n",
            "| epoch  47 |   600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.66 | ppl    38.84\n",
            "| epoch  47 |   800/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.72 | ppl    41.26\n",
            "| epoch  47 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.76 | ppl    42.77\n",
            "| epoch  47 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.71 | ppl    40.73\n",
            "| epoch  47 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.48 | loss  3.93 | ppl    50.95\n",
            "| epoch  47 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.46 | loss  3.85 | ppl    46.95\n",
            "| epoch  47 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.76 | ppl    42.83\n",
            "| epoch  47 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.73 | ppl    41.81\n",
            "| epoch  47 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.69 | ppl    39.92\n",
            "| epoch  47 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.65 | ppl    38.34\n",
            "| epoch  47 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.70 | ppl    40.34\n",
            "| epoch  47 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.68 | ppl    39.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 77.88s | valid loss  5.22 | valid ppl   184.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 7.8125e-06\n",
            "| epoch  48 |   200/ 2983 batches | lr 0.00 | ms/batch 25.51 | loss  3.85 | ppl    47.05\n",
            "| epoch  48 |   400/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.89 | ppl    48.94\n",
            "| epoch  48 |   600/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.66 | ppl    38.72\n",
            "| epoch  48 |   800/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.72 | ppl    41.11\n",
            "| epoch  48 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.34 | loss  3.76 | ppl    42.90\n",
            "| epoch  48 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.34 | loss  3.71 | ppl    40.72\n",
            "| epoch  48 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.93 | ppl    50.75\n",
            "| epoch  48 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.85 | ppl    46.91\n",
            "| epoch  48 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.75 | ppl    42.64\n",
            "| epoch  48 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.72 | ppl    41.06\n",
            "| epoch  48 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.70 | ppl    40.36\n",
            "| epoch  48 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.66 | ppl    38.76\n",
            "| epoch  48 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.33 | loss  3.69 | ppl    40.12\n",
            "| epoch  48 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.68 | ppl    39.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 77.83s | valid loss  5.23 | valid ppl   186.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 3.90625e-06\n",
            "| epoch  49 |   200/ 2983 batches | lr 0.00 | ms/batch 25.51 | loss  3.90 | ppl    49.44\n",
            "| epoch  49 |   400/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.97 | ppl    53.03\n",
            "| epoch  49 |   600/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.82 | ppl    45.57\n",
            "| epoch  49 |   800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.76 | ppl    42.89\n",
            "| epoch  49 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.84 | ppl    46.30\n",
            "| epoch  49 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.80 | ppl    44.56\n",
            "| epoch  49 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.98 | ppl    53.43\n",
            "| epoch  49 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.90 | ppl    49.18\n",
            "| epoch  49 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.78 | ppl    43.88\n",
            "| epoch  49 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.77 | ppl    43.40\n",
            "| epoch  49 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.73 | ppl    41.59\n",
            "| epoch  49 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.74 | ppl    42.07\n",
            "| epoch  49 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.72 | ppl    41.40\n",
            "| epoch  49 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.68 | ppl    39.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 77.85s | valid loss  5.18 | valid ppl   178.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 3.90625e-06\n",
            "| epoch  50 |   200/ 2983 batches | lr 0.00 | ms/batch 25.47 | loss  3.94 | ppl    51.18\n",
            "| epoch  50 |   400/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.95 | ppl    51.79\n",
            "| epoch  50 |   600/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.75 | ppl    42.44\n",
            "| epoch  50 |   800/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.75 | ppl    42.31\n",
            "| epoch  50 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.31 | loss  3.84 | ppl    46.49\n",
            "| epoch  50 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.32 | loss  3.78 | ppl    43.63\n",
            "| epoch  50 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.97 | ppl    53.07\n",
            "| epoch  50 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.91 | ppl    50.10\n",
            "| epoch  50 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.79 | ppl    44.28\n",
            "| epoch  50 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.76 | ppl    42.94\n",
            "| epoch  50 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.73 | ppl    41.51\n",
            "| epoch  50 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.72 | ppl    41.32\n",
            "| epoch  50 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.72 | ppl    41.11\n",
            "| epoch  50 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.33 | loss  3.70 | ppl    40.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 77.76s | valid loss  5.18 | valid ppl   178.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 3.90625e-06\n",
            "| epoch  51 |   200/ 2983 batches | lr 0.00 | ms/batch 25.50 | loss  3.93 | ppl    50.93\n",
            "| epoch  51 |   400/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.94 | ppl    51.51\n",
            "| epoch  51 |   600/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.73 | ppl    41.88\n",
            "| epoch  51 |   800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.75 | ppl    42.54\n",
            "| epoch  51 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.84 | ppl    46.69\n",
            "| epoch  51 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.76 | ppl    42.97\n",
            "| epoch  51 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.98 | ppl    53.26\n",
            "| epoch  51 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.91 | ppl    50.12\n",
            "| epoch  51 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.79 | ppl    44.42\n",
            "| epoch  51 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.76 | ppl    42.93\n",
            "| epoch  51 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.34 | loss  3.72 | ppl    41.15\n",
            "| epoch  51 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.71 | ppl    40.82\n",
            "| epoch  51 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.71 | ppl    40.79\n",
            "| epoch  51 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.34 | loss  3.70 | ppl    40.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 77.80s | valid loss  5.18 | valid ppl   177.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 3.90625e-06\n",
            "| epoch  52 |   200/ 2983 batches | lr 0.00 | ms/batch 25.50 | loss  3.92 | ppl    50.55\n",
            "| epoch  52 |   400/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.93 | ppl    50.99\n",
            "| epoch  52 |   600/ 2983 batches | lr 0.00 | ms/batch 25.34 | loss  3.72 | ppl    41.43\n",
            "| epoch  52 |   800/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.74 | ppl    42.28\n",
            "| epoch  52 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.83 | ppl    45.95\n",
            "| epoch  52 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.78 | ppl    43.65\n",
            "| epoch  52 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.98 | ppl    53.47\n",
            "| epoch  52 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.91 | ppl    49.72\n",
            "| epoch  52 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.78 | ppl    43.91\n",
            "| epoch  52 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.76 | ppl    42.80\n",
            "| epoch  52 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.72 | ppl    41.35\n",
            "| epoch  52 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.71 | ppl    40.92\n",
            "| epoch  52 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.71 | ppl    40.70\n",
            "| epoch  52 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.70 | ppl    40.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 77.85s | valid loss  5.18 | valid ppl   176.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 3.90625e-06\n",
            "| epoch  53 |   200/ 2983 batches | lr 0.00 | ms/batch 25.49 | loss  3.93 | ppl    50.74\n",
            "| epoch  53 |   400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.93 | ppl    51.01\n",
            "| epoch  53 |   600/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.72 | ppl    41.32\n",
            "| epoch  53 |   800/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.76 | ppl    42.83\n",
            "| epoch  53 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.83 | ppl    46.13\n",
            "| epoch  53 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.76 | ppl    42.74\n",
            "| epoch  53 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.99 | ppl    53.93\n",
            "| epoch  53 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.92 | ppl    50.15\n",
            "| epoch  53 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.78 | ppl    43.97\n",
            "| epoch  53 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.77 | ppl    43.29\n",
            "| epoch  53 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.71 | ppl    40.82\n",
            "| epoch  53 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.52 | loss  3.70 | ppl    40.39\n",
            "| epoch  53 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.70 | ppl    40.26\n",
            "| epoch  53 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.50 | loss  3.69 | ppl    40.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 77.93s | valid loss  5.18 | valid ppl   177.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 3.90625e-06\n",
            "| epoch  54 |   200/ 2983 batches | lr 0.00 | ms/batch 25.52 | loss  3.92 | ppl    50.45\n",
            "| epoch  54 |   400/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.93 | ppl    51.02\n",
            "| epoch  54 |   600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.71 | ppl    40.89\n",
            "| epoch  54 |   800/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.75 | ppl    42.42\n",
            "| epoch  54 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.85 | ppl    46.88\n",
            "| epoch  54 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.74 | ppl    42.29\n",
            "| epoch  54 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  4.00 | ppl    54.60\n",
            "| epoch  54 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.91 | ppl    49.84\n",
            "| epoch  54 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.44 | loss  3.79 | ppl    44.25\n",
            "| epoch  54 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.77 | ppl    43.45\n",
            "| epoch  54 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.70 | ppl    40.48\n",
            "| epoch  54 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.69 | ppl    40.00\n",
            "| epoch  54 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.69 | ppl    40.04\n",
            "| epoch  54 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.70 | ppl    40.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 77.88s | valid loss  5.18 | valid ppl   178.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 3.90625e-06\n",
            "| epoch  55 |   200/ 2983 batches | lr 0.00 | ms/batch 25.56 | loss  3.91 | ppl    49.87\n",
            "| epoch  55 |   400/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.92 | ppl    50.64\n",
            "| epoch  55 |   600/ 2983 batches | lr 0.00 | ms/batch 25.49 | loss  3.71 | ppl    40.88\n",
            "| epoch  55 |   800/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.75 | ppl    42.56\n",
            "| epoch  55 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.47 | loss  3.82 | ppl    45.70\n",
            "| epoch  55 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.74 | ppl    41.97\n",
            "| epoch  55 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.99 | ppl    54.31\n",
            "| epoch  55 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.46 | loss  3.91 | ppl    50.00\n",
            "| epoch  55 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.79 | ppl    44.07\n",
            "| epoch  55 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.75 | ppl    42.54\n",
            "| epoch  55 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.70 | ppl    40.49\n",
            "| epoch  55 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.68 | ppl    39.72\n",
            "| epoch  55 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.70 | ppl    40.57\n",
            "| epoch  55 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.70 | ppl    40.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 77.91s | valid loss  5.18 | valid ppl   177.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 3.90625e-06\n",
            "| epoch  56 |   200/ 2983 batches | lr 0.00 | ms/batch 25.55 | loss  3.92 | ppl    50.63\n",
            "| epoch  56 |   400/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.92 | ppl    50.43\n",
            "| epoch  56 |   600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.70 | ppl    40.46\n",
            "| epoch  56 |   800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.75 | ppl    42.72\n",
            "| epoch  56 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.83 | ppl    46.04\n",
            "| epoch  56 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.74 | ppl    42.06\n",
            "| epoch  56 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  4.00 | ppl    54.65\n",
            "| epoch  56 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.91 | ppl    49.79\n",
            "| epoch  56 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.49 | loss  3.78 | ppl    43.74\n",
            "| epoch  56 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.75 | ppl    42.40\n",
            "| epoch  56 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.70 | ppl    40.58\n",
            "| epoch  56 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.70 | ppl    40.53\n",
            "| epoch  56 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.70 | ppl    40.31\n",
            "| epoch  56 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.70 | ppl    40.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 77.93s | valid loss  5.18 | valid ppl   178.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 3.90625e-06\n",
            "| epoch  57 |   200/ 2983 batches | lr 0.00 | ms/batch 25.52 | loss  3.91 | ppl    50.10\n",
            "| epoch  57 |   400/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.94 | ppl    51.64\n",
            "| epoch  57 |   600/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.70 | ppl    40.45\n",
            "| epoch  57 |   800/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.75 | ppl    42.38\n",
            "| epoch  57 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.84 | ppl    46.41\n",
            "| epoch  57 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.74 | ppl    41.92\n",
            "| epoch  57 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  4.00 | ppl    54.37\n",
            "| epoch  57 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.90 | ppl    49.38\n",
            "| epoch  57 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.79 | ppl    44.12\n",
            "| epoch  57 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.76 | ppl    42.82\n",
            "| epoch  57 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.70 | ppl    40.61\n",
            "| epoch  57 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.68 | ppl    39.76\n",
            "| epoch  57 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.68 | ppl    39.82\n",
            "| epoch  57 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.69 | ppl    40.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 77.85s | valid loss  5.18 | valid ppl   178.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 3.90625e-06\n",
            "| epoch  58 |   200/ 2983 batches | lr 0.00 | ms/batch 25.46 | loss  3.91 | ppl    49.81\n",
            "| epoch  58 |   400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.94 | ppl    51.53\n",
            "| epoch  58 |   600/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.70 | ppl    40.40\n",
            "| epoch  58 |   800/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.76 | ppl    42.94\n",
            "| epoch  58 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.81 | ppl    45.18\n",
            "| epoch  58 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.75 | ppl    42.37\n",
            "| epoch  58 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  4.00 | ppl    54.77\n",
            "| epoch  58 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.89 | ppl    49.15\n",
            "| epoch  58 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.78 | ppl    44.03\n",
            "| epoch  58 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.35 | loss  3.75 | ppl    42.34\n",
            "| epoch  58 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.71 | ppl    40.69\n",
            "| epoch  58 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.70 | ppl    40.27\n",
            "| epoch  58 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.69 | ppl    39.93\n",
            "| epoch  58 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.71 | ppl    40.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 77.82s | valid loss  5.18 | valid ppl   178.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 1.953125e-06\n",
            "| epoch  59 |   200/ 2983 batches | lr 0.00 | ms/batch 25.52 | loss  3.93 | ppl    50.84\n",
            "| epoch  59 |   400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.98 | ppl    53.59\n",
            "| epoch  59 |   600/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.92 | ppl    50.59\n",
            "| epoch  59 |   800/ 2983 batches | lr 0.00 | ms/batch 25.36 | loss  3.90 | ppl    49.41\n",
            "| epoch  59 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.87 | ppl    47.87\n",
            "| epoch  59 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.34 | loss  3.90 | ppl    49.28\n",
            "| epoch  59 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  4.04 | ppl    56.64\n",
            "| epoch  59 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.99 | ppl    54.03\n",
            "| epoch  59 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.47 | loss  3.88 | ppl    48.38\n",
            "| epoch  59 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.38 | loss  3.78 | ppl    43.77\n",
            "| epoch  59 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.73 | ppl    41.76\n",
            "| epoch  59 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.77 | ppl    43.33\n",
            "| epoch  59 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.46 | loss  3.76 | ppl    42.80\n",
            "| epoch  59 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.42 | loss  3.69 | ppl    39.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 77.88s | valid loss  5.18 | valid ppl   176.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 1.953125e-06\n",
            "| epoch  60 |   200/ 2983 batches | lr 0.00 | ms/batch 25.50 | loss  3.96 | ppl    52.24\n",
            "| epoch  60 |   400/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.98 | ppl    53.44\n",
            "| epoch  60 |   600/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.87 | ppl    48.04\n",
            "| epoch  60 |   800/ 2983 batches | lr 0.00 | ms/batch 25.37 | loss  3.86 | ppl    47.61\n",
            "| epoch  60 |  1000/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.87 | ppl    47.81\n",
            "| epoch  60 |  1200/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.91 | ppl    50.09\n",
            "| epoch  60 |  1400/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  4.01 | ppl    55.27\n",
            "| epoch  60 |  1600/ 2983 batches | lr 0.00 | ms/batch 25.41 | loss  3.96 | ppl    52.52\n",
            "| epoch  60 |  1800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.86 | ppl    47.38\n",
            "| epoch  60 |  2000/ 2983 batches | lr 0.00 | ms/batch 25.40 | loss  3.80 | ppl    44.70\n",
            "| epoch  60 |  2200/ 2983 batches | lr 0.00 | ms/batch 25.45 | loss  3.75 | ppl    42.48\n",
            "| epoch  60 |  2400/ 2983 batches | lr 0.00 | ms/batch 25.43 | loss  3.74 | ppl    42.20\n",
            "| epoch  60 |  2600/ 2983 batches | lr 0.00 | ms/batch 25.48 | loss  3.73 | ppl    41.87\n",
            "| epoch  60 |  2800/ 2983 batches | lr 0.00 | ms/batch 25.39 | loss  3.68 | ppl    39.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 77.89s | valid loss  5.17 | valid ppl   176.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "Current learning rate: 1.953125e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the best saved model with safe globals\n",
        "# safe_globals = [\n",
        "#     RNNModel,\n",
        "#     TransformerModel,\n",
        "#     PositionalEncoding,\n",
        "#     torch.nn.modules.dropout.Dropout,\n",
        "#     torch.nn.modules.linear.Linear,\n",
        "#     torch.nn.modules.rnn.GRU,\n",
        "#     torch.nn.modules.rnn.LSTM,\n",
        "#     torch.nn.modules.rnn.RNN,\n",
        "#     torch.nn.modules.sparse.Embedding,\n",
        "#     torch.nn.modules.transformer.TransformerEncoder,\n",
        "#     torch.nn.modules.transformer.TransformerEncoderLayer,\n",
        "#     torch.nn.modules.activation.MultiheadAttention,\n",
        "#     torch.nn.modules.linear.NonDynamicallyQuantizableLinear,\n",
        "#     torch.nn.modules.normalization.LayerNorm,\n",
        "#     torch.nn.functional.relu\n",
        "# ]\n",
        "\n",
        "# with torch.serialization.safe_globals(safe_globals):\n",
        "#     with open(args.save, 'rb') as f:\n",
        "#         model = torch.load(f, map_location=device)\n",
        "\n",
        "# # Run on test data\n",
        "# test_loss = evaluate(test_data)\n",
        "# print('=' * 89)\n",
        "# print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "#     test_loss, math.exp(test_loss)))\n",
        "# print('=' * 89)\n",
        "\n",
        "# # Step 6: Generate Text\n",
        "# checkpoint = 'model.pt'\n",
        "# outf = 'generated.txt'\n",
        "# words = 1000\n",
        "# temperature = 1.0\n",
        "# log_interval = 100\n",
        "# accel = True\n",
        "\n",
        "# torch.manual_seed(1111)\n",
        "\n",
        "# if accel and torch.cuda.is_available():\n",
        "#     device = torch.device(\"cuda\")\n",
        "# else:\n",
        "#     device = torch.device(\"cpu\")\n",
        "\n",
        "# with torch.serialization.safe_globals(safe_globals):\n",
        "#     with open(checkpoint, 'rb') as f:\n",
        "#         model = torch.load(f, map_location=device)\n",
        "# model.eval()\n",
        "\n",
        "# corpus = Corpus(args.data)\n",
        "# ntokens = len(corpus.dictionary)\n",
        "\n",
        "# is_transformer_model = hasattr(model, 'model_type') and model.model_type == 'Transformer'\n",
        "# if not is_transformer_model:\n",
        "#     hidden = model.init_hidden(1)\n",
        "# input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "# with open(outf, 'w') as outf:\n",
        "#     with torch.no_grad():\n",
        "#         for i in range(words):\n",
        "#             if is_transformer_model:\n",
        "#                 output = model(input, False)\n",
        "#                 word_weights = output[-1].squeeze().div(temperature).exp().cpu()\n",
        "#                 word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "#                 word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
        "#                 input = torch.cat([input, word_tensor], 0)\n",
        "#             else:\n",
        "#                 output, hidden = model(input, hidden)\n",
        "#                 word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "#                 word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "#                 input.fill_(word_idx)\n",
        "\n",
        "#             word = corpus.dictionary.idx2word[word_idx]\n",
        "#             if word == '@-@':\n",
        "#                 word = ' '\n",
        "#             outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "#             if i % log_interval == 0:\n",
        "#                 print('| Generated {}/{} words'.format(i, words))\n",
        "\n",
        "# # Print generated text\n",
        "# !cat generated.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hESqQ5xIetVC",
        "outputId": "2a762406-a926-4c62-f259-12bfd5878f4d"
      },
      "id": "hESqQ5xIetVC",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.09 | test ppl   162.30\n",
            "=========================================================================================\n",
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n",
            "to do not incorporate , there was a little written gate in the top seven episode . It usually included\n",
            "as Main   T 12 Artists of Fresh Superior del <unk> ; an alternate character service was called to 9th\n",
            "visual Christian Mission records . 5 of 05 . <eos> <eos> = = = Post   time and philosophy of\n",
            "the ballots = = = <eos> <eos> The proper German script was accomplished by German forces between the \" Soviet\n",
            "Republic \" . <eos> <eos> = = = Controversies = = = <eos> <eos> During the 1950s , a nursery\n",
            "objector conducted by Arthur Dooley of the Death   Roman <unk> in February , having stated that its fifth seasons\n",
            "of the Metro ranges through Paris . Bill Woodward was offered this to the current spotlight . The men joined\n",
            "the North Yorkshire Campaign near night to the Federal Army ( 1918 ) to prepare he 's possessions in the\n",
            "military , as the third federal state government halted . <eos> Soon after , <unk> the organization of Wu Peron\n",
            "had met his last decision by turning out . After these recording officials in the event , he gained a\n",
            "boot , but Belgian forces greeted it with tradition that he pushed under being under the poem , but she\n",
            "traveled back from doing the youngest First League to find . Lawrence was very gifted in baseball , and Richardson\n",
            "came to a professional wrestler , saying Early . Two months later , he was able to resort for the\n",
            "singing and quickly had a gross would be permitted at future . <eos> On January 29 , 2011 , in\n",
            "numerous forest programs and publishing , Cullen called him a well   known gag which was not interested in being\n",
            "lost . In doing a competitor , the company also ordered the female no   handed clause . In the\n",
            "first period , Sisler denied another \" open   down \" temperature records up himself if he tried to swap\n",
            "the \" Grand Prix \" all taking on the wicket . Despite opposing criticism and they again met the role\n",
            "and did not enter the field , she went on to run back and advanced to the horse . However\n",
            ", Wilbur did not see the removal of the experiment . Vice described the <unk> available for him 's shift\n",
            "of the tombs of the Lisbon background by collecting power and sell their peace <unk> . A second leg was\n",
            "placed on for a year in a new lap , but started wrestling after seamlessly and earned four provisional accidents\n",
            "in the title , indicating him fill a wildly   timed record for nine years . These also attempted to\n",
            "do anything . Then , he lost the Phillies and striking them across it inside all the to four tenths\n",
            ". He took three tour in the weeks earlier . <eos> The team died at the end of the first\n",
            "double round session on 21 March 1910 . After a successful <unk> on the sidelines , the ball 's son\n",
            "had changed the gap back , and would not become a hit growing out of the run ; the Jeremy\n",
            "Courier had been a patron named Richard for a man who was now with the <unk> by the prime minister\n",
            ". Though the Blue Jackets objected to his car , the team was vulnerable to smoke , but because he\n",
            "died . Anderson 's mortar range took over into the race , and did not occasionally pay the children ,\n",
            "including the intervention of him to <unk> to his friend , missing Gutirrez , by other international loyalty , including\n",
            "those having died when he did not want to allow the 2016 relation to the United States . <unk> was\n",
            "said to drop their remnants in the 1957  33 season , which was assured into Somerset 's home appearance\n",
            ". Aaron Johnstone Dominic <unk> , <unk> manager Nelson ( Baltimore ) run throughout stint in 1988  97 ,\n",
            "he became part of his first two Ferrari Ten Grand Prix in 1925 . <eos> During the following season ,\n",
            "the deal was called that his support were at a <unk> and fan damage , setting him out . Jardine\n",
            "and his wife fought into Owl , and Park was stay down . \" Despite his father on home ,\n",
            "he was the youngest who co   opted he was who had missed ahead and attracted ties from McLaren ,\n",
            "pitching went with Barbarian as a promising driver of abuse . \" His tactics led by a battle of the\n",
            "line period from the early 19th   century events instead of what he supported as the racist medium of <unk>\n",
            ". <eos> At the start of a team cycle remained , Herg was re   raised to safety in the\n",
            "Red Sea ( AIF ) shut to seasonal crime , as until he was a key , and he soon\n",
            "carries some concessions forever at the Glastonbury Bridge in 1975 . He was inducted down until his 2nd <unk> as\n",
            "her efforts to find his influence . After the World War II was first recorded again on December 26 ,\n",
            "1910 , the Tom Race trades was sent to a takeover in the general election in <unk> . <eos> <eos>\n",
            "= = = World War Z = = = <eos> <eos> <eos> = = = Early career and retirement =\n",
            "= = <eos> <eos> Returning in people started in February 1907 , O 'Malley realised that he was <unk> for\n",
            "his <unk> , and he never held his league pitch . Other accounts were published alongside friends , who died\n",
            "the world at camp . During the war , he contributed it to the teams after being the first same\n",
            "woman line . At Frank 's election , he put the ethnic convoys for three minutes , while he took\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MAIN PART"
      ],
      "metadata": {
        "id": "BSnSaA8FXKGj"
      },
      "id": "BSnSaA8FXKGj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Data Preparation for Custom Data (English-French dataset as example)\n",
        "# Extract French sentences and create train/valid/test.txt\n",
        "DATA_PATH = '/content/drive/MyDrive/data_word_train/custom'\n",
        "INPUT_FILE = '/content/drive/MyDrive/data/eng-fra.txt'\n",
        "os.makedirs(DATA_PATH, exist_ok=True)\n",
        "random.seed(1111)\n",
        "\n",
        "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "french_sentences = [line.strip().split('\\t')[1] for line in lines if len(line.strip().split('\\t')) == 2]\n",
        "\n",
        "random.shuffle(french_sentences)\n",
        "n = len(french_sentences)\n",
        "train_end = int(n * 0.8)\n",
        "valid_end = train_end + int(n * 0.1)\n",
        "train_data = french_sentences[:train_end]\n",
        "valid_data = french_sentences[train_end:valid_end]\n",
        "test_data = french_sentences[valid_end:]\n",
        "\n",
        "def save_sentences(sentences, filename):\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for sentence in sentences:\n",
        "            sentence = sentence.replace('.', ' .').replace(',', ' ,').replace('!', ' !').replace('?', ' ?')\n",
        "            f.write(sentence + ' <eos>\\n')\n",
        "\n",
        "save_sentences(train_data, os.path.join(DATA_PATH, 'train.txt'))\n",
        "save_sentences(valid_data, os.path.join(DATA_PATH, 'valid.txt'))\n",
        "save_sentences(test_data, os.path.join(DATA_PATH, 'test.txt'))\n",
        "\n",
        "print(f\"Created datasets: {len(train_data)} train, {len(valid_data)} valid, {len(test_data)} test sentences\")\n",
        "\n",
        "# Step 5: Training Function from main.py (adapted for Colab)\n",
        "# Define args as a class for Colab\n",
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.data = '/content/drive/MyDrive/data_word_train/custom'  # Custom data path\n",
        "        self.model = 'LSTM'  # RNN_TANH, RNN_RELU, LSTM, GRU, Transformer\n",
        "        self.emsize = 200\n",
        "        self.nhid = 200\n",
        "        self.nlayers = 2\n",
        "        self.lr = 0.001\n",
        "        self.clip = 0.25\n",
        "        self.epochs = 20  # Reduced for faster training\n",
        "        self.batch_size = 20\n",
        "        self.bptt = 35\n",
        "        self.dropout = 0.2\n",
        "        self.tied = False\n",
        "        self.seed = 1111\n",
        "        self.log_interval = 200\n",
        "        self.save = 'model.pt'\n",
        "        self.onnx_export = ''\n",
        "        self.nhead = 2\n",
        "        self.dry_run = False\n",
        "        self.accel = True\n",
        "        self.use_optimizer = True  # Use AdamW\n",
        "\n",
        "args = Args()\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "if args.accel and torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "corpus = Corpus(args.data)\n",
        "\n",
        "# Starting from sequential data, batchify arranges the dataset into columns.\n",
        "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
        "#  a g m s \n",
        "#  b h n t \n",
        "#  c i o u \n",
        "#  d j p v \n",
        "#  e k q w \n",
        "#  f l r x .\n",
        "# These columns are treated as independent by the model, which means that the\n",
        "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
        "# batch processing.\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    nbatch = data.size(0) // bsz\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(corpus.train, args.batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "\n",
        "ntokens = len(corpus.dictionary)\n",
        "if args.model == 'Transformer':\n",
        "    model = TransformerModel(ntokens, args.emsize, args.nhead, args.nhid, args.nlayers, args.dropout).to(device)\n",
        "else:\n",
        "    model = RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied).to(device)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "if args.use_optimizer:\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
        "\n",
        "###############################################################################\n",
        "# Training code\n",
        "###############################################################################\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "# get_batch subdivides the source data into chunks of length args.bptt.\n",
        "# If source is equal to the example output of the batchify function, with\n",
        "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
        "#  a g m s   b h n t \n",
        "#  b h n t   c i o u \n",
        "# Note that despite the name of the function, the subdivison of data is not\n",
        "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
        "# by the batchify function. The chunks are along dimension 0, corresponding\n",
        "# to the seq_len dimension in the LSTM.\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "def evaluate(data_source):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if args.model != 'Transformer':\n",
        "        hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, args.bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            if args.model == 'Transformer':\n",
        "                output = model(data)\n",
        "                output = output.view(-1, ntokens)\n",
        "            else:\n",
        "                output, hidden = model(data, hidden)\n",
        "                hidden = repackage_hidden(hidden)\n",
        "            total_loss += len(data) * criterion(output, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "def train_func():\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if args.model != 'Transformer':\n",
        "        hidden = model.init_hidden(args.batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, args.bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        optimizer.zero_grad() if args.use_optimizer else model.zero_grad()\n",
        "        if args.model == 'Transformer':\n",
        "            output = model(data)\n",
        "            output = output.view(-1, ntokens)\n",
        "        else:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            output, hidden = model(data, hidden)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
        "        if args.use_optimizer:\n",
        "            optimizer.step()\n",
        "        else:\n",
        "            for p in model.parameters():\n",
        "                p.data.add_(p.grad, alpha=-lr)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % args.log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / args.log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // args.bptt, lr,\n",
        "                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "        if args.dry_run:\n",
        "            break\n",
        "\n",
        "lr = args.lr\n",
        "best_val_loss = None\n",
        "\n",
        "try:\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        train_func()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                         val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(args.save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(args.save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)\n",
        "\n",
        "# Step 6: Generate Text from generate.py (adapted for Colab)\n",
        "checkpoint = 'model.pt'  # Your saved model\n",
        "outf = 'generated.txt'\n",
        "words = 1000\n",
        "temperature = 1.0\n",
        "log_interval = 100\n",
        "accel = True  # Use CUDA\n",
        "\n",
        "torch.manual_seed(1111)\n",
        "\n",
        "if accel and torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "with open(checkpoint, 'rb') as f:\n",
        "    model = torch.load(f, map_location=device)\n",
        "model.eval()\n",
        "\n",
        "corpus = Corpus(args.data)\n",
        "ntokens = len(corpus.dictionary)\n",
        "\n",
        "is_transformer_model = hasattr(model, 'model_type') and model.model_type == 'Transformer'\n",
        "if not is_transformer_model:\n",
        "    hidden = model.init_hidden(1)\n",
        "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "with open(outf, 'w') as outf:\n",
        "    with torch.no_grad():\n",
        "        for i in range(words):\n",
        "            if is_transformer_model:\n",
        "                output = model(input, False)\n",
        "                word_weights = output[-1].squeeze().div(temperature).exp().cpu()\n",
        "                word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "                word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
        "                input = torch.cat([input, word_tensor], 0)\n",
        "            else:\n",
        "                output, hidden = model(input, hidden)\n",
        "                word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "                word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "                input.fill_(word_idx)\n",
        "\n",
        "            word = corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "            if i % log_interval == 0:\n",
        "                print('| Generated {}/{} words'.format(i, words))\n",
        "\n",
        "# Print generated text\n",
        "!head -n 20 generated.txt"
      ],
      "metadata": {
        "id": "cmr9R-Z5XIPa"
      },
      "id": "cmr9R-Z5XIPa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Script to prepare names dataset for Word-Level Language Modeling\n",
        "Converts multiple text files with names into train/valid/test splits\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import random\n",
        "from typing import List, Tuple\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def read_names_from_files(data_dir: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Read all names from text files in the directory.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory containing .txt files with names\n",
        "\n",
        "    Returns:\n",
        "        List of all names\n",
        "    \"\"\"\n",
        "    all_names: List[str] = []\n",
        "\n",
        "    for filename in Path(data_dir).glob('*.txt'):\n",
        "        print(f\"Reading {filename.name}...\")\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            names: List[str] = [line.strip() for line in f if line.strip()]\n",
        "            all_names.extend(names)\n",
        "            print(f\"  Found {len(names)} names\")\n",
        "\n",
        "    print(f\"\\nTotal names: {len(all_names)}\")\n",
        "    return all_names\n",
        "\n",
        "\n",
        "def create_train_valid_test_splits(\n",
        "    names: List[str],\n",
        "    train_ratio: float = 0.8,\n",
        "    valid_ratio: float = 0.1,\n",
        "    test_ratio: float = 0.1,\n",
        "    seed: int = 42\n",
        ") -> Tuple[List[str], List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Split names into train, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "        names: List of names\n",
        "        train_ratio: Proportion for training (default 0.8)\n",
        "        valid_ratio: Proportion for validation (default 0.1)\n",
        "        test_ratio: Proportion for testing (default 0.1)\n",
        "        seed: Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (train_names, valid_names, test_names)\n",
        "    \"\"\"\n",
        "    assert abs(train_ratio + valid_ratio + test_ratio - 1.0) < 1e-6, \\\n",
        "        \"Ratios must sum to 1.0\"\n",
        "\n",
        "    random.seed(seed)\n",
        "    names_copy: List[str] = names.copy()\n",
        "    random.shuffle(names_copy)\n",
        "\n",
        "    n: int = len(names_copy)\n",
        "    train_end: int = int(n * train_ratio)\n",
        "    valid_end: int = train_end + int(n * valid_ratio)\n",
        "\n",
        "    train_names: List[str] = names_copy[:train_end]\n",
        "    valid_names: List[str] = names_copy[train_end:valid_end]\n",
        "    test_names: List[str] = names_copy[valid_end:]\n",
        "\n",
        "    print(f\"\\nSplit sizes:\")\n",
        "    print(f\"  Train: {len(train_names)} ({len(train_names)/n*100:.1f}%)\")\n",
        "    print(f\"  Valid: {len(valid_names)} ({len(valid_names)/n*100:.1f}%)\")\n",
        "    print(f\"  Test:  {len(test_names)} ({len(test_names)/n*100:.1f}%)\")\n",
        "\n",
        "    return train_names, valid_names, test_names\n",
        "\n",
        "\n",
        "def save_names_to_file(names: List[str], filename: str) -> None:\n",
        "    \"\"\"\n",
        "    Save names to file, one per line.\n",
        "\n",
        "    Args:\n",
        "        names: List of names\n",
        "        filename: Output filename\n",
        "    \"\"\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for name in names:\n",
        "            # Write each name as a separate \"sentence\"\n",
        "            # The model will add <eos> automatically\n",
        "            f.write(name + '\\n')\n",
        "    print(f\"Saved {len(names)} names to {filename}\")\n",
        "\n",
        "\n",
        "def prepare_names_dataset(\n",
        "    input_dir: str,\n",
        "    output_dir: str,\n",
        "    train_ratio: float = 0.8,\n",
        "    valid_ratio: float = 0.1,\n",
        "    test_ratio: float = 0.1,\n",
        "    seed: int = 42\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Main function to prepare names dataset.\n",
        "\n",
        "    Args:\n",
        "        input_dir: Directory containing raw name files\n",
        "        output_dir: Directory to save train/valid/test files\n",
        "        train_ratio: Proportion for training\n",
        "        valid_ratio: Proportion for validation\n",
        "        test_ratio: Proportion for testing\n",
        "        seed: Random seed\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"Preparing Names Dataset for Language Modeling\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Read all names\n",
        "    all_names: List[str] = read_names_from_files(input_dir)\n",
        "\n",
        "    # Split into train/valid/test\n",
        "    train_names, valid_names, test_names = create_train_valid_test_splits(\n",
        "        all_names, train_ratio, valid_ratio, test_ratio, seed\n",
        "    )\n",
        "\n",
        "    # Save splits\n",
        "    print(\"\\nSaving splits...\")\n",
        "    save_names_to_file(train_names, os.path.join(output_dir, 'train.txt'))\n",
        "    save_names_to_file(valid_names, os.path.join(output_dir, 'valid.txt'))\n",
        "    save_names_to_file(test_names, os.path.join(output_dir, 'test.txt'))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Dataset preparation complete!\")\n",
        "    print(f\"Output directory: {output_dir}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Print sample names\n",
        "    print(\"\\nSample names from train set:\")\n",
        "    for name in train_names[:10]:\n",
        "        print(f\"  {name}\")\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# ALTERNATIVE: Word-level format\n",
        "# ===============================\n",
        "\n",
        "def prepare_wordlevel_format(\n",
        "    input_dir: str,\n",
        "    output_dir: str,\n",
        "    train_ratio: float = 0.8,\n",
        "    valid_ratio: float = 0.1,\n",
        "    test_ratio: float = 0.1,\n",
        "    seed: int = 42\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Prepare dataset where each character is a 'word'.\n",
        "    This allows character-level language modeling using word LM code.\n",
        "\n",
        "    Args:\n",
        "        input_dir: Directory containing raw name files\n",
        "        output_dir: Directory to save train/valid/test files\n",
        "        train_ratio: Proportion for training\n",
        "        valid_ratio: Proportion for validation\n",
        "        test_ratio: Proportion for testing\n",
        "        seed: Random seed\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"Preparing Character-Level (as words) Dataset\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Read all names\n",
        "    all_names: List[str] = read_names_from_files(input_dir)\n",
        "\n",
        "    # Split\n",
        "    train_names, valid_names, test_names = create_train_valid_test_splits(\n",
        "        all_names, train_ratio, valid_ratio, test_ratio, seed\n",
        "    )\n",
        "\n",
        "    # Convert to character-level\n",
        "    def names_to_char_words(names: List[str], filename: str) -> None:\n",
        "        \"\"\"Convert names to space-separated characters.\"\"\"\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            for name in names:\n",
        "                # Each character becomes a \"word\"\n",
        "                char_sequence: str = ' '.join(list(name))\n",
        "                f.write(char_sequence + '\\n')\n",
        "        print(f\"Saved {len(names)} names (as char sequences) to {filename}\")\n",
        "\n",
        "    print(\"\\nSaving character-level splits...\")\n",
        "    names_to_char_words(train_names, os.path.join(output_dir, 'train.txt'))\n",
        "    names_to_char_words(valid_names, os.path.join(output_dir, 'valid.txt'))\n",
        "    names_to_char_words(test_names, os.path.join(output_dir, 'test.txt'))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"Character-level dataset preparation complete!\")\n",
        "    print"
      ],
      "metadata": {
        "id": "o7FUkvYGbw7c"
      },
      "id": "o7FUkvYGbw7c",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}