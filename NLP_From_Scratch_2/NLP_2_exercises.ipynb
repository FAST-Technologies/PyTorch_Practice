{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-08T03:37:50.598044Z",
     "start_time": "2025-10-08T03:37:45.842314Z"
    }
   },
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T03:38:04.848122Z",
     "start_time": "2025-10-08T03:38:04.841179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===============================\n",
    "# DATA PREPARATION\n",
    "# ===============================\n",
    "\n",
    "all_letters: str = string.ascii_letters + \" .,;'-\"\n",
    "n_letters: int = len(all_letters) + 1  # Plus EOS marker\n",
    "SOS_token: int = n_letters  # Start of sentence token\n",
    "n_letters_with_sos: int = n_letters + 1  # Include SOS\n",
    "\n",
    "\n",
    "def findFiles(path: str) -> List[str]:\n",
    "    \"\"\"Find all files matching the given path pattern.\"\"\"\n",
    "    return glob.glob(path)\n",
    "\n",
    "\n",
    "def unicodeToAscii(s: str) -> str:\n",
    "    \"\"\"Turn a Unicode string to plain ASCII.\"\"\"\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "\n",
    "def readLines(filename: str) -> List[str]:\n",
    "    \"\"\"Read a file and split into lines.\"\"\"\n",
    "    with open(filename, encoding='utf-8') as some_file:\n",
    "        return [unicodeToAscii(line.strip()) for line in some_file]"
   ],
   "id": "503d31724906a803",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T03:38:20.556078Z",
     "start_time": "2025-10-08T03:38:20.480296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build the category_lines dictionary\n",
    "category_lines: Dict[str, List[str]] = {}\n",
    "all_categories: List[str] = []\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category: str = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines: List[str] = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories: int = len(all_categories)\n",
    "\n",
    "if n_categories == 0:\n",
    "    raise RuntimeError('Data not found. Make sure that you downloaded data '\n",
    "        'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n",
    "        'the current directory.')\n",
    "\n",
    "print('# categories:', n_categories, all_categories)\n",
    "print(f\"Example conversion: {unicodeToAscii('ONéàl')}\")"
   ],
   "id": "157978079e7de2bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# categories: 18 ['French', 'Korean', 'Greek', 'Dutch', 'Arabic', 'Portuguese', 'English', 'Czech', 'Chinese', 'Irish', 'Vietnamese', 'Japanese', 'Italian', 'Spanish', 'Russian', 'Polish', 'German', 'Scottish']\n",
      "Example conversion: ONeal\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T03:38:42.264573Z",
     "start_time": "2025-10-08T03:38:42.245156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===============================\n",
    "# MODEL ARCHITECTURES\n",
    "# ===============================\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \"\"\"Basic RNN for generating names conditioned on category.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int) -> None:\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size: int = hidden_size\n",
    "        \n",
    "        self.i2h: nn.Linear = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
    "        self.i2o: nn.Linear = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
    "        self.o2o: nn.Linear = nn.Linear(hidden_size + output_size, output_size)\n",
    "        self.dropout: nn.Dropout = nn.Dropout(0.1)\n",
    "        self.softmax: nn.LogSoftmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        category: torch.Tensor,\n",
    "        input: torch.Tensor,\n",
    "        hidden: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        input_combined: torch.Tensor = torch.cat((category, input, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output: torch.Tensor = self.i2o(input_combined)\n",
    "        output_combined: torch.Tensor = torch.cat((hidden, output), 1)\n",
    "        output = self.o2o(output_combined)\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self) -> torch.Tensor:\n",
    "        \"\"\"Initialize hidden state with zeros.\"\"\"\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "\n",
    "class LSTM_Generator(nn.Module):\n",
    "    \"\"\"LSTM-based name generator.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, dropout: float = 0.2) -> None:\n",
    "        super(LSTM_Generator, self).__init__()\n",
    "        self.hidden_size: int = hidden_size\n",
    "        \n",
    "        self.embedding: nn.Embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.category_embed: nn.Linear = nn.Linear(n_categories, hidden_size)\n",
    "        \n",
    "        self.lstm: nn.LSTM = nn.LSTM(hidden_size * 2, hidden_size, batch_first=False)\n",
    "        self.dropout: nn.Dropout = nn.Dropout(dropout)\n",
    "        self.out: nn.Linear = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax: nn.LogSoftmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        category: torch.Tensor,\n",
    "        input: torch.Tensor,\n",
    "        hidden: Tuple[torch.Tensor, torch.Tensor]\n",
    "    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        # Get index from one-hot\n",
    "        input_idx: torch.Tensor = input.argmax(dim=1)\n",
    "        embedded: torch.Tensor = self.embedding(input_idx)\n",
    "        \n",
    "        cat_embed: torch.Tensor = self.category_embed(category)\n",
    "        combined: torch.Tensor = torch.cat((embedded, cat_embed), 1).unsqueeze(0)\n",
    "        \n",
    "        output: torch.Tensor\n",
    "        output, hidden = self.lstm(combined, hidden)\n",
    "        output = self.dropout(output.squeeze(0))\n",
    "        output = self.out(output)\n",
    "        output = self.softmax(output)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Initialize LSTM hidden and cell state.\"\"\"\n",
    "        return (torch.zeros(1, 1, self.hidden_size),\n",
    "                torch.zeros(1, 1, self.hidden_size))\n",
    "\n",
    "\n",
    "class GRU_Generator(nn.Module):\n",
    "    \"\"\"GRU-based name generator.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int, dropout: float = 0.2) -> None:\n",
    "        super(GRU_Generator, self).__init__()\n",
    "        self.hidden_size: int = hidden_size\n",
    "        \n",
    "        self.i2h: nn.Linear = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
    "        self.i2o: nn.Linear = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
    "        self.o2o: nn.Linear = nn.Linear(hidden_size + output_size, output_size)\n",
    "        self.gru: nn.GRU = nn.GRU(hidden_size + output_size, hidden_size)\n",
    "        self.dropout: nn.Dropout = nn.Dropout(dropout)\n",
    "        self.softmax: nn.LogSoftmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        category: torch.Tensor,\n",
    "        input: torch.Tensor,\n",
    "        hidden: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        input_combined: torch.Tensor = torch.cat((category, input, hidden), 1)\n",
    "        hidden_new: torch.Tensor = self.i2h(input_combined)\n",
    "        output: torch.Tensor = self.i2o(input_combined)\n",
    "        output_combined: torch.Tensor = torch.cat((hidden_new, output), 1)\n",
    "        \n",
    "        # Use GRU for additional processing\n",
    "        gru_out: torch.Tensor\n",
    "        gru_out, hidden = self.gru(output_combined.unsqueeze(0), hidden.unsqueeze(0))\n",
    "        hidden = hidden.squeeze(0)\n",
    "        \n",
    "        output = self.o2o(gru_out.squeeze(0))\n",
    "        output = self.dropout(output)\n",
    "        output = self.softmax(output)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self) -> torch.Tensor:\n",
    "        \"\"\"Initialize hidden state.\"\"\"\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "\n",
    "class DeepRNN_Generator(nn.Module):\n",
    "    \"\"\"Deep RNN with multiple layers for name generation.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_size: int,\n",
    "        output_size: int,\n",
    "        n_layers: int = 2,\n",
    "        dropout: float = 0.3\n",
    "    ) -> None:\n",
    "        super(DeepRNN_Generator, self).__init__()\n",
    "        self.hidden_size: int = hidden_size\n",
    "        self.n_layers: int = n_layers\n",
    "        \n",
    "        self.category_embed: nn.Linear = nn.Linear(n_categories, hidden_size)\n",
    "        self.input_embed: nn.Linear = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        self.rnn: nn.RNN = nn.RNN(\n",
    "            hidden_size * 2,\n",
    "            hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout if n_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.hidden2hidden: nn.Linear = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.hidden2out: nn.Linear = nn.Linear(hidden_size // 2, output_size)\n",
    "        self.dropout: nn.Dropout = nn.Dropout(dropout)\n",
    "        self.softmax: nn.LogSoftmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        category: torch.Tensor,\n",
    "        input: torch.Tensor,\n",
    "        hidden: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        cat_embed: torch.Tensor = self.category_embed(category)\n",
    "        input_embed: torch.Tensor = self.input_embed(input)\n",
    "        combined: torch.Tensor = torch.cat((cat_embed, input_embed), 1)\n",
    "        \n",
    "        rnn_out: torch.Tensor\n",
    "        rnn_out, hidden = self.rnn(combined.unsqueeze(0), hidden)\n",
    "        \n",
    "        output: torch.Tensor = self.dropout(rnn_out.squeeze(0))\n",
    "        output = torch.relu(self.hidden2hidden(output))\n",
    "        output = self.hidden2out(output)\n",
    "        output = self.softmax(output)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self) -> torch.Tensor:\n",
    "        \"\"\"Initialize hidden state for all layers.\"\"\"\n",
    "        return torch.zeros(self.n_layers, 1, self.hidden_size)"
   ],
   "id": "ccbfa023dfc132c3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T03:39:02.073500Z",
     "start_time": "2025-10-08T03:39:02.067103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===============================\n",
    "# TRAINING UTILITIES\n",
    "# ===============================\n",
    "\n",
    "def randomChoice(l: List[str]) -> str:\n",
    "    \"\"\"Random item from a list.\"\"\"\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "\n",
    "\n",
    "def randomTrainingPair() -> Tuple[str, str]:\n",
    "    \"\"\"Get a random category and random line from that category.\"\"\"\n",
    "    category: str = randomChoice(all_categories)\n",
    "    line: str = randomChoice(category_lines[category])\n",
    "    return category, line\n",
    "\n",
    "\n",
    "def categoryTensor(category: str) -> torch.Tensor:\n",
    "    \"\"\"One-hot vector for category.\"\"\"\n",
    "    li: int = all_categories.index(category)\n",
    "    tensor: torch.Tensor = torch.zeros(1, n_categories)\n",
    "    tensor[0][li] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def inputTensor(line: str, use_sos: bool = False) -> torch.Tensor:\n",
    "    \"\"\"One-hot matrix of letters for input.\"\"\"\n",
    "    size: int = n_letters_with_sos if use_sos else n_letters\n",
    "    tensor: torch.Tensor = torch.zeros(len(line), 1, size)\n",
    "    for li in range(len(line)):\n",
    "        letter: str = line[li]\n",
    "        tensor[li][0][all_letters.find(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def targetTensor(line: str) -> torch.LongTensor:\n",
    "    \"\"\"LongTensor of second letter to end (EOS) for target.\"\"\"\n",
    "    letter_indexes: List[int] = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_letters - 1)  # EOS\n",
    "    return torch.LongTensor(letter_indexes)\n",
    "\n",
    "\n",
    "def randomTrainingExample(use_sos: bool = False) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Make category, input, and target tensors from a random pair.\"\"\"\n",
    "    category: str\n",
    "    line: str\n",
    "    category, line = randomTrainingPair()\n",
    "    category_tensor: torch.Tensor = categoryTensor(category)\n",
    "    input_line_tensor: torch.Tensor = inputTensor(line, use_sos)\n",
    "    target_line_tensor: torch.Tensor = targetTensor(line)\n",
    "    return category_tensor, input_line_tensor, target_line_tensor"
   ],
   "id": "119a0a8ce73f8059",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T03:39:20.934436Z",
     "start_time": "2025-10-08T03:39:20.922642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_step(\n",
    "    model: nn.Module,\n",
    "    category_tensor: torch.Tensor,\n",
    "    input_line_tensor: torch.Tensor,\n",
    "    target_line_tensor: torch.Tensor,\n",
    "    criterion: nn.Module,\n",
    "    learning_rate: float,\n",
    "    is_lstm: bool = False\n",
    ") -> Tuple[torch.Tensor, float]:\n",
    "    \"\"\"Train the model on one example.\"\"\"\n",
    "    target_line_tensor.unsqueeze_(-1)\n",
    "    hidden = model.initHidden()\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    loss: torch.Tensor = torch.Tensor([0])\n",
    "    \n",
    "    for i in range(input_line_tensor.size(0)):\n",
    "        output: torch.Tensor\n",
    "        output, hidden = model(category_tensor, input_line_tensor[i], hidden)\n",
    "        l: torch.Tensor = criterion(output, target_line_tensor[i])\n",
    "        loss += l\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # Manual gradient descent\n",
    "    for p in model.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "    \n",
    "    return output, loss.item() / input_line_tensor.size(0)\n",
    "\n",
    "\n",
    "def timeSince(since: float) -> str:\n",
    "    \"\"\"Calculate time elapsed since given timestamp.\"\"\"\n",
    "    now: float = time.time()\n",
    "    s: float = now - since\n",
    "    m: int = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    model_name: str,\n",
    "    n_iters: int = 100000,\n",
    "    learning_rate: float = 0.0005,\n",
    "    print_every: int = 5000,\n",
    "    plot_every: int = 500,\n",
    "    use_sos: bool = False,\n",
    "    is_lstm: bool = False\n",
    ") -> List[float]:\n",
    "    \"\"\"Train a model and return losses.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    criterion: nn.NLLLoss = nn.NLLLoss()\n",
    "    all_losses: List[float] = []\n",
    "    total_loss: float = 0\n",
    "    \n",
    "    start: float = time.time()\n",
    "    \n",
    "    for iter in range(1, n_iters + 1):\n",
    "        output: torch.Tensor\n",
    "        loss: float\n",
    "        output, loss = train_step(\n",
    "            model,\n",
    "            *randomTrainingExample(use_sos),\n",
    "            criterion,\n",
    "            learning_rate,\n",
    "            is_lstm\n",
    "        )\n",
    "        total_loss += loss\n",
    "        \n",
    "        if iter % print_every == 0:\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
    "        \n",
    "        if iter % plot_every == 0:\n",
    "            all_losses.append(total_loss / plot_every)\n",
    "            total_loss = 0\n",
    "    \n",
    "    return all_losses"
   ],
   "id": "6e5238ad2f107e0b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T03:39:41.945445Z",
     "start_time": "2025-10-08T03:39:41.937033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===============================\n",
    "# SAMPLING/GENERATION\n",
    "# ===============================\n",
    "\n",
    "max_length: int = 20\n",
    "\n",
    "\n",
    "def sample(\n",
    "    model: nn.Module,\n",
    "    category: str,\n",
    "    start_letter: Optional[str] = None,\n",
    "    use_sos: bool = False,\n",
    "    is_lstm: bool = False\n",
    ") -> str:\n",
    "    \"\"\"Sample from a category and optional starting letter.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        category_tensor: torch.Tensor = categoryTensor(category)\n",
    "        \n",
    "        if start_letter is None or use_sos:\n",
    "            # Use SOS token\n",
    "            if use_sos:\n",
    "                input: torch.Tensor = torch.zeros(1, n_letters_with_sos)\n",
    "                input[0][SOS_token] = 1\n",
    "                output_name: str = \"\"\n",
    "            else:\n",
    "                # Pick random starting letter\n",
    "                start_letter = all_letters[random.randint(0, len(all_letters) - 1)]\n",
    "                input = inputTensor(start_letter, use_sos)\n",
    "                output_name = start_letter\n",
    "        else:\n",
    "            input = inputTensor(start_letter, use_sos)\n",
    "            output_name = start_letter\n",
    "        \n",
    "        hidden = model.initHidden()\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            output: torch.Tensor\n",
    "            output, hidden = model(category_tensor, input[0] if not use_sos or i > 0 else input, hidden)\n",
    "            topv: torch.Tensor\n",
    "            topi: torch.Tensor\n",
    "            topv, topi = output.topk(1)\n",
    "            topi_item: int = topi[0][0].item()\n",
    "            \n",
    "            if topi_item == n_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter: str = all_letters[topi_item]\n",
    "                output_name += letter\n",
    "            \n",
    "            size: int = n_letters_with_sos if use_sos else n_letters\n",
    "            input = torch.zeros(1, size)\n",
    "            input[0][topi_item] = 1\n",
    "        \n",
    "        return output_name\n",
    "\n",
    "\n",
    "def samples(\n",
    "    model: nn.Module,\n",
    "    category: str,\n",
    "    start_letters: str = 'ABC',\n",
    "    use_sos: bool = False,\n",
    "    is_lstm: bool = False\n",
    ") -> None:\n",
    "    \"\"\"Get multiple samples from one category.\"\"\"\n",
    "    print(f\"\\n{category}:\")\n",
    "    for start_letter in start_letters:\n",
    "        print(f\"  {start_letter}: {sample(model, category, start_letter, use_sos, is_lstm)}\")"
   ],
   "id": "eedb2bdb4a38c59c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T03:47:20.651372Z",
     "start_time": "2025-10-08T03:47:20.641941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sample_with_temperature(\n",
    "    model: nn.Module,\n",
    "    category: str,\n",
    "    temperature: float = 1.0,\n",
    "    start_letter: Optional[str] = None,\n",
    "    use_sos: bool = False,\n",
    "    is_lstm: bool = False\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Sample with temperature scaling for controlling randomness.\n",
    "    \n",
    "    Args:\n",
    "        temperature: Higher values (>1.0) make output more random,\n",
    "                    lower values (<1.0) make it more deterministic.\n",
    "                    temperature=1.0 is standard sampling.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        category_tensor: torch.Tensor = categoryTensor(category)\n",
    "        \n",
    "        if start_letter is None or use_sos:\n",
    "            if use_sos:\n",
    "                input: torch.Tensor = torch.zeros(1, n_letters_with_sos)\n",
    "                input[0][SOS_token] = 1\n",
    "                output_name: str = \"\"\n",
    "            else:\n",
    "                start_letter = all_letters[random.randint(0, len(all_letters) - 1)]\n",
    "                input = inputTensor(start_letter, use_sos)\n",
    "                output_name = start_letter\n",
    "        else:\n",
    "            input = inputTensor(start_letter, use_sos)\n",
    "            output_name = start_letter\n",
    "        \n",
    "        hidden = model.initHidden()\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            output: torch.Tensor\n",
    "            output, hidden = model(category_tensor, input[0] if not use_sos or i > 0 else input, hidden)\n",
    "            \n",
    "            # Apply temperature scaling\n",
    "            output = output / temperature\n",
    "            \n",
    "            # Convert log probabilities to probabilities\n",
    "            probs: torch.Tensor = torch.exp(output)\n",
    "            probs = probs / probs.sum()  # Normalize\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            topi_item: int = torch.multinomial(probs, 1).item()\n",
    "            \n",
    "            if topi_item == n_letters - 1:\n",
    "                break\n",
    "            else:\n",
    "                letter: str = all_letters[topi_item]\n",
    "                output_name += letter\n",
    "            \n",
    "            size: int = n_letters_with_sos if use_sos else n_letters\n",
    "            input = torch.zeros(1, size)\n",
    "            input[0][topi_item] = 1\n",
    "        \n",
    "        return output_name"
   ],
   "id": "8ee100522624808f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T03:47:33.432536Z",
     "start_time": "2025-10-08T03:47:33.311317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def beam_search(\n",
    "    model: nn.Module,\n",
    "    category: str,\n",
    "    beam_width: int = 5,\n",
    "    start_letter: Optional[str] = None,\n",
    "    use_sos: bool = False,\n",
    "    is_lstm: bool = False\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Beam search for finding most probable sequences.\n",
    "    \n",
    "    Returns:\n",
    "        List of (name, score) tuples sorted by score.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        category_tensor: torch.Tensor = categoryTensor(category)\n",
    "        \n",
    "        # Initialize beams: (sequence, score, hidden, input)\n",
    "        if start_letter is None or use_sos:\n",
    "            if use_sos:\n",
    "                initial_input: torch.Tensor = torch.zeros(1, n_letters_with_sos)\n",
    "                initial_input[0][SOS_token] = 1\n",
    "                beams: List[Tuple[str, float, any, torch.Tensor]] = [\n",
    "                    (\"\", 0.0, model.initHidden(), initial_input)\n",
    "                ]\n",
    "            else:\n",
    "                start_letter = all_letters[random.randint(0, len(all_letters) - 1)]\n",
    "                initial_input = inputTensor(start_letter, use_sos)\n",
    "                beams = [(start_letter, 0.0, model.initHidden(), initial_input)]\n",
    "        else:\n",
    "            initial_input = inputTensor(start_letter, use_sos)\n",
    "            beams = [(start_letter, 0.0, model.initHidden(), initial_input)]\n",
    "        \n",
    "        completed: List[Tuple[str, float]] = []\n",
    "        \n",
    "        for step in range(max_length):\n",
    "            all_candidates: List[Tuple[str, float, any, torch.Tensor]] = []\n",
    "            \n",
    "            for seq, score, hidden, input_tensor in beams:\n",
    "                output: torch.Tensor\n",
    "                output, new_hidden = model(category_tensor, input_tensor[0], hidden)\n",
    "                \n",
    "                # Get top k predictions\n",
    "                topv: torch.Tensor\n",
    "                topi: torch.Tensor\n",
    "                topv, topi = output.topk(min(beam_width, output.size(1)))\n",
    "                \n",
    "                for i in range(topv.size(1)):\n",
    "                    token_id: int = topi[0][i].item()\n",
    "                    token_score: float = topv[0][i].item()\n",
    "                    \n",
    "                    if token_id == n_letters - 1:  # EOS\n",
    "                        completed.append((seq, score + token_score))\n",
    "                    else:\n",
    "                        letter: str = all_letters[token_id]\n",
    "                        new_seq: str = seq + letter\n",
    "                        new_score: float = score + token_score\n",
    "                        \n",
    "                        size: int = n_letters_with_sos if use_sos else n_letters\n",
    "                        new_input: torch.Tensor = torch.zeros(1, size)\n",
    "                        new_input[0][token_id] = 1\n",
    "                        \n",
    "                        all_candidates.append((new_seq, new_score, new_hidden, new_input))\n",
    "            \n",
    "            # Keep top beam_width candidates\n",
    "            if not all_candidates:\n",
    "                break\n",
    "            \n",
    "            beams = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "        \n",
    "        # Add remaining beams to completed\n",
    "        for seq, score, _, _ in beams:\n",
    "            completed.append((seq, score))\n",
    "        \n",
    "        # Sort by score and return\n",
    "        return sorted(completed, key=lambda x: x[1], reverse=True)\n"
   ],
   "id": "f18e63ae968a08f3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T03:47:52.054890Z",
     "start_time": "2025-10-08T03:47:52.046580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ensemble_sample(\n",
    "    models: List[nn.Module],\n",
    "    category: str,\n",
    "    start_letter: Optional[str] = None,\n",
    "    use_sos: bool = False,\n",
    "    voting_method: str = 'majority'\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate names using ensemble of models.\n",
    "    \n",
    "    Args:\n",
    "        voting_method: 'majority' for most common output,\n",
    "                      'average_char' for character-level averaging\n",
    "    \"\"\"\n",
    "    if voting_method == 'majority':\n",
    "        # Generate multiple samples and vote\n",
    "        votes: Dict[str, int] = {}\n",
    "        for model in models:\n",
    "            name: str = sample(model, category, start_letter, use_sos)\n",
    "            votes[name] = votes.get(name, 0) + 1\n",
    "        \n",
    "        # Return most voted name\n",
    "        if votes:\n",
    "            return max(votes.items(), key=lambda x: x[1])[0]\n",
    "        return \"\"\n",
    "    \n",
    "    elif voting_method == 'average_char':\n",
    "        # Average probabilities at each character position\n",
    "        with torch.no_grad():\n",
    "            category_tensor: torch.Tensor = categoryTensor(category)\n",
    "            \n",
    "            if start_letter is None:\n",
    "                start_letter = all_letters[random.randint(0, len(all_letters) - 1)]\n",
    "            \n",
    "            input: torch.Tensor = inputTensor(start_letter, use_sos)\n",
    "            output_name: str = start_letter\n",
    "            \n",
    "            # Initialize hidden states for all models\n",
    "            hiddens: List = [model.initHidden() for model in models]\n",
    "            \n",
    "            for i in range(max_length):\n",
    "                # Get outputs from all models\n",
    "                avg_output: torch.Tensor = torch.zeros(1, n_letters)\n",
    "                \n",
    "                for idx, model in enumerate(models):\n",
    "                    output: torch.Tensor\n",
    "                    output, hiddens[idx] = model(category_tensor, input[0], hiddens[idx])\n",
    "                    avg_output += torch.exp(output)  # Convert from log probs\n",
    "                \n",
    "                avg_output = avg_output / len(models)\n",
    "                avg_output = torch.log(avg_output)  # Back to log probs\n",
    "                \n",
    "                topv: torch.Tensor\n",
    "                topi: torch.Tensor\n",
    "                topv, topi = avg_output.topk(1)\n",
    "                topi_item: int = topi[0][0].item()\n",
    "                \n",
    "                if topi_item == n_letters - 1:\n",
    "                    break\n",
    "                else:\n",
    "                    letter: str = all_letters[topi_item]\n",
    "                    output_name += letter\n",
    "                \n",
    "                size: int = n_letters_with_sos if use_sos else n_letters\n",
    "                input = torch.zeros(1, size)\n",
    "                input[0][topi_item] = 1\n",
    "            \n",
    "            return output_name\n",
    "    \n",
    "    return \"\""
   ],
   "id": "6232b1d859878f38",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ===============================\n",
    "# EXPERIMENTS\n",
    "# ===============================\n",
    "\n",
    "results: Dict[str, Dict] = {}\n",
    "\n",
    "# Experiment 1: Basic RNN\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 1: Basic RNN (Baseline)\")\n",
    "print(\"=\"*60)\n",
    "rnn_basic: RNN = RNN(n_letters, 128, n_letters)\n",
    "losses_rnn: List[float] = train_model(rnn_basic, \"Basic RNN\", n_iters=50000, learning_rate=0.0005)\n",
    "results['Basic RNN'] = {'model': rnn_basic, 'losses': losses_rnn, 'use_sos': False, 'is_lstm': False}"
   ],
   "id": "bec240bee81fb26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Experiment 2: Larger RNN\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 2: Larger RNN (256 hidden)\")\n",
    "print(\"=\"*60)\n",
    "rnn_large: RNN = RNN(n_letters, 256, n_letters)\n",
    "losses_large: List[float] = train_model(rnn_large, \"Large RNN\", n_iters=50000, learning_rate=0.0005)\n",
    "results['Large RNN'] = {'model': rnn_large, 'losses': losses_large, 'use_sos': False, 'is_lstm': False}"
   ],
   "id": "cdb4e8afa59c6353"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Experiment 3: GRU\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 3: GRU Generator\")\n",
    "print(\"=\"*60)\n",
    "gru: GRU_Generator = GRU_Generator(n_letters, 128, n_letters, dropout=0.2)\n",
    "losses_gru: List[float] = train_model(gru, \"GRU\", n_iters=50000, learning_rate=0.0005)\n",
    "results['GRU'] = {'model': gru, 'losses': losses_gru, 'use_sos': False, 'is_lstm': False}"
   ],
   "id": "5a3ef474a9c1df2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Experiment 4: Deep RNN\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT 4: Deep RNN (2 layers)\")\n",
    "print(\"=\"*60)\n",
    "deep_rnn: DeepRNN_Generator = DeepRNN_Generator(n_letters, 128, n_letters, n_layers=2, dropout=0.3)\n",
    "losses_deep: List[float] = train_model(deep_rnn, \"Deep RNN\", n_iters=50000, learning_rate=0.0005)\n",
    "results['Deep RNN'] = {'model': deep_rnn, 'losses': losses_deep, 'use_sos': False, 'is_lstm': False}"
   ],
   "id": "e0e308a03e49f1c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot all losses\n",
    "plt.figure(figsize=(12, 6))\n",
    "for name, data in results.items():\n",
    "    plt.plot(data['losses'], label=name)\n",
    "plt.xlabel('Iterations (x500)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "id": "a91216ea3c441407"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Generate samples from all models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE GENERATION COMPARISON - Standard Sampling\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_categories: List[str] = ['Russian', 'German', 'Spanish', 'Chinese']\n",
    "test_letters: str = 'ABC'\n",
    "\n",
    "for name, data in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for category in test_categories:\n",
    "        samples(\n",
    "            data['model'],\n",
    "            category,\n",
    "            test_letters,\n",
    "            data['use_sos'],\n",
    "            data['is_lstm']\n",
    "        )\n",
    "\n",
    "# Generate with SOS token\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATION WITHOUT START LETTER (using best model)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_model = results['Deep RNN']['model']\n",
    "for category in test_categories:\n",
    "    print(f\"\\n{category} (no start letter):\")\n",
    "    for _ in range(5):\n",
    "        print(f\"  {sample(best_model, category, start_letter=None, use_sos=False)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*60)"
   ],
   "id": "c1c4715aa57cca9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEMPERATURE SAMPLING COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(\"Lower temperature = more conservative, Higher = more creative\\n\")\n",
    "\n",
    "temperatures: List[float] = [0.5, 0.8, 1.0, 1.2, 1.5]\n",
    "\n",
    "for category in ['Russian', 'Spanish']:\n",
    "    print(f\"\\n{category} names with 'A' (different temperatures):\")\n",
    "    for temp in temperatures:\n",
    "        names: List[str] = []\n",
    "        for _ in range(3):\n",
    "            name: str = sample_with_temperature(\n",
    "                best_model, \n",
    "                category, \n",
    "                temperature=temp,\n",
    "                start_letter='A'\n",
    "            )\n",
    "            names.append(name)\n",
    "        print(f\"  temp={temp:.1f}: {', '.join(names)}\")"
   ],
   "id": "b6b4dfc3e8f6c208"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEAM SEARCH - Finding Most Probable Names\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for category in ['Russian', 'German', 'Spanish']:\n",
    "    print(f\"\\n{category} names starting with 'M' (beam width=5):\")\n",
    "    beam_results: List[Tuple[str, float]] = beam_search(\n",
    "        best_model,\n",
    "        category,\n",
    "        beam_width=5,\n",
    "        start_letter='M'\n",
    "    )\n",
    "    for idx, (name, score) in enumerate(beam_results[:5], 1):\n",
    "        print(f\"  {idx}. {name:20s} (score: {score:6.2f})\")"
   ],
   "id": "4b8efb109cc90f99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENSEMBLE METHODS - Combining Multiple Models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get all trained models\n",
    "all_models: List[nn.Module] = [data['model'] for data in results.values()]\n",
    "\n",
    "print(\"\\nMajority Voting Ensemble:\")\n",
    "for category in ['Russian', 'Spanish', 'Chinese']:\n",
    "    print(f\"\\n{category} (starting with 'A'):\")\n",
    "    for _ in range(5):\n",
    "        name: str = ensemble_sample(\n",
    "            all_models,\n",
    "            category,\n",
    "            start_letter='A',\n",
    "            voting_method='majority'\n",
    "        )\n",
    "        print(f\"  {name}\")"
   ],
   "id": "ba3227b7e6aff7e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\nAverage Character Probabilities Ensemble:\")\n",
    "for category in ['German', 'Russian']:\n",
    "    print(f\"\\n{category} (starting with 'S'):\")\n",
    "    for _ in range(5):\n",
    "        name: str = ensemble_sample(\n",
    "            all_models,\n",
    "            category,\n",
    "            start_letter='S',\n",
    "            voting_method='average_char'\n",
    "        )\n",
    "        print(f\"  {name}\")"
   ],
   "id": "95626e983ad11889"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUALITY COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare different methods\n",
    "comparison_category: str = 'Russian'\n",
    "comparison_letter: str = 'V'\n",
    "n_samples: int = 10\n",
    "\n",
    "print(f\"\\nGenerating {n_samples} {comparison_category} names starting with '{comparison_letter}':\\n\")\n",
    "\n",
    "print(\"Standard Sampling:\")\n",
    "standard_names: List[str] = []\n",
    "for _ in range(n_samples):\n",
    "    name: str = sample(best_model, comparison_category, comparison_letter)\n",
    "    standard_names.append(name)\n",
    "print(f\"  {', '.join(standard_names)}\")\n",
    "\n",
    "print(\"\\nTemperature Sampling (0.8):\")\n",
    "temp_names: List[str] = []\n",
    "for _ in range(n_samples):\n",
    "    name: str = sample_with_temperature(best_model, comparison_category, 0.8, comparison_letter)\n",
    "    temp_names.append(name)\n",
    "print(f\"  {', '.join(temp_names)}\")\n",
    "\n",
    "print(\"\\nBeam Search (top result):\")\n",
    "beam_names: List[str] = []\n",
    "for _ in range(n_samples):\n",
    "    results_beam: List[Tuple[str, float]] = beam_search(best_model, comparison_category, beam_width=3, start_letter=comparison_letter)\n",
    "    if results_beam:\n",
    "        beam_names.append(results_beam[0][0])\n",
    "print(f\"  {', '.join(beam_names)}\")\n",
    "\n",
    "print(\"\\nEnsemble (majority voting):\")\n",
    "ensemble_names: List[str] = []\n",
    "for _ in range(n_samples):\n",
    "    name: str = ensemble_sample(all_models, comparison_category, comparison_letter, voting_method='majority')\n",
    "    ensemble_names.append(name)\n",
    "print(f\"  {', '.join(ensemble_names)}\")\n",
    "\n",
    "# Calculate diversity metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIVERSITY METRICS\")\n",
    "print(\"=\"*60)"
   ],
   "id": "b6090db472b892ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_diversity(names: List[str]) -> Tuple[float, int]:\n",
    "    \"\"\"Calculate diversity as unique names / total names.\"\"\"\n",
    "    unique: int = len(set(names))\n",
    "    diversity: float = unique / len(names) if names else 0\n",
    "    return diversity, unique\n",
    "\n",
    "print(f\"\\nDiversity for {n_samples} samples:\")\n",
    "std_div, std_uniq = calculate_diversity(standard_names)\n",
    "temp_div, temp_uniq = calculate_diversity(temp_names)\n",
    "beam_div, beam_uniq = calculate_diversity(beam_names)\n",
    "ens_div, ens_uniq = calculate_diversity(ensemble_names)\n",
    "\n",
    "print(f\"  Standard:       {std_div:.2%} ({std_uniq}/{n_samples} unique)\")\n",
    "print(f\"  Temperature:    {temp_div:.2%} ({temp_uniq}/{n_samples} unique)\")\n",
    "print(f\"  Beam Search:    {beam_div:.2%} ({beam_uniq}/{n_samples} unique)\")\n",
    "print(f\"  Ensemble:       {ens_div:.2%} ({ens_uniq}/{n_samples} unique)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATIVE GENERATION - High Temperature\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nVery creative names (temperature=2.0):\")\n",
    "for category in ['Russian', 'German', 'Spanish', 'Chinese']:\n",
    "    print(f\"\\n{category}:\")\n",
    "    creative_names: List[str] = []\n",
    "    for letter in 'ABCD':\n",
    "        name: str = sample_with_temperature(best_model, category, temperature=2.0, start_letter=letter)\n",
    "        creative_names.append(f\"{letter}: {name}\")\n",
    "    print(f\"  {', '.join(creative_names)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONSERVATIVE GENERATION - Low Temperature\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nVery conservative names (temperature=0.3):\")\n",
    "for category in ['Russian', 'German', 'Spanish', 'Chinese']:\n",
    "    print(f\"\\n{category}:\")\n",
    "    conservative_names: List[str] = []\n",
    "    for letter in 'ABCD':\n",
    "        name: str = sample_with_temperature(best_model, category, temperature=0.3, start_letter=letter)\n",
    "        conservative_names.append(f\"{letter}: {name}\")\n",
    "    print(f\"  {', '.join(conservative_names)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ALL EXPERIMENTS COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nSummary:\")\n",
    "print(\"  ✓ Temperature sampling: Control creativity (0.3-2.0)\")\n",
    "print(\"  ✓ Beam search: Find most probable sequences\")\n",
    "print(\"  ✓ Ensemble methods: Combine multiple models (majority/average)\")\n",
    "print(\"  ✓ Diversity metrics: Measure uniqueness of generations\")\n",
    "print(\"\\nKey Findings:\")\n",
    "print(\"  - Low temperature (0.3): Repetitive but realistic\")\n",
    "print(\"  - High temperature (2.0): Diverse but sometimes unusual\")\n",
    "print(\"  - Beam search: Most probable but less diverse\")\n",
    "print(\"  - Ensemble: Balanced quality and diversity\")# Generate samples from all models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE GENERATION COMPARISON - Standard Sampling\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_categories: List[str] = ['Russian', 'German', 'Spanish', 'Chinese']\n",
    "test_letters: str = 'ABC'\n",
    "\n",
    "for name, data in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for category in test_categories:\n",
    "        samples(\n",
    "            data['model'],\n",
    "            category,\n",
    "            test_letters,\n",
    "            data['use_sos'],\n",
    "            data['is_lstm']\n",
    "        )"
   ],
   "id": "cfeb44bb640e328a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
